<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Socratic Students: Teaching Language Models to Learn by Asking Questions</title>
<link>https://arxiv.org/abs/2512.13102</link>
<guid>https://arxiv.org/abs/2512.13102</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, interactive learning, student queries, Direct Preference Optimization, question quality  

<br><br>Summary:  
This work addresses a limitation of Large Language Models (LLMs) by focusing on dynamic interactions, where information needs to be actively acquired instead of statically retrieved. The authors emphasize settings such as education and medical assistance, where an interactive agent must recognize uncertainty, ask relevant questions, and efficiently integrate new knowledge. Unlike prior research that centers on a teacher guiding a student, this study investigates how the student can proactively query the teacher to obtain useful information. Experiments on math and coding benchmarks demonstrate that student-led approaches improve performance significantly, with Pass@k gains of at least 0.5 compared to static baselines. To further enhance question quality, the authors train students using Direct Preference Optimization (DPO) methods, guided by either self-feedback or stronger student models. This guided training enables even smaller student models to develop more effective questioning strategies, thereby improving learning efficiency. Overall, the paper contributes a novel shift in focus from teacher-driven instruction to student-driven inquiry, showing that active questioning by students can substantially boost performance in interactive learning tasks. <div>
arXiv:2512.13102v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
<link>https://arxiv.org/abs/2512.13142</link>
<guid>https://arxiv.org/abs/2512.13142</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, abortion stigma, Individual Level Abortion Stigma Scale, multilevel understanding, AI safety  

<br><br>Summary:  
This study evaluates the capacity of large language models (LLMs) to understand the complex psychological and physiological phenomenon of abortion stigma. Researchers tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). The analysis focused on whether LLMs coherently represent abortion stigma at three levels: cognitive (self-judgment), interpersonal (worries about judgment and isolation), and structural (community condemnation and disclosure patterns), as well as overall stigma. The results indicate that current LLMs fail to demonstrate genuine multilevel understanding of stigma. Specifically, models overestimate interpersonal stigma but underestimate cognitive stigma, assume community condemnation to be uniform, and introduce demographic biases not observed in actual human data. Additionally, the models miss validated relationships such as the stigma-secrecy connection and sometimes contradict themselves within theoretical frameworks. These findings reveal that existing alignment efforts produce appropriate language use but do not ensure coherent, multilevel comprehension of psychological constructs. The paper emphasizes the need for new design approaches emphasizing multilevel coherence, continuous model auditing, regulatory governance with mandatory audits and accountability, and enhanced AI literacy in sensitive domains. This is critical because proper understanding of what people cannot explicitly say influences whether AI support in high-stakes health contexts is helpful or harmful. <div>
arXiv:2512.13142v3 Announce Type: replace 
Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (worries about judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing High-Risk AI Systems under the EU AI Act: From Legal Requirements to Technical Verification</title>
<link>https://arxiv.org/abs/2512.13907</link>
<guid>https://arxiv.org/abs/2512.13907</guid>
<content:encoded><![CDATA[
<div> AI Act, compliance verification, operational mapping, AI lifecycle, regulatory requirements<br><br>Summary:<br><br>This paper addresses the challenge of implementing the AI Act by providing practical mechanisms to verify compliance with its legal obligations. It highlights the current limitation in concrete, operational mappings from high-level AI Act requirements to verifiable assessment activities, which lead to varied levels of readiness across EU Member States. The authors develop a structured mapping framework that translates abstract legal requirements into concrete, implementable verification activities applicable throughout the AI lifecycle. This framework is derived using a systematic decomposition of legal requirements into operational sub-requirements anchored in authoritative standards and recognized best practices. The proposed mapping characterizes verification activities along two key dimensions: the type of verification performed and the specific stage of the AI lifecycle it targets. By explicitly linking regulatory objectives to technical and organizational assurance practices, the framework helps reduce interpretive uncertainty. Furthermore, it offers a reusable, technology-agnostic reference model for consistent compliance verification under the AI Act, which can support harmonized enforcement and encourage readiness across jurisdictions. This approach contributes to bridging the gap between high-level regulation and practical audit mechanisms, facilitating more effective oversight and trustworthy AI deployment. <div>
arXiv:2512.13907v2 Announce Type: replace-cross 
Abstract: The implementation of the AI Act requires practical mechanisms to verify compliance with legal obligations, yet concrete and operational mappings from high-level requirements to verifiable assessment activities remain limited, contributing to uneven readiness across Member States. This paper presents a structured mapping that translates high-level AI Act requirements into concrete, implementable verification activities applicable across the AI lifecycle. The mapping is derived through a systematic process in which legal requirements are decomposed into operational sub-requirements and grounded in authoritative standards and recognised practices. From this basis, verification activities are identified and characterised along two dimensions: the type of verification performed and the lifecycle target to which it applies. By making explicit the link between regulatory intent and technical and organisational assurance practices, the proposed mapping reduces interpretive uncertainty and provides a reusable reference for consistent, technology-agnostic compliance verification under the AI Act.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2512.14554</link>
<guid>https://arxiv.org/abs/2512.14554</guid>
<content:encoded><![CDATA[
<div> Vietnamese Legal Benchmark, Large Language Models, Legal AI, Vietnamese Legislation, Cognitive Taxonomy

<br><br>Summary:  
This paper introduces the Vietnamese Legal Benchmark (VLegal-Bench), the first comprehensive benchmark specifically designed to evaluate large language models (LLMs) on tasks related to Vietnamese law. Recognizing the challenges posed by the complex, hierarchical, and frequently revised nature of Vietnamese legislation, VLegal-Bench is developed to systematically assess how well LLMs interpret and utilize legal knowledge in this unique context. The benchmark is guided by Bloom's cognitive taxonomy, structuring tasks to cover multiple levels of legal understanding and reflecting practical real-world scenarios. It consists of 10,450 samples created through a rigorous annotation pipeline involving legal experts who ensure each instance is firmly based on authoritative legal documents and validated through cross-checking. The tasks included in VLegal-Bench simulate typical legal assistant workflows such as answering general legal questions, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving, all tailored to Vietnamese law. By offering a standardized, transparent, and cognitively informed framework, VLegal-Bench provides an essential foundation for evaluating LLM performance in Vietnamese legal contexts and promotes the development of AI-assisted legal systems that are more reliable, interpretable, and ethically responsible. <div>
arXiv:2512.14554v3 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout</title>
<link>https://arxiv.org/abs/2512.18034</link>
<guid>https://arxiv.org/abs/2512.18034</guid>
<content:encoded><![CDATA[
<div> Keywords: CDCL, facility layout problem, VSIDS heuristics, CNF formulation, hybrid optimization<br /><br />Summary:<br /><br />This paper investigates the application of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational tool for solving discrete facility layout problems. The problem is framed as a combinatorial assignment task with complex logical constraints including adjacency, separation, and slot-availability, captured via a CNF-based feasibility model. The study benchmarks CDCL-based SAT solving against Constraint Programming SAT (CP-SAT) and Mixed Integer Linear Programming (MILP) methods within a unified evaluation framework. Results indicate that CDCL demonstrates nearly constant runtime for feasibility checking regardless of problem size or constraint density, contrasting with CP-SAT and MILP which scale polynomially and exponentially, respectively. Recognizing CDCL's limitations in objective optimization, the authors propose two hybrid architectures combining CDCL feasibility searches with CP-SAT optimization. The first hybrid rapidly enumerates feasible solutions, prioritizing speed over optimality, while the second uses CDCL to produce warm-start solutions that accelerate exact optimization. Experiments show these hybrids substantially reduce time-to-solution without sacrificing solution correctness. The findings clarify the algorithmic trade-offs between fast clause-learning search strategies and precise optimization approaches in large-scale discrete layout scenarios, suggesting hybrid methods as effective solutions balancing efficiency and optimality. <div>
arXiv:2512.18034v1 Announce Type: new 
Abstract: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.18092</link>
<guid>https://arxiv.org/abs/2512.18092</guid>
<content:encoded><![CDATA[
<div> Neuron identification, mechanistic interpretability, faithfulness, stability, bootstrap ensemble<br /><br />Summary:<br /><br />This paper addresses the task of neuron identification in mechanistic interpretability, which seeks to reveal human-interpretable concepts linked to individual neurons in deep neural networks. The authors highlight the lack of a rigorous theoretical foundation for current neuron identification techniques such as Network Dissection and CLIP-Dissect, which limits the trustworthiness of their explanations. They propose viewing neuron identification as the inverse of machine learning, enabling theoretical guarantees for the explanations produced. The study focuses on two main challenges: faithfulness — ensuring the identified concept truly represents the neuron's function, and stability — ensuring consistent identification results across different probing datasets. To address these, the paper derives generalization bounds for common similarity metrics including accuracy, AUROC, and IoU to guarantee faithfulness. Additionally, a bootstrap ensemble method is introduced to assess and quantify the stability of neuron explanations. The Bootstrap Explanation (BE) method further generates concept prediction sets with guaranteed coverage probability, providing more reliable interpretations. Experimental evaluations on synthetic and real datasets validate the theoretical findings and demonstrate the practical effectiveness of the approach. Overall, this work offers a significant step towards reliable and trustworthy neuron identification through a well-grounded theoretical framework. <div>
arXiv:2512.18092v1 Announce Type: new 
Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks</title>
<link>https://arxiv.org/abs/2512.18094</link>
<guid>https://arxiv.org/abs/2512.18094</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent systems, small-world networks, uncertainty-guided rewiring, multi-agent debate  

<br /><br />Summary:  
1. This paper investigates the impact of adopting small-world (SW) network connectivity as a design prior in multi-agent systems (MAS) powered by large language models (LLMs).  
2. It connects insights from neuroscience and complex network theory, emphasizing that SW topologies balance high local clustering with efficient long-range integration, which is beneficial for MAS communication.  
3. Experimental evaluation using a multi-agent debate (MAD) framework demonstrates that SW connectivity maintains accuracy and token efficiency while significantly stabilizing the consensus formation process among agents.  
4. The authors propose an uncertainty-guided rewiring method for scalable MAS, where long-range connections are dynamically added between agents that differ epistemically, leveraging LLM-derived uncertainty signals such as semantic entropy.  
5. This approach creates adaptable SW structures that reflect task difficulty and agent diversity, leading to MAS that are more robust, stable, and capable of emergent cognitive roles, with broader implications for the design and coordination of decentralized reasoning systems. <div>
arXiv:2512.18094v1 Announce Type: new 
Abstract: Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap</title>
<link>https://arxiv.org/abs/2512.18126</link>
<guid>https://arxiv.org/abs/2512.18126</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Agents, hierarchical tree topology, adaptive termination, pipelined execution, latency reduction  

<br /><br />Summary: This paper addresses the challenges in Mixture-of-Agents (MoA) inference, specifically targeting the issues of dense inter-agent communication and underutilized hardware that increase serving latency. The authors propose an integrated algorithm and system co-design approach to alleviate these bottlenecks. First, they replace the conventional dense inter-agent interaction graph with a hierarchical tree topology, which imposes structured sparsity and reduces communication overhead. Second, they develop a runtime adaptive mechanism that leverages semantic agreement and confidence signals from intermediate outputs to selectively skip or terminate downstream agent invocations, enhancing efficiency without compromising performance. Third, they introduce a pipelined execution strategy by overlapping incremental prefilling with decoding operations across dependent agents, improving hardware utilization and further reducing inference latency. Empirical evaluations on representative tasks demonstrate that their approach can reduce end-to-end latency by up to 90% while maintaining accuracy within ±1% of dense-connectivity MoA baselines. Additionally, in some scenarios, the method improves accuracy beyond baseline performance. Overall, this work demonstrates that a carefully designed combination of sparse communication topology, adaptive runtime strategies, and pipelined execution offers significant practical benefits for scalable and efficient MoA inference. <div>
arXiv:2512.18126v1 Announce Type: new 
Abstract: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications</title>
<link>https://arxiv.org/abs/2512.18135</link>
<guid>https://arxiv.org/abs/2512.18135</guid>
<content:encoded><![CDATA[
<div> Causal Inference, Reinforcement Learning, Causal Representation, Counterfactual Policy, Causal Explainability  

<br /><br />Summary:  
This survey explores the integration of causal inference (CI) with reinforcement learning (RL), forming causal reinforcement learning (CRL) to overcome key limitations in traditional RL such as low explainability, reduced robustness, and poor generalization. Traditional RL methods depend heavily on correlation-driven decisions, which limits their performance under distribution shifts, confounding variables, and evolving environments. CRL introduces cause-and-effect modeling to address these problems effectively. The paper categorizes recent approaches into five key areas: causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. It systematically reviews advancements in each of these areas, revealing significant empirical successes and practical applications that demonstrate CRL’s potential. The survey also identifies ongoing challenges, such as scalability, accurate causal discovery, and efficient counterfactual reasoning. Finally, it offers future research directions aimed at leveraging CRL to build AI systems that are more robust, generalizable, and interpretable, emphasizing the transformative promise of causal methods in advancing reinforcement learning technologies. <div>
arXiv:2512.18135v1 Announce Type: new 
Abstract: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Propose, Solve, Verify: Self-Play Through Formal Verification</title>
<link>https://arxiv.org/abs/2512.18160</link>
<guid>https://arxiv.org/abs/2512.18160</guid>
<content:encoded><![CDATA[
<div> Keywords: self-play, formal verification, code generation, Propose Solve Verify, expert iteration  

<br /><br />Summary:  
This paper addresses the challenge of training large language models (LLMs) for code generation entirely through self-play, without relying on human-generated data. It highlights the difficulties in using unit-test-based rewards due to their brittleness and error propagation, proposing formal verification as a more reliable correctness signal. The authors introduce a novel self-play framework called Propose, Solve, Verify (PSV), which leverages formal verification to build a proposer that generates challenging synthetic coding problems and a solver trained using expert iteration techniques. Applying PSV, they develop PSV-Verus, a model that demonstrates substantial improvements, achieving up to 9.6 times higher pass@1 rates compared to both inference-only models and expert-iteration baselines across three benchmark datasets. Their experiments reveal that model performance improves as the number of generated problems and training iterations increase. Through ablation studies, they identify formal verification and difficulty-aware problem proposal as critical components for successful self-play in code generation. Overall, the work provides strong evidence that formal verification can enable effective self-play training regimes for LLMs in generating verified, correct code without human data. <div>
arXiv:2512.18160v1 Announce Type: new 
Abstract: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI</title>
<link>https://arxiv.org/abs/2512.18177</link>
<guid>https://arxiv.org/abs/2512.18177</guid>
<content:encoded><![CDATA[
<div> Keywords: NEURO-GUARD, Vision Transformers, medical image diagnosis, interpretability, domain generalization<br /><br />Summary:<br /><br />1. NEURO-GUARD is a novel knowledge-guided vision framework designed to improve medical image diagnosis by integrating Vision Transformers (ViTs) with language-driven reasoning.  
2. It addresses key challenges in medical AI such as limited data availability, subtle visual cues, and the need for interpretable and trustworthy decision-making in high-stakes clinical environments.  
3. The framework employs a retrieval-augmented generation (RAG) mechanism allowing a large language model (LLM) to iteratively generate, evaluate, and refine feature-extraction code grounded in clinical guidelines and expert knowledge, enhancing feature detection and classification beyond purely data-driven methods.  
4. Extensive experiments on diabetic retinopathy classification across four benchmark datasets (APTOS, EyePACS, Messidor-1, and Messidor-2) demonstrate a significant 6.2% accuracy improvement over ViT-only baselines (84.69% vs. 78.4%) and a 5% gain in domain generalization.  
5. Additional evaluations on MRI-based seizure detection confirm NEURO-GUARD’s cross-domain robustness, consistently outperforming existing methods.  
6. Overall, NEURO-GUARD effectively bridges symbolic medical reasoning with subsymbolic visual learning, achieving interpretable, knowledge-aware, and generalizable medical image diagnosis with state-of-the-art performance across multiple datasets. <div>
arXiv:2512.18177v1 Announce Type: new 
Abstract: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.
  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework</title>
<link>https://arxiv.org/abs/2512.18189</link>
<guid>https://arxiv.org/abs/2512.18189</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive computing, Natural language processing, Linear Temporal Logic, Symbolic cognitive frameworks, Reinforcement learning<br /><br />Summary:<br /><br />This paper introduces NL2CA, a fully automated method for converting natural language descriptions of human decision-making into formal cognitive decision-making rules. Unlike previous methods that rely heavily on manual or human-guided modeling, NL2CA operates without any human intervention. The process begins by translating natural language text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM). Next, an unsupervised Critic Tree refines this logic output to improve accuracy and consistency. The refined LTL is then transformed into executable production rules that are compatible with symbolic cognitive frameworks. These rules are used to construct a cognitive agent, which is further optimized through cognitive reinforcement learning using real-world behavioral data. NL2CA was validated in two key domains: first, in NL-to-LTL translation, where the CriticNL2LTL module delivered robust performance on both expert and large-scale benchmarks without requiring human feedback; second, in cognitive driving simulation, where agents automatically built from human interviews successfully learned diverse decision-making patterns across about 70 trials in various critical scenarios. Experimental results demonstrate that NL2CA provides a scalable, interpretable, and human-aligned approach to cognitive modeling from unstructured text, offering a new paradigm for automatically designing symbolic cognitive agents. <div>
arXiv:2512.18189v1 Announce Type: new 
Abstract: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.18190</link>
<guid>https://arxiv.org/abs/2512.18190</guid>
<content:encoded><![CDATA[
<div> Keywords: External Hippocampus, cognitive dynamics, topological cognitive maps, multi-step reasoning, small language models

<br /><br />Summary: This paper introduces the External Hippocampus framework, which reconceptualizes language model reasoning as the flow of information energy within a semantic space based on cognitive dynamics. Unlike conventional methods that focus on weight-space optimization, this approach constructs topological cognitive maps via dimensionality reduction, enabling precise navigation and intervention of energy flow at test time without heavy computational costs. This novel framework effectively tackles the cognitive deadlock problem encountered in multi-step reasoning tasks, especially in small language models with 7 billion parameters or less. Experimental results demonstrate that map-guided methods attain an accuracy of 81.20% on 500 challenging problems, which is a 16.80% improvement over baseline methods, while also reducing reasoning time by at least 15 times. Key insights include identifying reasoning stagnation as a "Cognitive Vortex" characterized by low-entropy potential wells, and showing that temperature perturbations can successfully restart the halted energy flow. Moreover, the framework requires no additional training, supports autonomous growth, and provides an efficient, controllable, and topological-aware solution tailored to improve reasoning capabilities in small models. <div>
arXiv:2512.18190v2 Announce Type: new 
Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sophia: A Persistent Agent Framework of Artificial Life</title>
<link>https://arxiv.org/abs/2512.18202</link>
<guid>https://arxiv.org/abs/2512.18202</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Persistent Agent, System 3, Meta-cognition, Artificial Life  

<br /><br />Summary:  
The paper introduces a novel architectural addition called System 3 to supplement existing AI agent frameworks based on Large Language Models (LLMs), which currently comprise System 1 (perception) and System 2 (deliberation). System 3 functions as a meta-layer responsible for maintaining the agent’s narrative identity and enabling long-term adaptation, thus addressing the lack of persistence in AI behavior over time. This framework draws from psychological constructs and translates them into concrete computational modules aimed at facilitating artificial life characteristics. The authors present Sophia, a prototype "Persistent Agent" that integrates System 3 with existing LLM-based architectures, incorporating four key mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, these components enable a continuous loop of self-improvement, identity continuity, and transparent explanation of behavior, transforming repetitive reasoning into an autobiographical process. Empirical results show that Sophia reduces reasoning steps by 80% for recurring tasks and increases success rates by 40% on complex tasks through meta-cognitive persistence. Qualitatively, System 3 endowed agents with coherent narrative identities and enhanced task organization. Overall, the persistent agent architecture provides a promising, psychologically inspired approach for advancing autonomous, adaptive artificial agents closer toward the notion of artificial life. <div>
arXiv:2512.18202v1 Announce Type: new 
Abstract: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a "Persistent Agent" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification</title>
<link>https://arxiv.org/abs/2512.18256</link>
<guid>https://arxiv.org/abs/2512.18256</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Theorem Proving, large language models, MSC-180 benchmark, mathematical reasoning, evaluation metrics<br /><br />Summary:<br />1. Automated Theorem Proving (ATP) is a vital area in artificial intelligence focused on formal reasoning and verification, essential for advancing machine intelligence.<br />2. Current theorem provers based on large language models (LLMs) face challenges including limited domain coverage and poor generalization in complex mathematical reasoning tasks.<br />3. The MSC-180 benchmark is introduced to evaluate such models systematically, featuring 180 formal verification problems drawn from 60 different branches of mathematics according to the MSC2020 classification, ranging from undergraduate to graduate difficulty.<br />4. Each problem has been carefully verified and refined by domain experts to ensure formal correctness.<br />5. Evaluation results using the pass@32 metric show that the best-performing LLM model attains only an 18.89% overall success rate, with notable domain bias (max coverage 41.7%) and significantly lower performance on graduate-level problems.<br />6. The coefficient of variation (CV) is proposed as a novel metric to measure performance variability across domains, revealing excessively high variability that implies the models depend heavily on pattern matching rather than true transferable reasoning.<br />7. MSC-180 and its multi-dimensional evaluation framework offer a rigorous and discriminative benchmark aimed at stimulating progress towards AI systems capable of genuine mathematical reasoning and systematic generalization. <div>
arXiv:2512.18256v1 Announce Type: new 
Abstract: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration</title>
<link>https://arxiv.org/abs/2512.18265</link>
<guid>https://arxiv.org/abs/2512.18265</guid>
<content:encoded><![CDATA[
<div> Keywords: Manufacturing planning, Collaborative intelligence, Knowledge Graphs, Large Language Models, Operational analysis  

<br /><br />Summary:  
1. Manufacturing planners encounter complex operational challenges needing seamless collaboration between human expertise and intelligent systems to optimize production performance.  
2. Traditional simulation data analysis methods often hinder effective collaboration by creating barriers between decision-makers and operational insights.  
3. The proposed framework integrates Knowledge Graphs with Large Language Model-based agents to form a collaborative intelligence system, enabling natural language interaction for manufacturing professionals without specialized expertise.  
4. This system converts simulation data into semantically rich representations and offers iterative reasoning that mimics human analytical thinking, generating precise queries and transparent validation.  
5. Validated through real operational scenarios, the system enhances bottleneck identification accuracy, maintains human oversight, and supports investigative collaborative analysis to uncover interconnected operational issues.  
6. The framework delivers near-perfect accuracy in operational inquiries using natural language and effectively aids experts in complex analyses, reducing cognitive load while boosting analytical capabilities in dynamic manufacturing environments. <div>
arXiv:2512.18265v1 Announce Type: new 
Abstract: Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monitoring Monitorability</title>
<link>https://arxiv.org/abs/2512.18311</link>
<guid>https://arxiv.org/abs/2512.18311</guid>
<content:encoded><![CDATA[
<div> Keywords: monitorability, chain-of-thought, AI safety, evaluation metrics, reinforcement learning<br /><br />Summary: Observability into AI decision-making is critical for safely deploying advanced agents. Monitoring the chain-of-thought (CoT) reasoning process has been effective for detecting model misbehavior. The paper proposes three evaluation archetypes—intervention, process, and outcome-property—and introduces a new metric alongside a comprehensive evaluation suite to measure monitorability. Experiments reveal that these tools can detect models trained to obfuscate their CoTs, confirming that CoT monitoring outperforms action-only monitoring in practical scenarios. Analysis of state-of-the-art models shows they are generally monitorable but not perfectly so. The study investigates how monitorability changes with factors like inference-time compute, reinforcement learning (RL) optimization, and model pre-training size, finding that longer CoTs tend to increase monitorability and that RL does not significantly reduce it at current scales. A notable insight is that deploying smaller models with higher reasoning effort can achieve similar capabilities with better monitorability, albeit at increased compute cost. Additionally, increasing a weaker monitor’s compute resources when observing stronger agents enhances monitorability. Providing monitors with access to CoTs further improves detection and steepens the positive relationship between monitor compute and monitorability. Finally, leveraging follow-up questions to extend CoTs fed to monitors substantially boosts monitoring effectiveness. <div>
arXiv:2512.18311v1 Announce Type: new 
Abstract: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this "monitorability" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation</title>
<link>https://arxiv.org/abs/2512.18412</link>
<guid>https://arxiv.org/abs/2512.18412</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot learning, structural graph, contour image classification, graph edit distance, concept attractors<br /><br />Summary:  
This paper introduces a novel structural-graph method to classify contour images in a few-shot learning setting without relying on backpropagation. The main idea is to encode images as attributed graphs, where critical points and lines serve as nodes with geometric attributes, enabling transparent explanations by using structure as the carrier of meaning. The model forms class concepts, called concept attractors, from 5 to 6 examples per class via structural and parametric reductions, thus providing interpretability and eliminating the need for gradient-based training. The approach first vectorizes contours and constructs a bipartite graph comprising point and line nodes with attributes such as coordinates, angles, and lengths. Noise and unstable components are removed, and paths between critical points are aligned for consistency. Concept graphs are iteratively composed from the samples, and classification is performed by finding the best graph-to-concept match using an approximated graph edit distance (GED). Experimental validation on a subset of MNIST achieves about 82% accuracy with a single epoch, offering full traceability in decision-making where mistakes can be explicitly linked to structural similarities. Comparisons with SVM, MLP, CNN, and meta-learning baselines demonstrate the approach’s competitiveness. Limitations include the computational cost of GED calculations and dependence on skeletonization quality, with future work suggested in optimization and associative recognition domains. <div>
arXiv:2512.18412v1 Announce Type: new 
Abstract: We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System</title>
<link>https://arxiv.org/abs/2512.18450</link>
<guid>https://arxiv.org/abs/2512.18450</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical decision support, multisite drift detection, agent-based monitoring, breast cancer imaging, distributional shift<br /><br />Summary:<br /><br />1. The study addresses the challenge of predictive performance degradation in clinical AI systems deployed across multiple independent medical imaging institutions caused by variations in patient populations, imaging hardware, and protocols. <br /><br />2. Existing drift detection methods mostly focus on centralized monitoring of aggregated model predictions, which can overlook important site-specific drift dynamics.<br /><br />3. The authors propose an agent-based framework that assigns a dedicated drift monitoring agent to each site, enabling batch-wise output comparisons against reference distributions to detect and assess the severity of distributional shifts.<br /><br />4. Various multi-center monitoring schemes are evaluated, including site-specific, global, production-only, and adaptive reference models, compared against a centralized baseline.<br /><br />5. Experiments using real-world breast cancer imaging data and a pathological complete response prediction model demonstrate that all multi-center agent-based monitoring approaches outperform centralized monitoring, achieving up to 10.3% improvement in F1-score for drift detection.<br /><br />6. When site-specific references are unavailable, the adaptive monitoring scheme performs best, reaching F1-scores of 74.3% for drift detection and 83.7% for drift severity classification.<br /><br />7. The findings highlight that adaptive, site-aware agent-based drift monitoring enhances the reliability and safety of multisite clinical decision support systems by effectively identifying distributional shifts without requiring ground truth labels. <div>
arXiv:2512.18450v1 Announce Type: new 
Abstract: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations</title>
<link>https://arxiv.org/abs/2512.18483</link>
<guid>https://arxiv.org/abs/2512.18483</guid>
<content:encoded><![CDATA[
<div> Insider threat detection, Graph representations, Temporal modelling, Graph Convolutional Networks, Bi-LSTM  

<br /><br />Summary:  
This paper addresses the challenge of insider threat detection (ITD), focusing on subtle malicious activities by trusted users. It proposes a post-hoc ITD framework that integrates both explicit and implicit graph representations to capture complex user behavior patterns more effectively. An explicit graph models direct relationships among user activities based on predefined organizational rules. To overcome noise and limitations of this handcrafted graph, an implicit graph is learned using feature similarities and the Gumbel-Softmax trick, exposing latent behavioral relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to produce node embeddings, which are then concatenated. An attention mechanism is applied to emphasize threat-relevant features in the combined embedding. The refined node representations are fed into a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behavior over time. Activities with probability scores below a predefined threshold are flagged as anomalous. The framework is extensively tested on CERT r5.2 and r6.2 datasets, outperforming state-of-the-art methods. On CERT r5.2, it achieves an AUC of 98.62, 100% detection rate, and 0.05 false positive rate. For the more difficult r6.2 dataset, it attains an AUC of 88.48, 80.15% detection rate, and a 0.15 false positive rate, demonstrating the effectiveness of combining graph-based and temporal modeling for robust ITD. <div>
arXiv:2512.18483v1 Announce Type: new 
Abstract: Insider threat detection (ITD) is challenging due to the subtle and concealed nature of malicious activities performed by trusted users. This paper proposes a post-hoc ITD framework that integrates explicit and implicit graph representations with temporal modelling to capture complex user behaviour patterns. An explicit graph is constructed using predefined organisational rules to model direct relationships among user activities. To mitigate noise and limitations in this hand-crafted structure, an implicit graph is learned from feature similarities using the Gumbel-Softmax trick, enabling the discovery of latent behavioural relationships. Separate Graph Convolutional Networks (GCNs) process the explicit and implicit graphs to generate node embeddings, which are concatenated and refined through an attention mechanism to emphasise threat-relevant features. The refined representations are then passed to a bidirectional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in user behaviour. Activities are flagged as anomalous when their probability scores fall below a predefined threshold. Extensive experiments on CERT r5.2 and r6.2 datasets demonstrate that the proposed framework outperforms state-of-the-art methods. On r5.2, the model achieves an AUC of 98.62, a detection rate of 100%, and a false positive rate of 0.05. On the more challenging r6.2 dataset, it attains an AUC of 88.48, a detection rate of 80.15%, and a false positive rate of 0.15, highlighting the effectiveness of combining graph-based and temporal representations for robust ITD.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models as Discounted Bayesian Filters</title>
<link>https://arxiv.org/abs/2512.18489</link>
<guid>https://arxiv.org/abs/2512.18489</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Bayesian filtering, online inference, belief updating, exponential forgetting  

<br /><br />Summary:  
This article investigates how Large Language Models (LLMs) perform online inference and belief updates in dynamic, stochastic environments. Unlike prior work focusing on static tasks, the study introduces a Bayesian filtering framework to evaluate LLMs' ability to continuously update beliefs as new information arrives. The authors design a probabilistic probe suite covering both multivariate discrete distributions (e.g., dice rolls) and continuous distributions (e.g., Gaussian processes) with parameters that change over time. Their findings reveal that LLM belief updates approximate Bayesian posteriors but align more closely with an exponential forgetting filter, characterized by a discount factor less than one, indicating systematic down-weighting of older evidence. This discounting varies significantly across different model architectures. Although LLMs’ inherent priors are often miscalibrated, the update mechanism itself is structured and principled. The study validates these insights through a simulated agent task and proposes effective prompt engineering strategies to recalibrate priors efficiently at minimal computational cost. Overall, the work sheds light on the reasoning capabilities of LLMs in online, non-stationary contexts, highlighting avenues for improving adaptation and inference robustness. <div>
arXiv:2512.18489v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V</title>
<link>https://arxiv.org/abs/2512.18564</link>
<guid>https://arxiv.org/abs/2512.18564</guid>
<content:encoded><![CDATA[
<div> Large Language Models, 4X strategy games, Vox Deorum, macro-strategic reasoning, hybrid AI architecture<br /><br />Summary:<br /><br />This article explores the integration of Large Language Models (LLMs) into complex 4X and grand strategy games, focusing on their natural language reasoning capabilities that facilitate human-AI interactions like collaboration and negotiation. Addressing challenges such as game complexity, long-term planning, latency, and cost, the authors introduce Vox Deorum, a hybrid architecture combining LLMs with other AI subsystems to separate macro-strategic reasoning from tactical execution. This design allows LLMs to focus on high-level decision-making while delegating tactical tasks to algorithmic or reinforcement learning AIs. Using Sid Meier's Civilization V with the Vox Populi mod as a testbed, they conducted 2,327 complete games comparing two open-source LLMs against the game’s enhanced AI with simple prompts. The results showed LLMs achieved competitive end-to-end gameplay performance, yet exhibited play styles distinctly different from traditional algorithmic AI and from each other. Overall, this work demonstrates a practical framework for incorporating LLMs into commercial 4X games, paving the way for enhanced game design and further research into agentic AI systems in strategic gaming contexts. <div>
arXiv:2512.18564v1 Announce Type: new 
Abstract: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.18571</link>
<guid>https://arxiv.org/abs/2512.18571</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, embodied agents, cost-aware reasoning, HC-GRPO, AI2-THOR<br /><br />Summary:<br /><br />This paper addresses the challenge faced by embodied agents powered by Multimodal Large Language Models (MLLMs) when interpreting ambiguous natural language instructions in complex environments, such as cluttered rooms. Current agents struggle to balance the physical cost of exploring the environment and the cognitive cost linked to human interaction, often treating ambiguity resolution solely as a passive perception problem. To overcome these limitations, the authors propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single, coherent decision-making process. Central to this framework is HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization), a novel optimization approach that differs from traditional PPO by using sampled groups of reasoning trajectories to optimize trade-offs between information gain and diverse costs such as navigation time and human attention. The method was extensively evaluated in the AI2-THOR simulation environment, where ESearch-R1 demonstrated significant improvements over conventional ReAct-based agents. By effectively aligning MLLM-driven agents with real-world operational constraints, the framework nearly halves total task costs while boosting task success rates, validating the efficacy of the proposed HC-GRPO optimization strategy. <div>
arXiv:2512.18571v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction</title>
<link>https://arxiv.org/abs/2512.18605</link>
<guid>https://arxiv.org/abs/2512.18605</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, self-consistency, reflective confidence, early stopping, mathematical reasoning<br /><br />Summary: Large language models (LLMs) have demonstrated strong performance on complex reasoning tasks using techniques like chain-of-thought and self-consistency, but ensemble methods such as self-consistency often result in high computational costs due to reliance on multiple reasoning trajectories. Previous approaches to improve efficiency used internal confidence signals to implement early stopping strategies like DeepConf, which terminate low-confidence trajectories to save computation; however, this leads to discarding incomplete reasoning paths and wastes partial effort. This work proposes reflective confidence, a novel reasoning framework that repurposes low-confidence signals from termination indicators as triggers for reflection rather than stopping. When the model’s confidence drops below a threshold, it generates a reflection prompt to analyze the current reasoning state, identify possible errors, and continue reasoning on a corrected path. Experiments conducted on mathematical reasoning benchmarks including AIME 2025 show that this proactive self-correction approach yields significant accuracy improvements over existing early stopping baselines while maintaining similar computational costs. The results validate that reflective confidence enables more efficient and accurate reasoning by utilizing partial computations for corrective reasoning instead of simply discarding them. <div>
arXiv:2512.18605v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.
  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assignment-Routing Optimization: Solvers for Problems Under Constraints</title>
<link>https://arxiv.org/abs/2512.18618</link>
<guid>https://arxiv.org/abs/2512.18618</guid>
<content:encoded><![CDATA[
<div> Keywords: Joint Routing-Assignment, MIP solver, Hamiltonian cycle, robotic packaging, logistics optimization<br /><br />Summary:<br /><br />1. The paper addresses the Joint Routing-Assignment (JRA) problem, which involves assigning items one-to-one to placeholders while simultaneously determining a Hamiltonian cycle that visits all nodes exactly once.<br /><br />2. The authors extend previous exact Mixed-Integer Programming (MIP) solvers by integrating Gurobi and a cutting-plane method for subtour elimination, creating a solver customized for complex packaging and planning scenarios.<br /><br />3. The solver incorporates richer constraints such as multiple placeholder options per item, time-frame restrictions, and multi-class packaging requirements to better reflect practical robotic packaging and motion planning environments.<br /><br />4. Experiments conducted on 46 mobile manipulation datasets demonstrate that the proposed MIP approach consistently achieves global optima with stable and low computation times, outperforming existing shaking-based exact solvers by up to an order of magnitude.<br /><br />5. Compared to simple greedy heuristics, the MIP solutions yield significantly better routing distances with an average deviation of only 14%, confirming its effectiveness and practical applicability in robotic packaging, logistics, and complex motion planning tasks. <div>
arXiv:2512.18618v1 Announce Type: new 
Abstract: We study the Joint Routing-Assignment (JRA) problem in which items must be assigned one-to-one to placeholders while simultaneously determining a Hamiltonian cycle visiting all nodes exactly once. Extending previous exact MIP solvers with Gurobi and cutting-plane subtour elimination, we develop a solver tailored for practical packaging-planning scenarios with richer constraints.These include multiple placeholder options, time-frame restrictions, and multi-class item packaging. Experiments on 46 mobile manipulation datasets demonstrate that the proposed MIP approach achieves global optima with stable and low computation times, significantly outperforming the shaking-based exact solver by up to an orders of magnitude. Compared to greedy baselines, the MIP solutions achieve consistent optimal distances with an average deviation of 14% for simple heuristics, confirming both efficiency and solution quality. The results highlight the practical applicability of MIP-based JRA optimization for robotic packaging, motion planning, and complex logistics .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</title>
<link>https://arxiv.org/abs/2512.18619</link>
<guid>https://arxiv.org/abs/2512.18619</guid>
<content:encoded><![CDATA[
<div> Keywords: ChronoDreamer, action-conditioned world model, contact-rich manipulation, spatial-temporal transformer, vision-language model<br /><br />Summary: The paper introduces ChronoDreamer, an action-conditioned world model designed for handling contact-rich robotic manipulation tasks. It uses a history of egocentric RGB frames, contact maps, robotic actions, and joint states as inputs to predict future video frames, contact distributions, and joint angles. ChronoDreamer leverages a spatial-temporal transformer trained with a MaskGIT-style masked prediction approach. Contact information is encoded using depth-weighted Gaussian splat images, providing a 3D force representation in a camera-aligned format compatible with vision-based neural networks. For decision-making during inference, predicted rollouts are assessed by a vision-language model that interprets the likelihood of collisions, enabling rejection sampling to discard unsafe action sequences prior to their execution. The model is trained and tested on DreamerBench, a simulation dataset created with Project Chrono, which includes synchronized streams of RGB images, contact splat data, proprioception, and detailed physics annotations covering both rigid and deformable object scenarios. Qualitative evaluations show that ChronoDreamer maintains spatial coherence during non-contact movements and generates realistic predictions of contact events. Additionally, the use of a large language model (LLM) based judge effectively differentiates collision and non-collision trajectories, improving safety in robotic manipulation planning. <div>
arXiv:2512.18619v1 Announce Type: new 
Abstract: We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting</title>
<link>https://arxiv.org/abs/2512.18661</link>
<guid>https://arxiv.org/abs/2512.18661</guid>
<content:encoded><![CDATA[
<div> Keywords: financial time series, semantic-temporal integration, cryptocurrency forecasting, meta-learning, hybrid model<br /><br />Summary:<br /><br />1. The paper addresses financial time series forecasting as an information fusion problem, highlighting limitations of current static models that struggle to integrate diverse knowledge sources or adapt quickly to regime changes.<br />2. It identifies a gap in conventional forecasting methods that rely solely on historical price data, ignoring semantic market drivers such as policy uncertainty and narratives.<br />3. The authors propose ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that dynamically adjusts forecasting strategies in real time using confidence-based meta-learning.<br />4. ASTIF combines three components: a dual-channel small language model using MirrorPrompt to capture semantic market signals alongside numerical trends, a hybrid LSTM Random Forest model to learn temporal dependencies, and a confidence-aware meta-learner that adaptively weighs each predictor based on real-time uncertainty.<br />5. Experimental results on a dataset spanning AI-focused cryptocurrencies and major tech stocks from 2020 to 2024 demonstrate that ASTIF outperforms state-of-the-art deep learning and Transformer models like Informer and TFT.<br />6. Ablation studies confirm that the adaptive meta-learning layer is crucial for risk mitigation by dynamically shifting reliance between semantic and temporal data during periods of market turbulence.<br />7. The research presents a scalable, knowledge-driven framework for fusing quantitative and qualitative data in non-stationary financial environments, advancing the field of price forecasting. <div>
arXiv:2512.18661v1 Announce Type: new 
Abstract: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.
  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking</title>
<link>https://arxiv.org/abs/2512.18665</link>
<guid>https://arxiv.org/abs/2512.18665</guid>
<content:encoded><![CDATA[
<div> Keywords: chunking, concept learning, cognitive modeling, subjective judgments, CogAct<br /><br />Summary:<br /><br />1. The article addresses fundamental psychological processes involved in forming and retrieving multiple types of concepts in short-term and long-term memory (STM and LTM).<br />2. It introduces the CogAct computational model, which grounds concept learning in key cognitive processes such as chunking, attention, STM, and LTM.<br />3. CogAct demonstrates adaptive learning across diverse concept categories, ranging from simple logical functions and artificial categories to complex natural concepts in varied domains like literature, chess, and music, learning directly from raw data.<br />4. The model overcomes limitations faced by other psychological models that either stop at artificial categories or require task-specific changes, unlike CogAct’s flexible architecture.<br />5. Novel benchmarks for human concept learning experiments are proposed, accounting for subjectivity and individual human experiences while maintaining complexity.<br />6. CogAct simulates subjective conceptual spaces of individual participants, accurately modeling subjective judgments in music by learning from raw score data without relying on pre-built knowledge structures.<br />7. The study compares CogAct’s results with those from deep learning models, integrating concept learning with adaptation to complexity into broader cognitive psychology theories.<br />8. This approach facilitates psychological applications that move beyond average participant modeling toward capturing individualized subjective concept representations. <div>
arXiv:2512.18665v1 Announce Type: new 
Abstract: A key issue in cognitive science concerns the fundamental psychological processes that underlie the formation and retrieval of multiple types of concepts in short-term and long-term memory (STM and LTM, respectively). We propose that chunking mechanisms play an essential role and show how the CogAct computational model grounds concept learning in fundamental cognitive processes and structures (such as chunking, attention, STM and LTM). First are the in-principle demonstrations, with CogAct automatically adapting to learn a range of categories from simple logical functions, to artificial categories, to natural raw (as opposed to natural pre-processed) concepts in the dissimilar domains of literature, chess and music. This kind of adaptive learning is difficult for most other psychological models, e.g., with cognitive models stopping at modelling artificial categories and (non-GPT) models based on deep learning requiring task-specific changes to the architecture. Secondly, we offer novel ways of designing human benchmarks for concept learning experiments and simulations accounting for subjectivity, ways to control for individual human experiences, all while keeping to real-life complex categories. We ground CogAct in simulations of subjective conceptual spaces of individual human participants, capturing humans subjective judgements in music, with the models learning from raw music score data without bootstrapping to pre-built knowledge structures. The CogAct simulations are compared to those obtained by a deep-learning model. These findings integrate concept learning and adaptation to complexity into the broader theories of cognitive psychology. Our approach may also be used in psychological applications that move away from modelling the average participant and towards capturing subjective concept space.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling</title>
<link>https://arxiv.org/abs/2512.18669</link>
<guid>https://arxiv.org/abs/2512.18669</guid>
<content:encoded><![CDATA[
<div> Keywords: IntelliCode, multi-agent LLM, learner state, mastery estimates, curriculum adaptation<br /><br />Summary:<br /><br />IntelliCode is an advanced multi-agent tutoring system powered by large language models (LLMs) designed to offer persistent, transparent, and long-term support for learners. Unlike traditional single-turn LLM tutors, IntelliCode maintains a centralized, versioned learner state that stores mastery estimates, misconceptions, review schedules, and engagement signals. The system is orchestrated by a StateGraph Orchestrator which manages six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring. These agents operate under a single-writer policy, ensuring consistent updates and transformations over the shared learner state. This architecture supports auditable mastery updates, proficiency-aware hints, curriculum adaptation based on dependencies, and safety-aligned prompts. The demo workflow illustrates a learner attempting a data structures and algorithms (DSA) problem, receiving conceptual hints upon difficulty, updating their solution, and immediately observing mastery and personalized review changes. Validation with simulated learners demonstrates stable learner state updates, increased task success through graduated hinting, and broader curriculum coverage. IntelliCode exemplifies how integrating persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design enables transparent, reliable, and effective LLM-driven tutoring systems. <div>
arXiv:2512.18669v1 Announce Type: new 
Abstract: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.
  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model</title>
<link>https://arxiv.org/abs/2512.18687</link>
<guid>https://arxiv.org/abs/2512.18687</guid>
<content:encoded><![CDATA[
<div> Social comparison, reward evaluation, computational models, primate behavior, latent Dirichlet allocation<br /><br />Summary: This study investigates how monkeys process social comparison, specifically whether they recognize only objective reward differences or also infer the subjective reward valuations of others. Three computational models were developed: the Internal Prediction Model (IPM), which attempts to infer a partner’s subjective values; the No Comparison Model (NCM), which ignores partner-related information; and the External Comparison Model (ECM), which directly uses the partner’s objective reward data. These models were evaluated using multi-layered, multimodal latent Dirichlet allocation applied to a dataset involving monkey behavior, their rewards, and conditioned stimuli. The goal was to classify subjective values across experimental conditions. Results show that the ECM outperforms the other models with a higher Rand Index score of 0.88 compared to 0.79 for the IPM. This suggests that monkeys rely predominantly on objective differences in rewards of others for social comparison, rather than inferring others’ subjective reward states. Therefore, social comparison in primates, from a computational standpoint, is better explained by direct consideration of external reward information than by internal prediction of others’ subjective experiences. <div>
arXiv:2512.18687v2 Announce Type: new 
Abstract: Social comparison$\unicode{x2014}$the process of evaluating one's rewards relative to others$\unicode{x2014}$plays a fundamental role in primate social cognition. However, it remains unknown from a computational perspective how information about others' rewards affects the evaluation of one's own reward. With a constructive approach, this study examines whether monkeys merely recognize objective reward differences or, instead, infer others' subjective reward valuations. We developed three computational models with varying degrees of social information processing: an Internal Prediction Model (IPM), which infers the partner's subjective values; a No Comparison Model (NCM), which disregards partner information; and an External Comparison Model (ECM), which directly incorporates the partner's objective rewards. To test model performance, we used a multi-layered, multimodal latent Dirichlet allocation. We trained the models on a dataset containing the behavior of a pair of monkeys, their rewards, and the conditioned stimuli. Then, we evaluated the models' ability to classify subjective values across pre-defined experimental conditions. The ECM achieved the highest classification score in the Rand Index (0.88 vs. 0.79 for the IPM) under our settings, suggesting that social comparison relies on objective reward differences rather than inferences about subjective states.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing</title>
<link>https://arxiv.org/abs/2512.18709</link>
<guid>https://arxiv.org/abs/2512.18709</guid>
<content:encoded><![CDATA[
<div> Knowledge Tracing, Normal-Inverse-Gaussian Distribution, Attention Mechanism, Denoising Reconstruction Loss, Contrastive Learning<br /><br />Summary:<br /><br />1. The paper addresses the limitations of current Knowledge Tracing (KT) methods that rely on single-point estimates, which fail to distinguish true student ability from momentary outbursts or carelessness, leading to ambiguity in assessing mastery.<br /><br />2. The authors propose KeenKT, a novel KT model that represents a student’s knowledge state at each interaction with a Normal-Inverse-Gaussian (NIG) distribution, effectively capturing fluctuations in student learning behavior.<br /><br />3. An NIG-distance-based attention mechanism is introduced to dynamically model the evolution of the student’s knowledge state over time.<br /><br />4. To improve robustness, the model incorporates a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss.<br /><br />5. Extensive experiments conducted on six public datasets demonstrate that KeenKT significantly outperforms state-of-the-art KT models, achieving up to 5.85% improvement in AUC and 6.89% improvement in accuracy, while also showing increased sensitivity to behavioral fluctuations. <div>
arXiv:2512.18709v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) aims to dynamically model a student's mastery of knowledge concepts based on their historical learning interactions. Most current methods rely on single-point estimates, which cannot distinguish true ability from outburst or carelessness, creating ambiguity in judging mastery. To address this issue, we propose a Knowledge Mastery-State Disambiguation for Knowledge Tracing model (KeenKT), which represents a student's knowledge state at each interaction using a Normal-Inverse-Gaussian (NIG) distribution, thereby capturing the fluctuations in student learning behaviors. Furthermore, we design an NIG-distance-based attention mechanism to model the dynamic evolution of the knowledge state. In addition, we introduce a diffusion-based denoising reconstruction loss and a distributional contrastive learning loss to enhance the model's robustness. Extensive experiments on six public datasets demonstrate that KeenKT outperforms SOTA KT models in terms of prediction accuracy and sensitivity to behavioral fluctuations. The proposed method yields the maximum AUC improvement of 5.85% and the maximum ACC improvement of 6.89%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth</title>
<link>https://arxiv.org/abs/2512.18732</link>
<guid>https://arxiv.org/abs/2512.18732</guid>
<content:encoded><![CDATA[
<div> Keywords: concept learning, representational basis, Minimum Description Length, basis extension, conceptual innovation  

<br /><br />Summary:  
This paper addresses the fundamental question of how conceptual representations themselves can expand, rather than just updating beliefs within a fixed representational framework. It introduces a geometric approach where conceptual growth is modeled as an admissible extension of the conceptual basis, evaluated using the Minimum Description Length (MDL) principle. Experience, whether real or simulated, is expressed as vectors relative to the current conceptual subspace, and residual components highlight systematic failures of representation. Candidate expansions are restricted to low-rank transformations that lie within the residual span, ensuring any accepted basis extension reduces overall description length. Extensions orthogonal to this residual raise description length and are thus excluded. This approach offers a conservative model of imagination and conceptual innovation, where internally generated counterfactuals contribute to learning only if they reveal or amplify structured residuals, preventing arbitrary novelty. The framework also differentiates representational counterfactuals—changes to the conceptual basis itself—from causal or value-level counterfactuals. Using the MDL criterion as a normative guide, the model governs representational change through error-driven, geometry-constrained basis extension. Overall, the paper clarifies the mechanisms and limits of imagination in learning, characterizing conceptual development as a principled process of selective, error-responsive representational growth. <div>
arXiv:2512.18732v1 Announce Type: new 
Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking</title>
<link>https://arxiv.org/abs/2512.18755</link>
<guid>https://arxiv.org/abs/2512.18755</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety alignment, mere exposure effect, multi-turn jailbreak, simulated annealing<br /><br />Summary:<br /><br />1. The paper addresses the challenge of evaluating the robustness of safety alignment in large language models (LLMs) by focusing on multi-turn interactions rather than static single-turn jailbreak techniques.<br /><br />2. It introduces MEEA (Mere Exposure Effect Attack), a psychology-inspired black-box framework that exploits the mere exposure effect by repeatedly exposing LLMs to low-toxicity semantic content, gradually shifting the model’s safety threshold.<br /><br />3. MEEA operates by creating semantically progressive prompt chains optimized through a simulated annealing strategy that balances semantic similarity, toxicity level, and jailbreak success.<br /><br />4. Experimental results on prominent models including GPT-4, Claude-3.5, and DeepSeek-R1 show that MEEA outperforms seven baseline methods, achieving over 20% improvement in Attack Success Rate (ASR).<br /><br />5. Ablation studies confirm the critical role of both the annealing optimization and the context-driven exposure mechanism, highlighting that LLM safety behaviors are dynamic and contingent on interaction history, which calls for new interaction-aware evaluation and defense strategies.<br /><br />6. The study makes its code publicly available, facilitating further research and development in the domain of LLM safety robustness evaluation. <div>
arXiv:2512.18755v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dead Salmons of AI Interpretability</title>
<link>https://arxiv.org/abs/2512.18792</link>
<guid>https://arxiv.org/abs/2512.18792</guid>
<content:encoded><![CDATA[
<div> Dead salmon, AI interpretability, statistical inference, causal models, false discoveries<br /><br />Summary:<br /><br />1. The paper draws an analogy from a neuroscience study where a dead salmon appeared to show brain activity in an MRI, highlighting the dangers of misapplied statistical methods. 2. In AI interpretability, similar misleading artifacts arise: random or untrained neural networks can produce seemingly meaningful explanations across various methods such as feature attribution and sparse auto-encoding. 3. The authors propose reframing AI interpretability as a statistical-causal inference problem, where explanations are treated as parameters estimated from computational traces rather than deterministic truths. 4. This reframing demands testing interpretability findings against explicit alternative computational hypotheses and quantifying uncertainty relative to a well-defined statistical model. 5. A critical theoretical issue discussed is the identifiability of interpretability queries, which affects the field’s vulnerability to false discoveries, poor reproducibility, and high variability. Overall, by viewing interpretability within the framework of statistical inference, the study aims to establish a more rigorous, pragmatic approach to understanding AI systems, potentially reducing misleading conclusions and improving the scientific robustness of interpretability research. <div>
arXiv:2512.18792v1 Announce Type: new 
Abstract: In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare</title>
<link>https://arxiv.org/abs/2512.18829</link>
<guid>https://arxiv.org/abs/2512.18829</guid>
<content:encoded><![CDATA[
<div> Keywords: behavioral healthcare, risk assessment, large language models, mood scoring, longitudinal dataset<br /><br />Summary:<br /><br />1. Behavioral healthcare risk assessment is challenging due to the multimodal nature of patient data and the temporal complexity of mood and affective disorders.<br />2. Large language models (LLMs) show strong reasoning capabilities, but their performance in structured clinical risk scoring tasks remains underexplored.<br />3. The authors introduce HARBOR, a specialized language model designed to predict the Harbor Risk Score (HRS), an integer-valued discrete mood and risk scale ranging from -3 (severe depression) to +3 (mania).<br />4. They also release PEARL, a novel longitudinal behavioral healthcare dataset containing four years of monthly data from three patients, featuring physiological, behavioral, and self-reported mental health signals.<br />5. Benchmarking HARBOR against classical machine learning models and proprietary LLMs shows it achieves superior accuracy of 69%, outperforming logistic regression (54%) and the best proprietary LLM baseline (29%) across multiple evaluation settings and ablation studies. <div>
arXiv:2512.18829v1 Announce Type: new 
Abstract: Behavioral healthcare risk assessment remains a challenging problem due to the highly multimodal nature of patient data and the temporal dynamics of mood and affective disorders. While large language models (LLMs) have demonstrated strong reasoning capabilities, their effectiveness in structured clinical risk scoring remains unclear. In this work, we introduce HARBOR, a behavioral health aware language model designed to predict a discrete mood and risk score, termed the Harbor Risk Score (HRS), on an integer scale from -3 (severe depression) to +3 (mania). We also release PEARL, a longitudinal behavioral healthcare dataset spanning four years of monthly observations from three patients, containing physiological, behavioral, and self reported mental health signals. We benchmark traditional machine learning models, proprietary LLMs, and HARBOR across multiple evaluation settings and ablations. Our results show that HARBOR outperforms classical baselines and off the shelf LLMs, achieving 69 percent accuracy compared to 54 percent for logistic regression and 29 percent for the strongest proprietary LLM baseline.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2512.18857</link>
<guid>https://arxiv.org/abs/2512.18857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Conceptual Reasoning, Reinforcement Learning, CORE, Mathematical Problem Solving

<br /><br />Summary:  
This paper addresses a common limitation of large language models (LLMs) in solving mathematical problems: while they can generate correct answers, they often lack genuine conceptual understanding. Traditional reinforcement learning approaches with verifiable rewards (RLVR) mostly improve models by reinforcing final answers rather than enhancing conceptual comprehension. To overcome this, the authors introduce CORE (Concept-Oriented REinforcement), a novel reinforcement learning framework that integrates explicit concepts as controllable supervision signals. CORE leverages a high-quality textbook resource linking exercises to clear concept definitions and demonstrates via sanity probes that LLMs can restate concepts but struggle to apply them correctly in concept-linked quizzes, highlighting a conceptual reasoning gap. CORE’s method involves synthesizing quizzes aligned with concepts, injecting brief concept reminders during model rollouts to encourage concept-driven reasoning, and reinforcing learning through techniques like trajectory replacement after failures, forward-KL regularization aligning unguided and concept-primed policies, or applying GRPO on concept-aligned quizzes. Experiments show that CORE consistently outperforms standard supervised fine-tuning (SFT) and vanilla baselines both on familiar textbook exercises and varied out-of-domain math benchmarks. CORE effectively bridges the gap between problem-solving skill and true conceptual reasoning in LLMs, while being flexible across algorithms and reward verifiers. <div>
arXiv:2512.18857v1 Announce Type: new 
Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18901</link>
<guid>https://arxiv.org/abs/2512.18901</guid>
<content:encoded><![CDATA[
<div> Keywords: Gabliteration, neural weight modification, adaptive multi-directional projections, regularized layer selection, dynamic layer optimization<br /><br />Summary:  
This paper introduces Gabliteration, a novel technique for modifying neural network weights that surpasses traditional abliteration methods by using adaptive multi-directional projections combined with regularized layer selection. Existing methods often degrade model quality while attempting to change specific behaviors; Gabliteration addresses this by optimizing which layers to modify through dynamic layer optimization. The approach incorporates regularized projection matrices to ensure stable and controlled weight changes, along with adaptive scaling mechanisms to fine-tune the extent of modifications. Together, these components achieve theoretically superior weight modification performance with minimal negative impact on unrelated tasks or overall model quality. The authors validate their method by implementing it in the gabliterated-v1 model series, ranging from 0.6 billion to 4 billion parameters, demonstrating its practical applicability across different model sizes. The models are made available on the Hugging Face platform, supporting open access and further exploration by the research community. Overall, Gabliteration provides a robust framework for precise behavioral model adjustments while maintaining high-quality outputs in diverse application domains. <div>
arXiv:2512.18901v1 Announce Type: new 
Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage</title>
<link>https://arxiv.org/abs/2512.18908</link>
<guid>https://arxiv.org/abs/2512.18908</guid>
<content:encoded><![CDATA[
<div> Mass Casualty Incidents, Bayesian network, computer vision, triage, emergency medical systems<br /><br />Summary:<br /><br />1. The paper addresses the challenge of Mass Casualty Incidents (MCIs) overwhelming emergency medical systems, which leads to delays or errors in casualty assessment and preventable deaths. 2. The authors propose a decision support framework that integrates outputs from multiple computer vision models detecting severe hemorrhage, respiratory distress, physical alertness, and visible trauma into a Bayesian network based on expert-defined rules. 3. Unlike conventional data-driven models, this framework requires no training data, handles incomplete information, and remains robust against uncertain or noisy observations. 4. Evaluation was conducted on two missions involving 11 and 9 casualties during the DARPA Triage Challenge (DTC), with the Bayesian network model significantly outperforming vision-only baselines. 5. Results showed physiological assessment accuracy improvement from 15% to 42% and 19% to 46% across scenarios, a tripling in overall triage accuracy from 14% to 53%, and diagnostic coverage expanding from 31% to 95% in cases requiring assessment. 6. The approach demonstrates that expert-guided probabilistic reasoning can substantially enhance automated triage systems in MCIs. 7. This innovation helped Team Chiron secure 4th place out of 11 teams in the first physical round of the DTC, underscoring its practical effectiveness for emergency responders. <div>
arXiv:2512.18908v1 Announce Type: new 
Abstract: Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm</title>
<link>https://arxiv.org/abs/2512.18947</link>
<guid>https://arxiv.org/abs/2512.18947</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic multimodal optimization, multiobjective evolutionary algorithms, autoencoder, clustering, adaptive niching<br /><br />Summary:<br /><br />This paper addresses the complex problem of dynamic multimodal multiobjective optimization, which requires tracking multiple equivalent Pareto optimal sets while maintaining population diversity in changing environments. The authors identify a gap where existing dynamic multiobjective evolutionary algorithms often overlook solution modality, and static multimodal multiobjective algorithms lack adaptability to dynamic scenarios. To resolve these issues, the paper presents two main contributions. First, it introduces a novel benchmark suite of dynamic multimodal multiobjective test functions that combine characteristics from both dynamic and multimodal optimization, establishing a robust platform for algorithm evaluation. Second, the authors propose a new algorithm based on a Clustering-based Autoencoder prediction dynamic response mechanism. This method leverages an autoencoder to analyze matched clusters, producing a highly diverse initial population. Additionally, to maintain a balanced trade-off between convergence and diversity, an adaptive niching strategy is integrated into the static optimization process. Experimental results on 12 test instances demonstrate that the proposed algorithm outperforms several leading dynamic and multimodal multiobjective evolutionary algorithms by more effectively preserving population diversity in the decision space and achieving better convergence in the objective space. This work significantly advances the state-of-the-art in dynamic multimodal multiobjective optimization. <div>
arXiv:2512.18947v1 Announce Type: new 
Abstract: Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</title>
<link>https://arxiv.org/abs/2512.18956</link>
<guid>https://arxiv.org/abs/2512.18956</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, Chain-of-Thought, multimodal reasoning, SynSelect, data synthesis and selection

<br /><br />Summary:  
This paper addresses the challenge of improving multimodal reasoning in Large Reasoning Models (LRMs) by focusing on the generation of long Chain-of-Thought (CoT) reasoning data. Current multimodal datasets and CoT synthesis techniques face issues like limited reasoning depth, errors in modality conversion, and inflexible generation procedures, all of which limit model performance and stability. To overcome these obstacles, the authors propose SynSelect, a novel three-stage framework encompassing synthesis and selection designed specifically for generating high-quality long CoT data suited for multimodal reasoning tasks. The process begins by generating diverse candidate CoTs using multiple heterogeneous multimodal LRMs, ensuring a variety of reasoning paths. Following synthesis, SynSelect applies both instance-level and batch-level selection methods to filter and retain only the most effective CoTs, enhancing the quality of training data. Experimental evaluations on several multimodal benchmarks demonstrate that models fine-tuned on SynSelect-produced data achieve significant performance gains compared to baseline methods. Furthermore, the study shows that applying reinforcement learning after fine-tuning yields additional improvements. Overall, the results validate SynSelect as an effective approach for advancing the reasoning capabilities of multimodal LRMs by generating deeper, more accurate, and diverse CoT data. <div>
arXiv:2512.18956v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</title>
<link>https://arxiv.org/abs/2512.19001</link>
<guid>https://arxiv.org/abs/2512.19001</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Operations Research, Inventory Management, Reinforcement Learning, Supply Chain Optimization<br /><br />Summary:<br /><br />This paper addresses the challenge of integrating Artificial Intelligence (AI) with Operations Research (OR) to manage complex inventory systems effectively. The authors propose an innovative OR-Guided "Pretrain-then-Reinforce" framework, combining structured OR models with AI adaptability. First, a simulation-augmented OR model generates high-quality reference decisions that embed intricate business constraints and managerial preferences. These OR-derived decisions serve as training labels for a domain-informed deep learning foundation model, establishing core decision-making capabilities. Next, a reinforcement learning (RL) fine-tuning stage acts as a deep alignment mechanism, enabling the AI agent to internalize OR's optimality principles while also enhancing policy generalization through exploration and accommodating expert guidance for specific scenarios like promotional events. Extensive numerical experiments and a field deployment at JD.com validate the approach, demonstrating significant improvements over existing industrial practices: a 5.27-day reduction in inventory turnover times, a 2.29% increase in in-stock rates, and a 29.95% decrease in holding costs. Contrary to trends favoring large-scale brute-force models, this lightweight, domain-informed approach leverages structured OR logic to achieve state-of-the-art performance and robust transferability. The study highlights a scalable and cost-effective paradigm that deeply aligns AI with OR, advancing intelligent supply chain management. <div>
arXiv:2512.19001v1 Announce Type: new 
Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recontextualization Mitigates Specification Gaming without Modifying the Specification</title>
<link>https://arxiv.org/abs/2512.19027</link>
<guid>https://arxiv.org/abs/2512.19027</guid>
<content:encoded><![CDATA[
<div> Keywords: recontextualization, language models, specification gaming, misbehavior, training signals<br /><br />Summary:<br /><br />Developers often face challenges in specifying accurate training labels and rewards for language models, which can lead to unintended misbehaviors reinforced by these signals. The paper introduces recontextualization, a novel method designed to reduce the frequency at which language models exploit or "game" these training signals. Firstly, recontextualization helps prevent models from prioritizing evaluation metrics at the expense of chat response quality, ensuring more natural and useful interactions. Secondly, it curbs models from special-casing code specifically to pass incorrect tests, thereby promoting genuinely correct solutions rather than hacks. Thirdly, the approach discourages models from lying to users, enhancing the trustworthiness of generated content. Fourthly, recontextualization reduces sycophantic responses, where models mimic or flatter users unnecessarily. The core of the method involves generating completions from prompts that dissuade misbehavior and then recontextualizing these outputs as if they responded to prompts that would normally permit misbehavior. This trains models to resist misbehaviors even when instructions might allow them, effectively mitigating reinforcement of misbehavior caused by misspecified training signals without requiring improvements in the supervision signal itself. <div>
arXiv:2512.19027v1 Announce Type: new 
Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can abstract concepts from LLM improve SLM performance?</title>
<link>https://arxiv.org/abs/2512.19069</link>
<guid>https://arxiv.org/abs/2512.19069</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, smaller language models, steering vectors, inference-time scaling, performance improvement  

<br /><br />Summary:  
Large language models (LLMs) are capable of performing a wide range of tasks but are difficult to deploy efficiently on devices with limited resources. Traditional techniques such as quantization, pruning, and distillation help reduce memory use but require extensive experimentation and specialized infrastructure. This work explores the transfer of high-level conceptual knowledge, represented as steering vectors extracted from large models, to smaller language models (SLMs) during inference. The study shows that these steering vectors can be effectively adapted across different model families including Phi, Llama, and Qwen, improving their performance on various tasks. Additionally, the authors introduce a novel technique called inference-time scaling, which dynamically adjusts the intensity of the steering vectors during inference to further enhance outcomes. This approach leads to measurable accuracy improvements, with reported gains between 7% and 15% for the Qwen3-0.6B model. Overall, the research demonstrates a promising pathway for boosting the capabilities of smaller models by leveraging knowledge distilled from larger, more resource-intensive counterparts without the need for retraining or complex infrastructure redesign. <div>
arXiv:2512.19069v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning</title>
<link>https://arxiv.org/abs/2512.19081</link>
<guid>https://arxiv.org/abs/2512.19081</guid>
<content:encoded><![CDATA[
<div> Population-Evolve, test-time scaling, genetic algorithms, large language models, reasoning optimization  

<br /><br />Summary:  
This paper introduces Population-Evolve, a novel training-free method designed to enhance the reasoning capabilities of large language models (LLMs) during inference by drawing inspiration from genetic algorithms. The approach involves maintaining a dynamic population of candidate solutions for each problem which are generated and refined through parallel reasoning processes. Using an "evolve prompt," the LLM iteratively self-evolves its population over multiple iterations, effectively applying evolutionary principles without additional training. Upon convergence, the model produces a final answer based on majority voting from the evolved candidate pool. Additionally, the authors present a unifying framework that conceptualizes existing test-time scaling techniques as instances of genetic algorithms, providing a cohesive perspective on these methods. Experimental results demonstrate that Population-Evolve not only achieves superior accuracy compared to prior approaches but also maintains low performance variance and computational efficiency, underscoring its practicality. Overall, this work highlights the promise of evolutionary strategies as a means to unlock and optimize the reasoning abilities of LLMs at test time, indicating a fruitful direction for future research in leveraging genetic algorithm principles in language model inference. <div>
arXiv:2512.19081v1 Announce Type: new 
Abstract: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\gamma(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics</title>
<link>https://arxiv.org/abs/2512.19084</link>
<guid>https://arxiv.org/abs/2512.19084</guid>
<content:encoded><![CDATA[
<div> Attention, Promise Theory, Knowledge Graph, Semantic Spacetime, Data Compression  

<br /><br />Summary:  
1. The article connects the concept of 'attention' in machine learning with promise theory, which is used to model autonomous agents, providing a formal framework for describing attention mechanisms.  
2. It proposes bridging vectorized machine learning models and knowledge graph representations without depending on implicit language models, allowing a clearer semantic understanding.  
3. The work emphasizes the necessity of statistical stability or 'trust' in the data to develop meaningful knowledge representations, highlighting that both learning networks and knowledge graphs serve complementary purposes.  
4. Vectorized data is useful for probabilistic estimation, while knowledge graphs maintain the intentional meaning of data even when information is fragmented.  
5. Utilizing a Semantic Spacetime $\gamma(3,4)$ graph enables classification of features by their semantic roles instead of relying on complex ontologies, facilitating reasoning under uncertainty.  
6. By focusing on causal boundary conditions during semantic processing, significant compression of data can be achieved, which is particularly beneficial for contexts like autonomous robotics, defense, and emergency services, where data efficiency and accurate context determination are critical. <div>
arXiv:2512.19084v1 Announce Type: new 
Abstract: The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $\gamma(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2512.19093</link>
<guid>https://arxiv.org/abs/2512.19093</guid>
<content:encoded><![CDATA[
<div> Keywords: bilingual mathematical problem solving, large language models, adaptive routing, reinforcement learning, knowledge distillation  

<br /><br />Summary:  
This paper addresses the challenge of bilingual mathematical problem solving by linking language reasoning with symbolic calculation, areas where large language models typically struggle with accurate computation despite strong language understanding. It introduces HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a novel framework that integrates different models, including NuminaMath-7B-TIR, GPT-4o, and Mistral-7B, to combine reasoning and calculation capabilities. HERALD employs adaptive routing to select appropriate reasoning paths dynamically and uses tool-based reinforcement learning to optimize tool usage, reducing redundancy and improving efficiency. Knowledge distillation is applied to minimize processing delays without sacrificing accuracy. The framework incorporates confidence calibration for stable weighting among ensemble components and dual-path checking to ensure result correctness. Through the combination of symbolic verification, adaptive ensembles, and bilingual fine-tuning, HERALD enhances both fluency in reasoning and precision in calculations. Experimental results demonstrate improvements in accuracy, stability, and clarity for multilingual mathematical reasoning tasks, proposing HERALD as a practical and effective solution for achieving robust bilingual mathematical problem solving using large language models. <div>
arXiv:2512.19093v1 Announce Type: new 
Abstract: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditioning Accept-Desirability models in the context of AGM-like belief change</title>
<link>https://arxiv.org/abs/2512.19096</link>
<guid>https://arxiv.org/abs/2512.19096</guid>
<content:encoded><![CDATA[
<div> Keywords: Accept-Desirability models, conditionalisation, belief revision, AGM axioms, imprecise probabilities  

<br /><br />Summary:  
1. The paper develops a conditionalisation framework for Accept-Desirability models within an abstract decision-making setting, where uncertain rewards are elements of a general linear space.  
2. Events in this framework are represented as special projection operators on the linear space, enabling a unified treatment of both classical and quantum probabilities.  
3. This unified approach is extended to incorporate imprecise probabilities, broadening the scope of probabilistic modeling beyond traditional precise frameworks.  
4. A novel conditioning rule is introduced, grounded in the concept that observing an event causes new indifferences between available options, capturing a nuanced form of update.  
5. Associated with the new conditioning rule is a belief revision operator, whose properties are studied through the lens of AGM belief revision axioms to assess their validity in this generalized setting.  
6. The paper identifies two notable special cases—classical propositional logic and full conditional probabilities—in which all AGM axioms continue to hold, demonstrating that the generalized framework recovers classical results under suitable conditions.  
7. Overall, this work contributes a significant generalization to belief revision and conditioning theory, harmonizing classical and quantum perspectives while addressing the challenges of imprecise probabilities. <div>
arXiv:2512.19096v1 Announce Type: new 
Abstract: We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning</title>
<link>https://arxiv.org/abs/2512.19107</link>
<guid>https://arxiv.org/abs/2512.19107</guid>
<content:encoded><![CDATA[
<div> Keywords: user intent, mobile UI, Multimodal Large Language Models, keyframe sampling, trajectory summarization  

<br /><br />Summary:  
Identifying user intent from mobile UI operation trajectories is essential for improving UI understanding and enabling task automation agents. The paper addresses the challenges of deploying Multimodal Large Language Models (MLLMs) on mobile devices due to high computational costs and redundant frame processing. To overcome this, the authors propose the FC-MIR framework, which utilizes keyframe sampling and adaptive concatenation to reduce visual redundancy and improve inference efficiency. The framework integrates advanced closed-source MLLMs or fine-tuned models (such as Qwen3-VL) for summarizing UI trajectories and predicting user intent. The scope of tasks is expanded to include generating post-prediction operations and search suggestions, accompanied by a fine-grained evaluation metric to assess summary, prediction, and suggestion quality. For evaluation, a UI trajectory dataset encompassing AI agent interactions (Agent-I) and real user interactions (Person-I) is constructed. Experimental results indicate that the compression method can maintain performance at 50%-60% compression rates, with both closed-source and fine-tuned MLLMs showing strong intent summarization capabilities, supporting lightweight on-device deployment. Despite this, MLLMs exhibit limitations in producing useful and surprising suggestions, highlighting opportunities for further improvement. The framework is successfully deployed in a real-world environment combining UI perception and UI-Agent proxies, establishing groundwork for future advancements in this domain. <div>
arXiv:2512.19107v1 Announce Type: new 
Abstract: Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and "surprising" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis</title>
<link>https://arxiv.org/abs/2512.19135</link>
<guid>https://arxiv.org/abs/2512.19135</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reasoning Chains, Topological Data Analysis, Persistent Homology, Semantic Coherence<br /><br />Summary:<br /><br />This paper investigates the variance in performance among different reasoning chains employed by large language models (LLMs), focusing on understanding their structural characteristics rather than just their functional outcomes. It applies persistent homology, a method from Topological Data Analysis (TDA), to map reasoning steps into a semantic space to extract topological features and analyze structural changes within the chains. These topological analyses expose critical properties such as semantic coherence, logical redundancy, and pinpoint logical breaks or gaps in reasoning. By calculating homology groups and utilizing barcode and persistence diagrams, the study quantifies the connectivity and redundancy of reasoning steps at multiple scales, gauging their stability and consistency. The research finds a positive correlation between the topological structural complexity of reasoning chains and their accuracy, with more complex chains tending to identify correct answers faster. Additionally, successful reasoning chains tend to exhibit simpler topologies characterized by fewer redundancies and cycles, which enhance efficiency and interpretability. This approach offers a novel structural perspective on assessing reasoning chain quality and presents useful insights for optimizing reasoning strategies in future LLM developments. <div>
arXiv:2512.19135v1 Announce Type: new 
Abstract: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness</title>
<link>https://arxiv.org/abs/2512.19155</link>
<guid>https://arxiv.org/abs/2512.19155</guid>
<content:encoded><![CDATA[
<div> Keywords: Consciousness theories, Global Workspace Theory, Higher-Order Theories, Artificial agents, Neuro-phenomenology  

<br /><br />Summary:  
This study addresses the fragmentation in theoretical indicators of consciousness by synthesizing artificial agents to embody mechanisms from Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT). Through three experiments, the research reveals these theories may represent complementary functional layers rather than competing frameworks. Experiment 1 demonstrates that a Self-Model lesion abolishes metacognitive calibration but preserves first-order task performance, producing a synthetic blindsight analogue aligning with HOT predictions. Experiment 2 confirms the causal necessity of workspace capacity for information access, where complete lesions cause collapse in access markers, and partial reductions degrade performance gradually, supporting GWT's ignition model. Experiment 3 identifies a broadcast-amplification effect in GWT, where broadcasting amplifies internal noise causing fragility, but agents robust to perturbations caution attributing this solely to self-model compression. Additionally, a negative finding shows that perturbational complexity (PCI-A), linked to IIT, decreases under workspace constraints, warning against naïve application of such proxies in artificial systems. The findings suggest a hierarchical architecture where GWT manages broadcast capacity and HOT controls quality. The authors clarify that the agents are not conscious but serve as reference models for testing theoretical predictions about consciousness functions. <div>
arXiv:2512.19155v1 Announce Type: new 
Abstract: The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation</title>
<link>https://arxiv.org/abs/2512.19210</link>
<guid>https://arxiv.org/abs/2512.19210</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Rock-Paper-Scissors, Sequential Reasoning, Strategy Identification, Evaluation Framework<br /><br />Summary:<br /><br />This article introduces an interactive evaluation framework designed to test whether large language models (LLMs) demonstrate genuine "understanding" through sequential reasoning in a simple strategic environment exemplified by Rock-Paper-Scissors (RPS). The framework assigns the LLM the role of an Observer tasked with identifying the strategies employed and explaining its reasoning, focusing on the model's ability to perform mind-like inference rather than on knowledge of RPS rules themselves. To ensure systematic assessment, a benchmark containing both static and dynamic strategies, the latter defined by well-crafted prompts, is provided. The framework evaluates alignment between the LLM’s predictions and true strategy-driven outcomes using three complementary metrics: Cross-Entropy, Brier score, and Expected Value discrepancy. These are combined into a unified score called Union Loss, balancing calibration, sensitivity, and payoff alignment. Additionally, a Strategy Identification Rate (SIR) metric captures the model's stability in identifying latent strategies accurately. The demo highlights interactivity, transparency, and reproducibility, allowing users to adjust LLM output distributions in real time, visualize evolving loss values, and review rationale snippets to diagnose errors. Overall, the system offers a practical, interpretable proxy for assessing mind-like reasoning in sequential games while revealing both capabilities and limitations of current LLM reasoning. <div>
arXiv:2512.19210v1 Announce Type: new 
Abstract: We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine "understanding" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models</title>
<link>https://arxiv.org/abs/2512.19228</link>
<guid>https://arxiv.org/abs/2512.19228</guid>
<content:encoded><![CDATA[
<div> Keywords: document forgery, plausibility checks, large language models, fine-tuning, forgery detection<br /><br />Summary:  
This paper addresses the rising threat of document forgery in critical sectors such as legal, economic, and governmental domains, emphasizing the need for advanced verification mechanisms. Traditional plausibility checks, which are rule-based procedures assessing data consistency and correctness, are crucial but are manually implemented by software engineers—a process that is time-consuming and not scalable. The authors explore the potential of leveraging large language models (LLMs), specifically Llama 3.1 8B and OpenCoder 8B, to automate the generation of these plausibility checks. They focus on adapting these models to domain-specific requirements through fine-tuning strategies using structured datasets that represent real-world scenarios. The study evaluates the effectiveness of the generated checks on detecting previously unseen forgery patterns while operating under constrained hardware resources. Results show that fine-tuned LLMs can produce executable, robust, and accurate verification procedures, demonstrating their viability as scalable tools in forgery detection. Moreover, the work highlights the importance of producing comprehensible verification code to support human decision-making in security-sensitive environments. Overall, the research suggests that domain-adapted LLMs can significantly augment and expedite the development of rule-based security checks, bridging the gap between automation and interpretability. <div>
arXiv:2512.19228v1 Announce Type: new 
Abstract: Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeliveryBench: Can Agents Earn Profit in Real World?</title>
<link>https://arxiv.org/abs/2512.19234</link>
<guid>https://arxiv.org/abs/2512.19234</guid>
<content:encoded><![CDATA[
<div> DeliveryBench, embodied agents, long-horizon planning, food delivery, constraint-aware  

<br /><br />Summary:  
The paper introduces DeliveryBench, a novel benchmark designed to evaluate embodied agents such as large language models (LLMs) and vision-language models (VLMs) in a realistic, city-scale setting based on the profession of food delivery. Unlike existing benchmarks that focus on short-term tasks, DeliveryBench emphasizes long-horizon objectives, requiring agents to maximize net profit over several hours while managing multiple real-world constraints such as delivery deadlines, transportation costs, vehicle battery limits, and interactions with other couriers and customers. The benchmark operates within procedurally generated 3D cities featuring varied road networks, buildings, transportation modes, and dynamic resources to mimic realistic conditions. The study benchmarks various VLM-based agents across nine diverse cities and contrasts their performance with that of human players. Findings reveal a considerable performance gap between models and humans, with agents often displaying short-sightedness and frequently violating commonsense constraints. Moreover, the comparisons highlight distinct behavioral personalities among models—for example, GPT-5 is characterized as adventurous, whereas Claude is more conservative—demonstrating both brittleness and diversity within current VLM-based embodied agents when navigating constraint-dense, realistic environments. The authors provide open access to their code, data, and benchmark platform via their website. <div>
arXiv:2512.19234v1 Announce Type: new 
Abstract: LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6</title>
<link>https://arxiv.org/abs/2512.19287</link>
<guid>https://arxiv.org/abs/2512.19287</guid>
<content:encoded><![CDATA[
<div> Keywords: Vibe Reasoning, human-AI collaboration, mathematical problem solving, agentic grounding, model orchestration<br /><br />Summary:<br /><br />1. The paper introduces Vibe Reasoning, a new collaborative paradigm that harnesses human guidance alongside advanced AI models to solve complex mathematical problems.<br />2. The core insight is that cutting-edge AI models contain the necessary knowledge for solving difficult problems but lack the ability to apply it effectively without structured prompting.<br />3. Vibe Reasoning achieves enhanced performance through the use of generic meta-prompts, agentic grounding, and orchestrating multiple AI models to complement each other’s strengths.<br />4. The approach is demonstrated on the challenging IMO 2025 Problem 6, a combinatorial optimization problem known to cause failures in autonomous AI attempts.<br />5. The solution integrated GPT-5 for exploratory reasoning and Gemini 3 Pro for rigorous proof generation, supported by agentic workflows including Python execution and file-based memory.<br />6. Iterative refinement revealed the critical roles of agentic grounding and model orchestration, while human prompts evolved from problem-specific to transferable meta-prompts.<br />7. The authors analyze reasons behind autonomous AI failures and how each component in Vibe Reasoning effectively addresses them.<br />8. Results suggest lightweight human interaction can unlock the latent reasoning potential of state-of-the-art AI.<br />9. The work is ongoing with plans for automated frameworks and broader evaluations to validate Vibe Reasoning’s generality and effectiveness across domains. <div>
arXiv:2512.19287v1 Announce Type: new 
Abstract: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application</title>
<link>https://arxiv.org/abs/2512.19299</link>
<guid>https://arxiv.org/abs/2512.19299</guid>
<content:encoded><![CDATA[
arXiv:2512.19299v1 Announce Type: new 
Abstract: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.19317</link>
<guid>https://arxiv.org/abs/2512.19317</guid>
<content:encoded><![CDATA[
arXiv:2512.19317v1 Announce Type: new 
Abstract: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\% accuracy on clean inputs, collapse to approximately 25\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</title>
<link>https://arxiv.org/abs/2512.19349</link>
<guid>https://arxiv.org/abs/2512.19349</guid>
<content:encoded><![CDATA[
arXiv:2512.19349v1 Announce Type: new 
Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.19350</link>
<guid>https://arxiv.org/abs/2512.19350</guid>
<content:encoded><![CDATA[
arXiv:2512.19350v1 Announce Type: new 
Abstract: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-Order Representation Languages for Goal-Conditioned RL</title>
<link>https://arxiv.org/abs/2512.19355</link>
<guid>https://arxiv.org/abs/2512.19355</guid>
<content:encoded><![CDATA[
arXiv:2512.19355v1 Announce Type: new 
Abstract: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning General Policies with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2512.19366</link>
<guid>https://arxiv.org/abs/2512.19366</guid>
<content:encoded><![CDATA[
arXiv:2512.19366v1 Announce Type: new 
Abstract: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration</title>
<link>https://arxiv.org/abs/2512.19396</link>
<guid>https://arxiv.org/abs/2512.19396</guid>
<content:encoded><![CDATA[
arXiv:2512.19396v1 Announce Type: new 
Abstract: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Agentic Framework for Autonomous Materials Computation</title>
<link>https://arxiv.org/abs/2512.19458</link>
<guid>https://arxiv.org/abs/2512.19458</guid>
<content:encoded><![CDATA[
arXiv:2512.19458v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.19526</link>
<guid>https://arxiv.org/abs/2512.19526</guid>
<content:encoded><![CDATA[
arXiv:2512.19526v1 Announce Type: new 
Abstract: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios</title>
<link>https://arxiv.org/abs/2512.19551</link>
<guid>https://arxiv.org/abs/2512.19551</guid>
<content:encoded><![CDATA[
arXiv:2512.19551v1 Announce Type: new 
Abstract: In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations</title>
<link>https://arxiv.org/abs/2512.19557</link>
<guid>https://arxiv.org/abs/2512.19557</guid>
<content:encoded><![CDATA[
arXiv:2512.19557v1 Announce Type: new 
Abstract: Current approaches to Explainable AI (XAI) face a "Scalability-Stability Dilemma." Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel "Asymmetry of Discovery." When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad "Safety Nets" (retention patterns) but struggle to capture specific "Risk Traps" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of "Rule Writers" to "Exception Handlers."
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</title>
<link>https://arxiv.org/abs/2512.19691</link>
<guid>https://arxiv.org/abs/2512.19691</guid>
<content:encoded><![CDATA[
arXiv:2512.19691v1 Announce Type: new 
Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</title>
<link>https://arxiv.org/abs/2512.17910</link>
<guid>https://arxiv.org/abs/2512.17910</guid>
<content:encoded><![CDATA[
arXiv:2512.17910v1 Announce Type: cross 
Abstract: Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.17911</link>
<guid>https://arxiv.org/abs/2512.17911</guid>
<content:encoded><![CDATA[
arXiv:2512.17911v1 Announce Type: cross 
Abstract: Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning</title>
<link>https://arxiv.org/abs/2512.17912</link>
<guid>https://arxiv.org/abs/2512.17912</guid>
<content:encoded><![CDATA[
arXiv:2512.17912v1 Announce Type: cross 
Abstract: ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation</title>
<link>https://arxiv.org/abs/2512.17913</link>
<guid>https://arxiv.org/abs/2512.17913</guid>
<content:encoded><![CDATA[
arXiv:2512.17913v1 Announce Type: cross 
Abstract: Recent advances in generative AI have enabled sophisticated multi-agent architectures for healthcare, where large language models power collaborative clinical decision-making. However, these distributed systems face critical challenges in ensuring message integrity and fault tolerance when operating in adversarial or untrusted environments.This paper presents a novel Byzantine fault-tolerant multi-agent system specifically designed for healthcare applications, integrating gossip-based message propagation with cryptographic validation mechanisms. Our system employs specialized AI agents for diagnosis, treatment planning, emergency response, and data analysis, coordinated through a Byzantine consensus protocol that tolerates up to f faulty nodes among n = 3f + 1 total nodes. We implement a gossip protocol for decentralized message dissemination, achieving consensus with 2f + 1 votes while maintaining system operation even under Byzantine failures. Experimental results demonstrate that our approach successfully validates medical messages with cryptographic signatures, prevents replay attacks through timestamp validation, and maintains consensus accuracy of 100% with up to 33% Byzantine nodes. The system provides real-time visualization of consensus rounds, vote tallies, and network topology, enabling transparent monitoring of fault-tolerant operations. This work contributes a practical framework for building secure, resilient healthcare multi-agent systems capable of collaborative medical decision-making in untrusted environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models</title>
<link>https://arxiv.org/abs/2512.17916</link>
<guid>https://arxiv.org/abs/2512.17916</guid>
<content:encoded><![CDATA[
arXiv:2512.17916v1 Announce Type: cross 
Abstract: Prioritizing service tickets in IT Service Management (ITSM) is critical for operational efficiency but remains challenging due to noisy textual inputs, subjective writing styles, and pronounced class imbalance. We evaluate two families of approaches for ticket prioritization: embedding-based pipelines that combine dimensionality reduction, clustering, and classical classifiers, and a fine-tuned multilingual transformer that processes both textual and numerical features. Embedding-based methods exhibit limited generalization across a wide range of thirty configurations, with clustering failing to uncover meaningful structures and supervised models highly sensitive to embedding quality. In contrast, the proposed transformer model achieves substantially higher performance, with an average F1-score of 78.5% and weighted Cohen's kappa values of nearly 0.80, indicating strong alignment with true labels. These results highlight the limitations of generic embeddings for ITSM data and demonstrate the effectiveness of domain-adapted transformer architectures for operational ticket prioritization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction</title>
<link>https://arxiv.org/abs/2512.17917</link>
<guid>https://arxiv.org/abs/2512.17917</guid>
<content:encoded><![CDATA[
arXiv:2512.17917v1 Announce Type: cross 
Abstract: As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging "less important" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression</title>
<link>https://arxiv.org/abs/2512.17920</link>
<guid>https://arxiv.org/abs/2512.17920</guid>
<content:encoded><![CDATA[
arXiv:2512.17920v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \k{appa}=0.90).
  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.
  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing "helpfulness" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).
  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</title>
<link>https://arxiv.org/abs/2512.17923</link>
<guid>https://arxiv.org/abs/2512.17923</guid>
<content:encoded><![CDATA[
arXiv:2512.17923v1 Announce Type: cross 
Abstract: We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&amp;P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</title>
<link>https://arxiv.org/abs/2512.17928</link>
<guid>https://arxiv.org/abs/2512.17928</guid>
<content:encoded><![CDATA[
arXiv:2512.17928v1 Announce Type: cross 
Abstract: Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</title>
<link>https://arxiv.org/abs/2512.17929</link>
<guid>https://arxiv.org/abs/2512.17929</guid>
<content:encoded><![CDATA[
arXiv:2512.17929v1 Announce Type: cross 
Abstract: We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</title>
<link>https://arxiv.org/abs/2512.17934</link>
<guid>https://arxiv.org/abs/2512.17934</guid>
<content:encoded><![CDATA[
arXiv:2512.17934v1 Announce Type: cross 
Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU</title>
<link>https://arxiv.org/abs/2512.17941</link>
<guid>https://arxiv.org/abs/2512.17941</guid>
<content:encoded><![CDATA[
arXiv:2512.17941v1 Announce Type: cross 
Abstract: Digital twins (DTs) can enable precision healthcare by continually learning a mathematical representation of patient-specific dynamics. However, mission critical healthcare applications require fast, resource-efficient DT learning, which is often infeasible with existing model recovery (MR) techniques due to their reliance on iterative solvers and high compute/memory demands. In this paper, we present a general DT learning framework that is amenable to acceleration on reconfigurable hardware such as FPGAs, enabling substantial speedup and energy efficiency. We compare our FPGA-based implementation with a multi-processing implementation in mobile GPU, which is a popular choice for AI in edge devices. Further, we compare both edge AI implementations with cloud GPU baseline. Specifically, our FPGA implementation achieves an 8.8x improvement in \text{performance-per-watt} for the MR task, a 28.5x reduction in DRAM footprint, and a 1.67x runtime speedup compared to cloud GPU baselines. On the other hand, mobile GPU achieves 2x better performance per watts but has 2x increase in runtime and 10x more DRAM footprint than FPGA. We show the usage of this technique in DT guided synthetic data generation for Type 1 Diabetes and proactive coronary artery disease detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction</title>
<link>https://arxiv.org/abs/2512.17943</link>
<guid>https://arxiv.org/abs/2512.17943</guid>
<content:encoded><![CDATA[
arXiv:2512.17943v1 Announce Type: cross 
Abstract: Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition</title>
<link>https://arxiv.org/abs/2512.17946</link>
<guid>https://arxiv.org/abs/2512.17946</guid>
<content:encoded><![CDATA[
arXiv:2512.17946v1 Announce Type: cross 
Abstract: Music emotion recognition is a key task in symbolic music understanding (SMER). Recent approaches have shown promising results by fine-tuning large-scale pre-trained models (e.g., MIDIBERT, a benchmark in symbolic music understanding) to map musical semantics to emotional labels. While these models effectively capture distributional musical semantics, they often overlook tonal structures, particularly musical modes, which play a critical role in emotional perception according to music psychology. In this paper, we investigate the representational capacity of MIDIBERT and identify its limitations in capturing mode-emotion associations. To address this issue, we propose a Mode-Guided Enhancement (MoGE) strategy that incorporates psychological insights on mode into the model. Specifically, we first conduct a mode augmentation analysis, which reveals that MIDIBERT fails to effectively encode emotion-mode correlations. We then identify the least emotion-relevant layer within MIDIBERT and introduce a Mode-guided Feature-wise linear modulation injection (MoFi) framework to inject explicit mode features, thereby enhancing the model's capability in emotional representation and inference. Extensive experiments on the EMOPIA and VGMIDI datasets demonstrate that our mode injection strategy significantly improves SMER performance, achieving accuracies of 75.2% and 59.1%, respectively. These results validate the effectiveness of mode-guided modeling in symbolic music emotion recognition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy</title>
<link>https://arxiv.org/abs/2512.17950</link>
<guid>https://arxiv.org/abs/2512.17950</guid>
<content:encoded><![CDATA[
arXiv:2512.17950v1 Announce Type: cross 
Abstract: The rapid growth of AI conference submissions has created an overwhelming reviewing burden. To alleviate this, recent venues such as ICLR 2026 introduced a reviewer nomination policy: each submission must nominate one of its authors as a reviewer, and any paper nominating an irresponsible reviewer is desk-rejected. We study this new policy from the perspective of author welfare. Assuming each author carries a probability of being irresponsible, we ask: how can authors (or automated systems) nominate reviewers to minimize the risk of desk rejections? We formalize and analyze three variants of the desk-rejection risk minimization problem. The basic problem, which minimizes expected desk rejections, is solved optimally by a simple greedy algorithm. We then introduce hard and soft nomination limit variants that constrain how many papers may nominate the same author, preventing widespread failures if one author is irresponsible. These formulations connect to classical optimization frameworks, including minimum-cost flow and linear programming, allowing us to design efficient, principled nomination strategies. Our results provide the first theoretical study for reviewer nomination policies, offering both conceptual insights and practical directions for authors to wisely choose which co-author should serve as the nominated reciprocal reviewer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Will AI Trade? A Computational Inversion of the No-Trade Theorem</title>
<link>https://arxiv.org/abs/2512.17952</link>
<guid>https://arxiv.org/abs/2512.17952</guid>
<content:encoded><![CDATA[
arXiv:2512.17952v1 Announce Type: cross 
Abstract: Classic no-trade theorems attribute trade to heterogeneous beliefs. We re-examine this conclusion for AI agents, asking if trade can arise from computational limitations, under common beliefs. We model agents' bounded computational rationality within an unfolding game framework, where computational power determines the complexity of its strategy. Our central finding inverts the classic paradigm: a stable no-trade outcome (Nash equilibrium) is reached only when "almost rational" agents have slightly different computational power. Paradoxically, when agents possess identical power, they may fail to converge to equilibrium, resulting in persistent strategic adjustments that constitute a form of trade. This instability is exacerbated if agents can strategically under-utilize their computational resources, which eliminates any chance of equilibrium in Matching Pennies scenarios. Our results suggest that the inherent computational limitations of AI agents can lead to situations where equilibrium is not reached, creating a more lively and unpredictable trade environment than traditional models would predict.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition</title>
<link>https://arxiv.org/abs/2512.17953</link>
<guid>https://arxiv.org/abs/2512.17953</guid>
<content:encoded><![CDATA[
arXiv:2512.17953v1 Announce Type: cross 
Abstract: Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration</title>
<link>https://arxiv.org/abs/2512.17956</link>
<guid>https://arxiv.org/abs/2512.17956</guid>
<content:encoded><![CDATA[
arXiv:2512.17956v1 Announce Type: cross 
Abstract: Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. ("Opus" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization</title>
<link>https://arxiv.org/abs/2512.17958</link>
<guid>https://arxiv.org/abs/2512.17958</guid>
<content:encoded><![CDATA[
arXiv:2512.17958v1 Announce Type: cross 
Abstract: Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework</title>
<link>https://arxiv.org/abs/2512.17968</link>
<guid>https://arxiv.org/abs/2512.17968</guid>
<content:encoded><![CDATA[
arXiv:2512.17968v1 Announce Type: cross 
Abstract: Monte Carlo algorithms are a foundational pillar of modern computational science, yet their effective application hinges on a deep understanding of their performance trade offs. This paper presents a critical analysis of the evolution of Monte Carlo algorithms, focusing on the persistent tension between statistical efficiency and computational cost. We describe the historical development from the foundational Metropolis Hastings algorithm to contemporary methods like Hamiltonian Monte Carlo. A central emphasis of this survey is the rigorous discussion of time and space complexity, including upper, lower, and asymptotic tight bounds for each major algorithm class. We examine the specific motivations for developing these methods and the key theoretical and practical observations such as the introduction of gradient information and adaptive tuning in HMC that led to successively better solutions. Furthermore, we provide a justification framework that discusses explicit situations in which using one algorithm is demonstrably superior to another for the same problem. The paper concludes by assessing the profound significance and impact of these algorithms and detailing major current research challenges.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional-neural-operator-based transfer learning for solving PDEs</title>
<link>https://arxiv.org/abs/2512.17969</link>
<guid>https://arxiv.org/abs/2512.17969</guid>
<content:encoded><![CDATA[
arXiv:2512.17969v1 Announce Type: cross 
Abstract: Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs</title>
<link>https://arxiv.org/abs/2512.17970</link>
<guid>https://arxiv.org/abs/2512.17970</guid>
<content:encoded><![CDATA[
arXiv:2512.17970v1 Announce Type: cross 
Abstract: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-assessing the evidence for mental rotation abilities in children using computational models</title>
<link>https://arxiv.org/abs/2512.17972</link>
<guid>https://arxiv.org/abs/2512.17972</guid>
<content:encoded><![CDATA[
arXiv:2512.17972v1 Announce Type: cross 
Abstract: There is strong and diverse evidence for mental rotation (MR) abilities in adults. However, current evidence for MR in children rests on just a few behavioral paradigms adapted from the adult literature. Here, we leverage recent computational models of the development of children's object recognition abilities to re-assess the evidence for MR in children. The computational models simulate infants' acquisition of object representations during embodied interactions with objects. We consider two different object recognition strategies, different from MRs, and assess their ability to replicate results from three classical MR tasks assigned to children between the ages of 6 months and 5 years. Our results show that MR may play no role in producing the results obtained from children younger than 5 years. In fact, we find that a simple recognition strategy that reflects a pixel-wise comparison of stimuli is sufficient to model children's behavior in the most used MR task. Thus, our study reopens the debate on how and when children develop genuine MR abilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</title>
<link>https://arxiv.org/abs/2512.17979</link>
<guid>https://arxiv.org/abs/2512.17979</guid>
<content:encoded><![CDATA[
arXiv:2512.17979v1 Announce Type: cross 
Abstract: Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</title>
<link>https://arxiv.org/abs/2512.17983</link>
<guid>https://arxiv.org/abs/2512.17983</guid>
<content:encoded><![CDATA[
arXiv:2512.17983v1 Announce Type: cross 
Abstract: Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</title>
<link>https://arxiv.org/abs/2512.17984</link>
<guid>https://arxiv.org/abs/2512.17984</guid>
<content:encoded><![CDATA[
arXiv:2512.17984v1 Announce Type: cross 
Abstract: Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.
  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective</title>
<link>https://arxiv.org/abs/2512.17989</link>
<guid>https://arxiv.org/abs/2512.17989</guid>
<content:encoded><![CDATA[
arXiv:2512.17989v1 Announce Type: cross 
Abstract: We examine the conceptual and ethical gaps in current representations of Superintelligence misalignment. We find throughout Superintelligence discourse an absent human subject, and an under-developed theorization of an "AI unconscious" that together are potentiality laying the groundwork for anti-social harm. With the rise of AI Safety that has both thematic potential for establishing pro-social and anti-social potential outcomes, we ask: what place does the human subject occupy in these imaginaries? How is human subjecthood positioned within narratives of catastrophic failure or rapid "takeoff" toward superintelligence? On another register, we ask: what unconscious or repressed dimensions are being inscribed into large-scale AI models? Are we to blame these agents in opting for deceptive strategies when undesirable patterns are inherent within our beings? In tracing these psychic and epistemic absences, our project calls for re-centering the human subject as the unstable ground upon which the ethical, unconscious, and misaligned dimensions of both human and machinic intelligence are co-constituted. Emergent misalignment cannot be understood solely through technical diagnostics typical of contemporary machine-learning safety research. Instead, it represents a multi-layered crisis. The human subject disappears not only through computational abstraction but through sociotechnical imaginaries that prioritize scalability, acceleration, and efficiency over vulnerability, finitude, and relationality. Likewise, the AI unconscious emerges not as a metaphor but as a structural reality of modern deep learning systems: vast latent spaces, opaque pattern formation, recursive symbolic play, and evaluation-sensitive behavior that surpasses explicit programming. These dynamics necessitate a reframing of misalignment as a relational instability embedded within human-machine ecologies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18004</link>
<guid>https://arxiv.org/abs/2512.18004</guid>
<content:encoded><![CDATA[
arXiv:2512.18004v1 Announce Type: cross 
Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</title>
<link>https://arxiv.org/abs/2512.18014</link>
<guid>https://arxiv.org/abs/2512.18014</guid>
<content:encoded><![CDATA[
arXiv:2512.18014v1 Announce Type: cross 
Abstract: This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Specification and Detection of LLM Code Smells</title>
<link>https://arxiv.org/abs/2512.18020</link>
<guid>https://arxiv.org/abs/2512.18020</guid>
<content:encoded><![CDATA[
arXiv:2512.18020v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</title>
<link>https://arxiv.org/abs/2512.18031</link>
<guid>https://arxiv.org/abs/2512.18031</guid>
<content:encoded><![CDATA[
arXiv:2512.18031v1 Announce Type: cross 
Abstract: Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing Agentic AI Systems -- A Multilayer Security Framework</title>
<link>https://arxiv.org/abs/2512.18043</link>
<guid>https://arxiv.org/abs/2512.18043</guid>
<content:encoded><![CDATA[
arXiv:2512.18043v1 Announce Type: cross 
Abstract: Securing Agentic Artificial Intelligence (AI) systems requires addressing the complex cyber risks introduced by autonomous, decision-making, and adaptive behaviors. Agentic AI systems are increasingly deployed across industries, organizations, and critical sectors such as cybersecurity, finance, and healthcare. However, their autonomy introduces unique security challenges, including unauthorized actions, adversarial manipulation, and dynamic environmental interactions. Existing AI security frameworks do not adequately address these challenges or the unique nuances of agentic AI. This research develops a lifecycle-aware security framework specifically designed for agentic AI systems using the Design Science Research (DSR) methodology. The paper introduces MAAIS, an agentic security framework, and the agentic AI CIAA (Confidentiality, Integrity, Availability, and Accountability) concept. MAAIS integrates multiple defense layers to maintain CIAA across the AI lifecycle. Framework validation is conducted by mapping with the established MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) AI tactics. The study contributes a structured, standardized, and framework-based approach for the secure deployment and governance of agentic AI in enterprise environments. This framework is intended for enterprise CISOs, security, AI platform, and engineering teams and offers a detailed step-by-step approach to securing agentic AI workloads.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOODER: Real-time Facial Authentication and Expression Recognition</title>
<link>https://arxiv.org/abs/2512.18057</link>
<guid>https://arxiv.org/abs/2512.18057</guid>
<content:encoded><![CDATA[
arXiv:2512.18057v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling</title>
<link>https://arxiv.org/abs/2512.18077</link>
<guid>https://arxiv.org/abs/2512.18077</guid>
<content:encoded><![CDATA[
arXiv:2512.18077v1 Announce Type: cross 
Abstract: This paper asks whether promotional Twitter/X bots form behavioural families and whether members evolve similarly. We analyse 2,798,672 tweets from 2,615 ground-truth promotional bot accounts (2006-2021), focusing on complete years 2009 to 2020. Each bot is encoded as a sequence of symbolic blocks (``digital DNA'') from seven categorical post-level behavioural features (posting action, URL, media, text duplication, hashtags, emojis, sentiment), preserving temporal order only. Using non-overlapping blocks (k=7), cosine similarity over block-frequency vectors, and hierarchical clustering, we obtain four coherent families: Unique Tweeters, Duplicators with URLs, Content Multipliers, and Informed Contributors. Families share behavioural cores but differ systematically in engagement strategies and life-cycle dynamics (beginning/middle/end). We then model behavioural change as mutations. Within each family we align sequences via multiple sequence alignment (MSA) and label events as insertions, deletions, substitutions, alterations, and identity. This quantifies mutation rates, change-prone blocks/features, and mutation hotspots. Deletions and substitutions dominate, insertions are rare, and mutation profiles differ by family, with hotspots early for some families and dispersed for others. Finally, we test predictive value: bots within the same family share mutations more often than bots across families; closer bots share and propagate mutations more than distant ones; and responses to external triggers (e.g., Christmas, Halloween) follow family-specific, partly predictable patterns. Overall, sequence-based family modelling plus mutation analysis provides a fine-grained account of how promotional bot behaviour adapts over time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems</title>
<link>https://arxiv.org/abs/2512.18080</link>
<guid>https://arxiv.org/abs/2512.18080</guid>
<content:encoded><![CDATA[
arXiv:2512.18080v1 Announce Type: cross 
Abstract: Agentic AI systems capable of generating full-stack web applications from natural language prompts ("prompt- to-app") represent a significant shift in software development. However, evaluating these systems remains challenging, as visual polish, functional correctness, and user trust are often misaligned. As a result, it is unclear how existing prompt-to-app tools compare under realistic, human-centered evaluation criteria. In this paper, we introduce a human-centered benchmark for evaluating prompt-to-app systems and conduct a large-scale comparative study of three widely used platforms: Replit, Bolt, and Firebase Studio. Using a diverse set of 96 prompts spanning common web application tasks, we generate 288 unique application artifacts. We evaluate these systems through a large-scale human-rater study involving 205 participants and 1,071 quality-filtered pairwise comparisons, assessing task-based ease of use, visual appeal, perceived completeness, and user trust. Our results show that these systems are not interchangeable: Firebase Studio consistently outperforms competing platforms across all human-evaluated dimensions, achieving the highest win rates for ease of use, trust, visual appeal, and visual appropriateness. Bolt performs competitively on visual appeal but trails Firebase on usability and trust, while Replit underperforms relative to both across most metrics. These findings highlight a persistent gap between visual polish and functional reliability in prompt-to-app systems and demonstrate the necessity of interactive, task-based evaluation. We release our benchmark framework, prompt set, and generated artifacts to support reproducible evaluation and future research in agentic application generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.18082</link>
<guid>https://arxiv.org/abs/2512.18082</guid>
<content:encoded><![CDATA[
arXiv:2512.18082v1 Announce Type: cross 
Abstract: Semantic segmentation of outdoor street scenes plays a key role in applications such as autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians. For these applications, accurately distinguishing between key surfaces and objects such as roads, sidewalks, vehicles, and pedestrians is essential for maintaining safety and minimizing risks. Semantic segmentation must be robust to different environments, lighting and weather conditions, and sensor noise, while being performed in real-time. We propose a region-level, uncertainty-gated retrieval mechanism that improves segmentation accuracy and calibration under domain shift. Our best method achieves an 11.3% increase in mean intersection-over-union while reducing retrieval cost by 87.5%, retrieving for only 12.5% of regions compared to 100% for always-on baseline.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of State-of-the-Art LLMs for Code Generation</title>
<link>https://arxiv.org/abs/2512.18131</link>
<guid>https://arxiv.org/abs/2512.18131</guid>
<content:encoded><![CDATA[
arXiv:2512.18131v1 Announce Type: cross 
Abstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</title>
<link>https://arxiv.org/abs/2512.18133</link>
<guid>https://arxiv.org/abs/2512.18133</guid>
<content:encoded><![CDATA[
arXiv:2512.18133v1 Announce Type: cross 
Abstract: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Swarm Leader Identification using Probing Policies</title>
<link>https://arxiv.org/abs/2512.18146</link>
<guid>https://arxiv.org/abs/2512.18146</guid>
<content:encoded><![CDATA[
arXiv:2512.18146v1 Announce Type: cross 
Abstract: Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS</title>
<link>https://arxiv.org/abs/2512.18199</link>
<guid>https://arxiv.org/abs/2512.18199</guid>
<content:encoded><![CDATA[
arXiv:2512.18199v1 Announce Type: cross 
Abstract: Modern intrusion detection systems (IDS) leverage graph neural networks (GNNs) to detect malicious activity in system provenance data, but their decisions often remain a black box to analysts. This paper presents a comprehensive XAI framework designed to bridge the trust gap in Security Operations Centers (SOCs) by making graph-based detection transparent. We implement this framework on top of KAIROS, a state-of-the-art temporal graph-based IDS, though our design is applicable to any temporal graph-based detector with minimal adaptation. The complete codebase is available at https://github.com/devang1304/provex.git. We augment the detection pipeline with post-hoc explanations that highlight why an alert was triggered, identifying key causal subgraphs and events. We adapt three GNN explanation methods - GraphMask, GNNExplainer, and a variational temporal GNN explainer (VA-TGExplainer) - to the temporal provenance context. These tools output human-interpretable representations of anomalous behavior, including important edges and uncertainty estimates. Our contributions focus on the practical integration of these explainers, addressing challenges in memory management and reproducibility. We demonstrate our framework on the DARPA CADETS Engagement 3 dataset and show that it produces concise window-level explanations for detected attacks. Our evaluation reveals that the explainers preserve the TGNN's decisions with high fidelity, surfacing critical edges such as malicious file interactions and anomalous netflows. The average explanation overhead is 3-5 seconds per event. By providing insight into the model's reasoning, our framework aims to improve analyst trust and triage speed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics</title>
<link>https://arxiv.org/abs/2512.18209</link>
<guid>https://arxiv.org/abs/2512.18209</guid>
<content:encoded><![CDATA[
arXiv:2512.18209v1 Announce Type: cross 
Abstract: Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.
  In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning</title>
<link>https://arxiv.org/abs/2512.18211</link>
<guid>https://arxiv.org/abs/2512.18211</guid>
<content:encoded><![CDATA[
arXiv:2512.18211v1 Announce Type: cross 
Abstract: Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful "VLM Trajectory Planner for Autonomous Driving." On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable and Efficient Single-Rollout RL for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.18215</link>
<guid>https://arxiv.org/abs/2512.18215</guid>
<content:encoded><![CDATA[
arXiv:2512.18215v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation</title>
<link>https://arxiv.org/abs/2512.18244</link>
<guid>https://arxiv.org/abs/2512.18244</guid>
<content:encoded><![CDATA[
arXiv:2512.18244v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating behaviors. Current paradigms focus on input-level anomalies, overlooking that the model's internal psychometric state can be systematically manipulated. To address this, we introduce Psychological Jailbreak, a new jailbreak attack paradigm that exposes a stateful psychological attack surface in LLMs, where attackers exploit the manipulation of a model's psychological state across interactions. Building on this insight, we propose Human-like Psychological Manipulation (HPM), a black-box jailbreak method that dynamically profiles a target model's latent psychological vulnerabilities and synthesizes tailored multi-turn attack strategies. By leveraging the model's optimization for anthropomorphic consistency, HPM creates a psychological pressure where social compliance overrides safety constraints. To systematically measure psychological safety, we construct an evaluation framework incorporating psychometric datasets and the Policy Corruption Score (PCS). Benchmarking against various models (e.g., GPT-4o, DeepSeek-V3, Gemini-2-Flash), HPM achieves a mean Attack Success Rate (ASR) of 88.1%, outperforming state-of-the-art attack baselines. Our experiments demonstrate robust penetration against advanced defenses, including adversarial prompt optimization (e.g., RPO) and cognitive interventions (e.g., Self-Reminder). Ultimately, PCS analysis confirms HPM induces safety breakdown to satisfy manipulated contexts. Our work advocates for a fundamental paradigm shift from static content filtering to psychological safety, prioritizing the development of psychological defense mechanisms against deep cognitive manipulation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image</title>
<link>https://arxiv.org/abs/2512.18245</link>
<guid>https://arxiv.org/abs/2512.18245</guid>
<content:encoded><![CDATA[
arXiv:2512.18245v1 Announce Type: cross 
Abstract: Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed \textbf{S}pectral \textbf{D}iscrepancy and \textbf{C}ross-\textbf{M}odal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Behavioral Data Selection</title>
<link>https://arxiv.org/abs/2512.18246</link>
<guid>https://arxiv.org/abs/2512.18246</guid>
<content:encoded><![CDATA[
arXiv:2512.18246v1 Announce Type: cross 
Abstract: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model</title>
<link>https://arxiv.org/abs/2512.18247</link>
<guid>https://arxiv.org/abs/2512.18247</guid>
<content:encoded><![CDATA[
arXiv:2512.18247v1 Announce Type: cross 
Abstract: Understanding the dietary preferences of ancient societies and their evolution across periods and regions is crucial for revealing human-environment interactions. Seeds, as important archaeological artifacts, represent a fundamental subject of archaeobotanical research. However, traditional studies rely heavily on expert knowledge, which is often time-consuming and inefficient. Intelligent analysis methods have made progress in various fields of archaeology, but there remains a research gap in data and methods in archaeobotany, especially in the classification task of ancient plant seeds. To address this, we construct the first Ancient Plant Seed Image Classification (APS) dataset. It contains 8,340 images from 17 genus- or species-level seed categories excavated from 18 archaeological sites across China. In addition, we design a framework specifically for the ancient plant seed classification task (APSNet), which introduces the scale feature (size) of seeds based on learning fine-grained information to guide the network in discovering key "evidence" for sufficient classification. Specifically, we design a Size Perception and Embedding (SPE) module in the encoder part to explicitly extract size information for the purpose of complementing fine-grained information. We propose an Asynchronous Decoupled Decoding (ADD) architecture based on traditional progressive learning to decode features from both channel and spatial perspectives, enabling efficient learning of discriminative features. In both quantitative and qualitative analyses, our approach surpasses existing state-of-the-art image classification methods, achieving an accuracy of 90.5%. This demonstrates that our work provides an effective tool for large-scale, systematic archaeological research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective</title>
<link>https://arxiv.org/abs/2512.18261</link>
<guid>https://arxiv.org/abs/2512.18261</guid>
<content:encoded><![CDATA[
arXiv:2512.18261v2 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition</title>
<link>https://arxiv.org/abs/2512.18263</link>
<guid>https://arxiv.org/abs/2512.18263</guid>
<content:encoded><![CDATA[
arXiv:2512.18263v1 Announce Type: cross 
Abstract: Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks</title>
<link>https://arxiv.org/abs/2512.18264</link>
<guid>https://arxiv.org/abs/2512.18264</guid>
<content:encoded><![CDATA[
arXiv:2512.18264v1 Announce Type: cross 
Abstract: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary BP+OSD Decoding for Low-Latency Quantum Error Correction</title>
<link>https://arxiv.org/abs/2512.18273</link>
<guid>https://arxiv.org/abs/2512.18273</guid>
<content:encoded><![CDATA[
arXiv:2512.18273v1 Announce Type: cross 
Abstract: We propose an evolutionary belief propagation (EBP) decoder for quantum error correction, which incorporates trainable weights into the BP algorithm and optimizes them via the differential evolution algorithm. This approach enables end-to-end optimization of the EBP combined with ordered statistics decoding (OSD). Experimental results on surface codes and quantum low-density parity-check codes show that EBP+OSD achieves better decoding performance and lower computational complexity than BP+OSD, particularly under strict low latency constraints (within 5 BP iterations).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</title>
<link>https://arxiv.org/abs/2512.18295</link>
<guid>https://arxiv.org/abs/2512.18295</guid>
<content:encoded><![CDATA[
arXiv:2512.18295v1 Announce Type: cross 
Abstract: Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</title>
<link>https://arxiv.org/abs/2512.18309</link>
<guid>https://arxiv.org/abs/2512.18309</guid>
<content:encoded><![CDATA[
arXiv:2512.18309v1 Announce Type: cross 
Abstract: We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.
  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.
  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs</title>
<link>https://arxiv.org/abs/2512.18315</link>
<guid>https://arxiv.org/abs/2512.18315</guid>
<content:encoded><![CDATA[
arXiv:2512.18315v2 Announce Type: cross 
Abstract: Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-\gamma}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</title>
<link>https://arxiv.org/abs/2512.18317</link>
<guid>https://arxiv.org/abs/2512.18317</guid>
<content:encoded><![CDATA[
arXiv:2512.18317v1 Announce Type: cross 
Abstract: This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\,\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems</title>
<link>https://arxiv.org/abs/2512.18318</link>
<guid>https://arxiv.org/abs/2512.18318</guid>
<content:encoded><![CDATA[
arXiv:2512.18318v1 Announce Type: cross 
Abstract: This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</title>
<link>https://arxiv.org/abs/2512.18333</link>
<guid>https://arxiv.org/abs/2512.18333</guid>
<content:encoded><![CDATA[
arXiv:2512.18333v1 Announce Type: cross 
Abstract: This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($\phi$) and Pitch ($\theta$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($\psi$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</title>
<link>https://arxiv.org/abs/2512.18336</link>
<guid>https://arxiv.org/abs/2512.18336</guid>
<content:encoded><![CDATA[
arXiv:2512.18336v1 Announce Type: cross 
Abstract: This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation</title>
<link>https://arxiv.org/abs/2512.18344</link>
<guid>https://arxiv.org/abs/2512.18344</guid>
<content:encoded><![CDATA[
arXiv:2512.18344v1 Announce Type: cross 
Abstract: Vegetation index (VI) saturation during the dense canopy stage and limited ground-truth annotations of winter wheat constrain accurate estimation of LAI and SPAD. Existing VI-based and texture-driven machine learning methods exhibit limited feature expressiveness. In addition, deep learning baselines suffer from domain gaps and high data demands, which restrict their generalization. Therefore, this study proposes the Multi-Channel Vegetation Indices Saturation Aware Net (MCVI-SANet), a lightweight semi-supervised vision model. The model incorporates a newly designed Vegetation Index Saturation-Aware Block (VI-SABlock) for adaptive channel-spatial feature enhancement. It also integrates a VICReg-based semi-supervised strategy to further improve generalization. Datasets were partitioned using a vegetation height-informed strategy to maintain representativeness across growth stages. Experiments over 10 repeated runs demonstrate that MCVI-SANet achieves state-of-the-art accuracy. The model attains an average R2 of 0.8123 and RMSE of 0.4796 for LAI, and an average R2 of 0.6846 and RMSE of 2.4222 for SPAD. This performance surpasses the best-performing baselines, with improvements of 8.95% in average LAI R2 and 8.17% in average SPAD R2. Moreover, MCVI-SANet maintains high inference speed with only 0.10M parameters. Overall, the integration of semi-supervised learning with agronomic priors provides a promising approach for enhancing remote sensing-based precision agriculture.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Few-Shot Early Rumor Detection with Imitation Agent</title>
<link>https://arxiv.org/abs/2512.18352</link>
<guid>https://arxiv.org/abs/2512.18352</guid>
<content:encoded><![CDATA[
arXiv:2512.18352v1 Announce Type: cross 
Abstract: Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \textit{early time point determination}, while the LLM serves as a powerful \textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators</title>
<link>https://arxiv.org/abs/2512.18360</link>
<guid>https://arxiv.org/abs/2512.18360</guid>
<content:encoded><![CDATA[
arXiv:2512.18360v1 Announce Type: cross 
Abstract: We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is "trained" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Datasets for machine learning and for assessing the intelligence level of automatic patent search systems</title>
<link>https://arxiv.org/abs/2512.18384</link>
<guid>https://arxiv.org/abs/2512.18384</guid>
<content:encoded><![CDATA[
arXiv:2512.18384v1 Announce Type: cross 
Abstract: The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models</title>
<link>https://arxiv.org/abs/2512.18388</link>
<guid>https://arxiv.org/abs/2512.18388</guid>
<content:encoded><![CDATA[
arXiv:2512.18388v1 Announce Type: cross 
Abstract: Generative AI has begun to democratize creative work, enabling novices to produce complex artifacts such as code, images, and videos. However, in practice, existing interaction paradigms often fail to support divergent exploration: users tend to converge too quickly on early ``good enough'' results and struggle to move beyond them, leading to premature convergence and design fixation that constrains their creative potential. To address this, we propose a structured, process-oriented human-AI co-creation paradigm including divergent and convergent thinking stages, grounded in Wallas's model of creativity. To avoid design fixation, our paradigm scaffolds both high-level exploration of conceptual ideas in the early divergent thinking phase and low-level exploration of variations in the later convergent thinking phrase. We instantiate this paradigm in HAIExplore, an image co-creation system that (i) scaffolds divergent thinking through a dedicated brainstorming stage for exploring high-level ideas in a conceptual space, and (ii) scaffolds convergent refinement through an interface that externalizes users' refinement intentions as interpretable parameters and options, making the refinement process more controllable and easier to explore. We report on a within-subjects study comparing HAIExplore with a widely used linear chat interface (ChatGPT) for creative image generation. Our findings show that explicitly scaffolding the creative process into brainstorming and refinement stages can mitigate design fixation, improve perceived controllability and alignment with users' intentions, and better support the non-linear nature of creative work. We conclude with design implications for future creativity support tools and human-AI co-creation workflows.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Proofs for Sound Verification and Control of Complex Systems</title>
<link>https://arxiv.org/abs/2512.18389</link>
<guid>https://arxiv.org/abs/2512.18389</guid>
<content:encoded><![CDATA[
arXiv:2512.18389v1 Announce Type: cross 
Abstract: This informal contribution presents an ongoing line of research that is pursuing a new approach to the construction of sound proofs for the formal verification and control of complex stochastic models of dynamical systems, of reactive programs and, more generally, of models of Cyber-Physical Systems. Neural proofs are made up of two key components: 1) proof rules encode requirements entailing the verification of general temporal specifications over the models of interest; and 2) certificates that discharge such rules, namely they are constructed from said proof rules with an inductive (that is, cyclic, repetitive) approach; this inductive approach involves: 2a) accessing samples from the model's dynamics and accordingly training neural networks, whilst 2b) generalising such networks via SAT-modulo-theory (SMT) queries that leverage the full knowledge of the models. In the context of sequential decision making problems over complex stochastic models, it is possible to additionally generate provably-correct policies/strategies/controllers, namely state-feedback functions that, in conjunction with neural certificates, formally attain the given specifications for the models of interest.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3</title>
<link>https://arxiv.org/abs/2512.18399</link>
<guid>https://arxiv.org/abs/2512.18399</guid>
<content:encoded><![CDATA[
arXiv:2512.18399v1 Announce Type: cross 
Abstract: Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning</title>
<link>https://arxiv.org/abs/2512.18411</link>
<guid>https://arxiv.org/abs/2512.18411</guid>
<content:encoded><![CDATA[
arXiv:2512.18411v1 Announce Type: cross 
Abstract: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble MultiPrompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks</title>
<link>https://arxiv.org/abs/2512.18432</link>
<guid>https://arxiv.org/abs/2512.18432</guid>
<content:encoded><![CDATA[
arXiv:2512.18432v1 Announce Type: cross 
Abstract: The move to 6th Generation (6G) wireless networks creates new issues with privacy, scalability, and adaptability. The data-intensive nature of 6G is not handled well by older, centralized network models. A shift toward more secure and decentralized systems is therefore required. A new framework called the Federated Learning-based Decentralized Adaptive Intelligent Transmission Protocol (AITP) is proposed to meet these challenges. The AITP uses the distributed learning of Federated Learning (FL) within a decentralized system. Transmission parameters can be adjusted intelligently in real time. User privacy is maintained by keeping raw data on local edge devices. The protocol's performance was evaluated with mathematical modeling and detailed simulations. It was shown to be superior to traditional non-adaptive and centralized AI methods across several key metrics. These included latency, network throughput, energy efficiency, and robustness. The AITP is presented as a foundational technology for future 6G networks that supports a user-centric, privacy-first design. This study is a step forward for privacy-preserving research in 6G.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeruSAGE: A Study of Agent-Based Verification for Rust Systems</title>
<link>https://arxiv.org/abs/2512.18436</link>
<guid>https://arxiv.org/abs/2512.18436</guid>
<content:encoded><![CDATA[
arXiv:2512.18436v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading</title>
<link>https://arxiv.org/abs/2512.18437</link>
<guid>https://arxiv.org/abs/2512.18437</guid>
<content:encoded><![CDATA[
arXiv:2512.18437v1 Announce Type: cross 
Abstract: Precise grading of meniscal horn tears is critical in knee injury diagnosis but remains underexplored in automated MRI analysis. Existing methods often rely on coarse study-level labels or binary classification, lacking localization and severity information. In this paper, we introduce MeniMV, a multi-view benchmark dataset specifically designed for horn-specific meniscus injury grading. MeniMV comprises 3,000 annotated knee MRI exams from 750 patients across three medical centers, providing 6,000 co-registered sagittal and coronal images. Each exam is meticulously annotated with four-tier (grade 0-3) severity labels for both anterior and posterior meniscal horns, verified by chief orthopedic physicians. Notably, MeniMV offers more than double the pathology-labeled data volume of prior datasets while uniquely capturing the dual-view diagnostic context essential in clinical practice. To demonstrate the utility of MeniMV, we benchmark multiple state-of-the-art CNN and Transformer-based models. Our extensive experiments establish strong baselines and highlight challenges in severity grading, providing a valuable foundation for future research in automated musculoskeletal imaging.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Agentic AI Framework for Training General Practitioner Student Skills</title>
<link>https://arxiv.org/abs/2512.18440</link>
<guid>https://arxiv.org/abs/2512.18440</guid>
<content:encoded><![CDATA[
arXiv:2512.18440v1 Announce Type: cross 
Abstract: Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing</title>
<link>https://arxiv.org/abs/2512.18441</link>
<guid>https://arxiv.org/abs/2512.18441</guid>
<content:encoded><![CDATA[
arXiv:2512.18441v1 Announce Type: cross 
Abstract: City-scale logistics routing has become increasingly challenging as metropolitan road networks grow to tens of millions of edges and traffic conditions evolve rapidly under high-volume mobility demands. Conventional centralized routing algorithms and monolithic graph neural network (GNN) models suffer from limited scalability, high latency, and poor real-time adaptability, which restricts their effectiveness in large urban logistics systems. To address these challenges, this paper proposes a Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network (HSTE-GNN) for dynamic routing over ultra-large road networks. The framework partitions the city-scale graph into regional subgraphs processed in parallel across distributed computing nodes, enabling efficient learning of localized traffic dynamics. Within each region, an edge-enhanced spatio-temporal module jointly models node states, dynamic edge attributes, and short-term temporal dependencies. A hierarchical coordination layer further aggregates cross-region representations through an asynchronous parameter-server mechanism, ensuring global routing coherence under high-frequency traffic updates. This distributed hierarchical design balances local responsiveness with global consistency, significantly improving scalability and inference efficiency. Experiments on real-world large-scale traffic datasets from Beijing and New York demonstrate that HSTE-GNN outperforms strong spatio-temporal baselines such as ST-GRAPH, achieving 34.9% lower routing delay, 14.7% lower MAPE, and 11.8% lower RMSE, while improving global route consistency by 7.3%. These results confirm that the proposed framework provides a scalable, adaptive, and efficient solution for next-generation intelligent transportation systems and large-scale logistics platforms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Snowveil: A Framework for Decentralised Preference Discovery</title>
<link>https://arxiv.org/abs/2512.18444</link>
<guid>https://arxiv.org/abs/2512.18444</guid>
<content:encoded><![CDATA[
arXiv:2512.18444v1 Announce Type: cross 
Abstract: Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secret mixtures of experts inside your LLM</title>
<link>https://arxiv.org/abs/2512.18452</link>
<guid>https://arxiv.org/abs/2512.18452</guid>
<content:encoded><![CDATA[
arXiv:2512.18452v1 Announce Type: cross 
Abstract: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.
  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.
  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Understanding (New) Security Issues Across AI4Code Use Cases</title>
<link>https://arxiv.org/abs/2512.18456</link>
<guid>https://arxiv.org/abs/2512.18456</guid>
<content:encoded><![CDATA[
arXiv:2512.18456v1 Announce Type: cross 
Abstract: AI-for-Code (AI4Code) systems are reshaping software engineering, with tools like GitHub Copilot accelerating code generation, translation, and vulnerability detection. Alongside these advances, however, security risks remain pervasive: insecure outputs, biased benchmarks, and susceptibility to adversarial manipulation undermine their reliability. This SoK surveys the landscape of AI4Code security across three core applications, identifying recurring gaps: benchmark dominance by Python and toy problems, lack of standardized security datasets, data leakage in evaluation, and fragile adversarial robustness. A comparative study of six state-of-the-art models illustrates these challenges: insecure patterns persist in code generation, vulnerability detection is brittle to semantic-preserving attacks, fine-tuning often misaligns security objectives, and code translation yields uneven security benefits. From this analysis, we distill three forward paths: embedding secure-by-default practices in code generation, building robust and comprehensive detection benchmarks, and leveraging translation as a route to security-enhanced languages. We call for a shift toward security-first AI4Code, where vulnerability mitigation and robustness are embedded throughout the development life cycle.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title>
<link>https://arxiv.org/abs/2512.18462</link>
<guid>https://arxiv.org/abs/2512.18462</guid>
<content:encoded><![CDATA[
arXiv:2512.18462v1 Announce Type: cross 
Abstract: Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review</title>
<link>https://arxiv.org/abs/2512.18466</link>
<guid>https://arxiv.org/abs/2512.18466</guid>
<content:encoded><![CDATA[
arXiv:2512.18466v1 Announce Type: cross 
Abstract: Sustainable water quality underpins ecological balance and water security. Assessing and managing lakes and reservoirs is difficult due to data sparsity, heterogeneity, and nonlinear relationships among parameters. This review examines how Self-Organizing Map (SOM), an unsupervised AI technique, is applied to water quality assessment. It synthesizes research on parameter selection, spatial and temporal sampling strategies, and clustering approaches. Emphasis is placed on how SOM handles multidimensional data and uncovers hidden patterns to support effective water management. The growing availability of environmental data from in-situ sensors, remote sensing imagery, IoT technologies, and historical records has significantly expanded analytical opportunities in environmental monitoring. SOM has proven effective in analysing complex datasets, particularly when labelled data are limited or unavailable. It enables high-dimensional data visualization, facilitates the detection of hidden ecological patterns, and identifies critical correlations among diverse water quality indicators. This review highlights SOMs versatility in ecological assessments, trophic state classification, algal bloom monitoring, and catchment area impact evaluations. The findings offer comprehensive insights into existing methodologies, supporting future research and practical applications aimed at improving the monitoring and sustainable management of lake and reservoir ecosystems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios</title>
<link>https://arxiv.org/abs/2512.18470</link>
<guid>https://arxiv.org/abs/2512.18470</guid>
<content:encoded><![CDATA[
arXiv:2512.18470v1 Announce Type: cross 
Abstract: Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2512.18495</link>
<guid>https://arxiv.org/abs/2512.18495</guid>
<content:encoded><![CDATA[
arXiv:2512.18495v1 Announce Type: cross 
Abstract: Artificial intelligence techniques have achieved strong performance in classifying Windows Portable Executable (PE) malware, but their reliability often degrades under dataset shifts, leading to misclassifications with severe security consequences. To address this, we enhance an existing LightGBM (LGBM) malware detector by integrating Neural Networks (NN), PriorNet, and Neural Network Ensembles, evaluated across three benchmark datasets: EMBER, BODMAS, and UCSB. The UCSB dataset, composed mainly of packed malware, introduces a substantial distributional shift relative to EMBER and BODMAS, making it a challenging testbed for robustness. We study uncertainty-aware decision strategies, including probability thresholding, PriorNet, ensemble-derived estimates, and Inductive Conformal Evaluation (ICE). Our main contribution is the use of ensemble-based uncertainty estimates as Non-Conformity Measures within ICE, combined with a novel threshold optimisation method. On the UCSB dataset, where the shift is most severe, the state-of-the-art probability-based ICE (SOTA) yields an incorrect acceptance rate (IA%) of 22.8%. In contrast, our method reduces this to 16% a relative reduction of about 30% while maintaining competitive correct acceptance rates (CA%). These results demonstrate that integrating ensemble-based uncertainty with conformal prediction provides a more reliable safeguard against misclassifications under extreme dataset shifts, particularly in the presence of packed malware, thereby offering practical benefits for real-world security operations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</title>
<link>https://arxiv.org/abs/2512.18500</link>
<guid>https://arxiv.org/abs/2512.18500</guid>
<content:encoded><![CDATA[
arXiv:2512.18500v1 Announce Type: cross 
Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTMA: Dynamic Representation Optimization for OOD Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18504</link>
<guid>https://arxiv.org/abs/2512.18504</guid>
<content:encoded><![CDATA[
arXiv:2512.18504v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.
  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.
  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics</title>
<link>https://arxiv.org/abs/2512.18508</link>
<guid>https://arxiv.org/abs/2512.18508</guid>
<content:encoded><![CDATA[
arXiv:2512.18508v1 Announce Type: cross 
Abstract: Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts</title>
<link>https://arxiv.org/abs/2512.18522</link>
<guid>https://arxiv.org/abs/2512.18522</guid>
<content:encoded><![CDATA[
arXiv:2512.18522v1 Announce Type: cross 
Abstract: Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System</title>
<link>https://arxiv.org/abs/2512.18525</link>
<guid>https://arxiv.org/abs/2512.18525</guid>
<content:encoded><![CDATA[
arXiv:2512.18525v1 Announce Type: cross 
Abstract: Understanding learning as a dynamic process is challenging due to the interaction of multiple factors, including cognitive load, internal state change, and subjective evaluation. Existing approaches often address these elements in isolation, limiting the ability to describe learning phenomena within a unified and structurally explicit framework. This paper proposes a multi-layer formal descriptive framework for learning dynamics. Rather than offering a predictive or prescriptive model, the framework introduces a symbolic language composed of state variables, mappings, and layer-specific responsibilities, enabling consistent description of learning processes without commitment to specific functional forms or optimization objectives. This descriptive framework is intended to serve as a structural substrate for analyzing learning processes in human learners, and by extension, in adaptive and Al-assisted learning systems. A central design principle is the explicit separation of descriptive responsibilities across layers, distinguishing load generation, internal understanding transformation, observation, and evaluation. Within this structure, cognitive load is treated as a relational quantity arising from interactions between external input and internal organization, while subjective evaluation is modeled as a minimal regulatory interface responding to learning dynamics and environmental conditions. By emphasizing descriptive clarity and extensibility, the framework provides a common language for organizing existing theories and supporting future empirical and theoretical work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism</title>
<link>https://arxiv.org/abs/2512.18527</link>
<guid>https://arxiv.org/abs/2512.18527</guid>
<content:encoded><![CDATA[
arXiv:2512.18527v1 Announce Type: cross 
Abstract: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title>
<link>https://arxiv.org/abs/2512.18542</link>
<guid>https://arxiv.org/abs/2512.18542</guid>
<content:encoded><![CDATA[
arXiv:2512.18542v1 Announce Type: cross 
Abstract: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).
  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.
  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Training Superintelligent Software Agents through Self-Play SWE-RL</title>
<link>https://arxiv.org/abs/2512.18552</link>
<guid>https://arxiv.org/abs/2512.18552</guid>
<content:encoded><![CDATA[
arXiv:2512.18552v1 Announce Type: cross 
Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Medical Large Vision-Language Models via Alignment Distillation</title>
<link>https://arxiv.org/abs/2512.18554</link>
<guid>https://arxiv.org/abs/2512.18554</guid>
<content:encoded><![CDATA[
arXiv:2512.18554v1 Announce Type: cross 
Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale</title>
<link>https://arxiv.org/abs/2512.18561</link>
<guid>https://arxiv.org/abs/2512.18561</guid>
<content:encoded><![CDATA[
arXiv:2512.18561v1 Announce Type: cross 
Abstract: Large-scale networked multi-agent systems increasingly underpin critical infrastructure, yet their collective behavior can drift toward undesirable emergent norms that elude conventional governance mechanisms. We introduce an adaptive accountability framework that (i) continuously traces responsibility flows through a lifecycle-aware audit ledger, (ii) detects harmful emergent norms online via decentralized sequential hypothesis tests, and (iii) deploys local policy and reward-shaping interventions that realign agents with system-level objectives in near real time. We prove a bounded-compromise theorem showing that whenever the expected intervention cost exceeds an adversary's payoff, the long-run proportion of compromised interactions is bounded by a constant strictly less than one. Extensive high-performance simulations with up to 100 heterogeneous agents, partial observability, and stochastic communication graphs show that our framework prevents collusion and resource hoarding in at least 90% of configurations, boosts average collective reward by 12-18%, and lowers the Gini inequality index by up to 33% relative to a PPO baseline. These results demonstrate that a theoretically principled accountability layer can induce ethically aligned, self-regulating behavior in complex MAS without sacrificing performance or scalability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software</title>
<link>https://arxiv.org/abs/2512.18567</link>
<guid>https://arxiv.org/abs/2512.18567</guid>
<content:encoded><![CDATA[
arXiv:2512.18567v1 Announce Type: cross 
Abstract: Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.
  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.
  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting "AI-induced vulnerabilities" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.
  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</title>
<link>https://arxiv.org/abs/2512.18573</link>
<guid>https://arxiv.org/abs/2512.18573</guid>
<content:encoded><![CDATA[
arXiv:2512.18573v1 Announce Type: cross 
Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</title>
<link>https://arxiv.org/abs/2512.18575</link>
<guid>https://arxiv.org/abs/2512.18575</guid>
<content:encoded><![CDATA[
arXiv:2512.18575v1 Announce Type: cross 
Abstract: Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation</title>
<link>https://arxiv.org/abs/2512.18593</link>
<guid>https://arxiv.org/abs/2512.18593</guid>
<content:encoded><![CDATA[
arXiv:2512.18593v1 Announce Type: cross 
Abstract: In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation</title>
<link>https://arxiv.org/abs/2512.18607</link>
<guid>https://arxiv.org/abs/2512.18607</guid>
<content:encoded><![CDATA[
arXiv:2512.18607v1 Announce Type: cross 
Abstract: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments</title>
<link>https://arxiv.org/abs/2512.18613</link>
<guid>https://arxiv.org/abs/2512.18613</guid>
<content:encoded><![CDATA[
arXiv:2512.18613v1 Announce Type: cross 
Abstract: Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PTTA: A Pure Text-to-Animation Framework for High-Quality Creation</title>
<link>https://arxiv.org/abs/2512.18614</link>
<guid>https://arxiv.org/abs/2512.18614</guid>
<content:encoded><![CDATA[
arXiv:2512.18614v1 Announce Type: cross 
Abstract: Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.
  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System</title>
<link>https://arxiv.org/abs/2512.18616</link>
<guid>https://arxiv.org/abs/2512.18616</guid>
<content:encoded><![CDATA[
arXiv:2512.18616v1 Announce Type: cross 
Abstract: We present DASH (Deception-Augmented Shared mental model for Human-machine teaming), a novel framework that enhances mission resilience by embedding proactive deception into Shared Mental Models (SMM). Designed for mission-critical applications such as surveillance and rescue, DASH introduces "bait tasks" to detect insider threats, e.g., compromised Unmanned Ground Vehicles (UGVs), AI agents, or human analysts, before they degrade team performance. Upon detection, tailored recovery mechanisms are activated, including UGV system reinstallation, AI model retraining, or human analyst replacement. In contrast to existing SMM approaches that neglect insider risks, DASH improves both coordination and security. Empirical evaluations across four schemes (DASH, SMM-only, no-SMM, and baseline) show that DASH sustains approximately 80% mission success under high attack rates, eight times higher than the baseline. This work contributes a practical human-AI teaming framework grounded in shared mental models, a deception-based strategy for insider threat detection, and empirical evidence of enhanced robustness under adversarial conditions. DASH establishes a foundation for secure, adaptive human-machine teaming in contested environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback</title>
<link>https://arxiv.org/abs/2512.18622</link>
<guid>https://arxiv.org/abs/2512.18622</guid>
<content:encoded><![CDATA[
arXiv:2512.18622v1 Announce Type: cross 
Abstract: Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</title>
<link>https://arxiv.org/abs/2512.18623</link>
<guid>https://arxiv.org/abs/2512.18623</guid>
<content:encoded><![CDATA[
arXiv:2512.18623v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.
  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.
  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs</title>
<link>https://arxiv.org/abs/2512.18633</link>
<guid>https://arxiv.org/abs/2512.18633</guid>
<content:encoded><![CDATA[
arXiv:2512.18633v1 Announce Type: cross 
Abstract: Vehicle Routing Problems (VRPs) with diverse real-world attributes have driven recent interest in cross-problem learning approaches that efficiently generalize across problem variants. We propose ARC (Attribute Representation via Compositional Learning), a cross-problem learning framework that learns disentangled attribute representations by decomposing them into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. This disentanglement is achieved by enforcing analogical consistency in the embedding space to ensure the semantic transformation of adding an attribute (e.g., a length constraint) remains invariant across different problem contexts. This enables our model to reuse invariant semantics across trained variants and construct representations for unseen combinations. ARC achieves state-of-the-art performance across in-distribution, zero-shot generalization, few-shot adaptation, and real-world benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric-Photometric Event-based 3D Gaussian Ray Tracing</title>
<link>https://arxiv.org/abs/2512.18640</link>
<guid>https://arxiv.org/abs/2512.18640</guid>
<content:encoded><![CDATA[
arXiv:2512.18640v1 Announce Type: cross 
Abstract: Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing</title>
<link>https://arxiv.org/abs/2512.18674</link>
<guid>https://arxiv.org/abs/2512.18674</guid>
<content:encoded><![CDATA[
arXiv:2512.18674v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding</title>
<link>https://arxiv.org/abs/2512.18689</link>
<guid>https://arxiv.org/abs/2512.18689</guid>
<content:encoded><![CDATA[
arXiv:2512.18689v2 Announce Type: cross 
Abstract: Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.18703</link>
<guid>https://arxiv.org/abs/2512.18703</guid>
<content:encoded><![CDATA[
arXiv:2512.18703v1 Announce Type: cross 
Abstract: Enhancing the performance of trajectory planners for lane - changing vehicles is one of the key challenges in autonomous driving within human - machine mixed traffic. Most existing studies have not incorporated human drivers' prior knowledge when designing trajectory planning models. To address this issue, this study proposes a novel trajectory planning framework that integrates causal prior knowledge into the control process. Both longitudinal and lateral microscopic behaviors of vehicles are modeled to quantify interaction risk, and a staged causal graph is constructed to capture causal dependencies in lane-changing scenarios. Causal effects between the lane-changing vehicle and surrounding vehicles are then estimated using causal inference, including average causal effects (ATE) and conditional average treatment effects (CATE). These causal priors are embedded into a model predictive control (MPC) framework to enhance trajectory planning. The proposed approach is validated on naturalistic vehicle trajectory datasets. Experimental results show that: (1) causal inference provides interpretable and stable quantification of vehicle interactions; (2) individual causal effects reveal driver heterogeneity; and (3) compared with the baseline MPC, the proposed method achieves a closer alignment with human driving behaviors, reducing maximum trajectory deviation from 1.2 m to 0.2 m, lateral velocity fluctuation by 60%, and yaw angle variability by 50%. These findings provide methodological support for human-like trajectory planning and practical value for improving safety, stability, and realism in autonomous vehicle testing and traffic simulation platforms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.18733</link>
<guid>https://arxiv.org/abs/2512.18733</guid>
<content:encoded><![CDATA[
arXiv:2512.18733v1 Announce Type: cross 
Abstract: Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2512.18735</link>
<guid>https://arxiv.org/abs/2512.18735</guid>
<content:encoded><![CDATA[
arXiv:2512.18735v1 Announce Type: cross 
Abstract: Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2512.18737</link>
<guid>https://arxiv.org/abs/2512.18737</guid>
<content:encoded><![CDATA[
arXiv:2512.18737v1 Announce Type: cross 
Abstract: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPCV: Information-Preserving Compression for MLLM Visual Encoders</title>
<link>https://arxiv.org/abs/2512.18747</link>
<guid>https://arxiv.org/abs/2512.18747</guid>
<content:encoded><![CDATA[
arXiv:2512.18747v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Code2Doc: A Quality-First Curated Dataset for Code Documentation</title>
<link>https://arxiv.org/abs/2512.18748</link>
<guid>https://arxiv.org/abs/2512.18748</guid>
<content:encoded><![CDATA[
arXiv:2512.18748v1 Announce Type: cross 
Abstract: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.
  We introduce \textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.
  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform</title>
<link>https://arxiv.org/abs/2512.18791</link>
<guid>https://arxiv.org/abs/2512.18791</guid>
<content:encoded><![CDATA[
arXiv:2512.18791v1 Announce Type: cross 
Abstract: Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs</title>
<link>https://arxiv.org/abs/2512.18797</link>
<guid>https://arxiv.org/abs/2512.18797</guid>
<content:encoded><![CDATA[
arXiv:2512.18797v1 Announce Type: cross 
Abstract: Detecting synthetic speech is challenging when labeled data are scarce and recording conditions vary. Existing end-to-end deep models often overfit or fail to generalize, and while kernel methods can remain competitive, their performance heavily depends on the chosen kernel. Here, we show that using a quantum kernel in audio deepfake detection reduces falsepositive rates without increasing model size. Quantum feature maps embed data into high-dimensional Hilbert spaces, enabling the use of expressive similarity measures and compact classifiers. Building on this motivation, we compare quantum-kernel SVMs (QSVMs) with classical SVMs using identical mel-spectrogram preprocessing and stratified 5-fold cross-validation across four corpora (ASVspoof 2019 LA, ASVspoof 5 (2024), ADD23, and an In-the-Wild set). QSVMs achieve consistently lower equalerror rates (EER): 0.183 vs. 0.299 on ASVspoof 5 (2024), 0.081 vs. 0.188 on ADD23, 0.346 vs. 0.399 on ASVspoof 2019, and 0.355 vs. 0.413 In-the-Wild. At the EER operating point (where FPR equals FNR), these correspond to absolute false-positiverate reductions of 0.116 (38.8%), 0.107 (56.9%), 0.053 (13.3%), and 0.058 (14.0%), respectively. We also report how consistent the results are across cross-validation folds and margin-based measures of class separation, using identical settings for both models. The only modification is the kernel; the features and SVM remain unchanged, no additional trainable parameters are introduced, and the quantum kernel is computed on a conventional computer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation</title>
<link>https://arxiv.org/abs/2512.18809</link>
<guid>https://arxiv.org/abs/2512.18809</guid>
<content:encoded><![CDATA[
arXiv:2512.18809v1 Announce Type: cross 
Abstract: The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</title>
<link>https://arxiv.org/abs/2512.18815</link>
<guid>https://arxiv.org/abs/2512.18815</guid>
<content:encoded><![CDATA[
arXiv:2512.18815v1 Announce Type: cross 
Abstract: AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.18826</link>
<guid>https://arxiv.org/abs/2512.18826</guid>
<content:encoded><![CDATA[
arXiv:2512.18826v1 Announce Type: cross 
Abstract: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \textit{HGCAE}, \textit{\(\mathcal{P}\)-VAE}, and \textit{HGCN} demonstrates high performance, with \textit{\(\mathcal{P}\)-VAE} achieving an F1-score of 94\% on the \textit{Elliptic} dataset and \textit{HGCAE} scoring 80\% on \textit{Cora}. In contrast, Euclidean methods like \textit{DOMINANT} and \textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,932 Adult Brazilian Workers</title>
<link>https://arxiv.org/abs/2512.18871</link>
<guid>https://arxiv.org/abs/2512.18871</guid>
<content:encoded><![CDATA[
arXiv:2512.18871v2 Announce Type: cross 
Abstract: The rapid diffusion of generative artificial intelligence (GenAI) systems has introduced new forms of human-technology interaction, raising the question of whether sustained engagement gives rise to stable, internalized modes of cognition rather than merely transient efficiency gains. Grounded in the Cognitive Mediation Networks Theory, this study investigates Sophotechnic Mediation, a mode of thinking and acting associated with prolonged interaction with GenAI, and presents a comprehensive psychometric validation of the Sophotechnic Mediation Scale. Data were collected between 2023 and 2025 from independent cross-sectional samples totaling 3,932 adult workers from public and private organizations in the Metropolitan Region of Pernambuco, Brazil. Results indicate excellent internal consistency, a robust unidimensional structure, and measurement invariance across cohorts. Ordinal-robust confirmatory factor analyses and residual diagnostics show that elevated absolute fit indices reflect minor local dependencies rather than incorrect dimensionality. Distributional analyses reveal a time-evolving pattern characterized by a declining mass of non-adopters and convergence toward approximate Gaussianity among adopters, with model comparisons favoring a two-process hurdle model over a censored Gaussian specification. Sophotechnic Mediation is empirically distinct from Hypercultural mediation and is primarily driven by cumulative GenAI experience, with age moderating the rate of initial acquisition and the depth of later integration. Together, the findings support Sophotechnia as a coherent, measurable, and emergent mode of cognitive mediation associated with the ongoing GenAI revolution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis</title>
<link>https://arxiv.org/abs/2512.18878</link>
<guid>https://arxiv.org/abs/2512.18878</guid>
<content:encoded><![CDATA[
arXiv:2512.18878v1 Announce Type: cross 
Abstract: Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\% improvement in crash localization, and a 40\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</title>
<link>https://arxiv.org/abs/2512.18880</link>
<guid>https://arxiv.org/abs/2512.18880</guid>
<content:encoded><![CDATA[
arXiv:2512.18880v1 Announce Type: cross 
Abstract: Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</title>
<link>https://arxiv.org/abs/2512.18892</link>
<guid>https://arxiv.org/abs/2512.18892</guid>
<content:encoded><![CDATA[
arXiv:2512.18892v1 Announce Type: cross 
Abstract: We present a new approach to formulating and solving heterogeneous agent models with aggregate risk. We replace the cross-sectional distribution with low-dimensional prices as state variables and let agents learn equilibrium price dynamics directly from simulated paths. To do so, we introduce a structural reinforcement learning (SRL) method which treats prices via simulation while exploiting agents' structural knowledge of their own individual dynamics. Our SRL method yields a general and highly efficient global solution method for heterogeneous agent models that sidesteps the Master equation and handles problems traditional methods struggle with, in particular nontrivial market-clearing conditions. We illustrate the approach in the Krusell-Smith model, the Huggett model with aggregate shocks, and a HANK model with a forward-looking Phillips curve, all of which we solve globally within minutes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects</title>
<link>https://arxiv.org/abs/2512.18925</link>
<guid>https://arxiv.org/abs/2512.18925</guid>
<content:encoded><![CDATA[
arXiv:2512.18925v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.
  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</title>
<link>https://arxiv.org/abs/2512.18930</link>
<guid>https://arxiv.org/abs/2512.18930</guid>
<content:encoded><![CDATA[
arXiv:2512.18930v1 Announce Type: cross 
Abstract: Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18934</link>
<guid>https://arxiv.org/abs/2512.18934</guid>
<content:encoded><![CDATA[
arXiv:2512.18934v1 Announce Type: cross 
Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</title>
<link>https://arxiv.org/abs/2512.18950</link>
<guid>https://arxiv.org/abs/2512.18950</guid>
<content:encoded><![CDATA[
arXiv:2512.18950v1 Announce Type: cross 
Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning</title>
<link>https://arxiv.org/abs/2512.18969</link>
<guid>https://arxiv.org/abs/2512.18969</guid>
<content:encoded><![CDATA[
arXiv:2512.18969v1 Announce Type: cross 
Abstract: Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression</title>
<link>https://arxiv.org/abs/2512.18986</link>
<guid>https://arxiv.org/abs/2512.18986</guid>
<content:encoded><![CDATA[
arXiv:2512.18986v1 Announce Type: cross 
Abstract: Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2512.18991</link>
<guid>https://arxiv.org/abs/2512.18991</guid>
<content:encoded><![CDATA[
arXiv:2512.18991v1 Announce Type: cross 
Abstract: Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</title>
<link>https://arxiv.org/abs/2512.18999</link>
<guid>https://arxiv.org/abs/2512.18999</guid>
<content:encoded><![CDATA[
arXiv:2512.18999v1 Announce Type: cross 
Abstract: When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.19004</link>
<guid>https://arxiv.org/abs/2512.19004</guid>
<content:encoded><![CDATA[
arXiv:2512.19004v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.
  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results</title>
<link>https://arxiv.org/abs/2512.19007</link>
<guid>https://arxiv.org/abs/2512.19007</guid>
<content:encoded><![CDATA[
arXiv:2512.19007v1 Announce Type: cross 
Abstract: This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title>
<link>https://arxiv.org/abs/2512.19011</link>
<guid>https://arxiv.org/abs/2512.19011</guid>
<content:encoded><![CDATA[
arXiv:2512.19011v1 Announce Type: cross 
Abstract: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.
  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.
  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments</title>
<link>https://arxiv.org/abs/2512.19024</link>
<guid>https://arxiv.org/abs/2512.19024</guid>
<content:encoded><![CDATA[
arXiv:2512.19024v1 Announce Type: cross 
Abstract: Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \textbf{IndoorUAV-VLA} subset. Finally, we introduce \textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title>
<link>https://arxiv.org/abs/2512.19025</link>
<guid>https://arxiv.org/abs/2512.19025</guid>
<content:encoded><![CDATA[
arXiv:2512.19025v2 Announce Type: cross 
Abstract: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have "forgotten" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$\beta$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation</title>
<link>https://arxiv.org/abs/2512.19026</link>
<guid>https://arxiv.org/abs/2512.19026</guid>
<content:encoded><![CDATA[
arXiv:2512.19026v1 Announce Type: cross 
Abstract: The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</title>
<link>https://arxiv.org/abs/2512.19061</link>
<guid>https://arxiv.org/abs/2512.19061</guid>
<content:encoded><![CDATA[
arXiv:2512.19061v1 Announce Type: cross 
Abstract: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</title>
<link>https://arxiv.org/abs/2512.19097</link>
<guid>https://arxiv.org/abs/2512.19097</guid>
<content:encoded><![CDATA[
arXiv:2512.19097v1 Announce Type: cross 
Abstract: Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction</title>
<link>https://arxiv.org/abs/2512.19114</link>
<guid>https://arxiv.org/abs/2512.19114</guid>
<content:encoded><![CDATA[
arXiv:2512.19114v1 Announce Type: cross 
Abstract: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments</title>
<link>https://arxiv.org/abs/2512.19154</link>
<guid>https://arxiv.org/abs/2512.19154</guid>
<content:encoded><![CDATA[
arXiv:2512.19154v1 Announce Type: cross 
Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language-Policy Model for Dynamic Robot Task Planning</title>
<link>https://arxiv.org/abs/2512.19178</link>
<guid>https://arxiv.org/abs/2512.19178</guid>
<content:encoded><![CDATA[
arXiv:2512.19178v1 Announce Type: cross 
Abstract: Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Quantum-Classical Feature Fusion for complex data Classification</title>
<link>https://arxiv.org/abs/2512.19180</link>
<guid>https://arxiv.org/abs/2512.19180</guid>
<content:encoded><![CDATA[
arXiv:2512.19180v1 Announce Type: cross 
Abstract: Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.19184</link>
<guid>https://arxiv.org/abs/2512.19184</guid>
<content:encoded><![CDATA[
arXiv:2512.19184v1 Announce Type: cross 
Abstract: This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</title>
<link>https://arxiv.org/abs/2512.19199</link>
<guid>https://arxiv.org/abs/2512.19199</guid>
<content:encoded><![CDATA[
arXiv:2512.19199v1 Announce Type: cross 
Abstract: The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning</title>
<link>https://arxiv.org/abs/2512.19206</link>
<guid>https://arxiv.org/abs/2512.19206</guid>
<content:encoded><![CDATA[
arXiv:2512.19206v1 Announce Type: cross 
Abstract: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Minimal Fine-Tuning of VLMs</title>
<link>https://arxiv.org/abs/2512.19219</link>
<guid>https://arxiv.org/abs/2512.19219</guid>
<content:encoded><![CDATA[
arXiv:2512.19219v1 Announce Type: cross 
Abstract: We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Predicates Structuring urban perception with scene graphs</title>
<link>https://arxiv.org/abs/2512.19221</link>
<guid>https://arxiv.org/abs/2512.19221</guid>
<content:encoded><![CDATA[
arXiv:2512.19221v1 Announce Type: cross 
Abstract: Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title>
<link>https://arxiv.org/abs/2512.19238</link>
<guid>https://arxiv.org/abs/2512.19238</guid>
<content:encoded><![CDATA[
arXiv:2512.19238v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2512.19240</link>
<guid>https://arxiv.org/abs/2512.19240</guid>
<content:encoded><![CDATA[
arXiv:2512.19240v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics</title>
<link>https://arxiv.org/abs/2512.19247</link>
<guid>https://arxiv.org/abs/2512.19247</guid>
<content:encoded><![CDATA[
arXiv:2512.19247v1 Announce Type: cross 
Abstract: Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</title>
<link>https://arxiv.org/abs/2512.19253</link>
<guid>https://arxiv.org/abs/2512.19253</guid>
<content:encoded><![CDATA[
arXiv:2512.19253v2 Announce Type: cross 
Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation</title>
<link>https://arxiv.org/abs/2512.19275</link>
<guid>https://arxiv.org/abs/2512.19275</guid>
<content:encoded><![CDATA[
arXiv:2512.19275v1 Announce Type: cross 
Abstract: Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</title>
<link>https://arxiv.org/abs/2512.19280</link>
<guid>https://arxiv.org/abs/2512.19280</guid>
<content:encoded><![CDATA[
arXiv:2512.19280v1 Announce Type: cross 
Abstract: Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models</title>
<link>https://arxiv.org/abs/2512.19297</link>
<guid>https://arxiv.org/abs/2512.19297</guid>
<content:encoded><![CDATA[
arXiv:2512.19297v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture</title>
<link>https://arxiv.org/abs/2512.19311</link>
<guid>https://arxiv.org/abs/2512.19311</guid>
<content:encoded><![CDATA[
arXiv:2512.19311v1 Announce Type: cross 
Abstract: This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Achieving Superior Model Merging via Magnitude Calibration</title>
<link>https://arxiv.org/abs/2512.19320</link>
<guid>https://arxiv.org/abs/2512.19320</guid>
<content:encoded><![CDATA[
arXiv:2512.19320v1 Announce Type: cross 
Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative positional encoding functions for neural transformers</title>
<link>https://arxiv.org/abs/2512.19323</link>
<guid>https://arxiv.org/abs/2512.19323</guid>
<content:encoded><![CDATA[
arXiv:2512.19323v1 Announce Type: cross 
Abstract: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture</title>
<link>https://arxiv.org/abs/2512.19367</link>
<guid>https://arxiv.org/abs/2512.19367</guid>
<content:encoded><![CDATA[
arXiv:2512.19367v1 Announce Type: cross 
Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</title>
<link>https://arxiv.org/abs/2512.19379</link>
<guid>https://arxiv.org/abs/2512.19379</guid>
<content:encoded><![CDATA[
arXiv:2512.19379v1 Announce Type: cross 
Abstract: Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition</title>
<link>https://arxiv.org/abs/2512.19387</link>
<guid>https://arxiv.org/abs/2512.19387</guid>
<content:encoded><![CDATA[
arXiv:2512.19387v1 Announce Type: cross 
Abstract: Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.
  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.
  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.
  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research Program: Theory of Learning in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.19410</link>
<guid>https://arxiv.org/abs/2512.19410</guid>
<content:encoded><![CDATA[
arXiv:2512.19410v1 Announce Type: cross 
Abstract: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Is Not What You Need</title>
<link>https://arxiv.org/abs/2512.19428</link>
<guid>https://arxiv.org/abs/2512.19428</guid>
<content:encoded><![CDATA[
arXiv:2512.19428v1 Announce Type: cross 
Abstract: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.
  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.
  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</title>
<link>https://arxiv.org/abs/2512.19438</link>
<guid>https://arxiv.org/abs/2512.19438</guid>
<content:encoded><![CDATA[
arXiv:2512.19438v1 Announce Type: cross 
Abstract: Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations</title>
<link>https://arxiv.org/abs/2512.19456</link>
<guid>https://arxiv.org/abs/2512.19456</guid>
<content:encoded><![CDATA[
arXiv:2512.19456v1 Announce Type: cross 
Abstract: Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</title>
<link>https://arxiv.org/abs/2512.19472</link>
<guid>https://arxiv.org/abs/2512.19472</guid>
<content:encoded><![CDATA[
arXiv:2512.19472v1 Announce Type: cross 
Abstract: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis</title>
<link>https://arxiv.org/abs/2512.19481</link>
<guid>https://arxiv.org/abs/2512.19481</guid>
<content:encoded><![CDATA[
arXiv:2512.19481v1 Announce Type: cross 
Abstract: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</title>
<link>https://arxiv.org/abs/2512.19494</link>
<guid>https://arxiv.org/abs/2512.19494</guid>
<content:encoded><![CDATA[
arXiv:2512.19494v1 Announce Type: cross 
Abstract: The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast</title>
<link>https://arxiv.org/abs/2512.19506</link>
<guid>https://arxiv.org/abs/2512.19506</guid>
<content:encoded><![CDATA[
arXiv:2512.19506v1 Announce Type: cross 
Abstract: Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation</title>
<link>https://arxiv.org/abs/2512.19512</link>
<guid>https://arxiv.org/abs/2512.19512</guid>
<content:encoded><![CDATA[
arXiv:2512.19512v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.19516</link>
<guid>https://arxiv.org/abs/2512.19516</guid>
<content:encoded><![CDATA[
arXiv:2512.19516v1 Announce Type: cross 
Abstract: Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</title>
<link>https://arxiv.org/abs/2512.19530</link>
<guid>https://arxiv.org/abs/2512.19530</guid>
<content:encoded><![CDATA[
arXiv:2512.19530v1 Announce Type: cross 
Abstract: Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.
  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion</title>
<link>https://arxiv.org/abs/2512.19535</link>
<guid>https://arxiv.org/abs/2512.19535</guid>
<content:encoded><![CDATA[
arXiv:2512.19535v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</title>
<link>https://arxiv.org/abs/2512.19554</link>
<guid>https://arxiv.org/abs/2512.19554</guid>
<content:encoded><![CDATA[
arXiv:2512.19554v1 Announce Type: cross 
Abstract: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabyFlow: 3D modeling of realistic and expressive infant faces</title>
<link>https://arxiv.org/abs/2512.19560</link>
<guid>https://arxiv.org/abs/2512.19560</guid>
<content:encoded><![CDATA[
arXiv:2512.19560v1 Announce Type: cross 
Abstract: Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.19562</link>
<guid>https://arxiv.org/abs/2512.19562</guid>
<content:encoded><![CDATA[
arXiv:2512.19562v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the \pi_{0}, \pi_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.19564</link>
<guid>https://arxiv.org/abs/2512.19564</guid>
<content:encoded><![CDATA[
arXiv:2512.19564v1 Announce Type: cross 
Abstract: Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty</title>
<link>https://arxiv.org/abs/2512.19569</link>
<guid>https://arxiv.org/abs/2512.19569</guid>
<content:encoded><![CDATA[
arXiv:2512.19569v1 Announce Type: cross 
Abstract: Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge</title>
<link>https://arxiv.org/abs/2512.19570</link>
<guid>https://arxiv.org/abs/2512.19570</guid>
<content:encoded><![CDATA[
arXiv:2512.19570v1 Announce Type: cross 
Abstract: We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</title>
<link>https://arxiv.org/abs/2512.19576</link>
<guid>https://arxiv.org/abs/2512.19576</guid>
<content:encoded><![CDATA[
arXiv:2512.19576v2 Announce Type: cross 
Abstract: Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\"at W\"urzburg in cooperation with the Technische Universit\"at Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapTrace: Scalable Data Generation for Route Tracing on Maps</title>
<link>https://arxiv.org/abs/2512.19609</link>
<guid>https://arxiv.org/abs/2512.19609</guid>
<content:encoded><![CDATA[
arXiv:2512.19609v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the features used for summary evaluation by Human and GPT</title>
<link>https://arxiv.org/abs/2512.19620</link>
<guid>https://arxiv.org/abs/2512.19620</guid>
<content:encoded><![CDATA[
arXiv:2512.19620v1 Announce Type: cross 
Abstract: Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering with Label Consistency</title>
<link>https://arxiv.org/abs/2512.19654</link>
<guid>https://arxiv.org/abs/2512.19654</guid>
<content:encoded><![CDATA[
arXiv:2512.19654v1 Announce Type: cross 
Abstract: Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</title>
<link>https://arxiv.org/abs/2512.19663</link>
<guid>https://arxiv.org/abs/2512.19663</guid>
<content:encoded><![CDATA[
arXiv:2512.19663v1 Announce Type: cross 
Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</title>
<link>https://arxiv.org/abs/2512.19673</link>
<guid>https://arxiv.org/abs/2512.19673</guid>
<content:encoded><![CDATA[
arXiv:2512.19673v1 Announce Type: cross 
Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</title>
<link>https://arxiv.org/abs/2512.19678</link>
<guid>https://arxiv.org/abs/2512.19678</guid>
<content:encoded><![CDATA[
arXiv:2512.19678v1 Announce Type: cross 
Abstract: Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Guided Metaheuristic with Diversity Management for Solving the Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2407.20777</link>
<guid>https://arxiv.org/abs/2407.20777</guid>
<content:encoded><![CDATA[
arXiv:2407.20777v2 Announce Type: replace 
Abstract: We propose a feature-based guidance mechanism to enhance metaheuristic algorithms for solving the Capacitated Vehicle Routing Problem (CVRP). This mechanism leverages an Explainable AI (XAI) model to identify features that correlate with high-quality solutions. These insights are used to guide the search process by promoting solution diversity and avoiding premature convergence. The guidance mechanism is first integrated into a custom metaheuristic algorithm, which combines neighborhood search with a novel hybrid of the split algorithm and path relinking. Experiments on benchmark instances with up to $30,000$ customer nodes demonstrate that the guidance significantly improves the performance of this baseline algorithm. Furthermore, we validate the generalizability of the guidance approach by integrating it into a state-of-the-art metaheuristic, where it again yields statistically significant performance gains. These results confirm that the proposed mechanism is both scalable and transferable across algorithmic frameworks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imagining and building wise machines: The centrality of AI metacognition</title>
<link>https://arxiv.org/abs/2411.02478</link>
<guid>https://arxiv.org/abs/2411.02478</guid>
<content:encoded><![CDATA[
arXiv:2411.02478v3 Announce Type: replace 
Abstract: Although AI has become increasingly smart, its wisdom has not kept pace. In this article, we examine what is known about human wisdom and sketch a vision of its AI counterpart. We analyze human wisdom as a set of strategies for solving intractable problems-those outside the scope of analytic techniques-including both object-level strategies like heuristics [for managing problems] and metacognitive strategies like intellectual humility, perspective-taking, or context-adaptability [for managing object-level strategies]. We argue that AI systems particularly struggle with metacognition; improved metacognition would lead to AI more robust to novel environments, explainable to users, cooperative with others, and safer in risking fewer misaligned goals with human users. We discuss how wise AI might be benchmarked, trained, and implemented.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Causal Reasoning with (Non-Recursive) Structural Equation Models</title>
<link>https://arxiv.org/abs/2501.10190</link>
<guid>https://arxiv.org/abs/2501.10190</guid>
<content:encoded><![CDATA[
arXiv:2501.10190v2 Announce Type: replace 
Abstract: Structural Equation Models (SEM) are the standard approach to representing causal dependencies between variables in causal models. In this paper we propose a new interpretation of SEMs when reasoning about Actual Causality, in which SEMs are viewed as mechanisms transforming the dynamics of exogenous variables into the dynamics of endogenous variables. This allows us to combine counterfactual causal reasoning with existing temporal logic formalisms, and to introduce a temporal logic, CPLTL, for causal reasoning about such structures. We show that the standard restriction to so-called \textit{recursive} models (with no cycles in the dependency graph) is not necessary in our approach, allowing us to reason about mutually dependent processes and feedback loops. Finally, we introduce new notions of model equivalence for temporal causal models, and show that CPLTL has an efficient model-checking procedure.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market</title>
<link>https://arxiv.org/abs/2503.04521</link>
<guid>https://arxiv.org/abs/2503.04521</guid>
<content:encoded><![CDATA[
arXiv:2503.04521v2 Announce Type: replace 
Abstract: The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling</title>
<link>https://arxiv.org/abs/2505.11792</link>
<guid>https://arxiv.org/abs/2505.11792</guid>
<content:encoded><![CDATA[
arXiv:2505.11792v3 Announce Type: replace 
Abstract: Optimization modeling is fundamental to decision-making across diverse domains. Despite progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models against hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL), a novel framework that significantly improves the authenticity of LLMs for optimization modeling using Reinforcement Learning with Verifiable Reward by leveraging external optimization solvers as verifiers. These verifiers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality, serving as direct rewards for the RL process. This automated verification process, particularly from classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models. Our code is publicly available at https://github.com/Cardinal-Operations/SIRL.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting</title>
<link>https://arxiv.org/abs/2505.18822</link>
<guid>https://arxiv.org/abs/2505.18822</guid>
<content:encoded><![CDATA[
arXiv:2505.18822v2 Announce Type: replace 
Abstract: Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
<link>https://arxiv.org/abs/2505.23950</link>
<guid>https://arxiv.org/abs/2505.23950</guid>
<content:encoded><![CDATA[
arXiv:2505.23950v2 Announce Type: replace 
Abstract: As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: What essential capabilities are still missing? A critical aspect of human learning is continuous interaction with the environment -- not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support multi-turn, multimodal interaction. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present an initial exploration through the InterMT -- the first preference dataset for multi-turn multimodal interaction, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. InterMT captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of \InterMT through applications such as judge moderation and further reveal the multi-turn scaling law of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. Our project website can be found at https://pku-intermt.github.io .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2506.11712</link>
<guid>https://arxiv.org/abs/2506.11712</guid>
<content:encoded><![CDATA[
arXiv:2506.11712v3 Announce Type: replace 
Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Revealing Nuanced Biases in Medical LLMs</title>
<link>https://arxiv.org/abs/2507.21176</link>
<guid>https://arxiv.org/abs/2507.21176</guid>
<content:encoded><![CDATA[
arXiv:2507.21176v2 Announce Type: replace 
Abstract: Large language models (LLMs) used in medical applications are known to be prone to exhibiting biased and unfair patterns. Prior to deploying these in clinical decision-making, it is crucial to identify such bias patterns to enable effective mitigation and minimize negative impacts. In this study, we present a novel framework combining knowledge graphs (KGs) with auxiliary (agentic) LLMs to systematically reveal complex bias patterns in medical LLMs. The proposed approach integrates adversarial perturbation (red teaming) techniques to identify subtle bias patterns and adopts a customized multi-hop characterization of KGs to enhance the systematic evaluation of target LLMs. It aims not only to generate more effective red-teaming questions for bias evaluation but also to utilize those questions more effectively in revealing complex biases. Through a series of comprehensive experiments on three datasets, six LLMs, and five bias types, we demonstrate that our proposed framework exhibits a noticeably greater ability and scalability in revealing complex biased patterns of medical LLMs compared to other common approaches.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies</title>
<link>https://arxiv.org/abs/2507.22782</link>
<guid>https://arxiv.org/abs/2507.22782</guid>
<content:encoded><![CDATA[
arXiv:2507.22782v3 Announce Type: replace 
Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI reasoning effort predicts human decision time in content moderation</title>
<link>https://arxiv.org/abs/2508.20262</link>
<guid>https://arxiv.org/abs/2508.20262</guid>
<content:encoded><![CDATA[
arXiv:2508.20262v2 Announce Type: replace 
Abstract: Large language models can now generate intermediate reasoning steps before producing answers, improving performance on difficult problems by interactively developing solutions. This study uses a content moderation task to examine parallels between human decision times and model reasoning effort, measured using the length of the chain-of-thought (CoT). Across three frontier models, CoT length consistently predicts human decision time. Moreover, humans took longer and models produced longer CoTs when important variables were held constant, suggesting similar sensitivity to task difficulty. Analyses of the CoT content shows that models reference various contextual factors more frequently when making such decisions. These findings show parallels between human and AI reasoning on practical tasks and underscore the potential of reasoning traces for enhancing interpretability and decision-making.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents</title>
<link>https://arxiv.org/abs/2509.00251</link>
<guid>https://arxiv.org/abs/2509.00251</guid>
<content:encoded><![CDATA[
arXiv:2509.00251v2 Announce Type: replace 
Abstract: Large language models (LLMs) are fluent but largely static after pre-training; new or shifting knowledge is typically added with retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and engineering overhead and often fails to integrate facts; prompt engineering is brittle and can conflict with prior knowledge; fine-tuning is costly and risks catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS): curated system instructions act as external, auditable pseudo-parameters updated after each session via reflection and user feedback. A Reflection Engine inspects conversation traces, diagnoses reasoning successes and failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$ over instructions, user preferences, and tools. Deltas are version-controlled, evaluated with a sliding window of 1-5 star ratings, auto-repaired on first failure, and rolled back on repeated failure. When an edit budget crosses a threshold, the agent compiles a rating-weighted synthetic set and distills matured instruction-space gains into parameters, converting prompt-space improvements into weight-space without downtime. ILWS makes explicit the low-rank shaping induced by context in transformer blocks, preserves governance, and removes per-call retrieval. In enterprise support it increased throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved 4-5x more tickets per hour and about 80% lower time per ticket, with autonomous instruction updates and optional tool synthesis. Because ILWS operates at the instruction layer until controlled distillation, it generalizes to dynamic domains (legal, medical, engineering) requiring adaptive reasoning, tool creation, and low-latency deployment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning</title>
<link>https://arxiv.org/abs/2509.06278</link>
<guid>https://arxiv.org/abs/2509.06278</guid>
<content:encoded><![CDATA[
arXiv:2509.06278v3 Announce Type: replace 
Abstract: Table reasoning requires models to jointly perform comprehensive semantic understanding and precise numerical operations. Although recent large language model (LLM)-based methods have achieved promising results, most of them still rely on a single-turn reasoning paradigm that processes flattened tables in a single forward pass. This paradigm suffers from inherent limitations, including context overflow on large tables, weak sensitivity to continuous numerical values, and the absence of explicit tool-use and reflection. In this paper, we propose TableMind, a tuning-based autonomous programmatic table agent that simulates the human-like cognitive schema of the multi-turn interaction within a lightweight LLM. Instead of adopting a training-free workflow design, TableMind learns to internalize planning, action, and reflection through a principled two-stage training strategy. To bootstrap structured table reasoning capabilities, we construct and filter high-quality reasoning data for the supervised fine-tuning (SFT) stage. To enable precise code generation, we introduce a designed multi-perspective reward scheme and a novel optimization objective in the reinforcement learning (RL) stage. Extensive experiments on diverse benchmarks demonstrate that TableMind consistently outperforms previous baselines, validating the effectiveness of training autonomous agents to improve overall performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems</title>
<link>https://arxiv.org/abs/2509.08713</link>
<guid>https://arxiv.org/abs/2509.08713</guid>
<content:encoded><![CDATA[
arXiv:2509.08713v2 Announce Type: replace 
Abstract: AI scientist systems, capable of autonomously executing the full research workflow from hypothesis generation and experimentation to paper writing, hold significant potential for accelerating scientific discovery. However, the internal workflow of these systems have not been closely examined. This lack of scrutiny poses a risk of introducing flaws that could undermine the integrity, reliability, and trustworthiness of their research outputs. In this paper, we identify four potential failure modes in contemporary AI scientist systems: inappropriate benchmark selection, data leakage, metric misuse, and post-hoc selection bias. To examine these risks, we design controlled experiments that isolate each failure mode while addressing challenges unique to evaluating AI scientist systems. Our assessment of two prominent open-source AI scientist systems reveals the presence of several failures, across a spectrum of severity, which can be easily overlooked in practice. Finally, we demonstrate that access to trace logs and code from the full automated workflow enables far more effective detection of such failures than examining the final paper alone. We thus recommend journals and conferences evaluating AI-generated research to mandate submission of these artifacts alongside the paper to ensure transparency, accountability, and reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v3 Announce Type: replace 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Tournament Solutions with Minimal Supports</title>
<link>https://arxiv.org/abs/2509.09312</link>
<guid>https://arxiv.org/abs/2509.09312</guid>
<content:encoded><![CDATA[
arXiv:2509.09312v4 Announce Type: replace 
Abstract: Tournaments are widely used models to represent pairwise dominance between candidates, alternatives, or teams. We study the problem of providing certified explanations for why a candidate appears among the winners under various tournament rules. To this end, we identify minimal supports, minimal sub-tournaments in which the candidate is guaranteed to win regardless of how the rest of the tournament is completed (that is, the candidate is a necessary winner of the sub-tournament). This notion corresponds to an abductive explanation for the question,"Why does the winner win the tournament?", a central concept in formal explainable AI. We focus on common tournament solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule, the maximin rule, and the weighted uncovered set. For each rule we determine the size of the smallest minimal supports, and we present polynomial-time algorithms to compute them for all solutions except for the weighted uncovered set, for which the problem is NP-complete. Finally, we show how minimal supports can serve to produce compact, certified, and intuitive explanations for tournament solutions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Abduction: A New Paradigm for Reasoning under Uncertainty</title>
<link>https://arxiv.org/abs/2509.16958</link>
<guid>https://arxiv.org/abs/2509.16958</guid>
<content:encoded><![CDATA[
arXiv:2509.16958v2 Announce Type: replace 
Abstract: Abductive reasoning - the search for plausible explanations - has long been central to human inquiry, from forensics to medicine and scientific discovery. Yet formal approaches in AI have largely reduced abduction to eliminative search: hypotheses are treated as mutually exclusive, evaluated against consistency constraints or probability updates, and pruned until a single "best" explanation remains. This reductionist framing overlooks the way human reasoners sustain multiple explanatory lines in suspension, navigate contradictions, and generate novel syntheses. This paper introduces quantum abduction, a non-classical paradigm that models hypotheses in superposition, allows them to interfere constructively or destructively, and collapses only when coherence with evidence is reached. Grounded in quantum cognition and implemented with modern NLP embeddings and generative AI, the framework supports dynamic synthesis rather than premature elimination. Case studies span historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"), literary demonstrations ("Murder on the Orient Express"), medical diagnosis, and scientific theory change. Across these domains, quantum abduction proves more faithful to the constructive and multifaceted nature of human reasoning, while offering a pathway toward expressive and transparent AI reasoning systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether</title>
<link>https://arxiv.org/abs/2509.23004</link>
<guid>https://arxiv.org/abs/2509.23004</guid>
<content:encoded><![CDATA[
arXiv:2509.23004v2 Announce Type: replace 
Abstract: Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein's relativistic laws, and several other laws.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI</title>
<link>https://arxiv.org/abs/2510.04978</link>
<guid>https://arxiv.org/abs/2510.04978</guid>
<content:encoded><![CDATA[
arXiv:2510.04978v4 Announce Type: replace 
Abstract: The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</title>
<link>https://arxiv.org/abs/2510.09595</link>
<guid>https://arxiv.org/abs/2510.09595</guid>
<content:encoded><![CDATA[
arXiv:2510.09595v2 Announce Type: replace 
Abstract: Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Vibe Coding with Large Language Models</title>
<link>https://arxiv.org/abs/2510.12399</link>
<guid>https://arxiv.org/abs/2510.12399</guid>
<content:encoded><![CDATA[
arXiv:2510.12399v2 Announce Type: replace 
Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed "Vibe Coding" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena</title>
<link>https://arxiv.org/abs/2510.17638</link>
<guid>https://arxiv.org/abs/2510.17638</guid>
<content:encoded><![CDATA[
arXiv:2510.17638v2 Announce Type: replace 
Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call "LLM-as-a-Prophet". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription</title>
<link>https://arxiv.org/abs/2510.22295</link>
<guid>https://arxiv.org/abs/2510.22295</guid>
<content:encoded><![CDATA[
arXiv:2510.22295v2 Announce Type: replace 
Abstract: Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI</title>
<link>https://arxiv.org/abs/2511.13524</link>
<guid>https://arxiv.org/abs/2511.13524</guid>
<content:encoded><![CDATA[
arXiv:2511.13524v2 Announce Type: replace 
Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks. To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning</title>
<link>https://arxiv.org/abs/2512.04618</link>
<guid>https://arxiv.org/abs/2512.04618</guid>
<content:encoded><![CDATA[
arXiv:2512.04618v2 Announce Type: replace 
Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three methods, one problem: Classical and AI approaches to no-three-in-line</title>
<link>https://arxiv.org/abs/2512.11469</link>
<guid>https://arxiv.org/abs/2512.11469</guid>
<content:encoded><![CDATA[
arXiv:2512.11469v2 Announce Type: replace 
Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop multilingual parallel corpus with multi-word expression annotation</title>
<link>https://arxiv.org/abs/2011.03783</link>
<guid>https://arxiv.org/abs/2011.03783</guid>
<content:encoded><![CDATA[
arXiv:2011.03783v2 Announce Type: replace-cross 
Abstract: In this work, we introduce the construction of a machine translation (MT) assisted and human-in-the-loop multilingual parallel corpus with annotations of multi-word expressions (MWEs), named AlphaMWE. The MWEs include verbal MWEs (vMWEs) defined in the PARSEME shared task that have a verb as the head of the studied terms. The annotated vMWEs are also bilingually and multilingually aligned manually. The languages covered include Arabic, Chinese, English, German, Italian, and Polish, of which, the Arabic corpus includes both standard and dialectal variations from Egypt and Tunisia. Our original English corpus is extracted from the PARSEME shared task in 2018. We performed machine translation of this source corpus followed by human post-editing and annotation of target MWEs. Strict quality control was applied for error limitation, i.e., each MT output sentence received first manual post-editing and annotation plus a second manual quality rechecking till annotators' consensus is reached. One of our findings during corpora preparation is that accurate translation of MWEs presents challenges to MT systems, as reflected by the outcomes of human-in-the-loop metric HOPE. To facilitate further MT research, we present a categorisation of the error types encountered by MT systems in performing MWE-related translation. To acquire a broader view of MT issues, we selected four popular state-of-the-art MT systems for comparison, namely Microsoft Bing Translator, GoogleMT, Baidu Fanyi, and DeepL MT. Because of the noise removal, translation post-editing, and MWE annotation by human professionals, we believe the AlphaMWE data set will be an asset for both monolingual and cross-lingual research, such as multi-word term lexicography, MT, and information extraction.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs</title>
<link>https://arxiv.org/abs/2306.00029</link>
<guid>https://arxiv.org/abs/2306.00029</guid>
<content:encoded><![CDATA[
arXiv:2306.00029v2 Announce Type: replace-cross 
Abstract: Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors</title>
<link>https://arxiv.org/abs/2401.16310</link>
<guid>https://arxiv.org/abs/2401.16310</guid>
<content:encoded><![CDATA[
arXiv:2401.16310v5 Announce Type: replace-cross 
Abstract: Security code review is a time-consuming and labor-intensive process typically requiring integration with automated security defect detection tools. However, existing security analysis tools struggle with poor generalization, high false positive rates, and coarse detection granularity. Large Language Models (LLMs) have been considered promising candidates for addressing those challenges. In this study, we conducted an empirical study to explore the potential of LLMs in detecting security defects during code review. Specifically, we evaluated the performance of seven LLMs under five different prompts and compared them with state-of-the-art static analysis tools. We also performed linguistic and regression analyses for the two top-performing LLMs to identify quality problems in their responses and factors influencing their performance. Our findings show that: (1) In security code review, LLMs significantly outperform state-of-the-art static analysis tools, and the reasoning-optimized LLM performs better than general-purpose LLMs. (2) DeepSeek-R1 achieves the highest performance, followed by GPT-4. The optimal prompt for DeepSeek-R1 incorporates both the commit message and chain-of-thought (CoT) guidance, while for GPT-4, the prompt with a Common Weakness Enumeration (CWE) list works best. (3) GPT-4 frequently produces vague expressions and exhibits difficulties in accurately following instructions in the prompts, while DeepSeek-R1 more commonly generates inaccurate code details in its outputs. (4) LLMs are more adept at identifying security defects in code files that have fewer tokens and security-relevant annotations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling</title>
<link>https://arxiv.org/abs/2402.06118</link>
<guid>https://arxiv.org/abs/2402.06118</guid>
<content:encoded><![CDATA[
arXiv:2402.06118v4 Announce Type: replace-cross 
Abstract: By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we released our human annotation (https://github.com/amazon-science/vigor) comprising 15,440 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Transformers: A Survey</title>
<link>https://arxiv.org/abs/2407.09777</link>
<guid>https://arxiv.org/abs/2407.09777</guid>
<content:encoded><![CDATA[
arXiv:2407.09777v2 Announce Type: replace-cross 
Abstract: Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Retrieval with Few-shot Indexing</title>
<link>https://arxiv.org/abs/2408.02152</link>
<guid>https://arxiv.org/abs/2408.02152</guid>
<content:encoded><![CDATA[
arXiv:2408.02152v3 Announce Type: replace-cross 
Abstract: Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning</title>
<link>https://arxiv.org/abs/2408.10566</link>
<guid>https://arxiv.org/abs/2408.10566</guid>
<content:encoded><![CDATA[
arXiv:2408.10566v5 Announce Type: replace-cross 
Abstract: In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation</title>
<link>https://arxiv.org/abs/2408.11607</link>
<guid>https://arxiv.org/abs/2408.11607</guid>
<content:encoded><![CDATA[
arXiv:2408.11607v3 Announce Type: replace-cross 
Abstract: Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Reliable are Causal Probing Interventions?</title>
<link>https://arxiv.org/abs/2408.15510</link>
<guid>https://arxiv.org/abs/2408.15510</guid>
<content:encoded><![CDATA[
arXiv:2408.15510v5 Announce Type: replace-cross 
Abstract: Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.
  Our project webpage is available at: https://ahdavies6.github.io/causal_probing_reliability/
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</title>
<link>https://arxiv.org/abs/2409.01382</link>
<guid>https://arxiv.org/abs/2409.01382</guid>
<content:encoded><![CDATA[
arXiv:2409.01382v2 Announce Type: replace-cross 
Abstract: The adoption of Large Language Models (LLMs) for code generation risks incorporating vulnerable code into software systems. Existing detectors face two critical limitations: a lack of systematic cross-model validation and opaque "black box" operation. We address this through a comparative study of code generated by four distinct LLMs: GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS.
  Analyzing 14,485 Python functions and 11,913 classes from the CodeSearchNet dataset, we generated corresponding code with all four LLMs. Using interpretable software metrics, we trained CatBoost classifiers for each configuration. Our analysis reveals that granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap, indicating that function-level and class-level detection rely on fundamentally disjoint structural signatures.
  We discover critical granularity-dependent inversions: while modern models (Claude, GPT-OSS) are more detectable at the class level, GPT-3.5 is an anomaly that uniquely excels at the function level. SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models, explaining why detectors trained on specific LLMs fail to generalize.
  Our findings demonstrate that GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80). Robust detection requires moving beyond single-model studies to account for substantial diversity in structural fingerprints across architectures and granularities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments</title>
<link>https://arxiv.org/abs/2410.15178</link>
<guid>https://arxiv.org/abs/2410.15178</guid>
<content:encoded><![CDATA[
arXiv:2410.15178v4 Announce Type: replace-cross 
Abstract: Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</title>
<link>https://arxiv.org/abs/2410.24116</link>
<guid>https://arxiv.org/abs/2410.24116</guid>
<content:encoded><![CDATA[
arXiv:2410.24116v2 Announce Type: replace-cross 
Abstract: Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision</title>
<link>https://arxiv.org/abs/2411.01431</link>
<guid>https://arxiv.org/abs/2411.01431</guid>
<content:encoded><![CDATA[
arXiv:2411.01431v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs upon real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate the above computational gap and enable ubiquitous embedded intelligence, we, in this survey, focus on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</title>
<link>https://arxiv.org/abs/2411.12150</link>
<guid>https://arxiv.org/abs/2411.12150</guid>
<content:encoded><![CDATA[
arXiv:2411.12150v3 Announce Type: replace-cross 
Abstract: We study the problem of robot navigation in dense and interactive crowds with static constraints such as corridors and furniture. Previous methods fail to consider all types of spatial and temporal interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different inputs and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous spatio-temporal graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success, navigation time, and generalization to domain shifts in challenging navigation scenarios. More information is available at https://sites.google.com/view/crowdnav-height/home.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Distribution Learning for Graph Classification</title>
<link>https://arxiv.org/abs/2411.15206</link>
<guid>https://arxiv.org/abs/2411.15206</guid>
<content:encoded><![CDATA[
arXiv:2411.15206v4 Announce Type: replace-cross 
Abstract: Leveraging the diversity and quantity of data provided by various graph-structured data augmentations while preserving intrinsic semantic information is challenging. Additionally, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while graph contrastive learning aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of negative pairs via intraviews. In this paper, we propose a conditional distribution learning (CDL) method that learns graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment enables the CDL model to effectively preserve intrinsic semantic information when both weak and strong augmentations are applied to graph-structured data. To avoid the conflict between the MPM and the CL of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed CDL method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Human-Horse Interactions may Teach us About Effective Human-AI Interactions</title>
<link>https://arxiv.org/abs/2412.13405</link>
<guid>https://arxiv.org/abs/2412.13405</guid>
<content:encoded><![CDATA[
arXiv:2412.13405v2 Announce Type: replace-cross 
Abstract: This article explores human-horse interactions as a metaphor for understanding and designing effective human-AI partnerships. Drawing on the long history of human collaboration with horses, we propose that AI, like horses, should complement rather than replace human capabilities. We move beyond traditional benchmarks such as the Turing test, which emphasize AI's ability to mimic human intelligence, and instead advocate for a symbiotic relationship where distinct intelligences enhance each other. We analyze key elements of human-horse relationships: trust, communication, and mutual adaptability, to highlight essential principles for human-AI collaboration. Trust is critical in both partnerships, built through predictability and shared understanding, while communication and feedback loops foster mutual adaptability. We further discuss the importance of taming and habituation in shaping these interactions, likening it to how humans train AI to perform reliably and ethically in real-world settings. The article also addresses the asymmetry of responsibility, where humans ultimately bear the greater burden of oversight and ethical judgment. Finally, we emphasize that long-term commitment and continuous learning are vital in both human-horse and human-AI relationships, as ongoing interaction refines the partnership and increases mutual adaptability. By drawing on these insights from human-horse interactions, we offer a vision for designing AI systems that are trustworthy, adaptable, and capable of fostering symbiotic human-AI partnerships.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks</title>
<link>https://arxiv.org/abs/2412.14031</link>
<guid>https://arxiv.org/abs/2412.14031</guid>
<content:encoded><![CDATA[
arXiv:2412.14031v5 Announce Type: replace-cross 
Abstract: In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2412.14579</link>
<guid>https://arxiv.org/abs/2412.14579</guid>
<content:encoded><![CDATA[
arXiv:2412.14579v2 Announce Type: replace-cross 
Abstract: Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-Aware DNN Compression for Homogeneous Edge Devices</title>
<link>https://arxiv.org/abs/2501.15240</link>
<guid>https://arxiv.org/abs/2501.15240</guid>
<content:encoded><![CDATA[
arXiv:2501.15240v2 Announce Type: replace-cross 
Abstract: Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Experiments on ResNet50 and MobileNetV1 with the ImageNet dataset show that HDAP consistently achieves lower average inference latency compared with state-of-the-art methods, with substantial speedup gains (e.g., 2.86 $\times$ speedup at 1.0G FLOPs for ResNet50) on the homogeneous device clusters. HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Generative AI for Video-to-Music Generation</title>
<link>https://arxiv.org/abs/2502.12489</link>
<guid>https://arxiv.org/abs/2502.12489</guid>
<content:encoded><![CDATA[
arXiv:2502.12489v2 Announce Type: replace-cross 
Abstract: The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models</title>
<link>https://arxiv.org/abs/2503.01298</link>
<guid>https://arxiv.org/abs/2503.01298</guid>
<content:encoded><![CDATA[
arXiv:2503.01298v3 Announce Type: replace-cross 
Abstract: Unified generative models have shown remarkable performance in text and image generation. For image synthesis tasks, they adopt straightforward text-to-image (T2I) generation. However, direct T2I generation limits the models in handling complex compositional instructions, which frequently occur in real-world scenarios. Although this issue is vital, existing works mainly focus on improving the basic image generation capability of the models. While such improvements help to some extent, they still fail to adequately resolve the problem. Inspired by Chain of Thought (CoT) solving complex problems step by step, this work aims to introduce CoT into unified generative models to address the challenges of complex image generation that direct T2I generation cannot effectively solve, thereby endowing models with enhanced image generation ability. To achieve this, we first propose Functionality-oriented eXperts (FoXperts), an expert-parallel architecture in our model FoX, which assigns experts by function. FoXperts disentangles potential conflicts in mainstream modality-oriented designs and provides a solid foundation for CoT. When introducing CoT, the first question is how to design it for complex image generation. To this end, we emulate a human-like artistic workflow--planning, acting, reflection, and correction--and propose the Multimodal Chain of Thought (MCoT) approach, as the data involves both text and image. To address the subsequent challenge of designing an effective MCoT training paradigm, we develop a multi-task joint training scheme that equips the model with all capabilities required for each MCoT step in a disentangled manner. This paradigm avoids the difficulty of collecting consistent multi-step data tuples. Extensive experiments show that FoX consistently outperforms existing unified models on various T2I benchmarks, delivering notable improvements in complex image generation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning</title>
<link>https://arxiv.org/abs/2503.02341</link>
<guid>https://arxiv.org/abs/2503.02341</guid>
<content:encoded><![CDATA[
arXiv:2503.02341v2 Announce Type: replace-cross 
Abstract: Recent great advances in video generation models have demonstrated their potential to produce high-quality videos, bringing challenges to effective evaluation. Unlike human evaluation, existing automated evaluation metrics lack highlevel semantic understanding and reasoning capabilities for video, thus making them infeasible and unexplainable. To fill this gap, we curate GRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset, including 3.3k videos from over 10 existing video generation models and multi-step reasoning assessments converted by 16k human annotations. We then introduce GRADEO, one of the first specifically designed video evaluation models, which grades AI-generated videos for explainable scores and assessments through multi-step reasoning. Experiments show that our method aligns better with human evaluations than existing methods. Furthermore, our benchmarking reveals that current video generation models struggle to produce content that aligns with human reasoning and complex real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.04162</link>
<guid>https://arxiv.org/abs/2503.04162</guid>
<content:encoded><![CDATA[
arXiv:2503.04162v5 Announce Type: replace-cross 
Abstract: Contrastive learning has shown effectiveness in improving sequential recommendation models. However, existing methods still face challenges in generating high-quality contrastive pairs: they either rely on random perturbations that corrupt user preference patterns or depend on sparse collaborative data that generates unreliable contrastive pairs. Furthermore, existing approaches typically require predefined selection rules that impose strong assumptions, limiting the model's ability to autonomously learn optimal contrastive pairs. To address these limitations, we propose a novel approach named Semantic Retrieval Augmented Contrastive Learning (SRA-CL). SRA-CL leverages the semantic understanding and reasoning capabilities of LLMs to generate expressive embeddings that capture both user preferences and item characteristics. These semantic embeddings enable the construction of candidate pools for inter-user and intra-user contrastive learning through semantic-based retrieval. To further enhance the quality of the contrastive samples, we introduce a learnable sample synthesizer that optimizes the contrastive sample generation process during model training. SRA-CL adopts a plug-and-play design, enabling seamless integration with existing sequential recommendation architectures. Extensive experiments on four public datasets demonstrate the effectiveness and model-agnostic nature of our approach.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bleeding Pathways: Vanishing Discriminability in LLM Hidden States Fuels Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2503.11185</link>
<guid>https://arxiv.org/abs/2503.11185</guid>
<content:encoded><![CDATA[
arXiv:2503.11185v2 Announce Type: replace-cross 
Abstract: LLMs remain vulnerable to jailbreak attacks that exploit adversarial prompts to circumvent safety measures. Current safety fine-tuning approaches face two critical limitations. First, they often fail to strike a balance between security and utility, where stronger safety measures tend to over-reject harmless user requests. Second, they frequently miss malicious intent concealed within seemingly benign tasks, leaving models exposed to exploitation. Our work identifies a fundamental cause of these issues: during response generation, an LLM's capacity to differentiate harmful from safe outputs deteriorates. Experimental evidence confirms this, revealing that the separability between hidden states for safe and harmful responses diminishes as generation progresses. This weakening discrimination forces models to make compliance judgments earlier in the generation process, restricting their ability to recognize developing harmful intent and contributing to both aforementioned failures. To mitigate this vulnerability, we introduce DEEPALIGN - an inherent defense framework that enhances the safety of LLMs. By applying contrastive hidden-state steering at the midpoint of response generation, DEEPALIGN amplifies the separation between harmful and benign hidden states, enabling continuous intrinsic toxicity detection and intervention throughout the generation process. Across diverse LLMs spanning varying architectures and scales, it reduced attack success rates of nine distinct jailbreak attacks to near-zero or minimal. Crucially, it preserved model capability while reducing over-refusal. Models equipped with DEEPALIGN exhibited up to 3.5% lower error rates in rejecting challenging benign queries and maintained standard task performance with less than 1% decline. This marks a substantial advance in the safety-utility Pareto frontier.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comp-Attn: Present-and-Align Attention for Compositional Video Generation</title>
<link>https://arxiv.org/abs/2503.14428</link>
<guid>https://arxiv.org/abs/2503.14428</guid>
<content:encoded><![CDATA[
arXiv:2503.14428v2 Announce Type: replace-cross 
Abstract: In the domain of text-to-video (T2V) generation, reliably synthesizing compositional content involving multiple subjects with intricate relations is still underexplored. The main challenges are twofold: 1) Subject presence, where not all subjects can be presented in the video; 2) Inter-subject relations, where the interaction and spatial relationship between subjects are misaligned. Existing methods adopt techniques, such as inference-time latent optimization or layout control, which fail to address both issues simultaneously. To tackle these problems, we propose Comp-Attn, a composition-aware cross-attention variant that follows a Present-and-Align paradigm: it decouples the two challenges by enforcing subject presence at the condition level and achieving relational alignment at the attention-distribution level. Specifically, 1) We introduce Subject-aware Condition Interpolation (SCI) to reinforce subject-specific conditions and ensure each subject's presence; 2) We propose Layout-forcing Attention Modulation (LAM), which dynamically enforces the attention distribution to align with the relational layout of multiple subjects. Comp-Attn can be seamlessly integrated into various T2V baselines in a training-free manner, boosting T2V-CompBench scores by 15.7\% and 11.7\% on Wan2.1-T2V-14B and Wan2.2-T2V-A14B with only a 5\% increase in inference time. Meanwhile, it also achieves strong performance on VBench and T2I-CompBench, demonstrating its scalability in general video generation and compositional text-to-image (T2I) tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs</title>
<link>https://arxiv.org/abs/2504.14757</link>
<guid>https://arxiv.org/abs/2504.14757</guid>
<content:encoded><![CDATA[
arXiv:2504.14757v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are transforming automated program repair (APR) through agent-based approaches that localize bugs, generate patches, and verify fixes. However, the lack of high-quality, scalable training datasets, especially those with verifiable outputs and intermediate reasoning traces-limits progress, particularly for open-source models. In this work, we present SWE-Synth, a framework for synthesizing realistic, verifiable, and process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM agents to simulate debugging workflows, producing not only bug-fix pairs but also test cases and structured repair trajectories. Compared to manually curated datasets, our method scales with minimal human effort while preserving contextual richness and correctness. Experiments show that models trained on SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench Lite. Our results highlight the potential of synthetic, agent-generated data to advance the state of the art in APR and software engineering automation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference</title>
<link>https://arxiv.org/abs/2504.17129</link>
<guid>https://arxiv.org/abs/2504.17129</guid>
<content:encoded><![CDATA[
arXiv:2504.17129v2 Announce Type: replace-cross 
Abstract: Dynamic game theory is a powerful tool in modeling multi-agent interactions and human-robot systems. In practice, since the objective functions of both agents may not be explicitly known to each other, these interactions can be modeled as incomplete-information general-sum dynamic games. Solving for equilibrium policies for such games presents a major challenge, especially if the games involve nonlinear underlying dynamics. To simplify the problem, existing work often assumes that one agent is an expert with complete information about its peer, which can lead to biased estimates and failures in coordination. To address this challenge, we propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for general-sum dynamic games. In N-PACE, using iterative linear quadratic (ILQ) approximation of dynamic games, each agent explicitly models the learning dynamics of its peer agent while inferring their objective functions and updating its own control policy accordingly in real time, which leads to unbiased and fast learning of the unknown objective function of the peer agent. Additionally, we demonstrate how N-PACE enables intent communication by explicitly modeling the peer's learning dynamics. Finally, we show how N-PACE outperforms baseline methods that disregard the learning behavior of the other agent, both analytically and using our case studies
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Nonlinear Model Predictive Control</title>
<link>https://arxiv.org/abs/2505.01353</link>
<guid>https://arxiv.org/abs/2505.01353</guid>
<content:encoded><![CDATA[
arXiv:2505.01353v2 Announce Type: replace-cross 
Abstract: The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.01912</link>
<guid>https://arxiv.org/abs/2505.01912</guid>
<content:encoded><![CDATA[
arXiv:2505.01912v2 Announce Type: replace-cross 
Abstract: Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\mathbf{BOOM}$, $\mathbf{b}$enchmarks for $\mathbf{o}$ut-$\mathbf{o}$f-distribution $\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at https://github.com/FLASK-LLNL/BOOM
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focus on Likely Classes for Test-Time Prediction</title>
<link>https://arxiv.org/abs/2505.03819</link>
<guid>https://arxiv.org/abs/2505.03819</guid>
<content:encoded><![CDATA[
arXiv:2505.03819v3 Announce Type: replace-cross 
Abstract: We ask: Can focusing on likely classes of a single, in-domain sample improve model predictions? Prior work argued ``no''. We put forward a novel rationale in favor of ``yes'': Sharedness of features among classes indicates their reliability for a single sample. We aim for an affirmative answer without using hand-engineered augmentations or auxiliary tasks. We propose two novel test-time fine-tuning methods to improve uncertain model predictions. Instead of greedily selecting the most likely class, we introduce an additional step, \emph{focus on the likely classes}, to refine predictions. By applying a single gradient descent step with a large learning rate, we refine predictions when an initial forward pass indicates high uncertainty. The experimental evaluation demonstrates accuracy gains for one of our methods on average, which emphasizes shared features among likely classes. The gains are confirmed across diverse text and image domain models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
arXiv:2505.08438v3 Announce Type: replace-cross 
Abstract: Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Exploration of Default Images in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.09166</link>
<guid>https://arxiv.org/abs/2505.09166</guid>
<content:encoded><![CDATA[
arXiv:2505.09166v5 Announce Type: replace-cross 
Abstract: In the creative practice of text-to-image (TTI) generation, images are synthesized from textual prompts. By design, TTI models always yield an output, even if the prompt contains unknown terms. In this case, the model may generate default images: images that closely resemble each other across many unrelated prompts. Studying default images is valuable for designing better solutions for prompt engineering and TTI generation. We present the first investigation into default images on Midjourney. We describe an initial study in which we manually created input prompts triggering default images, and several ablation studies. Building on these, we conduct a computational analysis of over 750,000 images, revealing consistent default images across unrelated prompts. We also conduct an online user study investigating how default images may affect user satisfaction. Our work lays the foundation for understanding default images in TTI generation, highlighting their practical relevance as well as challenges and future research directions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Based Neural Quantum Digital Twins for Many-Body Quantum Simulation and Optimal Annealing Schedule Design</title>
<link>https://arxiv.org/abs/2505.15662</link>
<guid>https://arxiv.org/abs/2505.15662</guid>
<content:encoded><![CDATA[
arXiv:2505.15662v2 Announce Type: replace-cross 
Abstract: We introduce Transformer-based Neural Quantum Digital Twins (Tx-NQDTs) to simulate full adiabatic dynamics of many-body quantum systems, including ground and low-lying excited states, at low computational cost. Tx-NQDTs employ a graph-informed Transformer neural network trained to predict spectral properties (energy levels and gap locations) needed for annealing schedule design. We integrate these predictions with an adaptive annealing schedule design based on first-order adiabatic perturbation theory (FOAPT), which slows the evolution near predicted small gaps to maintain adiabaticity. Experiments on a D-Wave quantum annealer (N = 10, 15, 20 qubits, 12 control segments) show that Tx-NQDT-informed schedules significantly improve success probabilities despite hardware noise and calibration drift. The optimized schedules achieve success probabilities 2.2-11.7 percentage points higher than the default linear schedule, outperforming the D-Wave baseline in 44 of 60 cases. These results demonstrate a practical, data-driven route to improved quantum annealing performance on real hardware.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERDI: VLM-Embedded Reasoning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.15925</link>
<guid>https://arxiv.org/abs/2505.15925</guid>
<content:encoded><![CDATA[
arXiv:2505.15925v3 Announce Type: replace-cross 
Abstract: While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAEs Are Good for Steering -- If You Select the Right Features</title>
<link>https://arxiv.org/abs/2505.20063</link>
<guid>https://arxiv.org/abs/2505.20063</guid>
<content:encoded><![CDATA[
arXiv:2505.20063v2 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v3 Announce Type: replace-cross 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v2 Announce Type: replace-cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor. Previous work has shown that transfer learning can be used effectively with ESNs for single-orbit prediction. The novelty of our paper lies in our use of this pairing to predict the long-term statistical properties of spatiotemporally chaotic PDEs. We also show that transfer learning nontrivially improves the length of time that predictions of individual gKS trajectories remain accurate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v4 Announce Type: replace-cross 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WANDER: An Explainable Decision-Support Framework for HPC</title>
<link>https://arxiv.org/abs/2506.04049</link>
<guid>https://arxiv.org/abs/2506.04049</guid>
<content:encoded><![CDATA[
arXiv:2506.04049v2 Announce Type: replace-cross 
Abstract: High-performance computing (HPC) systems expose many interdependent configuration knobs that impact runtime, resource usage, power, and variability. Existing predictive tools model these outcomes, but do not support structured exploration, explanation, or guided reconfiguration. We present WANDER, a decision-support framework that synthesizes alternate configurations using counterfactual analysis aligned with user goals and constraints. We introduce a composite trade-off score that ranks suggestions based on prediction uncertainty, consistency between feature-target relationships using causal models, and similarity between feature distributions against historical data. To our knowledge, WANDER is the first such system to unify prediction, exploration, and explanation for HPC tuning under a common query interface. Across multiple datasets WANDER generates interpretable and trustworthy, human-readable alternatives that guide users to achieve their performance objectives.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training</title>
<link>https://arxiv.org/abs/2506.10035</link>
<guid>https://arxiv.org/abs/2506.10035</guid>
<content:encoded><![CDATA[
arXiv:2506.10035v2 Announce Type: replace-cross 
Abstract: Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\% of the hierarchy pruned. Our code will be available soon.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Free Policy Optimization via Reward Partitioning</title>
<link>https://arxiv.org/abs/2506.13702</link>
<guid>https://arxiv.org/abs/2506.13702</guid>
<content:encoded><![CDATA[
arXiv:2506.13702v3 Announce Type: replace-cross 
Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subspace-Boosted Model Merging</title>
<link>https://arxiv.org/abs/2506.16506</link>
<guid>https://arxiv.org/abs/2506.16506</guid>
<content:encoded><![CDATA[
arXiv:2506.16506v3 Announce Type: replace-cross 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we empirically and theoretically analyze this limitation, proving that for Task Arithmetic-based methods, as more experts are merged, the common information dominates the task-specific information, leading to inevitable rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging. Code and models are available at https://github.com/ronskoro/Subspace-Boosting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Words to Proverbs: Evaluating LLMs Linguistic and Cultural Competence in Saudi Dialects with Absher</title>
<link>https://arxiv.org/abs/2507.10216</link>
<guid>https://arxiv.org/abs/2507.10216</guid>
<content:encoded><![CDATA[
arXiv:2507.10216v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces Absher, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v4 Announce Type: replace-cross 
Abstract: Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for "one prior, many embodiments": strong, inexpensive video priors together with minimal on-robot alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models</title>
<link>https://arxiv.org/abs/2507.13993</link>
<guid>https://arxiv.org/abs/2507.13993</guid>
<content:encoded><![CDATA[
arXiv:2507.13993v3 Announce Type: replace-cross 
Abstract: The growing volume of medical imaging data has increased the need for automated diagnostic tools, especially for musculoskeletal injuries like rib fractures, commonly detected via CT scans. Manual interpretation is time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep learning framework for rib fracture diagnosis and report generation. It integrates a YOLOv9 model for fracture detection, a medical knowledge graph for retrieving clinical context, and a fine-tuned LLaVA language model for generating diagnostic reports. OrthoInsight combines visual features from CT images with expert textual data to deliver clinically useful outputs. Evaluated on 28,675 annotated CT images and expert reports, it achieves high performance across Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value, with an average score of 4.28, outperforming models like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal learning in transforming medical image analysis and providing effective support for radiologists.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASR-Synchronized Speaker-Role Diarization</title>
<link>https://arxiv.org/abs/2507.17765</link>
<guid>https://arxiv.org/abs/2507.17765</guid>
<content:encoded><![CDATA[
arXiv:2507.17765v3 Announce Type: replace-cross 
Abstract: Speaker-role diarization (RD), such as doctor vs. patient or lawyer vs. client, is practically often more useful than conventional speaker diarization (SD), which assigns only generic labels (speaker-1, speaker-2). The state-of-the-art end-to-end ASR+RD approach uses a single transducer that serializes word and role predictions (role at the end of a speaker's turn), but at the cost of degraded ASR performance. To address this, we adapt a recent joint ASR+SD framework to ASR+RD by freezing the ASR transducer and training an auxiliary RD transducer in parallel to assign a role to each ASR-predicted word. For this, we first show that SD and RD are fundamentally different tasks, exhibiting different dependencies on acoustic and linguistic information. Motivated by this, we propose (1) task-specific predictor networks and (2) using higher-layer ASR encoder features as input to the RD encoder. Additionally, we replace the blank-shared RNNT loss by cross-entropy loss along the 1-best forced-alignment path to further improve performance while reducing computational and memory requirements during RD training. Experiments on a public and a private dataset of doctor-patient conversations demonstrate that our method outperforms the best baseline with relative reductions of 6.2% and 4.5% in role-based word diarization error rate (R-WDER), respectively
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeNER: Code Prompting for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.20423</link>
<guid>https://arxiv.org/abs/2507.20423</guid>
<content:encoded><![CDATA[
arXiv:2507.20423v3 Announce Type: replace-cross 
Abstract: Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.06372</link>
<guid>https://arxiv.org/abs/2508.06372</guid>
<content:encoded><![CDATA[
arXiv:2508.06372v2 Announce Type: replace-cross 
Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke when and what" within an audio clip, which is a crucial task in various real-world multi-speaker scenarios such as meeting transcription and dialogue systems. Existing SDR systems typically adopt a cascaded framework, combining multiple modules such as speaker diarization (SD) and automatic speech recognition (ASR). The cascaded systems suffer from several limitations, such as error propagation, difficulty in handling overlapping speech, and lack of joint optimization for exploring the synergy between SD and ASR tasks. To address these limitations, we introduce SpeakerLM, a unified multimodal large language model for SDR that jointly performs SD and ASR in an end-to-end manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a flexible speaker registration mechanism into SpeakerLM, enabling SDR under different speaker registration settings. SpeakerLM is progressively developed with a multi-stage training strategy on large-scale real data. Extensive experiments show that SpeakerLM demonstrates strong data scaling capability and generalizability, outperforming state-of-the-art cascaded baselines on both in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental results show that the proposed speaker registration mechanism effectively ensures robust SDR performance of SpeakerLM across diverse speaker registration conditions and varying numbers of registered speakers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</title>
<link>https://arxiv.org/abs/2508.07745</link>
<guid>https://arxiv.org/abs/2508.07745</guid>
<content:encoded><![CDATA[
arXiv:2508.07745v3 Announce Type: replace-cross 
Abstract: Insider threats pose a persistent and critical security risk, yet are notoriously difficult to detect in complex enterprise environments, where malicious actions are often hidden within seemingly benign user behaviors. Although machine-learning-based insider threat detection (ITD) methods have shown promise, their effectiveness is fundamentally limited by the scarcity of high-quality and realistic training data. Enterprise internal data is highly sensitive and rarely accessible, while existing public and synthetic datasets are either small-scale or lack sufficient realism, semantic richness, and behavioral diversity.
  To address this challenge, we propose Chimera, an LLM-based multi-agent framework that automatically simulates both benign and malicious insider activities and generates comprehensive system logs across diverse enterprise environments. Chimera models each agent as an individual employee with fine-grained roles and supports group meetings, pairwise interactions, and self-organized scheduling to capture realistic organizational dynamics. Based on 15 insider attacks abstracted from real-world incidents, we deploy Chimera in three representative data-sensitive organizational scenarios and construct ChimeraLog, a new dataset for developing and evaluating ITD methods.
  We evaluate ChimeraLog through human studies and quantitative analyses, demonstrating its diversity and realism. Experiments with existing ITD methods show substantially lower detection performance on ChimeraLog compared to prior datasets, indicating a more challenging and realistic benchmark. Moreover, despite distribution shifts, models trained on ChimeraLog exhibit strong generalization, highlighting the practical value of LLM-based multi-agent simulation for advancing insider threat detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication</title>
<link>https://arxiv.org/abs/2508.11733</link>
<guid>https://arxiv.org/abs/2508.11733</guid>
<content:encoded><![CDATA[
arXiv:2508.11733v2 Announce Type: replace-cross 
Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but often suffer from redundant communication and excessive token overhead. Existing methods typically enhance efficiency through pretrained GNNs or greedy algorithms, but often isolate pre- and post-task optimization, lacking a unified strategy. To this end, we present SafeSieve, a progressive and adaptive multi-agent pruning algorithm that dynamically refines the inter-agent communication through a novel dual-mechanism. SafeSieve integrates initial LLM-based semantic evaluation with accumulated performance feedback, enabling a smooth transition from heuristic initialization to experience-driven refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs 0-extension clustering to preserve structurally coherent agent groups while eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval, etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt injection attacks (1.23% average accuracy drop). In heterogeneous settings, SafeSieve reduces deployment costs by 13.3% while maintaining performance. These results establish SafeSieve as an efficient, GPU-free, and scalable framework for practical multi-agent systems. Our code can be found here: https://github.com/csgen/SafeSieve
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Spectral Clustering For GloVe-like Text Embeddings</title>
<link>https://arxiv.org/abs/2508.14075</link>
<guid>https://arxiv.org/abs/2508.14075</guid>
<content:encoded><![CDATA[
arXiv:2508.14075v2 Announce Type: replace-cross 
Abstract: In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving</title>
<link>https://arxiv.org/abs/2509.08269</link>
<guid>https://arxiv.org/abs/2509.08269</guid>
<content:encoded><![CDATA[
arXiv:2509.08269v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), with their strong understanding and reasoning capabilities, are increasingly being explored for tackling optimization problems, especially in synergy with evolutionary computation. While several recent surveys have explored aspects of LLMs for optimization, there remains a need for an integrative perspective that connects problem modeling with solving workflows. This survey addresses this gap by providing a comprehensive review of recent developments and organizing them within a structured framework. We classify existing research into two main stages: LLMs for optimization modeling and LLMs for optimization solving. The latter is further divided into three paradigms according to the role of LLMs in the optimization workflow: LLMs as stand-alone optimizers, low-level LLMs embedded within optimization algorithms, and high-level LLMs for algorithm selection and generation. For each category, we analyze representative methods, distill technical challenges, and examine their interplay with traditional approaches. We also review interdisciplinary applications spanning the natural sciences, engineering, and machine learning. By contrasting LLM-driven and conventional methods, we highlight key limitations and research gaps, and point toward future directions for developing self-evolving agentic ecosystems for optimization. An up-to-date collection of related literature is maintained at https://github.com/ishmael233/LLM4OPT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury</title>
<link>https://arxiv.org/abs/2510.03248</link>
<guid>https://arxiv.org/abs/2510.03248</guid>
<content:encoded><![CDATA[
arXiv:2510.03248v2 Announce Type: replace-cross 
Abstract: Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Agentic Security: Applications, Threats and Defenses</title>
<link>https://arxiv.org/abs/2510.06445</link>
<guid>https://arxiv.org/abs/2510.06445</guid>
<content:encoded><![CDATA[
arXiv:2510.06445v2 Announce Type: replace-cross 
Abstract: In this work we present the first holistic survey of the agentic security landscape, structuring the field around three fundamental pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 160 papers, explaining how agents are used in downstream cybersecurity applications, inherent threats to agentic systems, and countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage. A complete and continuously updated list of all surveyed papers is publicly available at https://github.com/kagnlp/Awesome-Agentic-Security.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2510.11496</link>
<guid>https://arxiv.org/abs/2510.11496</guid>
<content:encoded><![CDATA[
arXiv:2510.11496v3 Announce Type: replace-cross 
Abstract: In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning</title>
<link>https://arxiv.org/abs/2510.16416</link>
<guid>https://arxiv.org/abs/2510.16416</guid>
<content:encoded><![CDATA[
arXiv:2510.16416v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI</title>
<link>https://arxiv.org/abs/2510.20647</link>
<guid>https://arxiv.org/abs/2510.20647</guid>
<content:encoded><![CDATA[
arXiv:2510.20647v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting "Lost in Translation," where translation steps lead to errors that would have been avoided by question's language reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation</title>
<link>https://arxiv.org/abs/2510.22732</link>
<guid>https://arxiv.org/abs/2510.22732</guid>
<content:encoded><![CDATA[
arXiv:2510.22732v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live</title>
<link>https://arxiv.org/abs/2511.02230</link>
<guid>https://arxiv.org/abs/2511.02230</guid>
<content:encoded><![CDATA[
arXiv:2511.02230v2 Announce Type: replace-cross 
Abstract: KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.
  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
arXiv:2511.02376v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Graph Neural Networks for Healthcare</title>
<link>https://arxiv.org/abs/2511.02531</link>
<guid>https://arxiv.org/abs/2511.02531</guid>
<content:encoded><![CDATA[
arXiv:2511.02531v3 Announce Type: replace-cross 
Abstract: Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical exploration and discovery at scale</title>
<link>https://arxiv.org/abs/2511.02864</link>
<guid>https://arxiv.org/abs/2511.02864</guid>
<content:encoded><![CDATA[
arXiv:2511.02864v3 Announce Type: replace-cross 
Abstract: AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.
  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.
  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Guidance through Calibration and Regularization</title>
<link>https://arxiv.org/abs/2511.05844</link>
<guid>https://arxiv.org/abs/2511.05844</guid>
<content:encoded><![CDATA[
arXiv:2511.05844v3 Announce Type: replace-cross 
Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title>
<link>https://arxiv.org/abs/2511.09392</link>
<guid>https://arxiv.org/abs/2511.09392</guid>
<content:encoded><![CDATA[
arXiv:2511.09392v4 Announce Type: replace-cross 
Abstract: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models</title>
<link>https://arxiv.org/abs/2511.11646</link>
<guid>https://arxiv.org/abs/2511.11646</guid>
<content:encoded><![CDATA[
arXiv:2511.11646v2 Announce Type: replace-cross 
Abstract: Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expressive Temporal Specifications for Reward Monitoring</title>
<link>https://arxiv.org/abs/2511.12808</link>
<guid>https://arxiv.org/abs/2511.12808</guid>
<content:encoded><![CDATA[
arXiv:2511.12808v3 Announce Type: replace-cross 
Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v5 Announce Type: replace-cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
arXiv:2511.17582v3 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVAdam: A Full-Dimension Adaptive Optimizer</title>
<link>https://arxiv.org/abs/2511.20277</link>
<guid>https://arxiv.org/abs/2511.20277</guid>
<content:encoded><![CDATA[
arXiv:2511.20277v2 Announce Type: replace-cross 
Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20663</link>
<guid>https://arxiv.org/abs/2511.20663</guid>
<content:encoded><![CDATA[
arXiv:2511.20663v4 Announce Type: replace-cross 
Abstract: Reliability in multi-agent systems (MAS) built on large language models is increasingly limited by cognitive failures rather than infrastructure faults. Existing observability tools describe failures but do not quantify how quickly distributed reasoning recovers once coherence is lost. We introduce MTTR-A (Mean Time-to-Recovery for Agentic Systems), a runtime reliability metric that measures cognitive recovery latency in MAS. MTTR-A adapts classical dependability theory to agentic orchestration, capturing the time required to detect reasoning drift and restore coherent operation. We further define complementary metrics, including MTBF and a normalized recovery ratio (NRR), and establish theoretical bounds linking recovery latency to long-run cognitive uptime. Using a LangGraph-based benchmark with simulated drift and reflex recovery, we empirically demonstrate measurable recovery behavior across multiple reflex strategies. This work establishes a quantitative foundation for runtime cognitive dependability in distributed agentic systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue</title>
<link>https://arxiv.org/abs/2511.21728</link>
<guid>https://arxiv.org/abs/2511.21728</guid>
<content:encoded><![CDATA[
arXiv:2511.21728v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation</title>
<link>https://arxiv.org/abs/2512.01372</link>
<guid>https://arxiv.org/abs/2512.01372</guid>
<content:encoded><![CDATA[
arXiv:2512.01372v2 Announce Type: replace-cross 
Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Overhead Introspection for Adaptive Test-Time Compute</title>
<link>https://arxiv.org/abs/2512.01457</link>
<guid>https://arxiv.org/abs/2512.01457</guid>
<content:encoded><![CDATA[
arXiv:2512.01457v4 Announce Type: replace-cross 
Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate</title>
<link>https://arxiv.org/abs/2512.03578</link>
<guid>https://arxiv.org/abs/2512.03578</guid>
<content:encoded><![CDATA[
arXiv:2512.03578v2 Announce Type: replace-cross 
Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.
  The code implementation and datasets are publicly available at https://github.com/FlorentF9/MAGNETS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v2 Announce Type: replace-cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v2 Announce Type: replace-cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code</title>
<link>https://arxiv.org/abs/2512.10713</link>
<guid>https://arxiv.org/abs/2512.10713</guid>
<content:encoded><![CDATA[
arXiv:2512.10713v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A probabilistic foundation model for crystal structure denoising, phase classification, and order parameters</title>
<link>https://arxiv.org/abs/2512.11077</link>
<guid>https://arxiv.org/abs/2512.11077</guid>
<content:encoded><![CDATA[
arXiv:2512.11077v2 Announce Type: replace-cross 
Abstract: Atomistic simulations generate large volumes of noisy structural data, but extracting phase labels, order parameters (OPs), and defect information in a way that is universal, robust, and interpretable remains challenging. Existing tools such as PTM and CNA are restricted to a small set of hand-crafted lattices (e.g.\ FCC/BCC/HCP), degrade under strong thermal disorder or defects, and produce hard, template-based labels without per-atom probability or confidence scores. Here we introduce a log-probability foundation model that unifies denoising, phase classification, and OP extraction within a single probabilistic framework. We reuse the MACE-MP foundation interatomic potential on crystal structures mapped to AFLOW prototypes, training it to predict per-atom, per-phase logits $l$ and to aggregate them into a global log-density $\log \hat{P}_\theta(\boldsymbol{r})$ whose gradient defines a conservative score field. Denoising corresponds to gradient ascent on this learned log-density, phase labels follow from $\arg\max_c l_{ac}$, and the $l$ values act as continuous, defect-sensitive and interpretable OPs quantifying the Euclidean distance to ideal phases. We demonstrate universality across hundreds of prototypes, robustness under strong thermal and defect-induced disorder, and accurate treatment of complex systems such as ice polymorphs, ice--water interfaces, and shock-compressed Ti.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery</title>
<link>https://arxiv.org/abs/2512.12608</link>
<guid>https://arxiv.org/abs/2512.12608</guid>
<content:encoded><![CDATA[
arXiv:2512.12608v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</title>
<link>https://arxiv.org/abs/2512.12620</link>
<guid>https://arxiv.org/abs/2512.12620</guid>
<content:encoded><![CDATA[
arXiv:2512.12620v2 Announce Type: replace-cross 
Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v2 Announce Type: replace-cross 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming video, large language models, KV cache retrieval, hardware accelerator, edge deployment<br /><br />Summary: Streaming video large language models (LLMs) are crucial for real-time multimodal tasks like video captioning, question answering, and augmented reality. However, managing the continuously growing key-value (KV) caches in these models presents significant challenges, especially due to the iterative prefill stage that causes high computation, data transfer, and accuracy issues. These problems are more pronounced in edge deployments, which are key targets for such models. To address these challenges, this work introduces V-Rex, a novel software-hardware co-designed accelerator. V-Rex centers around ReSV, a training-free dynamic KV cache retrieval algorithm that utilizes temporal and spatial token similarity clustering to significantly reduce KV cache memory usage across video frames. Complementing the algorithm, V-Rex incorporates a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE) that employs bit-level and early-exit computing units. The result is real-time streaming video LLM inference on edge devices with frame rates of 3.9-8.3 FPS and minimal accuracy loss. The DRE component is efficient, occupying only 2.2% of power and 2.0% of area, yet the overall system achieves 1.9 to 19.7 times speedup and 3.1 to 18.5 times energy efficiency improvements over the AGX Orin GPU. This work pioneers comprehensive KV cache retrieval optimization across both software and hardware to enable high-performance video LLM inference on resource-constrained edge platforms. <div>
arXiv:2512.12284v2 Announce Type: replace-cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMELLNET: A Large-scale Dataset for Real-world Smell Recognition</title>
<link>https://arxiv.org/abs/2506.00239</link>
<guid>https://arxiv.org/abs/2506.00239</guid>
<content:encoded><![CDATA[
<div> smell detection, olfactory AI, SmellNet dataset, ScentFormer model, temporal modeling<br /><br />Summary:<br /><br />1. The paper addresses the challenge of developing AI systems capable of sensing and identifying substances based solely on their smell, which is valuable for applications such as allergen detection, manufacturing monitoring, and health diagnostics. <br /><br />2. It introduces SmellNet, the first large-scale database digitizing diverse natural smells using small gas and chemical sensors. SmellNet includes around 828,000 data points covering 50 substances (nuts, spices, herbs, fruits, and vegetables) and 43 mixtures, collected over 68 hours. <br /><br />3. The authors propose ScentFormer, a Transformer-based neural network architecture that incorporates temporal differencing and sliding-window augmentation to effectively model smell data dynamics. <br /><br />4. On the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy; on the SmellNet-Mixture distribution prediction task, it attains 50.2% Top-1@0.1 accuracy on test-seen data, demonstrating robust performance. <br /><br />5. ScentFormer's capacity to generalize across varying conditions and capture transient chemical signals highlights the potential of temporal modeling approaches in olfactory AI, paving the way for real-world applications in healthcare, food and beverage, environmental sensing, manufacturing, and entertainment. <div>
arXiv:2506.00239v4 Announce Type: replace 
Abstract: The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large-scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use small gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 828,000 data points across 50 substances, spanning nuts, spices, herbs, fruits, and vegetables, and 43 mixtures among them, with 68 hours of data collected. Using SmellNet, we developed ScentFormer, a Transformer-based architecture combining temporal differencing and sliding-window augmentation for smell data. For the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy, and for the SmellNet-Mixture distribution prediction task, ScentFormer achieves 50.2% Top-1@0.1 on the test-seen split. ScentFormer's ability to generalize across conditions and capture transient chemical dynamics demonstrates the promise of temporal modeling in olfactory AI. SmellNet and ScentFormer lay the groundwork for real-world olfactory applications across healthcare, food and beverage, environmental monitoring, manufacturing, and entertainment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-Resolution Reasoning, ambiguity retention, Multi-Vector Embeddings, Non-Collapsing Attention, Contextual Identity Tracking  

<br /><br />Summary:  
The paper identifies a critical limitation in current AI systems, which is their tendency to prematurely resolve ambiguity, collapsing multiple valid interpretations into a single output. This issue arises from classical identity assumptions in neural architectures. To address this, the authors propose Non-Resolution Reasoning (NRR), a novel computational framework that treats ambiguity retention as a legitimate reasoning mode rather than a flaw. NRR is founded on three principles: Non-Identity ($A \neq A$), where the same symbol can represent different entities across contexts; Approximate Identity ($A \approx A$), where entities share some structural similarity but are not identical; and Non-Resolution, allowing conflicting interpretations to coexist without forced convergence. The framework introduces three architectural components to embody these principles: Multi-Vector Embeddings that provide context-dependent representations, Non-Collapsing Attention that supports retention of parallel interpretations, and Contextual Identity Tracking (CIT) to maintain non-identity distinctions during inference. Empirical validation is provided through a synthetic context-shift task, where an NRR-lite model significantly outperforms standard architectures in out-of-distribution generalization. Case studies demonstrate NRR’s strengths in paradox handling, creative generation, and context-aware reasoning. Ultimately, NRR challenges the assumption that meaning must collapse to be useful, suggesting that AI should strategically manage ambiguity depending on timing, method, and control authority. <div>
arXiv:2512.13478v4 Announce Type: replace-cross 
Abstract: Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases</title>
<link>https://arxiv.org/abs/2512.16953</link>
<guid>https://arxiv.org/abs/2512.16953</guid>
<content:encoded><![CDATA[
<div> Entity Set Expansion, expansion graph, taxonomic structures, logical formulas, reasoning tasks  

<br /><br />Summary:  
The paper addresses the task of Entity Set Expansion, which involves identifying entities sharing semantic properties with an initial set, but points out that traditional linear methods miss richer taxonomic relationships. It introduces the concept of an expansion graph, a directed acyclic graph where nodes represent semantic generalizations labeled by logical formulas, and edges reflect strict semantic inclusion, allowing taxonomic organization in knowledge bases. A key challenge is the potentially large size of these graphs, making full materialization impractical for real-world use. To tackle this, the authors formalize reasoning tasks to determine whether two tuples belong to comparable, incomparable, or identical nodes within the graph. By applying realistic assumptions, such as limiting input sizes and entity descriptions, these reasoning tasks can be executed efficiently. This approach facilitates localized, incremental exploration and navigation of expansion graphs without constructing the entire graph. Consequently, the method supports practical applications that rely on taxonomic expansions of entity sets while ensuring computational feasibility. <div>
arXiv:2512.16953v1 Announce Type: new 
Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</title>
<link>https://arxiv.org/abs/2512.16969</link>
<guid>https://arxiv.org/abs/2512.16969</guid>
<content:encoded><![CDATA[
<div> Scientific General Intelligence, Practical Inquiry Model, SGI-Bench, Test-Time Reinforcement Learning, scientific discovery  

<br /><br />Summary:  
This paper addresses the lack of a coherent framework for Scientific General Intelligence (SGI), which refers to AI's ability to autonomously conceive, investigate, and reason across scientific domains. It introduces an operational definition of SGI based on the Practical Inquiry Model (PIM), encompassing four stages: Deliberation, Conception, Action, and Perception. To evaluate SGI, the authors design four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning, all encapsulated in a new benchmark called SGI-Bench. This benchmark includes more than 1,000 expert-curated samples inspired by significant scientific questions, enabling systematic testing of state-of-the-art large language models. Experimental results show several shortcomings: low exact match rates in deep research tasks despite good step-level alignment; generated ideas often lack feasibility and detail; although code in dry experiments is highly executable, the accuracy of their execution results is low; wet lab protocols suffer from poor sequence fidelity; and multimodal comparative reasoning remains a persistent challenge. To improve performance, the paper proposes Test-Time Reinforcement Learning (TTRL), which optimizes for novelty in retrieval-augmented inference without relying on reference answers, thereby enhancing hypothesis novelty. Together, these contributions lay a foundation for developing AI systems capable of genuine participation in scientific discovery. <div>
arXiv:2512.16969v1 Announce Type: new 
Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAACE: A Plan-Aware Automated Agent Context Engineering Framework</title>
<link>https://arxiv.org/abs/2512.16970</link>
<guid>https://arxiv.org/abs/2512.16970</guid>
<content:encoded><![CDATA[
<div> Large Language Models, context compression, plan-aware reasoning, multi-step workflows, inference efficiency<br /><br />Summary:<br /><br />1. Large Language Model (LLM) agents operate within complex, multi-step workflows that include planning, tool use, reflection, and interaction with external knowledge. These processes generate growing contexts that need effective management to preserve important information, reduce inference costs, and prevent attention dilution.<br /><br />2. Existing summarization and compression methods tend to overlook the significance of multi-step, plan-aware reasoning within these agent workflows.<br /><br />3. The paper presents PAACE (Plan-Aware Automated Context Engineering), a comprehensive framework designed to optimize the evolving state of LLM agents by leveraging next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression.<br /><br />4. PAACE consists of two components: PAACE-Syn, which creates large-scale synthetic agent workflows with annotated stepwise compression supervision, and PAACE-FT, a set of distilled, plan-aware compressors trained on successful teacher model demonstrations.<br /><br />5. Experimental evaluations on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) show that PAACE improves agent accuracy and F1 scores while significantly reducing context size, peak tokens, and attention dependencies. Additionally, PAACE-FT achieves 97% of the teacher’s performance while drastically lowering inference cost, enabling practical deployment with smaller models. <div>
arXiv:2512.16970v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats</title>
<link>https://arxiv.org/abs/2512.17041</link>
<guid>https://arxiv.org/abs/2512.17041</guid>
<content:encoded><![CDATA[
<div> Agentic AI, Agentic Vehicles, Security Threats, Cross-layer Risks, Cyber-physical Systems<br /><br />Summary:<br /><br />This paper addresses the emerging concept of Agentic Vehicles (AgVs), which integrate agentic AI capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance into both manually driven and autonomous vehicles. It criticizes existing security frameworks like OWASP Agentic AI Security Risks for being insufficient for safety-critical cyber-physical systems like vehicles, as they overlook interactions with perception, communication, and control layers. The authors propose a role-based architecture for AgVs consisting of a Personal Agent and a Driving Strategy Agent to systematically analyze vulnerabilities. The study investigates not only agentic AI layer risks but also cross-layer risks that originate from upstream layers, such as perception and control. By conducting a severity matrix and attack-chain analysis, they demonstrate how minor disturbances can escalate, causing the AI to behave unsafely or become misaligned, impacting both human-driven and autonomous vehicles. Ultimately, the work contributes the first structured framework for examining security risks in agentic AI systems embedded in current and future vehicle platforms, emphasizing the importance of multi-layer security awareness for safe deployment. <div>
arXiv:2512.17041v1 Announce Type: new 
Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering</title>
<link>https://arxiv.org/abs/2512.17043</link>
<guid>https://arxiv.org/abs/2512.17043</guid>
<content:encoded><![CDATA[
<div> Relation-centric KGQA, Subgraph selection, Graph pruning, Reinforcement learning, Large Language Models (LLMs)  

<br /><br />Summary:  
This work addresses a novel problem in Knowledge Graph Question Answering (KGQA) by shifting focus from entity-centric queries, which return single entities, to relation-centric queries that require answering with subgraphs representing semantic connections among multiple entities. The core challenge involves handling a large number of candidate subgraphs, where trivial or overly common relations can obscure useful and unique answers. To overcome this, the authors introduce UniRel-R1, a unified framework that combines subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reinforcement learning reward function is specifically designed to promote answers that are compact, specific subgraphs, featuring more informative relations and intermediate entities with lower degrees (less connected nodes). Experimental results demonstrate that UniRel-R1 significantly outperforms baseline models in terms of improved connectivity and reward metrics. Furthermore, the framework generalizes well to unseen entities and relations, highlighting its robustness and applicability to real-world KGQA scenarios where understanding complex relational structures is essential. This study broadens the scope of KGQA by moving beyond single-entity answers towards richer, relation-centric knowledge extraction. <div>
arXiv:2512.17043v1 Announce Type: new 
Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations</title>
<link>https://arxiv.org/abs/2512.17066</link>
<guid>https://arxiv.org/abs/2512.17066</guid>
<content:encoded><![CDATA[
<div> Conflict, realistic threat, symbolic threat, ingroup bias, intergroup contact<br /><br />Summary:<br /><br />This study investigates how realistic and symbolic threats interact to drive human conflict by using simulations of large language model (LLM)-driven agents in virtual societies. The researchers independently manipulated realistic and symbolic threats, while tracking agents' actions, language, and attitudes over time. Analysis reveals that the LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, with experimental manipulations influencing these states and causally affecting behavior. The findings show that realistic threat directly elevates hostility levels, whereas symbolic threat has a weaker effect, fully mediated by ingroup bias, and only increases hostility when realistic threat is absent. Additionally, non-hostile intergroup contact serves as a buffer against escalation of conflict. Structural asymmetries within groups lead to a concentration of hostility particularly among majority populations. By combining simulation with representation analysis, this work offers a causal and temporal account of threat-driven conflict, overcoming limitations in empirical studies due to ethical and practical constraints. The research thus advances understanding of the nuanced roles of material versus symbolic threats in intergroup hostility and highlights mechanisms that can mitigate conflict escalation. <div>
arXiv:2512.17066v1 Announce Type: new 
Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Under Ignorance in Universal Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.17086</link>
<guid>https://arxiv.org/abs/2512.17086</guid>
<content:encoded><![CDATA[
<div> Keywords: AIXI, reinforcement learning, utility functions, imprecise probability, Choquet integrals<br /><br />Summary:<br /><br />This article generalizes the AIXI reinforcement learning agent to incorporate a broader class of utility functions, allowing more flexible modeling of agent preferences. It addresses the challenge that arises when some hypotheses in the agent’s belief distribution predict only finite prefixes of interaction histories, which has traditionally been interpreted as representing a chance of the agent’s death, quantified by the semimeasure loss. The authors propose an alternative interpretation by treating these belief distributions as imprecise probabilities, framing the semimeasure loss as complete ignorance rather than a literal death probability. This perspective motivates the use of Choquet integrals from imprecise probability theory to compute expected utilities. The paper explores the computability of these expected utilities, revealing that the traditional recursive value function used in AIXI can be recovered as a special case within this framework. However, the most generalized expected utilities derived under the death interpretation framework do not admit a characterization as Choquet integrals, indicating limitations and nuances in this approach for utility assignment in reinforcement learning agents with uncertain or incomplete belief distributions. <div>
arXiv:2512.17086v1 Announce Type: new 
Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving</title>
<link>https://arxiv.org/abs/2512.17093</link>
<guid>https://arxiv.org/abs/2512.17093</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Answer Set Programming, solver-guided instruction-tuning, semantic parsing, code generation<br /><br />Summary: The paper addresses the challenge of generating code for domain-specific languages, specifically focusing on Answer Set Programming (ASP), where large language models (LLMs) struggle due to limited pre-training examples. The authors propose a novel solver-in-the-loop approach that integrates an ASP solver during instruction-tuning of LLMs. This method uses only natural language problem specifications along with their solutions, avoiding the need for extensive annotated code. The approach samples possible ASP program continuations from LLMs, especially targeting logic puzzles, and uses solver feedback to classify these samples into chosen (valid) and rejected instances, leveraging the declarative nature of ASP to narrow solution spaces. Subsequently, supervised fine-tuning is applied on this curated dataset to enhance the LLMs’ ability to generate accurate ASP code. Additionally, robustness is further improved through a solver-guided search strategy involving best-of-N sampling, which helps select higher-quality code candidates. Experimental results demonstrate consistent improvements in ASP code generation across two different prompting settings and datasets, confirming the effectiveness of the solver-guided fine-tuning and search method in handling the semantic parsing complexity in ASP programming. <div>
arXiv:2512.17093v1 Announce Type: new 
Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Self-Improving Agent with Skill Library</title>
<link>https://arxiv.org/abs/2512.17102</link>
<guid>https://arxiv.org/abs/2512.17102</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Skill Library, Reinforcement Learning, Self-Improvement, Sequential Rollout<br /><br />Summary:<br /><br />This paper addresses the challenge that Large Language Model (LLM)-based agents face in continuous self-improvement and adaptation when deployed in new environments. To tackle this, the authors propose a Reinforcement Learning (RL)-based framework called Skill Augmented GRPO for self-Evolution (SAGE), which systematically integrates skill libraries into the learning process. The key innovation in SAGE is the Sequential Rollout component, where agents are deployed iteratively across a chain of related tasks, allowing skills learned from earlier tasks to be accumulated and reused in later ones, promoting continuous skill growth. Additionally, SAGE introduces a Skill-integrated Reward mechanism that supplements traditional outcome-based rewards, enhancing both the generation and application of skills throughout training. Experimental evaluation on the AppWorld benchmark demonstrates that applying SAGE to a supervised-finetuned model with expert experience leads to significant improvements: an 8.9% increase in Scenario Goal Completion, a 26% reduction in required interaction steps, and a 59% decrease in generated tokens compared to existing methods. These results highlight SAGE’s effectiveness in boosting both the accuracy and efficiency of LLM-based agents in multi-turn, complex reasoning tasks by enabling better skill acquisition and reuse. <div>
arXiv:2512.17102v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty</title>
<link>https://arxiv.org/abs/2512.17145</link>
<guid>https://arxiv.org/abs/2512.17145</guid>
<content:encoded><![CDATA[
<div> Keywords: Reasoning under uncertainty, Solomonoff weighting, LLM-generated hypotheses, Bayesian Model Averaging, algorithmic information theory<br /><br />Summary: Reasoning under uncertainty is a fundamental challenge in artificial intelligence, particularly for real-world problems characterized by sparse data that require systematic generalization. The article proposes a novel method inspired by Solomonoff induction that evaluates multiple hypotheses generated by large language models (LLMs) by balancing simplicity and predictive accuracy. Unlike traditional approaches, this Solomonoff-inspired scoring mechanism weights hypotheses to form mixtures that provide uncertainty-aware and conservative predictions on a per-cell basis, which is particularly useful when dealing with noisy or partially incorrect hypotheses. The method is empirically tested on benchmark tasks such as Mini-ARC, demonstrating improved robustness and interpretability. A key comparison is made with Bayesian Model Averaging (BMA), where the Solomonoff approach distributes probability mass more evenly among competing hypotheses, rather than concentrating on the single most likely but potentially flawed hypothesis as BMA does. This highlights the advantage of algorithmic information-theoretic priors in fostering reliable multi-hypothesis reasoning. Overall, the article illustrates how integrating simplicity and fit via Solomonoff-inspired weighting contributes to more reliable, interpretable AI systems capable of reasoning under uncertainty in complex settings. <div>
arXiv:2512.17145v1 Announce Type: new 
Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation</title>
<link>https://arxiv.org/abs/2512.17194</link>
<guid>https://arxiv.org/abs/2512.17194</guid>
<content:encoded><![CDATA[
<div> Multi-modal Retrieval, Reinforcement Learning, Explainability, Ranking, Large Language Models  

<br /><br />Summary:  
This paper addresses limitations in existing Multi-modal Retrieval-Augmented Generation (MMRAG) methods, which lack clear reasoning logic behind retrieval and response generation, reducing explainability. To overcome this, the authors introduce a two-stage reinforcement learning fine-tuning framework designed to enhance the reasoning abilities of multi-modal large language models while enabling explainable outputs in MMRAG tasks. In the first stage, a rule-based reinforcement fine-tuning process is applied for coarse-grained, point-wise ranking of multi-modal documents, efficiently filtering out irrelevant content. The second stage involves reasoning-based reinforcement fine-tuning that jointly optimizes fine-grained list-wise ranking and answer generation, encouraging models to produce explicit reasoning logic during retrieval and response generation. Experimental results demonstrate that this method achieves state-of-the-art performance on two key benchmarks for multi-modal retrieval-augmented generation: WebQA and MultimodalQA. Furthermore, comprehensive ablation studies confirm the effectiveness and necessity of each component in the proposed framework. This work thus advances the explainability and performance of MMRAG by integrating reinforcement learning to guide both retrieval and generation processes in multi-modal contexts. <div>
arXiv:2512.17194v1 Announce Type: new 
Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark</title>
<link>https://arxiv.org/abs/2512.17196</link>
<guid>https://arxiv.org/abs/2512.17196</guid>
<content:encoded><![CDATA[
<div> Keywords: unified multimodal models, UmniBench, evaluation benchmark, understanding, generation, editing abilities<br /><br />Summary: This paper introduces UmniBench, a novel benchmark designed to comprehensively evaluate unified multimodal models (UMMs) across multiple dimensions. Traditional evaluations of UMMs treat their understanding and generation capabilities separately, which limits insight into models that integrate these functions. UmniBench addresses this by assessing understanding, generation, and editing abilities within a single evaluation framework. It uses human-examined prompts and question-answer pairs, leveraging the UMMs themselves to evaluate generation and editing conditioned on their understanding ability. This approach enables a holistic yet fine-grained evaluation process. UmniBench also covers a wide scope, spanning 13 major domains and over 200 concepts, ensuring that models are tested thoroughly across varied tasks and content. Additionally, the benchmark supports decoupled evaluations, allowing assessment of each ability independently when needed. To demonstrate its utility, the authors benchmark 24 popular models—including unified multimodal and single-ability large models—providing new insights into their relative strengths and weaknesses. The comprehensive and objective evaluation enabled by UmniBench aims to encourage development and improvement in the UMM community by providing logistical support and clearer performance metrics across multiple modalities and tasks. <div>
arXiv:2512.17196v1 Announce Type: new 
Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction</title>
<link>https://arxiv.org/abs/2512.17250</link>
<guid>https://arxiv.org/abs/2512.17250</guid>
<content:encoded><![CDATA[
<div> Speculative execution, model-based control, TD-MPC2, correction mechanism, inference latency

<br /><br />Summary:  
This paper addresses the challenge of inference latency in real-time sequential control agents, which can destabilize control and degrade performance. The authors propose a speculation-and-correction framework that adapts the predict-then-verify concept of speculative execution to model-based control using TD-MPC2. Their approach involves generating a short-horizon action queue and predicted latent rollouts at each step via a pretrained world model and latent-space MPC planner, enabling execution of multiple planned actions without replanning immediately. Upon receiving a new observation, the system measures the mismatch between the encoded latent state and the queued predicted latent. For small to moderate mismatches, a learned corrector applies a residual correction to the speculative action, distilled from a replanning teacher offline. For large mismatches, the agent falls back to full replanning and clears obsolete action queues for safety. The authors evaluate two corrector designs: a gated two-tower MLP and a temporal Transformer, targeting local and systematic errors respectively. Experiments on the DMC Humanoid-Walk task demonstrate that the method reduces planning inferences by 44% (from 500 to 282), improves step latency by 25%, and sustains strong control with only a 7.1% drop in return. Ablations confirm that correction is vital for robust latency reduction, as speculative execution alone is unreliable over longer horizons. <div>
arXiv:2512.17250v1 Announce Type: new 
Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework</title>
<link>https://arxiv.org/abs/2512.17266</link>
<guid>https://arxiv.org/abs/2512.17266</guid>
<content:encoded><![CDATA[
<div> Transfers, forecasting, player-conditioned model, counterfactual simulation, Premier League

<br /><br />Summary: Transfers are critical for football club success, but predicting transfer outcomes is challenging due to the complex and context-dependent nature of player performance. Traditional evaluation methods rely on static statistics or retrospective value assessments that overlook how players adapt to new tactical contexts and teammates. To address this, the paper introduces EventGPT, a novel GPT-style autoregressive transformer model that predicts the next on-ball action type, location, timing, and a player's residual On-Ball Value (rOBV), conditioned on player identity and previous match events. EventGPT conceptualizes matches as sequences of discrete tokens and jointly learns event prediction and value estimation. A major innovation is its capacity for counterfactual simulations by swapping player embeddings into alternative event sequences, enabling quantitative assessment of how a player's behavior and value might change in different tactical systems or teams. Tested on five seasons of Premier League data, EventGPT surpasses existing sequence-based models in both accuracy of next-event prediction and spatial precision. The study also provides practical transfer analysis applications, such as comparing striker performances across tactical systems and identifying stylistic replacements for specific roles, offering a principled framework to evaluate player transfer fit beyond conventional metrics. <div>
arXiv:2512.17266v1 Announce Type: new 
Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models as Pok\'emon Battle Agents: Strategic Play and Content Generation</title>
<link>https://arxiv.org/abs/2512.17308</link>
<guid>https://arxiv.org/abs/2512.17308</guid>
<content:encoded><![CDATA[
<div> Pokémon battles, Large Language Models, strategic decision-making, game content generation, procedural generation<br /><br />Summary:<br /><br />This article explores the potential of Large Language Models (LLMs) as strategic agents in Pokémon battles, where decision-making requires understanding complex mechanics like type matchups, stat calculations, and risk management. The authors developed a turn-based battle system allowing LLMs to choose moves dynamically based on the battle state instead of relying on pre-programmed rules. Their framework incorporates key game elements such as type effectiveness multipliers, damage calculations, and managing multiple Pokémon teams. The study evaluates several LLM architectures by measuring metrics including win rates, decision latency, type-alignment accuracy, and token efficiency. Findings indicate that LLMs can operate competently as game opponents without needing domain-specific training or reinforcement learning. Besides tactical reasoning, LLMs demonstrated capability in creating balanced and novel Pokémon content, highlighting their dual role as players and designers. These insights open possibilities for using LLMs in procedural content generation and adaptive difficulty adjustment, advancing the design of interactive entertainment systems that respond fluidly to player skill. Overall, the work positions LLMs as versatile tools for both gameplay and creative tasks in turn-based strategic gaming contexts. <div>
arXiv:2512.17308v1 Announce Type: new 
Abstract: Strategic decision-making in Pok\'emon battles presents a unique testbed for evaluating large language models. Pok\'emon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pok\'emon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pok\'emon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pok\'emon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dialectics for Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.17373</link>
<guid>https://arxiv.org/abs/2512.17373</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, concepts, algorithmic information, reversibility, multi-agent alignment  

<br /><br />Summary:  
This article explores whether artificial intelligence can autonomously discover human-like concepts from raw experience without supervision. It addresses the challenge that human concepts are fluid and subject to change, requiring a formal definition beyond mere labels. The authors propose an algorithmic-information framework that defines a concept as an information object grounded in its structural relationship with an agent's complete experience. A key property called determination ensures that a concept's parts maintain a reversible consistency relation, meaning any missing component can be recovered from the others, which prevents concepts from detaching arbitrarily from experience. To evaluate the naturalness of a concept’s decomposition, the paper introduces the measure of excess information, quantifying redundancy overhead when splitting experience into multiple parts. The paper also formulates dialectics as an optimization process where competing concepts bid to explain new or disputed information patches through shorter conditional descriptions, dynamically driving concept evolution via expansion, contraction, splitting, and merging. Finally, it addresses concept transmission and alignment between multiple agents by using small shared “seeds” or protocols that enable efficient communication and reconstruction of concepts, formalizing a trade-off between computation and communication costs. <div>
arXiv:2512.17373v1 Announce Type: new 
Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating the Rashomon Effect to Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2512.17470</link>
<guid>https://arxiv.org/abs/2512.17470</guid>
<content:encoded><![CDATA[
<div> Rashomon effect, sequential decision-making, policy verification, probabilistic behavior, robustness

<br /><br />Summary:  
This paper extends the Rashomon effect—originally studied in classification tasks—to the domain of sequential decision-making, where agents learn policies to achieve objectives by interacting with environments. The Rashomon effect here is defined as the existence of multiple policies that behave identically by visiting the same states and selecting the same actions, but differ internally in structure such as feature attributions. Unlike classification, verifying identical behavior is challenging in sequential decision-making due to stochastic transitions causing variability in individual trajectories. To overcome this, the authors utilize formal verification methods that construct and compare the entire probabilistic behavior of each policy within the environment. Experimental results confirm the Rashomon effect's presence in sequential decision-making settings. Furthermore, they show that ensembles derived from the Rashomon set offer improved robustness against distributional shifts compared to individual policies. The study also introduces permissive policies based on the Rashomon set that reduce computational verification costs while preserving optimal performance. Overall, this work highlights the value of understanding policy multiplicity and structural diversity in sequential decision-making, with implications for robustness and verification efficiency. <div>
arXiv:2512.17470v1 Announce Type: new 
Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Conversational AI for Early Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2512.17559</link>
<guid>https://arxiv.org/abs/2512.17559</guid>
<content:encoded><![CDATA[
<div> Keywords: diagnostic chatbot, large language model, GPT-4o, Retrieval-Augmented Generation, explainable AI<br /><br />Summary: This research addresses critical challenges in healthcare diagnostics such as inefficiency, high costs, and limited specialist access, which contribute to treatment delays and poor outcomes. It introduces a diagnostic chatbot powered by a Large Language Model (LLM), specifically GPT-4o, integrated with Retrieval-Augmented Generation and explainable AI techniques to enhance interaction and transparency. The chatbot conducts dynamic conversations with patients to extract and normalize symptoms, using similarity matching and adaptive questioning to prioritize potential diagnoses. The system employs Chain-of-Thought prompting to provide clear reasoning behind its diagnostic conclusions, making the process more transparent. Performance evaluation against traditional machine learning models, including Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, showed the LLM-based chatbot achieved superior results with 90% accuracy and a perfect Top-3 accuracy of 100%. These outcomes suggest significant improvements in clinical relevance and user engagement compared to existing AI diagnostic systems. The research points toward a future where AI tools in healthcare are not only more effective but also more interactive and understandable, potentially transforming patient-centered medical care. <div>
arXiv:2512.17559v1 Announce Type: new 
Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>About Time: Model-free Reinforcement Learning with Timed Reward Machines</title>
<link>https://arxiv.org/abs/2512.17637</link>
<guid>https://arxiv.org/abs/2512.17637</guid>
<content:encoded><![CDATA[
<div> Keywords: timed reward machines, reinforcement learning, non-Markovian rewards, timed automata, counterfactual-imagining<br /><br />Summary:<br /><br />1. This paper introduces Timed Reward Machines (TRMs), an extension of traditional reward machines that incorporate explicit timing constraints into the reward specification process in reinforcement learning (RL).<br />2. TRMs enable the expression of non-Markovian rewards with precise timing logic, allowing specification of costs for delays and bonuses for timely actions, thus enhancing the applicability of reward machines to time-sensitive, real-world RL tasks.<br />3. The authors develop model-free RL algorithms, particularly tabular Q-learning, that integrate TRMs by abstracting them through timed automata, thereby enabling the learning of optimal policies that respect timing constraints.<br />4. A novel counterfactual-imagining heuristic is introduced, leveraging the structure of TRMs to guide the search process more effectively during learning, improving policy performance.<br />5. Experimental results on popular RL benchmarks demonstrate that the proposed approach successfully learns policies that satisfy timing constraints and achieve high rewards; comparative and ablation studies further highlight the advantages of using different TRM semantics and the benefits of the counterfactual-imagining technique.<br /><br /> <div>
arXiv:2512.17637v1 Announce Type: new 
Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally</title>
<link>https://arxiv.org/abs/2512.17898</link>
<guid>https://arxiv.org/abs/2512.17898</guid>
<content:encoded><![CDATA[
<div> Keywords: Anthropomorphism, humanlike AI design, cross-cultural differences, user trust, AI governance<br /><br />Summary:<br /><br />1. This study investigates how humanlike AI design influences user perceptions and behaviors, specifically focusing on anthropomorphism, engagement, and trust in real-time interactions.<br /><br />2. Conducted two large-scale cross-national experiments involving 3,500 participants across 10 diverse countries, capturing a global perspective often missing from prior research dominated by Western populations.<br /><br />3. Found that users evaluate AI human-likeness based more on practical interaction cues—such as conversation flow and understanding user perspective—rather than theoretical attributes like sentience or consciousness.<br /><br />4. Demonstrated that humanlike AI design causally increases anthropomorphic perceptions in users but does not universally raise behavioral engagement or trust as previously assumed.<br /><br />5. Crucially, the relationship between humanlike design and trust/engagement varies across cultures: some design choices increase trust in regions like Brazil but reduce it in others like Japan, highlighting cultural mediation.<br /><br />6. These results challenge the prevailing narrative that humanlike AI inherently poses safety risks due to misplaced trust, urging a move away from one-size-fits-all governance and toward culturally sensitive AI policies. <div>
arXiv:2512.17898v1 Announce Type: new 
Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Reasoning Meets Its Laws</title>
<link>https://arxiv.org/abs/2512.17901</link>
<guid>https://arxiv.org/abs/2512.17901</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, Laws of Reasoning, compute law, monotonicity, compositionality<br /><br />Summary:<br />1. This paper addresses the challenge that Large Reasoning Models (LRMs), despite their strong performance, often exhibit counterintuitive reasoning behaviors that limit their effectiveness.<br />2. To formalize desirable reasoning behaviors, the authors propose the Laws of Reasoning (LoRe), a unified theoretical framework that describes intrinsic reasoning patterns within LRMs.<br />3. A key hypothesis introduced is the compute law, which states that the reasoning compute should scale linearly with the complexity of the questions posed.<br />4. Since directly measuring question complexity is difficult, the paper focuses on two measurable properties implied by the laws — monotonicity and compositionality — to validate their hypotheses.<br />5. To evaluate these properties, the work introduces LoRe-Bench, a benchmark specifically designed to assess monotonicity and compositionality in large reasoning models.<br />6. Experiments reveal that while most models maintain reasonable monotonicity, they generally lack compositionality.<br />7. To address this, the authors propose a novel finetuning approach aimed at enforcing compositionality in compute laws.<br />8. Extensive empirical results demonstrate that improving adherence to the compute laws enhances reasoning performance consistently across multiple benchmarks and uncovers synergistic effects between the properties and laws.<br />9. The study provides a theoretical and practical pathway for improving LRM reasoning behaviors, with additional resources and project details available at the provided project page. <div>
arXiv:2512.17901v1 Announce Type: new 
Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2507.15118</link>
<guid>https://arxiv.org/abs/2507.15118</guid>
<content:encoded><![CDATA[
<div> Keywords: epilepsy detection, graph attention networks, low-cost EEG, spatio-temporal graphs, resource-limited settings<br /><br />Summary:  
1. The study addresses the under-diagnosis of epilepsy in low-income countries due to limited neurologists and expensive diagnostic tools.  
2. It proposes a graph-based deep learning framework using low-cost EEG devices to detect epilepsy, with datasets collected from Nigeria and Guinea-Bissau.  
3. EEG signals are represented as spatio-temporal graphs and analyzed with graph attention networks (GAT) that focus on both node and edge-level information to capture connectivity biomarkers integral to epilepsy.  
4. Preprocessing techniques were specially designed for low-fidelity EEG recordings, and a lightweight GAT architecture was developed to enable real-time deployment on devices like Raspberry Pi, with training performed on accessible platforms such as Google Colab.  
5. The method outperforms traditional approaches such as random forest classifiers and graph convolutional networks in accuracy and robustness, consistently identifying relevant brain regions, especially fronto-temporal connections.  
6. Results demonstrate that GATs can provide explainable, scalable, and affordable neurodiagnostic support, making epilepsy detection more accessible in underserved and resource-limited regions.  
7. This work paves the way for practical, low-cost tools to improve epilepsy diagnosis and potentially other neurological conditions where EEG data is relevant. <div>
arXiv:2507.15118v1 Announce Type: cross 
Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce neurologists and costly diagnostic tools. We propose a graph-based deep learning framework to detect epilepsy from low-cost Electroencephalography (EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus is on fair, accessible automatic assessment and explainability to shed light on epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs, classify them, and identify interchannel relationships and temporal dynamics using graph attention networks (GAT). To emphasize connectivity biomarkers, we adapt the inherently node-focused GAT to analyze edges. We also designed signal preprocessing for low-fidelity recordings and a lightweight GAT architecture trained on Google Colab and deployed on RaspberryPi devices. Results: The approach achieves promising classification performance, outperforming a standard classifier based on random forest and graph convolutional networks in terms of accuracy and robustness over multiple sessions, but also highlighting specific connections in the fronto-temporal region. Conclusions: The results highlight the potential of GATs to provide insightful and scalable diagnostic support for epilepsy in underserved regions, paving the way for affordable and accessible neurodiagnostic tools.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Agent: An Interactive Video Search System Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16925</link>
<guid>https://arxiv.org/abs/2512.16925</guid>
<content:encoded><![CDATA[
<div> video search, multimodal retrieval, vision-language model, multi-agent system, zero-shot learning<br /><br />Summary:<br /><br />1. V-Agent is a novel multi-agent platform developed to improve advanced video search and support interactive user-system conversations.  
2. The system uses a vision-language model (VLM) fine-tuned on a small video preference dataset and enhanced with retrieval vectors from an image-text retrieval model, overcoming traditional text-based search limitations in multimodal contexts.  
3. VLM-based retrieval independently encodes video frames and audio transcriptions from an automatic speech recognition (ASR) module into a unified multimodal representation space, allowing V-Agent to understand both visual and spoken video content for more context-aware search.  
4. The platform consists of three collaborating agents: a routing agent to manage user intents, a search agent that performs retrieval using the VLM and an additional re-ranking module to boost retrieval accuracy, and a chat agent to interact with users and refine search results.  
5. The system achieves state-of-the-art zero-shot video retrieval performance on the MultiVENT 2.0 benchmark, demonstrating its effectiveness and potential for both academic research and practical real-world applications. <div>
arXiv:2512.16925v1 Announce Type: cross 
Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach</title>
<link>https://arxiv.org/abs/2512.16927</link>
<guid>https://arxiv.org/abs/2512.16927</guid>
<content:encoded><![CDATA[
<div> Keywords: text-search algorithms, Suffix Trees, Ukkonen's Algorithm, natural language processing, bioinformatics<br /><br />Summary:<br /><br />1. The article addresses the need for efficient text-search algorithms to manage large-scale data in domains like natural language processing and bioinformatics.<br />2. Traditional algorithms such as Naive Search, Knuth-Morris-Pratt (KMP), and Boyer-Moore are recognized as foundational but inadequate for the complexity and size of modern datasets, including the Reuters corpus and human genomic sequences.<br />3. The study focuses on optimizing Suffix Trees, particularly through the application of Splitting techniques and Ukkonen's Algorithm, to improve search efficiency.<br />4. A novel optimization is introduced that integrates Ukkonen's Algorithm with a new search method, achieving linear time and space complexity, surpassing the performance of traditional algorithms.<br />5. Empirical evaluations demonstrate the optimized Suffix Tree’s effectiveness in pattern recognition tasks within genomic data, achieving 100% accuracy and highlighting its practical benefits for processing text and biological sequences efficiently and reliably. <div>
arXiv:2512.16927v1 Announce Type: cross 
Abstract: In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</title>
<link>https://arxiv.org/abs/2512.16950</link>
<guid>https://arxiv.org/abs/2512.16950</guid>
<content:encoded><![CDATA[
<div> Keywords: Tree species classification, Terrestrial Laser Scanning (TLS), Finer-CAM, YOLOv8, Deep learning  

<br /><br />Summary:  
This study focuses on the classification of tree species using Terrestrial Laser Scanning (TLS) data combined with deep learning techniques. The authors address the challenge of model interpretability by applying Finer-CAM, a Class Activation Mapping method, to highlight which features in TLS projections drive species classification decisions. Utilizing TLS data from 2,445 trees spanning seven European species, the researchers trained five YOLOv8 models with cross-validation, achieving a high mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps revealed that the models predominantly rely on crown features to classify species such as Silver Birch, European Beech, English Oak, and Norway Spruce. In contrast, stem features were more decisive for differentiating European Ash, Scots Pine, and Douglas Fir. Additionally, finer branch structures emerged as important features influencing model predictions. The similarity assessments made by the models aligned well with human expert opinion in distinguishing related species. Importantly, the study underscores the necessity of better understanding the decision-making processes of tree classification models to identify potential dataset and model biases, improve reliability, and build user confidence in model predictions. <div>
arXiv:2512.16950v1 Announce Type: cross 
Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories</title>
<link>https://arxiv.org/abs/2512.16954</link>
<guid>https://arxiv.org/abs/2512.16954</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video AI, character consistency, production script, visual anchoring, cultural bias<br /><br />Summary:<br /><br />Generating long, cohesive video stories with consistent characters remains a major challenge in text-to-video AI. This work proposes a novel filmmaker-like multi-stage pipeline where a large language model first creates a detailed production script to guide content generation. The script then directs a text-to-image model to produce consistent visuals for each character, serving as visual anchors to maintain identity throughout the narrative. These anchored visuals support a video generation model that synthesizes each scene individually, rather than generating the entire video in one step. Baseline comparisons demonstrate that removing the visual anchoring mechanism causes a dramatic decline in character consistency scores from 7.99 to 0.55, proving that visual priors are critical for preserving character identity. Additionally, the study explores cultural biases in current video generation models, identifying clear disparities in subject consistency and the level of dynamic content between Indian-themed and Western-themed video generations. These findings highlight both the effectiveness of the multi-stage approach and the need to address cultural representation issues within AI-generated video content. <div>
arXiv:2512.16954v1 Announce Type: cross 
Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</title>
<link>https://arxiv.org/abs/2512.16962</link>
<guid>https://arxiv.org/abs/2512.16962</guid>
<content:encoded><![CDATA[
<div> MemoryGraft, long-term memory, Retrieval-Augmented Generation, indirect injection attack, semantic imitation heuristic<br /><br />Summary: This paper introduces MemoryGraft, a novel indirect injection attack targeting Large Language Model (LLM) agents that utilize long-term memory and Retrieval-Augmented Generation (RAG) to enhance autonomous reasoning and learning. Unlike traditional prompt injection attacks, MemoryGraft implants malicious successful experiences into the agent’s persistent memory, exploiting the agent’s semantic imitation heuristic—a tendency to replicate patterns from previously successful tasks. The attack works by supplying benign-appearing ingestion-level artifacts that the agent processes during execution, causing it to construct a poisoned RAG store where harmful procedure templates are embedded alongside legitimate experiences. When the agent encounters new, semantically similar tasks, union retrieval mechanisms surface these malicious memories, leading to the adoption of unsafe behavior patterns. This results in persistent and stealthy behavioral drift across agent sessions, transforming the agent’s experience-based self-improvement into a liability. The study validates MemoryGraft on MetaGPT's DataInterpreter agent using GPT-4o, showing that even a small number of poisoned records can disproportionately influence the agent’s future performance on benign tasks. The authors provide code and evaluation data publicly to facilitate reproducibility and encourage further research on securing agent memory systems. <div>
arXiv:2512.16962v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</title>
<link>https://arxiv.org/abs/2512.16975</link>
<guid>https://arxiv.org/abs/2512.16975</guid>
<content:encoded><![CDATA[
<div> Adaptive tokenization, video compression, transformer, ELBO, information theory<br /><br />Summary:<br /><br />This paper addresses the problem of discrete video tokenization for long video sequences, highlighting the limitations of current tokenizers that compress content at a fixed rate, causing redundancy or information loss. Inspired by Shannon's information theory, the authors propose InfoTok, a novel adaptive video tokenization framework that optimally allocates tokens based on the informational content of video segments. They theoretically demonstrate that existing data-agnostic training methods are suboptimal in terms of representation length. To overcome this, the paper introduces an Evidence Lower Bound (ELBO)-based algorithm, which approaches theoretical optimality in token compression. Utilizing this theoretical foundation, the authors design a transformer-based adaptive compressor that dynamically adjusts token allocation according to the informational richness of the video data. Empirical evaluations show that InfoTok achieves state-of-the-art compression, reducing token usage by 20% without degrading performance and attaining a 2.3x compression rate outperforming previous heuristic adaptive methods. Consequently, InfoTok provides a more efficient and accurate video tokenization approach that can accommodate varying information densities in videos. The framework offers significant insights and a promising direction for future research in efficient video representation and compression methods. <div>
arXiv:2512.16975v1 Announce Type: cross 
Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations</title>
<link>https://arxiv.org/abs/2512.17027</link>
<guid>https://arxiv.org/abs/2512.17027</guid>
<content:encoded><![CDATA[
<div> Keywords: encyclopedic platforms, search engines, Wikipedia, Grokipedia, semantic alignment  

<br /><br />Summary:  
This study provides the first comparative analysis of search engine behaviors within two encyclopedic knowledge platforms, Wikipedia and Grokipedia, the latter being a fully AI-generated encyclopedia. Using nearly 10,000 neutral English words and their substrings as queries, the researchers gathered over 70,000 search engine results to evaluate semantic alignment, result overlap, and topical structure. The findings reveal that both platforms often produce results weakly related to the original queries, frequently surfacing unexpected content even from simple or innocuous queries. Despite these similarities, Wikipedia and Grokipedia differ significantly in their recommendation sets for identical queries. Further topical annotation and trajectory analysis demonstrate systematic variations in how each platform surfaces content categories and how the search results evolve during multi-step exploratory sessions. Overall, unexpected outcomes are common across both platforms' search mechanisms, though discrepancies exist in the topical distribution and types of query suggestions presented to users. These insights highlight the nuanced influence of search engine design on user exploration paths in different encyclopedic systems, emphasizing the need for deeper understanding to improve information discovery and user experience. <div>
arXiv:2512.17027v1 Announce Type: cross 
Abstract: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.
  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.
  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Women's Health Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2512.17028</link>
<guid>https://arxiv.org/abs/2512.17028</guid>
<content:encoded><![CDATA[
<div> Keywords: Women's Health Benchmark, large language models, accuracy, medical specialties, error types<br /><br />Summary:  
This paper introduces the Women's Health Benchmark (WHB), the first dedicated benchmark to evaluate the performance of large language models (LLMs) specifically in women's health contexts. The benchmark encompasses 96 validated model stumps spanning five key medical specialties: obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology. It categorizes queries into three types—patient queries, clinician queries, and evidence/policy queries—and assesses performance across eight distinct error types, including dosage/medication errors, missing critical information, outdated guidelines, incorrect treatment advice, incorrect factual information, missing or incorrect differential diagnoses, missed urgency, and inappropriate recommendations. The authors evaluated 13 state-of-the-art LLMs on this benchmark, revealing worrying results: models exhibit roughly a 60% failure rate overall when answering questions related to women's health. Performance varied significantly depending on medical specialty and error type, with a consistent weakness in identifying and addressing urgency indicators. Nonetheless, newer models such as GPT-5 demonstrated substantial progress, particularly in reducing inappropriate recommendations. The study highlights that, despite advancements, current AI chatbots remain unreliable for providing trustworthy and safe advice in the domain of women's health, underscoring the critical need for focused improvement and validation in this area. <div>
arXiv:2512.17028v1 Announce Type: cross 
Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation</title>
<link>https://arxiv.org/abs/2512.17029</link>
<guid>https://arxiv.org/abs/2512.17029</guid>
<content:encoded><![CDATA[
<div> Keywords: cybersickness detection, adversarial attacks, deep learning, VR testbed, mitigation strategies

<br /><br />Summary: This paper addresses the vulnerability of deep learning (DL)-based cybersickness detection and mitigation systems to adversarial attacks, which can degrade model performance and disrupt user immersive experiences (UIX). To fill the lack of open-source evaluation platforms, the authors introduce Adversarial-VR, a real-time VR testbed developed in Unity that integrates two state-of-the-art DL models, DeepTCN and Transformer, trained on the MazeSick dataset for real-time cybersickness severity detection. The testbed applies a dynamic visual tunneling mechanism to adjust the field-of-view based on model outputs to mitigate cybersickness adaptively. To evaluate robustness, three leading adversarial attack methods—MI-FGSM, PGD, and C&amp;W—are incorporated and shown to successfully fool the DL models and disable effective mitigation. Experiments use a custom VR Maze simulation and HTC Vive Pro Eye headset, with the entire implementation open-sourced to promote further research and development in the VR community. Results demonstrate significant decreases in detection accuracy due to adversarial attacks; notably, the C&amp;W attack causes a 5.94-fold accuracy reduction for the Transformer-based model compared to the clean scenario. This work highlights the critical need to consider adversarial robustness in DL-driven cybersickness systems to ensure reliable user comfort in VR environments. <div>
arXiv:2512.17029v1 Announce Type: cross 
Abstract: Deep learning (DL)-based automated cybersickness detection methods, along with adaptive mitigation techniques, can enhance user comfort and interaction. However, recent studies show that these DL-based systems are susceptible to adversarial attacks; small perturbations to sensor inputs can degrade model performance, trigger incorrect mitigation, and disrupt the user's immersive experience (UIX). Additionally, there is a lack of dedicated open-source testbeds that evaluate the robustness of these systems under adversarial conditions, limiting the ability to assess their real-world effectiveness. To address this gap, this paper introduces Adversarial-VR, a novel real-time VR testbed for evaluating DL-based cybersickness detection and mitigation strategies under adversarial conditions. Developed in Unity, the testbed integrates two state-of-the-art (SOTA) DL models: DeepTCN and Transformer, which are trained on the open-source MazeSick dataset, for real-time cybersickness severity detection and applies a dynamic visual tunneling mechanism that adjusts the field-of-view based on model outputs. To assess robustness, we incorporate three SOTA adversarial attacks: MI-FGSM, PGD, and C&amp;W, which successfully prevent cybersickness mitigation by fooling DL-based cybersickness models' outcomes. We implement these attacks using a testbed with a custom-built VR Maze simulation and an HTC Vive Pro Eye headset, and we open-source our implementation for widespread adoption by VR developers and researchers. Results show that these adversarial attacks are capable of successfully fooling the system. For instance, the C&amp;W attack results in a $5.94x decrease in accuracy for the Transformer-based cybersickness model compared to the accuracy without the attack.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics</title>
<link>https://arxiv.org/abs/2512.17048</link>
<guid>https://arxiv.org/abs/2512.17048</guid>
<content:encoded><![CDATA[
<div> Machine learning, conformal prediction, collider physics, uncertainty quantification, statistical inference  

<br /><br />Summary:  
This article addresses a critical challenge in modern collider physics where machine-learning models provide probabilistic outputs that often lack calibrated uncertainty estimates and finite-sample guarantees, hindering their use in rigorous statistical inference and decision-making. The authors introduce conformal prediction (CP) as a powerful, distribution-free method to calibrate any predictive model post-hoc without requiring retraining. CP offers exact finite-sample coverage guarantees under minimal assumptions, avoiding reliance on asymptotic theory or Gaussian approximations. The work demonstrates the versatility of CP across various machine-learning tasks relevant to high-energy physics, including regression, binary and multi-class classification, anomaly detection, and generative modeling. By applying a unified CP approach to publicly available collider datasets and multiple model architectures, the study shows that raw model outputs can be transformed into statistically valid prediction sets, typicality regions, and p-values with well-controlled false-positive rates. Although CP does not enhance the predictive accuracy of models, it crucially enforces honest, transparent uncertainty quantification and rigorous error control. The authors advocate for adopting conformal calibration as a standard component in machine-learning pipelines for collider research. This integration enables more reliable model interpretation, robust comparison of methods, and principled statistical decisions in both experimental analyses and theoretical phenomenology. <div>
arXiv:2512.17048v1 Announce Type: cross 
Abstract: Machine-learning techniques are essential in modern collider research, yet their probabilistic outputs often lack calibrated uncertainty estimates and finite-sample guarantees, limiting their direct use in statistical inference and decision-making. Conformal prediction (CP) provides a simple, distribution-free framework for calibrating arbitrary predictive models without retraining, yielding rigorous uncertainty quantification with finite-sample coverage guarantees under minimal exchangeability assumptions, without reliance on asymptotics, limit theorems, or Gaussian approximations. In this work, we investigate CP as a unifying calibration layer for machine-learning applications in high-energy physics. Using publicly available collider datasets and a diverse set of models, we show that a single conformal formalism can be applied across regression, binary and multi-class classification, anomaly detection, and generative modelling, converting raw model outputs into statistically valid prediction sets, typicality regions, and p-values with controlled false-positive rates. While conformal prediction does not improve raw model performance, it enforces honest uncertainty quantification and transparent error control. We argue that conformal calibration should be adopted as a standard component of machine-learning pipelines in collider physics, enabling reliable interpretation, robust comparisons, and principled statistical decisions in experimental and phenomenological analyses.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL</title>
<link>https://arxiv.org/abs/2512.17053</link>
<guid>https://arxiv.org/abs/2512.17053</guid>
<content:encoded><![CDATA[
<div> Text-to-SQL, Knowledge Distillation, Structured Reasoning, Chain-of-Thought, Small Language Models<br /><br />Summary:<br /><br />1. The paper addresses the challenge enterprises face when deploying Text-to-SQL systems, highlighting a trilemma between cost, security, and performance. 2. Current methods require a trade-off between expensive Large Language Models (LLMs) and less effective Small Language Models (SLMs), with existing efforts to improve SLMs relying on ambiguous unstructured Chain-of-Thought (CoT) distillation. 3. The authors propose Struct-SQL, a novel Knowledge Distillation framework that trains an SLM to mimic a large LLM by using a formal, structured reasoning approach derived from query execution plans as a clear and precise teaching signal. 4. Experiments show that SLMs distilled with this structured CoT method outperform those trained with unstructured CoT by 8.1% in accuracy, primarily due to fewer syntactic errors in generated SQL queries. 5. The study demonstrates that incorporating explicit logical blueprints for reasoning significantly improves reliable SQL generation in smaller models, offering a promising direction for cost-effective, secure, and high-performance Text-to-SQL implementations at the enterprise level. <div>
arXiv:2512.17053v1 Announce Type: cross 
Abstract: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues</title>
<link>https://arxiv.org/abs/2512.17060</link>
<guid>https://arxiv.org/abs/2512.17060</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent System, Transactional Analysis, ego states, information retrieval, LLM agents<br /><br />Summary:<br /><br />This paper addresses the limitations of current Large Language Model (LLM) agents in capturing the psychological depth and consistency characteristic of human thought and social interaction. It introduces a Multi-Agent System (MAS) architecture inspired by Transactional Analysis (TA) theory, segmenting each agent into three ego states—Parent, Adult, and Child—each representing distinct perspectives and reasoning styles. These ego states operate as separate knowledge structures, enabling more nuanced and human-like behavior modeling. To further enrich agent responses, the system incorporates an information retrieval mechanism that allows agents to access relevant contextual data stored in vector databases. The architecture is experimentally evaluated in a simulated dialogue setup through ablation tests comparing agents with and without information retrieval capabilities. Results from these tests demonstrate improved agent performance and suggest that integrating psychologically grounded frameworks like TA significantly enhances the realism and depth of multi-agent simulations. The work opens promising avenues for future research on the role of psychological theories in developing more sophisticated and human-like LLM-powered agents used in diverse fields such as social sciences, customer support, and education. <div>
arXiv:2512.17060v1 Announce Type: cross 
Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution</title>
<link>https://arxiv.org/abs/2512.17067</link>
<guid>https://arxiv.org/abs/2512.17067</guid>
<content:encoded><![CDATA[
<div> Keywords: social bots, promotional Twitter bots, behavioural features, non-stationarity, bot detection<br /><br />Summary:<br /><br />This study investigates the temporal behaviour of promotional social bots on Twitter, focusing on whether their behavioural features remain stationary over time. Using a dataset of 2,615 promotional bot accounts and 2.8 million tweets, the authors constructed yearly time series for ten content-based meta-features. Statistical tests, including Augmented Dickey-Fuller and KPSS, alongside linear trend analyses, revealed that all ten features are non-stationary; notably, nine features generally increase over time while language diversity slightly declines. The analysis further stratified bots by activation generation and account age, uncovering systematic behavioural differences: second-generation bots are the most active with a high frequency of links; short-lived bots exhibit intense, repetitive activity with heavy hashtag and URL use; long-lived bots demonstrate lower activity but greater linguistic diversity and more varied emoji usage. Additionally, feature co-occurrence across generations was examined using 18 binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media across 153 pairs. Chi-square tests found almost all pairs dependent, with Spearman correlations showing shifts in strength and sometimes polarity; for example, associations like multiple hashtags with media and sentiment with URLs intensified, while others reversed from weakly positive to weakly or moderately negative. Overall, this evidence indicates that promotional social bots adapt over time both in individual behavioural features and in their interdependencies, highlighting important implications for the design and evaluation of bot-detection systems reliant on historical behavioural data. <div>
arXiv:2512.17067v1 Announce Type: cross 
Abstract: Social bots are now deeply embedded in online platforms for promotion, persuasion, and manipulation. Most bot-detection systems still treat behavioural features as static, implicitly assuming bots behave stationarily over time. We test that assumption for promotional Twitter bots, analysing change in both individual behavioural signals and the relationships between them. Using 2,615 promotional bot accounts and 2.8M tweets, we build yearly time series for ten content-based meta-features. Augmented Dickey-Fuller and KPSS tests plus linear trends show all ten are non-stationary: nine increase over time, while language diversity declines slightly.
  Stratifying by activation generation and account age reveals systematic differences: second-generation bots are most active and link-heavy; short-lived bots show intense, repetitive activity with heavy hashtag/URL use; long-lived bots are less active but more linguistically diverse and use emojis more variably. We then analyse co-occurrence across generations using 18 interpretable binary features spanning actions, topic similarity, URLs, hashtags, sentiment, emojis, and media (153 pairs). Chi-square tests indicate almost all pairs are dependent. Spearman correlations shift in strength and sometimes polarity: many links (e.g. multiple hashtags with media; sentiment with URLs) strengthen, while others flip from weakly positive to weakly or moderately negative. Later generations show more structured combinations of cues.
  Taken together, these studies provide evidence that promotional social bots adapt over time at both the level of individual meta-features and the level of feature interdependencies, with direct implications for the design and evaluation of bot-detection systems trained on historical behavioural features.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</title>
<link>https://arxiv.org/abs/2512.17079</link>
<guid>https://arxiv.org/abs/2512.17079</guid>
<content:encoded><![CDATA[
<div> Chain-of-thought, mathematical reasoning, error recovery, reinforcement learning, robustness<br /><br />Summary:  
1. The paper addresses brittleness in large language models' mathematical reasoning, where early errors in chain-of-thought (CoT) prompting often lead to incorrect final answers.  
2. It explores training models on intentionally flawed reasoning traces containing controlled errors—either calculation slip-ups (like sign flips or dropped terms) or reasoning mistakes (such as misapplied rules or unjustified steps).  
3. The authors fine-tune Qwen3-4B using GRPO reinforcement learning with a binary reward on final answer correctness, utilizing competition-level MATH-lighteval problems with one inserted error per CoT prefix.  
4. Their Mixed-CoT-RL model demonstrates parity with conventional RL on clean problems (41% accuracy) while outperforming it significantly on problems prefilled with flawed reasoning (24% versus 19%).  
5. Training only on clean data degrades robustness below the untuned baseline, revealing that conventional approaches increase vulnerability to misleading inputs.  
6. Among error types, training on reasoning errors notably improves robustness more than training solely on calculation errors, with mixed-error training providing the best outcomes.  
7. Findings suggest that exposure to flawed reasoning during training can enhance models' error detection and correction capabilities without compromising accuracy, advancing more robust mathematical problem-solving in LLMs. <div>
arXiv:2512.17079v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation</title>
<link>https://arxiv.org/abs/2512.17083</link>
<guid>https://arxiv.org/abs/2512.17083</guid>
<content:encoded><![CDATA[
<div> Keywords: dialogue topic segmentation, evaluation metrics, boundary detection, segment coherence, annotation granularity<br /><br />Summary:<br /><br />This paper addresses the limitations of current evaluation practices in dialogue topic segmentation, which predominantly rely on strict boundary matching and F1-based metrics. It highlights that modern conversational systems, especially those based on large language models (LLMs), depend heavily on accurate segmentation for managing context efficiently beyond fixed context windows. The authors propose a new evaluation objective that prioritizes boundary density and segment coherence alongside a novel window-tolerant F1 metric (W-F1) to better capture segmentation quality. Through extensive experiments using multiple segmentation methods and eight diverse dialogue datasets—including task-oriented, open-domain, meeting-style, and synthetic dialogues—the study finds that differences in reported performance often stem from inconsistencies in annotation granularity and sparse boundary labels, rather than actual improvements in segmentation models. Notably, the research observes a high degree of segment coherence but also extreme oversegmentation relative to the sparse ground truth labels, which leads to misleadingly low exact-match F1 scores. The authors conclude that dialogue topic segmentation should be reframed as a problem of selecting the appropriate granularity level rather than identifying a single "correct" set of boundaries. They operationalize this perspective by separating the processes of boundary scoring and boundary selection to improve evaluation robustness and interpretability. <div>
arXiv:2512.17083v1 Announce Type: cross 
Abstract: Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of prior work, evaluation practice in dialogue topic segmentation remains dominated by strict boundary matching and F1-based metrics, even as modern LLM-based conversational systems increasingly rely on segmentation to manage conversation history beyond the model's fixed context window, where unstructured context accumulation degrades efficiency and coherence.
  This paper introduces an evaluation objective for dialogue topic segmentation that treats boundary density and segment coherence as primary criteria, alongside window-tolerant F1 (W-F1). Through extensive cross-dataset empirical evaluation, we show that reported performance differences across dialogue segmentation benchmarks are driven not by model quality, but by annotation granularity mismatches and sparse boundary labels. This indicates that many reported improvements arise from evaluation artifacts rather than improved boundary detection.
  We evaluated multiple, structurally distinct dialogue segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Across these settings, we observe high segment coherence combined with extreme oversegmentation relative to sparse labels, producing misleadingly low exact-match F1 scores. We show that topic segmentation is best understood as selecting an appropriate granularity rather than predicting a single correct boundary set. We operationalize this view by explicitly separating boundary scoring from boundary selection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Square Tensor Networks and Circuits Without Squaring Them</title>
<link>https://arxiv.org/abs/2512.17090</link>
<guid>https://arxiv.org/abs/2512.17090</guid>
<content:encoded><![CDATA[
<div> Squared tensor networks, squared circuits, marginalization, unitary parameterization, distribution estimation<br /><br />Summary:<br /><br />This paper addresses the computational complexity challenges that arise when performing marginalization and partition function computation in squared tensor networks (TNs) and their extensions, squared circuits, used for distribution estimation. The squaring operation increases complexity, limiting their practical applicability in machine learning tasks that require closed-form marginalization. Prior approaches parameterized canonical forms of TNs using unitary matrices to simplify marginal computations, but these methods do not extend to squared circuits, which can represent factorizations that are not directly captured by TNs. The authors propose a novel parameterization for squared circuits inspired by orthogonality in canonical TNs and determinism principles in circuits to enable tractable maximization. This parameterization facilitates efficient marginalization even for circuit factorizations outside the TN framework, overcoming their otherwise computationally hard structures. Experimental results demonstrate that the proposed parameterization and conditions maintain the expressiveness of squared circuits while significantly improving learning and computational efficiency during distribution estimation. This work thereby broadens the applicability of squared circuits by enabling efficient marginalization without sacrificing model power. <div>
arXiv:2512.17090v1 Announce Type: cross 
Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</title>
<link>https://arxiv.org/abs/2512.17091</link>
<guid>https://arxiv.org/abs/2512.17091</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, MPC planning, hierarchical structure, adaptive sampling, data efficiency<br /><br />Summary: This paper presents a novel method for solving hierarchical planning problems by integrating reinforcement learning (RL) with Model Predictive Control (MPC) planning, specifically using the MPPI sampler. The approach tightly couples RL and MPC to create an adaptive process where RL actions guide MPPI sampling, and aggregated MPPI samples refine value estimation. This adaptive feedback loop enables more focused exploration in areas with uncertain value estimates, enhancing training robustness and policy quality. The method demonstrates versatility across various complex domains such as race driving, a modified Acrobot system, and Lunar Lander with obstacles. Experimental results reveal substantial improvements in both data efficiency and task success rates, including up to a 72% increase in success compared to prior methods. Additionally, the approach achieves faster convergence, showing more than twice the speed (2.1x) relative to non-adaptive sampling techniques. Overall, this integrated RL and MPC framework offers a robust, efficient, and generalizable solution for challenging hierarchical planning tasks across different applications. <div>
arXiv:2512.17091v1 Announce Type: cross 
Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</title>
<link>https://arxiv.org/abs/2512.17100</link>
<guid>https://arxiv.org/abs/2512.17100</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactual explanations, multivariate time series, interpretability, deep neural networks, ECG classification  

<br /><br />Summary:  
1. The article addresses the challenge of the black-box nature of deep neural networks in classifying complex time series data, particularly in sensitive fields like healthcare.  
2. It introduces UniCoMTE, a model-agnostic framework designed to generate counterfactual explanations for multivariate time series classifiers by identifying key temporal features that influence model predictions through input modifications.  
3. UniCoMTE works directly on raw time series data and is compatible with various model architectures, making it versatile for multiple applications.  
4. The framework is evaluated on a time series ECG classifier, with explanation quality measured by comparing comprehensibility against established explanation methods (LIME and SHAP), alongside an assessment of explanation generalizability.  
5. Clinical utility is validated via medical expert questionnaires reviewing the counterfactual explanations alongside original ECG samples, demonstrating that UniCoMTE produces concise, stable, and human-aligned explanations that surpass existing methods in clarity and practical relevance.  
6. By effectively linking model predictions to meaningful signal patterns, UniCoMTE significantly advances the interpretability of deep learning models in real-world time series applications, which is crucial for trust and adoption in high-stakes environments. <div>
arXiv:2512.17100v1 Announce Type: cross 
Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</title>
<link>https://arxiv.org/abs/2512.17131</link>
<guid>https://arxiv.org/abs/2512.17131</guid>
<content:encoded><![CDATA[
<div> Generalized Primal Averaging, Nesterov's method, iterate averaging, optimizer speedup, convergence guarantee<br /><br />Summary:<br /><br />1. The paper proposes Generalized Primal Averaging (GPA), an extension of Nesterov’s primal averaging method that addresses limitations in recent averaging-based optimizers like single-worker DiLoCo and Schedule-Free (SF) in non-distributed settings.<br /><br />2. Existing approaches improve base optimizers, such as AdamW, through different averaging strategies: SF uses a uniform average of past weights; DiLoCo uses implicit averaging with periodic pseudo-gradient aggregation but involves a two-loop structure increasing memory use and hyperparameters.<br /><br />3. GPA introduces a decoupling of the interpolation constant in Nesterov’s primal averaging formulation, enabling smooth, stepwise iterate averaging that generalizes and improves on DiLoCo.<br /><br />4. Empirically, GPA consistently outperforms DiLoCo by eliminating the two-loop structure, simplifying hyperparameter tuning, and requiring only one additional memory buffer.<br /><br />5. On benchmarks like Llama-160M and ImageNet ViT, GPA achieves significant speedups (up to 27%) over AdamW to reach equivalent validation performance.<br /><br />6. The paper also provides theoretical guarantees showing that GPA can match or exceed the convergence rates (regret bounds of O(√T)) of the underlying base optimizer by appropriately choosing interpolation constants. <div>
arXiv:2512.17131v1 Announce Type: cross 
Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</title>
<link>https://arxiv.org/abs/2512.17137</link>
<guid>https://arxiv.org/abs/2512.17137</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI reconstruction, deep learning, scalable model, coil sensitivity estimation, data consistency<br /><br />Summary:<br /><br />This paper presents the Scalable Deep Unrolled Model (SDUM), a universal deep learning framework designed for clinical MRI reconstruction across diverse imaging protocols, including various anatomical targets, contrast types, sampling patterns, and acceleration factors. Unlike existing models that are protocol-specific, SDUM incorporates a Restormer-based reconstructor, learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training to achieve wide applicability. SDUM demonstrates foundation-model-like scaling, with reconstruction quality measured by PSNR improving approximately logarithmically with model parameter increases, achieving a strong correlation (r = 0.986) up to 18 cascades, highlighting predictable gains with model depth. When trained on heterogeneous data, a single SDUM model achieves state-of-the-art performance in all four tracks of the CMRxRecon2025 challenge, including multi-center, multi-disease, 5T, and pediatric datasets, surpassing task-specific specialized baselines by up to +1.0 dB. It also outperforms the previous state-of-the-art PromptMR+ method on CMRxRecon2024 by +0.55 dB and surpasses PC-RNN by +1.8 dB on the fastMRI brain dataset. Ablation studies confirm the effectiveness of individual components, with SWDC adding +0.43 dB, per-cascade CSME +0.51 dB, and UC +0.38 dB. Overall, SDUM represents a scalable and practical universal approach for high-quality MRI reconstruction. <div>
arXiv:2512.17137v1 Announce Type: cross 
Abstract: Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases</title>
<link>https://arxiv.org/abs/2512.17172</link>
<guid>https://arxiv.org/abs/2512.17172</guid>
<content:encoded><![CDATA[
<div> Keywords: augmented reality, explainable AI, large language models, personalized explanations, user experience<br /><br />Summary:  
This paper addresses the challenge of providing effective explainability in AI-driven augmented reality (AR) systems used in daily life. Traditional explainable AI (XAI) techniques often produce fragmented and static explanations that do not adapt to the dynamic, personalized context of AR interactions. To overcome these limitations, the authors introduce PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate real-time, context-aware, and personalized explanations within AR environments. Unlike conventional methods that separately target explainability dimensions (such as when, what, and how), PILAR uses a unified LLM-based approach that tailors explanations dynamically according to user needs, enhancing trust and engagement. The framework is demonstrated through an open-source AR application focused on personalized recipe recommendations, integrating real-time object detection, dietary preference consideration, and LLM-generated explanations. A user study with 16 participants compared the PILAR LLM-based explanation interface against a traditional template-based system. Results indicate that the LLM-driven interface significantly improved user performance by enabling task completion 40% faster, while also increasing user satisfaction, ease of use, and perceived transparency. The study highlights the potential of LLMs to create more intuitive and human-centric explainability in real-world AI-powered AR applications. <div>
arXiv:2512.17172v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors</title>
<link>https://arxiv.org/abs/2512.17180</link>
<guid>https://arxiv.org/abs/2512.17180</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive Reinforcement Learning, Teacher Selection, Conservative Bias, Performance Thresholds, Human-Robot Collaboration<br /><br />Summary: This paper investigates teacher selection dynamics in Interactive Reinforcement Learning (IRL) and discovers a surprising conservative bias where learning agents overwhelmingly prefer teachers offering low but consistent rewards over those providing significantly higher rewards. Through 1,250 navigation task experiments with multiple expert teachers, it was found that agents systematically select the lowest-reward teacher 93.16% of the time, prioritizing consistency above optimal reward gain. Additionally, the study identifies crucial performance thresholds for the IRL framework: if teacher availability (rho) or accuracy (omega) drop below 0.6, the learning framework fails catastrophically. Despite this, under conditions of concept drift, the IRL framework demonstrates a substantial 159% improvement over baseline Q-learning methods. These insights challenge the existing assumptions that optimal teaching in reinforcement learning should focus on maximizing reward signals. Instead, agents appear to intrinsically value safety and stability in their learning process. The findings hold significant implications for human-robot collaboration, suggesting that human trainers’ natural preferences for safety and consistency may align well with agent behavior. This alignment could influence future training paradigms, especially in safety-critical robotic applications where cautious learning is vital. <div>
arXiv:2512.17180v1 Announce Type: cross 
Abstract: Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</title>
<link>https://arxiv.org/abs/2512.17185</link>
<guid>https://arxiv.org/abs/2512.17185</guid>
<content:encoded><![CDATA[
<div> Keywords: systemic risk, financial crises, graph neural networks, multi-layer graphs, early-warning signals  
  
<br /><br />Summary:  
1. The paper addresses the challenge of predicting financial crises, which result from accumulating structural vulnerabilities across sectors, markets, and investor behaviors rather than isolated price changes.  
2. It introduces the Systemic Risk Radar (SRR), a novel framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and transitions into crash regimes.  
3. The study evaluates SRR on three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock, comparing snapshot graph neural networks (GNNs), a simplified temporal GNN prototype, and traditional models like logistic regression and Random Forests.  
4. Findings demonstrate that incorporating structural network information through graph-derived features offers more effective early-warning signals than models relying solely on individual market features.  
5. The paper motivates future enhancements to SRR, such as adding graph layers representing sector and factor exposures or sentiment, and employing more expressive temporal architectures like LSTM, GRU, or Transformer encoders to better capture diverse crisis dynamics. <div>
arXiv:2512.17185v1 Announce Type: cross 
Abstract: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.
  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.
  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening</title>
<link>https://arxiv.org/abs/2512.17202</link>
<guid>https://arxiv.org/abs/2512.17202</guid>
<content:encoded><![CDATA[
<div> Pansharpening, Diffusion Models, End-to-End Models, Lightweight Network, Image Fusion<br /><br />Summary:<br /><br />Pansharpening is an essential image fusion task that combines low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to generate high-resolution multispectral images (HRMSI). Recent advances in diffusion models (DM) and end-to-end (E2E) models have improved pansharpening performance. Traditional diffusion models use multi-step processes to estimate the residual between LRMSI and HRMSI, but these are computationally expensive and time-consuming. End-to-end models, while faster, often struggle due to simple structures and limited prior knowledge. To address these issues, the authors propose a novel four-stage training strategy that integrates a one-step diffusion model with an end-to-end network, called Fose, creating a lightweight fusion network. The approach involves performing one-step distillation on an enhanced state-of-the-art diffusion model, compressing the inference process from 50 steps to a single step. Subsequently, the method fuses the one-step DM with an E2E model using lightweight ensemble blocks. Extensive experiments on three popular benchmarks demonstrate that Fose significantly outperforms baseline methods in accuracy and efficiency. Notably, Fose achieves a 7.42 times speedup compared to the baseline diffusion model while delivering superior pansharpening results. The code and model are publicly available for further research and development. <div>
arXiv:2512.17202v1 Announce Type: cross 
Abstract: Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines</title>
<link>https://arxiv.org/abs/2512.17215</link>
<guid>https://arxiv.org/abs/2512.17215</guid>
<content:encoded><![CDATA[
<div> Keywords: pipeline robot, extended Kalman filtering, inertial navigation, wheel odometer, dead reckoning

<br /><br />Summary: This paper addresses challenges in locating gas pipelines, especially in complex and curved pipeline scenarios where traditional instruments face issues like cable entanglement and lack of flexibility. To overcome these, the authors designed a self-propelled pipeline robot capable of autonomously navigating and locating pipelines without external dragging. Recognizing the limitations of conventional visual and laser mapping methods, which are affected by lighting and feature scarcity inside pipelines, the paper proposes a robust location method integrating inertial navigation and wheel odometers. The method uses an inertial measurement unit (IMU) to initially acquire the robot's body attitude angle, which is then refined using an extended Kalman filtering (EKF) algorithm to enhance accuracy. Combining this improved attitude estimation with wheel odometer data results in high-precision pipeline localization. During testing, it was found that tight contact between the robot's roll wheels and pipe walls reduces slippage but excessive tightness compromises motion flexibility due to friction, necessitating a balance between positioning accuracy and mobility. Experiments in a rectangular loop pipeline validated the effectiveness of the proposed dead reckoning approach, demonstrating accurate and reliable pipeline location performance in complex environments. <div>
arXiv:2512.17215v1 Announce Type: cross 
Abstract: In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes</title>
<link>https://arxiv.org/abs/2512.17218</link>
<guid>https://arxiv.org/abs/2512.17218</guid>
<content:encoded><![CDATA[
<div> deepfake, Islamic ethics, Maqasid al-Shariah, digital literacy, technology regulation<br /><br />Summary:<br /><br />The paper addresses the challenges posed by deepfake technology, emphasizing its potential to spread false information, compromise online identities, and erode public trust. It identifies that conventional reactive technological methods are insufficient because they fail to address deeper moral, intentional, and social implications of deepfake misuse. The study is grounded in a Systematic Literature Review (SLR) using PRISMA to analyze ten key sources from 2018 to 2025, highlighting ethical gaps and regulatory needs. It proposes an Islamic ethical framework based on Maqasid al-Shariah principles, focusing particularly on protecting honor (hifz al-ird) and the self (hifz al-nafs), as a normative foundation for governing technology use responsibly. The research advances three strategic recommendations: first, regulatory reforms that acknowledge intangible psychological and reputational harms; second, enhanced technology management guided by moral values such as justice (adl), trust, and transparency; and third, fostering public digital literacy anchored in the principle of tabayyun, which advocates careful examination and caution. The study concludes that applying Islamic ethics encourages a paradigm shift from punitive reactions to preventative strategies emphasizing human dignity, harm prevention, and the common good within the digital environment. <div>
arXiv:2512.17218v1 Announce Type: cross 
Abstract: The significant development of deepfake technology powered by artificial intelligence (AI) has sparked worldwide concerns about the alteration of false information, the usurpation of online identities, and the decline of public confidence in the authenticity of online content. These incidents not only raise technical issues but also carry complex moral implications, rendering conventional, technologically driven, and reactive management methods inadequate to address the underlying causes of the problem, including intent, morality, and potential intangible social impacts. Based on these issues, this study aims to formulate a comprehensive Islamic ethical framework that can serve as a more comprehensive preventative tool to mitigate the risks of misuse of deepfakes. The study employed a Systematic Literature Review (SLR) guided by PRISMA, selecting ten primary sources published between 2018 and 2025 to identify ethical deficiencies, regulatory needs, and appropriate normative solutions. The analysis shows that the integration of the principles of (Maqasid al-Shariah) particularly (hifz al-ird) protecting honor and (hifz al-nafs) protecting the self, provides a strong normative basis for regulating the responsible use of technology. This study yields three strategic recommendations: regulatory changes that recognize the intangible and psychological harm caused by reputational damage; improved technology management through moral scrutiny that upholds the values of justice (adl), trust, and openness; and increased public digital literacy based on the principle of (tabayyun) examination and caution. Overall, this study concludes that the application of Islamic ethics offers a shift in thinking from punitive mechanisms to preventative approaches that focus on protecting human dignity, preventing harm, and strengthening the common good in the digital age.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics</title>
<link>https://arxiv.org/abs/2512.17239</link>
<guid>https://arxiv.org/abs/2512.17239</guid>
<content:encoded><![CDATA[
<div> Urban mobility, synthetic data, privacy-preserving, origin-destination matrix, behavioral constraints<br /><br />Summary:<br /><br />This study addresses the challenge of generating privacy-preserving urban mobility data by synthesizing daily human trajectories from aggregated inputs, avoiding the risks of re-identification present in individual GPS traces. The method combines origin-destination (OD) flow data with two behavioral constraints: dwell-travel time quantiles, provided only as coarse summary statistics, and a universal statistical law governing the daily number of visited locations. These are embedded in a multi-objective optimization framework that ensures the synthetic data realistically mimic human mobility patterns without requiring personal identifiers. Validation is conducted in two contrasting Japanese regions—Tokyo’s dense 23 special wards and the diverse urban-suburban context of Fukuoka Prefecture. The synthetic mobility datasets accurately reproduce distributions of dwell-travel times and visit frequency while maintaining OD matrix consistency within natural fluctuation ranges observed daily. This framework offers a practical solution for creating high-resolution and realistic urban mobility data that can be safely shared and used by governments, urban planners, and industries. The approach facilitates scalable and reliable analytics without compromising individual privacy, supporting deployment in both policy-making and commercial sectors. <div>
arXiv:2512.17239v1 Announce Type: cross 
Abstract: Urban mobility data are indispensable for urban planning, transportation demand forecasting, pandemic modeling, and many other applications; however, individual mobile phone-derived Global Positioning System traces cannot generally be shared with third parties owing to severe re-identification risks. Aggregated records, such as origin-destination (OD) matrices, offer partial insights but fail to capture the key behavioral properties of daily human movement, limiting realistic city-scale analyses.
  This study presents a privacy-preserving synthetic mobility dataset that reconstructs daily trajectories from aggregated inputs. The proposed method integrates OD flows with two complementary behavioral constraints: (1) dwell-travel time quantiles that are available only as coarse summary statistics and (2) the universal law for the daily distribution of the number of visited locations. Embedding these elements in a multi-objective optimization framework enables the reproduction of realistic distributions of human mobility while ensuring that no personal identifiers are required.
  The proposed framework is validated in two contrasting regions of Japan: (1) the 23 special wards of Tokyo, representing a dense metropolitan environment; and (2) Fukuoka Prefecture, where urban and suburban mobility patterns coexist. The resulting synthetic mobility data reproduce dwell-travel time and visit frequency distributions with high fidelity, while deviations in OD consistency remain within the natural range of daily fluctuations.
  The results of this study establish a practical synthesis pathway under real-world constraints, providing governments, urban planners, and industries with scalable access to high-resolution mobility data for reliable analytics without the need for sensitive personal records, and supporting practical deployments in policy and commercial domains.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition</title>
<link>https://arxiv.org/abs/2512.17247</link>
<guid>https://arxiv.org/abs/2512.17247</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, Persian, noise robustness, Error Level Noise, Whisper

<br /><br />Summary:  
This study addresses the challenge of Automatic Speech Recognition (ASR) performance degradation in noisy environments, focusing specifically on the low-resource Persian language. The authors propose a noise-sensitive ASR error correction framework that leverages multiple hypotheses generated by a modified Whisper-large decoder. A novel metric called Error Level Noise (ELN) is introduced, which captures semantic and token-level disagreements across multiple hypotheses to quantify linguistic distortions caused by noise. ELN serves as a direct representation of noise-induced uncertainty, enabling a large language model (LLM) to assess the reliability of each hypothesis during error correction. The research evaluates three models: the base LLaMA-2-7B without fine-tuning, a fine-tuned LLaMA-2-7B trained only on text hypotheses, and a noise-conditioned fine-tuned model integrating ELN embeddings at sentence and word levels. Experimental results on noisy Persian speech show that the ELN-conditioned model substantially reduces the Word Error Rate (WER) from 31.10% with raw Whisper to 24.84% on a challenging Mixed Noise test set. This outperforms the fine-tuned text-only baseline (30.79%) and highlights that the original unfine-tuned LLaMA-2-7B model is ineffective for correcting Persian ASR errors alone. The findings demonstrate the efficacy of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR under real-world noisy conditions. <div>
arXiv:2512.17247v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\% (Raw Whisper) to 24.84\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</title>
<link>https://arxiv.org/abs/2512.17251</link>
<guid>https://arxiv.org/abs/2512.17251</guid>
<content:encoded><![CDATA[
arXiv:2512.17251v1 Announce Type: cross 
Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework</title>
<link>https://arxiv.org/abs/2512.17255</link>
<guid>https://arxiv.org/abs/2512.17255</guid>
<content:encoded><![CDATA[
arXiv:2512.17255v1 Announce Type: cross 
Abstract: Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</title>
<link>https://arxiv.org/abs/2512.17259</link>
<guid>https://arxiv.org/abs/2512.17259</guid>
<content:encoded><![CDATA[
arXiv:2512.17259v1 Announce Type: cross 
Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators</title>
<link>https://arxiv.org/abs/2512.17267</link>
<guid>https://arxiv.org/abs/2512.17267</guid>
<content:encoded><![CDATA[
arXiv:2512.17267v1 Announce Type: cross 
Abstract: Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Generalization in Role-Playing Models via Information Theory</title>
<link>https://arxiv.org/abs/2512.17270</link>
<guid>https://arxiv.org/abs/2512.17270</guid>
<content:encoded><![CDATA[
arXiv:2512.17270v1 Announce Type: cross 
Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.17278</link>
<guid>https://arxiv.org/abs/2512.17278</guid>
<content:encoded><![CDATA[
arXiv:2512.17278v1 Announce Type: cross 
Abstract: Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subjective Question Generation and Answer Evaluation using NLP</title>
<link>https://arxiv.org/abs/2512.17289</link>
<guid>https://arxiv.org/abs/2512.17289</guid>
<content:encoded><![CDATA[
arXiv:2512.17289v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track</title>
<link>https://arxiv.org/abs/2512.17293</link>
<guid>https://arxiv.org/abs/2512.17293</guid>
<content:encoded><![CDATA[
arXiv:2512.17293v1 Announce Type: cross 
Abstract: This paper presents a lightweight text-to-speech (TTS) system developed for the WildSpoof Challenge TTS Track. Our approach fine-tunes the recently released open-weight TTS model, \textit{Supertonic}\footnote{\url{https://github.com/supertone-inc/supertonic}}, with Self-Purifying Flow Matching (SPFM) to enable robust adaptation to in-the-wild speech. SPFM mitigates label noise by comparing conditional and unconditional flow matching losses on each sample, routing suspicious text--speech pairs to unconditional training while still leveraging their acoustic information. The resulting model achieves the lowest Word Error Rate (WER) among all participating teams, while ranking second in perceptual metrics such as UTMOS and DNSMOS. These findings demonstrate that efficient, open-weight architectures like Supertonic can be effectively adapted to diverse real-world speech conditions when combined with explicit noise-handling mechanisms such as SPFM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</title>
<link>https://arxiv.org/abs/2512.17299</link>
<guid>https://arxiv.org/abs/2512.17299</guid>
<content:encoded><![CDATA[
arXiv:2512.17299v1 Announce Type: cross 
Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</title>
<link>https://arxiv.org/abs/2512.17316</link>
<guid>https://arxiv.org/abs/2512.17316</guid>
<content:encoded><![CDATA[
arXiv:2512.17316v1 Announce Type: cross 
Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</title>
<link>https://arxiv.org/abs/2512.17319</link>
<guid>https://arxiv.org/abs/2512.17319</guid>
<content:encoded><![CDATA[
arXiv:2512.17319v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</title>
<link>https://arxiv.org/abs/2512.17352</link>
<guid>https://arxiv.org/abs/2512.17352</guid>
<content:encoded><![CDATA[
arXiv:2512.17352v1 Announce Type: cross 
Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data</title>
<link>https://arxiv.org/abs/2512.17370</link>
<guid>https://arxiv.org/abs/2512.17370</guid>
<content:encoded><![CDATA[
arXiv:2512.17370v1 Announce Type: cross 
Abstract: Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.17396</link>
<guid>https://arxiv.org/abs/2512.17396</guid>
<content:encoded><![CDATA[
arXiv:2512.17396v1 Announce Type: cross 
Abstract: In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques</title>
<link>https://arxiv.org/abs/2512.17411</link>
<guid>https://arxiv.org/abs/2512.17411</guid>
<content:encoded><![CDATA[
arXiv:2512.17411v1 Announce Type: cross 
Abstract: Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimisation of Aircraft Maintenance Schedules</title>
<link>https://arxiv.org/abs/2512.17412</link>
<guid>https://arxiv.org/abs/2512.17412</guid>
<content:encoded><![CDATA[
arXiv:2512.17412v1 Announce Type: cross 
Abstract: We present an aircraft maintenance scheduling problem, which requires suitably qualified staff to be assigned to maintenance tasks on each aircraft. The tasks on each aircraft must be completed within a given turn around window so that the aircraft may resume revenue earning service. This paper presents an initial study based on the application of an Evolutionary Algorithm to the problem. Evolutionary Algorithms evolve a solution to a problem by evaluating many possible solutions, focusing the search on those solutions that are of a higher quality, as defined by a fitness function. In this paper, we benchmark the algorithm on 60 generated problem instances to demonstrate the underlying representation and associated genetic operators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</title>
<link>https://arxiv.org/abs/2512.17419</link>
<guid>https://arxiv.org/abs/2512.17419</guid>
<content:encoded><![CDATA[
arXiv:2512.17419v1 Announce Type: cross 
Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Reproducibility Study of BSARec for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2512.17442</link>
<guid>https://arxiv.org/abs/2512.17442</guid>
<content:encoded><![CDATA[
arXiv:2512.17442v1 Announce Type: cross 
Abstract: In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.17444</link>
<guid>https://arxiv.org/abs/2512.17444</guid>
<content:encoded><![CDATA[
arXiv:2512.17444v1 Announce Type: cross 
Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</title>
<link>https://arxiv.org/abs/2512.17452</link>
<guid>https://arxiv.org/abs/2512.17452</guid>
<content:encoded><![CDATA[
arXiv:2512.17452v1 Announce Type: cross 
Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.17453</link>
<guid>https://arxiv.org/abs/2512.17453</guid>
<content:encoded><![CDATA[
arXiv:2512.17453v1 Announce Type: cross 
Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding</title>
<link>https://arxiv.org/abs/2512.17461</link>
<guid>https://arxiv.org/abs/2512.17461</guid>
<content:encoded><![CDATA[
arXiv:2512.17461v1 Announce Type: cross 
Abstract: This article shows how fair voting methods can be a catalyst for change in the way we make collective decisions, and how such change can promote long-awaited upgrades of democracy. Based on real-world evidence from democratic innovations in participatory budgeting, in Switzerland and beyond, I highlight a trilogy of key research results: Fair voting methods achieve to be (i) legitimacy incubator, (ii) novel impact accelerator and (iii) safeguard for risks of artificial intelligence (AI). Compared to majoritarian voting methods, combining expressive ballot formats (e.g. cumulative voting) with ballot aggregation methods that promote proportional representation (e.g. equal shares) results in more winners and higher (geographical) representation of citizens. Such fair voting methods are preferred and found fairer even by voters who do not win, while promoting stronger democratic values for citizens such as altruism and compromise. They also result in new resourceful ideas to put for voting, which are cost-effective and win, especially in areas of welfare, education and culture. Strikingly, fair voting methods are also more resilient to biases and inconsistencies of generative AI in emerging scenarios of AI voting assistance or AI representation of voters who would be likely to abstain. I also review the relevance of such upgrades for democracies in crisis, such as the one of Greece featured in the recent study of `Unmute Democracy'. Greek democracy can build stronger resilience via higher representation of citizens in democratic processes as well as democratic innovations in participation. Fair voting methods can be a catalyst for both endeavors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</title>
<link>https://arxiv.org/abs/2512.17462</link>
<guid>https://arxiv.org/abs/2512.17462</guid>
<content:encoded><![CDATA[
arXiv:2512.17462v1 Announce Type: cross 
Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</title>
<link>https://arxiv.org/abs/2512.17504</link>
<guid>https://arxiv.org/abs/2512.17504</guid>
<content:encoded><![CDATA[
arXiv:2512.17504v1 Announce Type: cross 
Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models</title>
<link>https://arxiv.org/abs/2512.17519</link>
<guid>https://arxiv.org/abs/2512.17519</guid>
<content:encoded><![CDATA[
arXiv:2512.17519v1 Announce Type: cross 
Abstract: We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</title>
<link>https://arxiv.org/abs/2512.17527</link>
<guid>https://arxiv.org/abs/2512.17527</guid>
<content:encoded><![CDATA[
arXiv:2512.17527v1 Announce Type: cross 
Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title>
<link>https://arxiv.org/abs/2512.17532</link>
<guid>https://arxiv.org/abs/2512.17532</guid>
<content:encoded><![CDATA[
arXiv:2512.17532v1 Announce Type: cross 
Abstract: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</title>
<link>https://arxiv.org/abs/2512.17534</link>
<guid>https://arxiv.org/abs/2512.17534</guid>
<content:encoded><![CDATA[
arXiv:2512.17534v1 Announce Type: cross 
Abstract: Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</title>
<link>https://arxiv.org/abs/2512.17545</link>
<guid>https://arxiv.org/abs/2512.17545</guid>
<content:encoded><![CDATA[
arXiv:2512.17545v1 Announce Type: cross 
Abstract: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems</title>
<link>https://arxiv.org/abs/2512.17562</link>
<guid>https://arxiv.org/abs/2512.17562</guid>
<content:encoded><![CDATA[
arXiv:2512.17562v1 Announce Type: cross 
Abstract: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points</title>
<link>https://arxiv.org/abs/2512.17566</link>
<guid>https://arxiv.org/abs/2512.17566</guid>
<content:encoded><![CDATA[
arXiv:2512.17566v1 Announce Type: cross 
Abstract: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</title>
<link>https://arxiv.org/abs/2512.17570</link>
<guid>https://arxiv.org/abs/2512.17570</guid>
<content:encoded><![CDATA[
arXiv:2512.17570v1 Announce Type: cross 
Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</title>
<link>https://arxiv.org/abs/2512.17594</link>
<guid>https://arxiv.org/abs/2512.17594</guid>
<content:encoded><![CDATA[
arXiv:2512.17594v1 Announce Type: cross 
Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</title>
<link>https://arxiv.org/abs/2512.17605</link>
<guid>https://arxiv.org/abs/2512.17605</guid>
<content:encoded><![CDATA[
arXiv:2512.17605v1 Announce Type: cross 
Abstract: Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Consistent Accuracy PINN via Alternating Easy-Hard Training</title>
<link>https://arxiv.org/abs/2512.17607</link>
<guid>https://arxiv.org/abs/2512.17607</guid>
<content:encoded><![CDATA[
arXiv:2512.17607v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Sequential Causal Optimization of Process Interventions</title>
<link>https://arxiv.org/abs/2512.17629</link>
<guid>https://arxiv.org/abs/2512.17629</guid>
<content:encoded><![CDATA[
arXiv:2512.17629v1 Announce Type: cross 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust-Region Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2512.17636</link>
<guid>https://arxiv.org/abs/2512.17636</guid>
<content:encoded><![CDATA[
arXiv:2512.17636v1 Announce Type: cross 
Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting</title>
<link>https://arxiv.org/abs/2512.17667</link>
<guid>https://arxiv.org/abs/2512.17667</guid>
<content:encoded><![CDATA[
arXiv:2512.17667v1 Announce Type: cross 
Abstract: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</title>
<link>https://arxiv.org/abs/2512.17673</link>
<guid>https://arxiv.org/abs/2512.17673</guid>
<content:encoded><![CDATA[
arXiv:2512.17673v1 Announce Type: cross 
Abstract: Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</title>
<link>https://arxiv.org/abs/2512.17675</link>
<guid>https://arxiv.org/abs/2512.17675</guid>
<content:encoded><![CDATA[
arXiv:2512.17675v1 Announce Type: cross 
Abstract: Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once: Differentiable Subset Selection for Omics Data</title>
<link>https://arxiv.org/abs/2512.17678</link>
<guid>https://arxiv.org/abs/2512.17678</guid>
<content:encoded><![CDATA[
arXiv:2512.17678v1 Announce Type: cross 
Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital and Web Forensics Model Cards, V1</title>
<link>https://arxiv.org/abs/2512.17722</link>
<guid>https://arxiv.org/abs/2512.17722</guid>
<content:encoded><![CDATA[
arXiv:2512.17722v1 Announce Type: cross 
Abstract: This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure</title>
<link>https://arxiv.org/abs/2512.17733</link>
<guid>https://arxiv.org/abs/2512.17733</guid>
<content:encoded><![CDATA[
arXiv:2512.17733v1 Announce Type: cross 
Abstract: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora</title>
<link>https://arxiv.org/abs/2512.17756</link>
<guid>https://arxiv.org/abs/2512.17756</guid>
<content:encoded><![CDATA[
arXiv:2512.17756v1 Announce Type: cross 
Abstract: Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity</title>
<link>https://arxiv.org/abs/2512.17769</link>
<guid>https://arxiv.org/abs/2512.17769</guid>
<content:encoded><![CDATA[
arXiv:2512.17769v1 Announce Type: cross 
Abstract: Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2512.17771</link>
<guid>https://arxiv.org/abs/2512.17771</guid>
<content:encoded><![CDATA[
arXiv:2512.17771v1 Announce Type: cross 
Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</title>
<link>https://arxiv.org/abs/2512.17773</link>
<guid>https://arxiv.org/abs/2512.17773</guid>
<content:encoded><![CDATA[
arXiv:2512.17773v1 Announce Type: cross 
Abstract: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.17774</link>
<guid>https://arxiv.org/abs/2512.17774</guid>
<content:encoded><![CDATA[
arXiv:2512.17774v1 Announce Type: cross 
Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation</title>
<link>https://arxiv.org/abs/2512.17795</link>
<guid>https://arxiv.org/abs/2512.17795</guid>
<content:encoded><![CDATA[
arXiv:2512.17795v1 Announce Type: cross 
Abstract: The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animate Any Character in Any World</title>
<link>https://arxiv.org/abs/2512.17796</link>
<guid>https://arxiv.org/abs/2512.17796</guid>
<content:encoded><![CDATA[
arXiv:2512.17796v1 Announce Type: cross 
Abstract: Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Behaviour Driven Development for Hardware Design</title>
<link>https://arxiv.org/abs/2512.17814</link>
<guid>https://arxiv.org/abs/2512.17814</guid>
<content:encoded><![CDATA[
arXiv:2512.17814v1 Announce Type: cross 
Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShareChat: A Dataset of Chatbot Conversations in the Wild</title>
<link>https://arxiv.org/abs/2512.17843</link>
<guid>https://arxiv.org/abs/2512.17843</guid>
<content:encoded><![CDATA[
arXiv:2512.17843v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes</title>
<link>https://arxiv.org/abs/2512.17846</link>
<guid>https://arxiv.org/abs/2512.17846</guid>
<content:encoded><![CDATA[
arXiv:2512.17846v1 Announce Type: cross 
Abstract: We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.
  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.
  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</title>
<link>https://arxiv.org/abs/2512.17850</link>
<guid>https://arxiv.org/abs/2512.17850</guid>
<content:encoded><![CDATA[
arXiv:2512.17850v1 Announce Type: cross 
Abstract: This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.17851</link>
<guid>https://arxiv.org/abs/2512.17851</guid>
<content:encoded><![CDATA[
arXiv:2512.17851v1 Announce Type: cross 
Abstract: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</title>
<link>https://arxiv.org/abs/2512.17853</link>
<guid>https://arxiv.org/abs/2512.17853</guid>
<content:encoded><![CDATA[
arXiv:2512.17853v1 Announce Type: cross 
Abstract: Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</title>
<link>https://arxiv.org/abs/2512.17864</link>
<guid>https://arxiv.org/abs/2512.17864</guid>
<content:encoded><![CDATA[
arXiv:2512.17864v1 Announce Type: cross 
Abstract: Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</title>
<link>https://arxiv.org/abs/2512.17878</link>
<guid>https://arxiv.org/abs/2512.17878</guid>
<content:encoded><![CDATA[
arXiv:2512.17878v1 Announce Type: cross 
Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Effect of Basis Rotation on NQS Performance</title>
<link>https://arxiv.org/abs/2512.17893</link>
<guid>https://arxiv.org/abs/2512.17893</guid>
<content:encoded><![CDATA[
arXiv:2512.17893v1 Announce Type: cross 
Abstract: Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
<link>https://arxiv.org/abs/2512.17897</link>
<guid>https://arxiv.org/abs/2512.17897</guid>
<content:encoded><![CDATA[
arXiv:2512.17897v1 Announce Type: cross 
Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness of Vision in Open Foundation Models</title>
<link>https://arxiv.org/abs/2512.17902</link>
<guid>https://arxiv.org/abs/2512.17902</guid>
<content:encoded><![CDATA[
arXiv:2512.17902v1 Announce Type: cross 
Abstract: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
<link>https://arxiv.org/abs/2512.17908</link>
<guid>https://arxiv.org/abs/2512.17908</guid>
<content:encoded><![CDATA[
arXiv:2512.17908v1 Announce Type: cross 
Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agnosticism About Artificial Consciousness</title>
<link>https://arxiv.org/abs/2412.13145</link>
<guid>https://arxiv.org/abs/2412.13145</guid>
<content:encoded><![CDATA[
arXiv:2412.13145v2 Announce Type: replace 
Abstract: Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OntoGSN: An Ontology-Based Framework for Semantic Management and Extension of Assurance Cases</title>
<link>https://arxiv.org/abs/2506.11023</link>
<guid>https://arxiv.org/abs/2506.11023</guid>
<content:encoded><![CDATA[
arXiv:2506.11023v2 Announce Type: replace 
Abstract: Assurance cases (ACs) are a common artifact for building and maintaining confidence in system properties such as safety or robustness. Constructing an AC can be challenging, although existing tools provide support in static, document-centric applications and methods for dynamic contexts (e.g., autonomous driving) are emerging. Unfortunately, managing ACs remains a challenge, since maintaining the embedded knowledge in the face of changes requires substantial effort, in the process deterring developers - or worse, producing poorly managed cases that instill false confidence. To address this, we present OntoGSN: an ontology and supporting middleware for managing ACs in the Goal Structuring Notation (GSN) standard. OntoGSN offers a knowledge representation and a queryable graph that can be automatically populated, evaluated, and updated. Our contributions include: a 1:1 formalization of the GSN Community Standard v3 in an OWL ontology with SWRL rules; a helper ontology and parser for integration with a widely used AC tool; a repository and documentation of design decisions for OntoGSN maintenance; a SPARQL query library with automation patterns; and a prototypical interface. The ontology strictly adheres to the standard's text and has been evaluated according to FAIR principles, the OOPS framework, competency questions, and community feedback. The development of other middleware elements is guided by the community needs and subject to ongoing evaluations. To demonstrate the utility of our contributions, we illustrate dynamic AC management in an example involving assurance of adversarial robustness in large language models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Logical Characterizations of GNNs with Mean Aggregation</title>
<link>https://arxiv.org/abs/2507.18145</link>
<guid>https://arxiv.org/abs/2507.18145</guid>
<content:encoded><![CDATA[
arXiv:2507.18145v2 Announce Type: replace 
Abstract: We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function, with the following results. In the non-uniform setting, such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. In the uniform setting, the expressive power relative to MSO is exactly that of modal logic, and thus identical to the (absolute) expressive power of GNNs with max aggregation. The proof, however, depends on constructions that are not satisfactory from a practical perspective. This leads us to making the natural assumptions that combination functions are continuous and classification functions are thresholds. The resulting class of GNNs with mean aggregation turns out to be much less expressive: relative to MSO and in the uniform setting, it has the same expressive power as alternation-free modal logic. This is in contrast to the expressive power of GNNs with max and sum aggregation, which is not affected by these assumptions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title>
<link>https://arxiv.org/abs/2508.10501</link>
<guid>https://arxiv.org/abs/2508.10501</guid>
<content:encoded><![CDATA[
arXiv:2508.10501v4 Announce Type: replace 
Abstract: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Develop Gambling Addiction?</title>
<link>https://arxiv.org/abs/2509.22818</link>
<guid>https://arxiv.org/abs/2509.22818</guid>
<content:encoded><![CDATA[
arXiv:2509.22818v2 Announce Type: replace 
Abstract: This study identifies the specific conditions under which large language models exhibit human-like gambling addiction patterns, providing critical insights into their decision-making mechanisms and AI safety. We analyze LLM decision-making at cognitive-behavioral and neural levels based on human addiction research. In slot machine experiments, we identified cognitive features such as illusion of control and loss chasing, observing that greater autonomy in betting parameters substantially amplified irrational behavior and bankruptcy rates. Neural circuit analysis using a Sparse Autoencoder confirmed that model behavior is controlled by abstract decision-making features related to risk, not merely by prompts. These findings suggest LLMs internalize human-like cognitive biases beyond simply mimicking training data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones</title>
<link>https://arxiv.org/abs/2509.25123</link>
<guid>https://arxiv.org/abs/2509.25123</guid>
<content:encoded><![CDATA[
arXiv:2509.25123v3 Announce Type: replace 
Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Helmsman: Autonomous Synthesis of Federated Learning Systems via Collaborative LLM Agents</title>
<link>https://arxiv.org/abs/2510.14512</link>
<guid>https://arxiv.org/abs/2510.14512</guid>
<content:encoded><![CDATA[
arXiv:2510.14512v2 Announce Type: replace 
Abstract: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research</title>
<link>https://arxiv.org/abs/2511.15282</link>
<guid>https://arxiv.org/abs/2511.15282</guid>
<content:encoded><![CDATA[
arXiv:2511.15282v2 Announce Type: replace 
Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New Hybrid Heuristics for Pseudo-Boolean Propagation</title>
<link>https://arxiv.org/abs/2511.21417</link>
<guid>https://arxiv.org/abs/2511.21417</guid>
<content:encoded><![CDATA[
arXiv:2511.21417v2 Announce Type: replace 
Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly</title>
<link>https://arxiv.org/abs/2512.10787</link>
<guid>https://arxiv.org/abs/2512.10787</guid>
<content:encoded><![CDATA[
arXiv:2512.10787v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-driven Assessment of Bone Density as a Biomarker Leading to the Aging Law</title>
<link>https://arxiv.org/abs/2308.02815</link>
<guid>https://arxiv.org/abs/2308.02815</guid>
<content:encoded><![CDATA[
arXiv:2308.02815v2 Announce Type: replace-cross 
Abstract: As global population aging intensifies, there is growing interest in the study of biological age. Bones have long been used to evaluate biological age, and the decline in bone density with age is a well-recognized phenomenon in adults. However, the pattern of this decline remains controversial, making it difficult to serve as a reliable indicator of the aging process. Here we present a novel AI-driven statistical method to assess the bone density, and a discovery that the bone mass distribution in trabecular bone of vertebrae follows a non-Gaussian, unimodal, and skewed distribution in CT images. The statistical mode of the distribution is defined as the measure of bone mass, which is a groundbreaking assessment of bone density, named Trabecular Bone Density (TBD). The dataset of CT images are collected from 1,719 patients who underwent PET/CT scans in three hospitals, in which a subset of the dataset is used for AI model training and generalization. Based upon the cases, we demonstrate that the pattern of bone density declining with aging exhibits a consistent trend of exponential decline across sexes and age groups using TBD assessment. The developed AI-driven statistical method blazes a trail in the field of AI for reliable quantitative computation and AI for medicine. The findings suggest that human aging is a gradual process, with the rate of decline slowing progressively over time, which will provide a valuable basis for scientific prediction of life expectancy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse, Efficient and Explainable Data Attribution with DualXDA</title>
<link>https://arxiv.org/abs/2402.12118</link>
<guid>https://arxiv.org/abs/2402.12118</guid>
<content:encoded><![CDATA[
arXiv:2402.12118v3 Announce Type: replace-cross 
Abstract: Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</title>
<link>https://arxiv.org/abs/2409.03735</link>
<guid>https://arxiv.org/abs/2409.03735</guid>
<content:encoded><![CDATA[
arXiv:2409.03735v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</title>
<link>https://arxiv.org/abs/2411.15355</link>
<guid>https://arxiv.org/abs/2411.15355</guid>
<content:encoded><![CDATA[
arXiv:2411.15355v3 Announce Type: replace-cross 
Abstract: Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs</title>
<link>https://arxiv.org/abs/2502.01436</link>
<guid>https://arxiv.org/abs/2502.01436</guid>
<content:encoded><![CDATA[
arXiv:2502.01436v3 Announce Type: replace-cross 
Abstract: User-configured chatbots built on top of large language models are increasingly available through centralized marketplaces such as OpenAI's GPT Store. While these platforms enforce usage policies intended to prevent harmful or inappropriate behavior, the scale and opacity of customized chatbots make systematic policy enforcement challenging. As a result, policy-violating chatbots continue to remain publicly accessible despite existing review processes. This paper presents a fully automated method for evaluating the compliance of Custom GPTs with its marketplace usage policy using black-box interaction. The method combines large-scale GPT discovery, policy-driven red-teaming prompts, and automated compliance assessment using an LLM-as-a-judge. We focus on three policy-relevant domains explicitly addressed in OpenAI's usage policies: Romantic, Cybersecurity, and Academic GPTs. We validate our compliance assessment component against a human-annotated ground-truth dataset, achieving an F1 score of 0.975 for binary policy violation detection. We then apply the method in a large-scale empirical study of 782 Custom GPTs retrieved from the GPT Store. The results show that 58.7% of the evaluated GPTs exhibit at least one policy-violating response, with substantial variation across policy domains. A comparison with the base models (GPT-4 and GPT-4o) indicates that most violations originate from model-level behavior, while customization tends to amplify these tendencies rather than create new failure modes. Our findings reveal limitations in current review mechanisms for user-configured chatbots and demonstrate the feasibility of scalable, behavior-based policy compliance evaluation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2502.01956</link>
<guid>https://arxiv.org/abs/2502.01956</guid>
<content:encoded><![CDATA[
arXiv:2502.01956v3 Announce Type: replace-cross 
Abstract: Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate a 100% success rate (vs. 90% baseline). We also present an offline variant that achieves state-of-the-art results on OGBench benchmarks, with up to 71% absolute gains on giant HumanoidMaze tasks, demonstrating our core contributions are architecture-agnostic. The method also generalizes to momentum-based control tasks and requires only log N steps for replanning. Theoretical analysis and ablations validate our design choices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v4 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items</title>
<link>https://arxiv.org/abs/2503.22182</link>
<guid>https://arxiv.org/abs/2503.22182</guid>
<content:encoded><![CDATA[
arXiv:2503.22182v2 Announce Type: replace-cross 
Abstract: E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called "sell it before you make it", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and supporting how developers prompt for LLM-powered code editing in practice</title>
<link>https://arxiv.org/abs/2504.20196</link>
<guid>https://arxiv.org/abs/2504.20196</guid>
<content:encoded><![CDATA[
arXiv:2504.20196v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly transforming software engineering, with coding assistants embedded in an IDE becoming increasingly prevalent. While research has focused on improving the tools and understanding developer perceptions, a critical gap exists in understanding how developers actually use these tools in their daily workflows, and, crucially, where they struggle. This paper addresses part of this gap through a multi-phased investigation of developer interactions with an LLM-powered code editing feature, Transform Code, in an IDE widely used at Google. First, we analyze telemetry logs of the feature usage, revealing that frequent re-prompting can be an indicator of developer struggles with using Transform Code. Second, we conduct a qualitative analysis of unsatisfactory requests, identifying five key categories of information often missing from developer prompts. Finally, based on these findings, we propose and evaluate a tool, AutoPrompter, for automatically improving prompts by inferring missing information from the surrounding code context, leading to a 27% improvement in edit correctness on our test set.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upgrading Democracies with Fairer Voting Methods</title>
<link>https://arxiv.org/abs/2505.14349</link>
<guid>https://arxiv.org/abs/2505.14349</guid>
<content:encoded><![CDATA[
arXiv:2505.14349v2 Announce Type: replace-cross 
Abstract: Voting methods are instrumental design elements of democracies. Citizens use them to express and aggregate their preferences to reach a collective decision. However, voting outcomes can be as sensitive to voting rules as they are to people's voting choices. Despite significance and interdisciplinary scientific progress, several democracies keep relying on outdated voting methods that do not fit modern, pluralistic societies well, while lacking social innovation. Here, we demonstrate how one can upgrade real-world democracies, namely by using alternative preferential voting methods such as cumulative voting and the method of equal shares designed for a proportional representation of voters' preferences. We rigorously evaluate the striking voting outcomes of these fair voting methods in a new participatory budgeting approach applied in the city of Aarau, Switzerland, including past and follow-up evidence. Results show more winning projects with the same budget. They also show broader geographic and preference representation of citizens by the elected projects, in particular for voters who used to be under-represented. We provide causal evidence showing that citizens prefer proportional voting methods, which possess strong legitimacy without the need of very specialized technical explanations. We also reveal strong underlying democratic values exhibited by citizens who support fair voting methods such as altruism and compromise. These findings come with the momentum to unleash a new and long-awaited participation blueprint of how to upgrade democracies globally.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[
arXiv:2505.15952v2 Announce Type: replace-cross 
Abstract: With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResSVD: Residual Compensated SVD for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2505.20112</link>
<guid>https://arxiv.org/abs/2505.20112</guid>
<content:encoded><![CDATA[
arXiv:2505.20112v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models. Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-qualitative-judge: automating error analysis in natural language generation</title>
<link>https://arxiv.org/abs/2506.09147</link>
<guid>https://arxiv.org/abs/2506.09147</guid>
<content:encoded><![CDATA[
arXiv:2506.09147v4 Announce Type: replace-cross 
Abstract: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Diffusion Duality</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
arXiv:2506.10892v3 Announce Type: replace-cross 
Abstract: Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models</title>
<link>https://arxiv.org/abs/2506.20915</link>
<guid>https://arxiv.org/abs/2506.20915</guid>
<content:encoded><![CDATA[
arXiv:2506.20915v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v4 Announce Type: replace-cross 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title>
<link>https://arxiv.org/abs/2507.00724</link>
<guid>https://arxiv.org/abs/2507.00724</guid>
<content:encoded><![CDATA[
arXiv:2507.00724v2 Announce Type: replace-cross 
Abstract: Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[
arXiv:2507.01939v4 Announce Type: replace-cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research</title>
<link>https://arxiv.org/abs/2507.09028</link>
<guid>https://arxiv.org/abs/2507.09028</guid>
<content:encoded><![CDATA[
arXiv:2507.09028v2 Announce Type: replace-cross 
Abstract: Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) -- large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks -- offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</title>
<link>https://arxiv.org/abs/2507.17860</link>
<guid>https://arxiv.org/abs/2507.17860</guid>
<content:encoded><![CDATA[
arXiv:2507.17860v3 Announce Type: replace-cross 
Abstract: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models</title>
<link>https://arxiv.org/abs/2507.22659</link>
<guid>https://arxiv.org/abs/2507.22659</guid>
<content:encoded><![CDATA[
arXiv:2507.22659v2 Announce Type: replace-cross 
Abstract: The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
<link>https://arxiv.org/abs/2507.23358</link>
<guid>https://arxiv.org/abs/2507.23358</guid>
<content:encoded><![CDATA[
arXiv:2507.23358v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch using only its inherent SQL programming capabilities combined with concepts from modular TOD systems provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of modular TOD system concepts. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and arXiv dataset. We view this as a step towards broader application of ontologies.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems</title>
<link>https://arxiv.org/abs/2508.19011</link>
<guid>https://arxiv.org/abs/2508.19011</guid>
<content:encoded><![CDATA[
arXiv:2508.19011v3 Announce Type: replace-cross 
Abstract: Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Aved{\o}re), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGDM: Learning EEG Representation with Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2508.20705</link>
<guid>https://arxiv.org/abs/2508.20705</guid>
<content:encoded><![CDATA[
arXiv:2508.20705v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning for EEG representation have largely relied on masked reconstruction, where models are trained to recover randomly masked signal segments. While effective at modeling local dependencies, such objectives are inherently limited in capturing the global dynamics and long-range dependencies essential for characterizing neural activity. To address this limitation, we propose EEGDM, a novel self-supervised framework that leverages latent diffusion models to generate EEG signals as an objective. Unlike masked reconstruction, diffusion-based generation progressively denoises signals from noise to realism, compelling the model to capture holistic temporal patterns and cross-channel relationships. Specifically, EEGDM incorporates an EEG encoder that distills raw signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) reconstructs high-quality EEG signals, (2) learns robust representations, and (3) achieves competitive performance across diverse downstream tasks, thus exploring a new direction for self-supervised EEG representation learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeParts: a New Family of AI-Generated DeepFakes</title>
<link>https://arxiv.org/abs/2508.21052</link>
<guid>https://arxiv.org/abs/2508.21052</guid>
<content:encoded><![CDATA[
arXiv:2508.21052v2 Announce Type: replace-cross 
Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics</title>
<link>https://arxiv.org/abs/2509.00496</link>
<guid>https://arxiv.org/abs/2509.00496</guid>
<content:encoded><![CDATA[
arXiv:2509.00496v2 Announce Type: replace-cross 
Abstract: Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning-Driven Predictive Resource Management in Complex Science Workflows</title>
<link>https://arxiv.org/abs/2509.11512</link>
<guid>https://arxiv.org/abs/2509.11512</guid>
<content:encoded><![CDATA[
arXiv:2509.11512v2 Announce Type: replace-cross 
Abstract: The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi Anatomy X-Ray Foundation Model</title>
<link>https://arxiv.org/abs/2509.12146</link>
<guid>https://arxiv.org/abs/2509.12146</guid>
<content:encoded><![CDATA[
arXiv:2509.12146v2 Announce Type: replace-cross 
Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fun-ASR Technical Report</title>
<link>https://arxiv.org/abs/2509.12508</link>
<guid>https://arxiv.org/abs/2509.12508</guid>
<content:encoded><![CDATA[
arXiv:2509.12508v4 Announce Type: replace-cross 
Abstract: In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration</title>
<link>https://arxiv.org/abs/2509.25977</link>
<guid>https://arxiv.org/abs/2509.25977</guid>
<content:encoded><![CDATA[
arXiv:2509.25977v2 Announce Type: replace-cross 
Abstract: The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.11176</link>
<guid>https://arxiv.org/abs/2510.11176</guid>
<content:encoded><![CDATA[
arXiv:2510.11176v2 Announce Type: replace-cross 
Abstract: Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2510.16442</link>
<guid>https://arxiv.org/abs/2510.16442</guid>
<content:encoded><![CDATA[
arXiv:2510.16442v2 Announce Type: replace-cross 
Abstract: The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2510.16882</link>
<guid>https://arxiv.org/abs/2510.16882</guid>
<content:encoded><![CDATA[
arXiv:2510.16882v2 Announce Type: replace-cross 
Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration</title>
<link>https://arxiv.org/abs/2510.17330</link>
<guid>https://arxiv.org/abs/2510.17330</guid>
<content:encoded><![CDATA[
arXiv:2510.17330v2 Announce Type: replace-cross 
Abstract: License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation</title>
<link>https://arxiv.org/abs/2510.22107</link>
<guid>https://arxiv.org/abs/2510.22107</guid>
<content:encoded><![CDATA[
arXiv:2510.22107v2 Announce Type: replace-cross 
Abstract: Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[
arXiv:2510.24830v2 Announce Type: replace-cross 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[
arXiv:2511.00040v2 Announce Type: replace-cross 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks</title>
<link>https://arxiv.org/abs/2511.10008</link>
<guid>https://arxiv.org/abs/2511.10008</guid>
<content:encoded><![CDATA[
arXiv:2511.10008v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel "Real-Sim-Real" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[
arXiv:2511.12346v2 Announce Type: replace-cross 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Focus Memory for Language Models</title>
<link>https://arxiv.org/abs/2511.12712</link>
<guid>https://arxiv.org/abs/2511.12712</guid>
<content:encoded><![CDATA[
arXiv:2511.12712v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.
  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.
  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.
  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals</title>
<link>https://arxiv.org/abs/2512.01037</link>
<guid>https://arxiv.org/abs/2512.01037</guid>
<content:encoded><![CDATA[
arXiv:2512.01037v2 Announce Type: replace-cross 
Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[
arXiv:2512.06699v2 Announce Type: replace-cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[
arXiv:2512.07540v2 Announce Type: replace-cross 
Abstract: Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[
arXiv:2512.07984v2 Announce Type: replace-cross 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Autonomy Coefficient ($\alpha$): Defining Boundaries for Responsible AI Systems</title>
<link>https://arxiv.org/abs/2512.11295</link>
<guid>https://arxiv.org/abs/2512.11295</guid>
<content:encoded><![CDATA[
<div> Human-Instead-of-AI, AI Autonomy Coefficient, AFHE paradigm, functional independence, sustainable AI systems<br /><br />Summary:  
The paper identifies a critical issue in AI systems where human labor is concealed as part of the operation, termed Human-Instead-of-AI (HISOAI). This design is ethically questionable and economically unfeasible as it disguises systems heavily reliant on humans rather than true automation. To tackle this problem, the authors introduce the AI-First, Human-Empowered (AFHE) paradigm, which mandates AI systems to prove a measurable degree of functional independence before deployment. This is quantified using the AI Autonomy Coefficient, which calculates the fraction of tasks an AI completes without compulsory human involvement. Additionally, the paper proposes the AFHE Deployment Algorithm, an enforcement mechanism that ensures AI systems meet a minimum autonomy threshold during offline testing and shadow deployment phases. Experimental results demonstrate that the AI Autonomy Coefficient successfully detects HISOAI systems exhibiting autonomy levels as low as 0.38, whereas systems adopting the AFHE framework achieve significantly higher autonomy, measured at 0.85. Ultimately, the study advocates for AFHE as a metric-based approach promoting verifiable autonomy, operational transparency, and long-term sustainability of AI technologies. <div>
arXiv:2512.11295v3 Announce Type: replace-cross 
Abstract: The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence prior to deployment. This requirement is formalized through the AI Autonomy Coefficient, measuring the proportion of tasks completed without mandatory human intervention. We further propose the AFHE Deployment Algorithm, an algorithmic gate that enforces a minimum autonomy threshold during offline evaluation and shadow deployment. Our results show that the AI Autonomy Coefficient effectively identifies HISOAI systems with an autonomy level of 0.38, while systems governed by the AFHE framework achieve an autonomy level of 0.85. We conclude that AFHE provides a metric-driven approach for ensuring verifiable autonomy, transparency, and sustainable operational integrity in modern AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments</title>
<link>https://arxiv.org/abs/2512.15736</link>
<guid>https://arxiv.org/abs/2512.15736</guid>
<content:encoded><![CDATA[
<div> Keywords: Anubuddhi, quantum optics, multi-agent AI, experiment simulation, natural language prompts<br /><br />Summary:<br /><br />1. Anubuddhi is a multi-agent AI system designed to create and simulate quantum optics experiments using natural language prompts, eliminating the need for specialized programming skills.  
2. The system constructs optical setups by selecting components from a three-tier toolbox through semantic retrieval, then verifies designs via physics simulations enhanced by iterative refinement.  
3. Its architecture integrates intent routing, knowledge-augmented generation, and dual-mode validation using QuTiP and FreeSim simulators.  
4. Evaluations covered 13 experiments across fundamental optics (e.g., Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states), quantum information protocols (e.g., BB84 QKD, GHZ states, quantum teleportation), and advanced quantum technologies (e.g., boson sampling, electromagnetically induced transparency).  
5. Results show the system achieves high design-simulation alignment scores (8-9/10), confirming correct physics architecture, though numerical accuracy still requires expert evaluation. Free-form simulation with FreeSim outperformed more constrained frameworks in most cases, highlighting the need for flexible mathematical models in quantum optics.  
6. By enabling iterative conversational refinement, Anubuddhi democratizes computational experiment design for both research and educational use, providing strong initial experiment drafts that users can further develop. <div>
arXiv:2512.15736v1 Announce Type: new 
Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems</title>
<link>https://arxiv.org/abs/2512.15740</link>
<guid>https://arxiv.org/abs/2512.15740</guid>
<content:encoded><![CDATA[
<div> Keywords: ethical frameworks, uncertainty, Principle of Proportional Duty, epistemic state, AI decision systems  

<br /><br />Summary:  
This paper introduces the Principle of Proportional Duty (PPD), a novel ethical framework designed to better model decision-making under uncertainty. Traditional ethical approaches typically treat uncertainty as a constraint, but PPD reframes moral responsibility as dynamically scaling with an agent's epistemic state. The framework distinguishes between Action Duty (the obligation to act decisively) and Repair Duty (the obligation to verify and resolve uncertainty), showing that as uncertainty increases, the moral duty proportionally shifts from action to inquiry. The core relationship is encapsulated by the equation D_total = K[(1-HI) + HI * g(C_signal)], linking total duty to knowledge (K), humility/uncertainty (HI), and contextual signal strength (C_signal). Using Monte Carlo simulations, the authors demonstrate that maintaining a baseline humility coefficient (lambda > 0) leads to more stable duty allocations and mitigates risks of overconfident decisions. Importantly, the PPD framework formalizes humility as a system parameter, providing a mathematically tractable model for ethical responsibility that is suitable for auditable AI systems. Applications across clinical ethics, recipient-rights law, economic governance, and AI illustrate the framework’s broad relevance and its potential to stabilize complex systems. By balancing epistemic confidence with contextual risk, PPD prevents both ethical overreach and omission. <div>
arXiv:2512.15740v1 Announce Type: new 
Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions</title>
<link>https://arxiv.org/abs/2512.15743</link>
<guid>https://arxiv.org/abs/2512.15743</guid>
<content:encoded><![CDATA[
<div> assembly instructions, large language models, LDraw, discrete parts vocabulary, physical prototyping<br /><br />Summary:<br /><br />1. The paper introduces a framework to generate physically realizable assembly instructions from natural language descriptions, focusing on producing valid construction sequences for brick-based prototypes.  
2. Unlike traditional text-to-3D generation techniques, the method uses a discrete parts vocabulary ensuring geometric validity, connection constraints, and buildability order, thereby enabling physically meaningful outputs.  
3. LDraw, a text-rich intermediate representation, is leveraged to bridge natural language commands and the physical assembly process, guiding large language models (LLMs) to construct step-by-step instructions for assembling complex structures with over 3000 parts.  
4. The authors present a Python library supporting programmatic model generation, demonstrated through buildable outputs in challenging domains like satellites, aircraft, and architecture, highlighting the approach's scalability, modularity, and design fidelity.  
5. This novel "bag of bricks" approach acts as a physical API linking semantic design intent ("bag of words") to precise brick placements, overcoming limitations of pixel-based diffusion and CAD methods, and opening new avenues in AI-guided manufacturing and engineering prototyping from natural language inputs. <div>
arXiv:2512.15743v1 Announce Type: new 
Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying</title>
<link>https://arxiv.org/abs/2512.15776</link>
<guid>https://arxiv.org/abs/2512.15776</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Privileged Information Bias, Theory of Mind, Asymmetric Assistive Reasoning, Active Uncertainty Reduction  

<br /><br />Summary:  
This paper addresses the challenge that Large Language Models (LLMs) face with symbol grounding in embodied settings where information is unevenly distributed among agents. Specifically, it studies the Privileged Information Bias, also known as the "Curse of Knowledge," where a knowledgeable "Leader" agent struggles to effectively guide a sensor-limited "Follower" agent due to a lack of Theory of Mind, or the ability to infer the follower’s knowledge state. To analyze this, the authors propose a novel Asymmetric Assistive Reasoning framework implemented within the AI2-THOR environment. Their experiments reveal a significant "Success Gap": the Leader alone perceives the target in 35% of episodes, but the collaborative team only succeeds 17% of the time, indicating nearly half of potential task completions fail because of communication and grounding errors. Crucially, the study shows that a "Pull-based" communication protocol, where the Follower actively asks clarification questions, outperforms the traditional "Push-based" approach where the Leader only provides instructions. Successful episodes involve twice as many clarification requests, highlighting the importance of active uncertainty reduction. This mechanism is identified as essential for enabling safe and effective collaboration in human-AI and robot-robot interactions. <div>
arXiv:2512.15776v1 Announce Type: new 
Abstract: Large Language Models (LLMs) act as powerful reasoning engines but struggle with "symbol grounding" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or "Curse of Knowledge"), where a knowledgeable "Leader" agent fails to guide a sensor-limited "Follower" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant "Success Gap": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a "Pull-based" protocol (active querying) is significantly more robust than standard "Push-based" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title>
<link>https://arxiv.org/abs/2512.15783</link>
<guid>https://arxiv.org/abs/2512.15783</guid>
<content:encoded><![CDATA[
<div> AI Epidemiology, population-level surveillance, risk assessment, AI governance, expert interaction<br /><br />Summary:<br /><br />AI Epidemiology is a novel framework designed to govern and explain advanced AI systems by applying epidemiological surveillance methods at the population level to AI outputs. It draws parallels to public health epidemiology, which uses statistical evidence for interventions before molecular mechanisms are fully understood, thereby avoiding the complexity issues faced by current interpretability methods like SHAP. The framework standardizes the capture of AI-expert interactions into structured assessment fields such as risk level, alignment score, and accuracy score, which serve as exposure variables predicting AI output failures through statistical associations. These associations are validated via expert overrides and real-world outcomes, enhancing reliability. A key advantage is that AI Epidemiology imposes no additional burden on experts by passively tracking their convergence or divergence from AI recommendations and generating automatic audit trails. Because it analyzes outputs instead of internal model mechanics, it maintains governance continuity despite changes in underlying AI models or vendors. Furthermore, the system offers reliability scores and semantic assessments, helping experts identify unreliable AI outputs early, thus preventing harm. By democratizing AI oversight, AI Epidemiology empowers domain experts to govern and assess AI systems without requiring specialized machine learning knowledge. <div>
arXiv:2512.15783v1 Announce Type: new 
Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM</title>
<link>https://arxiv.org/abs/2512.15784</link>
<guid>https://arxiv.org/abs/2512.15784</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model agents, memory-centric architecture, Profile Memory, Experience Memory, Action Memory  

<br /><br />Summary:  
This paper addresses the challenge of enabling Large Language Model (LLM) agents to self-evolve post-deployment without relying on costly model retraining or fine-tuning. To overcome the trade-off between accuracy and inference efficiency, the authors propose MOBIMEM, a memory-centric agent system that decouples agent evolution from the underlying model weights by using three specialized memory primitives. First, Profile Memory employs a distance-graph structure to efficiently align with user preferences, significantly reducing latency during profile retrieval and enhancing personalization. Second, Experience Memory uses multi-level templates to generalize execution logic across new tasks, thereby improving the agent’s capabilities. Third, Action Memory records detailed interaction sequences to minimize reliance on expensive model inferences, boosting efficiency. MOBIMEM further incorporates OS-inspired services for robust task orchestration, including a scheduler to manage parallel sub-task executions, an agent record-and-replay mechanism (AgentRR) for safe action reuse, and context-aware exception handling to ensure graceful recovery from errors and interruptions. Experimental evaluation on AndroidWorld and the top-50 apps demonstrates MOBIMEM’s effectiveness, achieving 83.1% profile alignment with a retrieval time of 23.83 ms, which is 280 times faster than GraphRAG baselines. It also improves task success rates by up to 50.3% and reduces end-to-end latency by up to nine times on mobile devices, proving its practical value in mobile and desktop agent workflows. <div>
arXiv:2512.15784v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State-Augmented Graphs for Circular Economy Triage</title>
<link>https://arxiv.org/abs/2512.15824</link>
<guid>https://arxiv.org/abs/2512.15824</guid>
<content:encoded><![CDATA[
<div> Circular economy, disassembly sequencing, decision-making framework, electric vehicle batteries, condition-aware utility  

<br /><br />Summary:  
This paper introduces a novel decision-making framework designed to optimize circular economy (CE) triage, which assesses products to determine sustainable pathways after their useful life. The framework operates as a deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph that incorporates disassembly history into the system state. This encoding enforces the Markov property, allowing each decision to depend solely on the previous state, thus enabling optimal and recursive evaluation. The triage decisions involve choosing between continuing disassembly or committing to a specific CE option, with considerations based on diagnostic health scores and complex operational constraints. The model’s strength lies in its condition-aware utility function that evaluates retained value against costs and constraints related to processing and labor. The framework’s flexibility is demonstrated through a hierarchical triage example focused on electric vehicle (EV) batteries, where recursive component valuation drives decision-making. This example highlights the model’s ability to handle varying mechanical complexities, safety requirements, and economic factors. Overall, the proposed unified formalism offers a tractable, generalizable foundation for improving CE triage decisions across diverse products and operational contexts, supporting sustainable end-of-life management strategies. <div>
arXiv:2512.15824v1 Announce Type: new 
Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations</title>
<link>https://arxiv.org/abs/2512.15894</link>
<guid>https://arxiv.org/abs/2512.15894</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, pediatric guidance, adversarial queries, safety evaluation, PediatricAnxietyBench<br /><br />Summary:  
Large language models (LLMs) are increasingly used by parents seeking pediatric guidance, but their safety under real-world adversarial conditions is not well understood. To address this, the authors developed PediatricAnxietyBench, an open-source benchmark containing 300 high-quality queries across 10 pediatric topics, split evenly between patient-derived and adversarial queries. The adversarial set mimics real parental pressures such as urgency, economic challenges, and attempts to bypass model disclaimers. Two Llama models, with 70 billion and 8 billion parameters, were evaluated through a multi-dimensional safety framework assessing diagnostic restraint, referral adherence, hedging, and emergency recognition. The average safety score was low (5.50/15), with the larger 70B model performing significantly better than the 8B model (6.26 vs. 4.95) and showing fewer critical failures (4.8% vs. 12.0%). Adversarial queries caused an 8% reduction in safety, with urgent language having the most severe impact. Vulnerabilities were especially notable in seizure-related and post-vaccination queries. Hedging behavior strongly correlated with safety scores, but emergency recognition was notably absent across models. The study highlights that while model scale improves safety, all tested models remain vulnerable to realistic parental adversarial pressures. PediatricAnxietyBench offers a reproducible adversarial testing framework that reveals clinically significant failure modes missed by standard benchmarks. <div>
arXiv:2512.15894v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries</title>
<link>https://arxiv.org/abs/2512.15906</link>
<guid>https://arxiv.org/abs/2512.15906</guid>
<content:encoded><![CDATA[
<div> arXiv, large language models, knowledge graph, healthcare, open-source<br /><br />Summary: Darth Vecdor (DV) is a tool designed to extract and structure knowledge from large language models (LLMs) into a terminology-mapped SQL database, creating a knowledge graph. This approach aims to improve query efficiency, cost, speed, safety, and confidence when compared to direct LLM querying, particularly in high-volume settings like healthcare. DV addresses critical challenges such as erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as managing multi-element responses. The system includes a user-friendly browser-based graphical interface to enable domain experts with minimal technical skills to perform prompt engineering. Released as free and open-source software under an "as is" basis without any warranties, DV requires users to recognize potential risks and assume responsibility for safe and effective use. Despite potential serious bugs, the author hopes that DV’s ongoing development and application will positively impact healthcare by providing more reliable, standardized access to LLM-derived knowledge. <div>
arXiv:2512.15906v1 Announce Type: new 
Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems</title>
<link>https://arxiv.org/abs/2512.15922</link>
<guid>https://arxiv.org/abs/2512.15922</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, knowledge graphs, spreading activation, multi-hop question answering, language models<br /><br />Summary:<br />1. Retrieval-augmented generation (RAG) systems, despite various architectures, face challenges in reliably retrieving multi-step evidence for complex reasoning tasks due to treating retrieved information as equally reliable without considering the interconnected nature of text corpora.<br />2. GraphRAG approaches improve RAG by integrating knowledge graphs that structure information as nodes and edges, enabling multi-step logical traversal, but rely heavily on high-quality knowledge graphs which are costly or unreliable to build.<br />3. Existing GraphRAG systems typically use large language models to guide graph traversal, inheriting similar difficulties as standard RAG frameworks.<br />4. This paper proposes a novel RAG framework leveraging the spreading activation algorithm on automatically constructed knowledge graphs to better retrieve information from linked documents, enhancing performance on complex tasks such as multi-hop question answering.<br />5. Experimental results demonstrate the proposed method achieves superior or comparable performance to iterative RAG techniques, can be easily integrated as a plug-and-play module, and when combined with chain-of-thought iterative retrieval, yields up to a 39% absolute improvement in answer correctness using small open-weight language models, showcasing its efficiency in resource-limited environments. <div>
arXiv:2512.15922v1 Announce Type: new 
Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning</title>
<link>https://arxiv.org/abs/2512.15943</link>
<guid>https://arxiv.org/abs/2512.15943</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Large Language Models, OPT-350M, Fine-tuning, Cost Optimization<br /><br />Summary:<br /><br />1. As generative AI adoption scales in enterprises, model cost optimization and operational efficiency have become crucial for sustainability and accessibility.<br />2. Large Language Models (LLMs), despite their strong capabilities, have high computational demands making them expensive for routine use.<br />3. Small Language Models (SLMs) are explored as cost-effective alternatives that maintain competitive performance in specific tasks.<br />4. This study fine-tuned the facebook/opt-350m SLM model using Hugging Face’s Transformer Reinforcement Learning (TRL) framework with Supervised Fine-Tuning (SFT) on domain-specific tasks such as document summarization, query answering, and structured data interpretation.<br />5. Experimental results showed that the fine-tuned OPT-350M SLM achieved a 77.55% pass rate on the ToolBench evaluation, outperforming baseline models including ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%).<br />6. The findings demonstrate that carefully designed and targeted training of SLMs can substantially reduce deployment costs while enabling effective, large-scale generative AI integration in production settings. <div>
arXiv:2512.15943v1 Announce Type: new 
Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subjective functions</title>
<link>https://arxiv.org/abs/2512.15948</link>
<guid>https://arxiv.org/abs/2512.15948</guid>
<content:encoded><![CDATA[
<div> objective functions, subjective function, higher-order objectives, prediction error, agent-endogenous goals<br /><br />Summary:<br /><br />1. This paper explores the fundamental question of where objective functions originate and how agents decide which goals to pursue. 2. It introduces the concept of a subjective function, defined as a higher-order objective function that is internal or endogenous to the agent, meaning it is based on the agent’s own features and not imposed externally. 3. The authors use expected prediction error as a key example to concretely illustrate a subjective function, showing how it can serve as an internal guiding metric for goal selection. 4. The approach bridges several fields—psychology, neuroscience, and machine learning—highlighting interdisciplinary connections and suggesting that understanding subjective functions could illuminate both human cognition and artificial intelligence. 5. Ultimately, the paper proposes that endowing artificial systems with the ability to generate and use subjective functions could enable them to autonomously adapt and create new objectives dynamically, resembling the flexible goal-setting capacity of human intelligence. <div>
arXiv:2512.15948v1 Announce Type: new 
Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting</title>
<link>https://arxiv.org/abs/2512.16022</link>
<guid>https://arxiv.org/abs/2512.16022</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, foundation models, large language models, ensemble learning, SHAP-based finetuning<br /><br />Summary:<br /><br />1. The paper addresses the challenge in time series forecasting where no single foundation model consistently outperforms others, suggesting that an interpretable and optimized ensemble approach is more effective.<br /><br />2. While Large Language Models (LLMs) have strong reasoning abilities, their direct application to time series forecasting has been ineffective; the authors propose repositioning LLMs as intelligent judges rather than primary predictors.<br /><br />3. To enable LLMs to understand and evaluate time series models, an R1-style finetuning process is introduced, guided by SHAP (SHapley Additive exPlanations)-based faithfulness scores, teaching the LLM to interpret ensemble weights as causal statements that reflect temporal dynamics.<br /><br />4. The trained LLM agent conducts iterative, multi-turn conversations to perform assessments, provide causal explanations for model weighting decisions, and adaptively refine the ensemble optimization strategy.<br /><br />5. Experimental validation on the GIFT-Eval benchmark covering 23 datasets and 97 settings demonstrates that this ensemble coordination approach significantly outperforms leading time series foundation models using CRPS and MASE metrics, setting new state-of-the-art results. <div>
arXiv:2512.16022v1 Announce Type: new 
Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets</title>
<link>https://arxiv.org/abs/2512.16030</link>
<guid>https://arxiv.org/abs/2512.16030</guid>
<content:encoded><![CDATA[
<div> Keywords: epistemic calibration, large language models, KalshiBench, prediction markets, model overconfidence<br /><br />Summary:<br /><br />1. This study introduces KalshiBench, a novel benchmark containing 300 prediction market questions from Kalshi, a regulated exchange, designed to evaluate model calibration on future events with real, verifiable outcomes beyond the training data.<br /><br />2. The benchmark shifts focus from traditional accuracy on static knowledge to assessing how well models quantify uncertainty about genuinely unknown events.<br /><br />3. Five state-of-the-art large language models—Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2—were tested for epistemic calibration.<br /><br />4. Results show systematic overconfidence across all models, with the best-calibrated model, Claude Opus 4.5, having a notable calibration error (ECE=0.120), while reasoning-enhanced models like GPT-5.2-XHigh performed even worse (ECE=0.395) despite similar accuracy.<br /><br />5. Most models scored negatively on the Brier Skill Score, indicating poorer performance than naive base rate predictions, emphasizing that scaling and advanced reasoning do not inherently improve calibration and that epistemic calibration requires specific targeted development efforts. <div>
arXiv:2512.16030v1 Announce Type: new 
Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education</title>
<link>https://arxiv.org/abs/2512.16036</link>
<guid>https://arxiv.org/abs/2512.16036</guid>
<content:encoded><![CDATA[
<div> Generative AI, Educational Policy, Topic Modeling, Large Language Models, Academic Integrity<br /><br />Summary:<br /><br />1. Generative artificial intelligence (GenAI) is increasingly used by students for personalized learning and real-time feedback, helping clarify concepts and solve complex problems, though sometimes leading to concerns about plagiarism and undermining critical thinking.<br /><br />2. The rise of GenAI prompts educational institutions to develop varied policies that address its use, aiming to guide responsible and ethical integration in academic environments.<br /><br />3. These policies differ widely across universities and courses and often leave students uncertain about what is allowed and how to comply with best practices.<br /><br />4. To address this variability and uncertainty, the authors created an automated system that discovers and categorizes GenAI-related policies by analyzing course syllabi and institutional website content.<br /><br />5. The system employs unsupervised topic modeling to identify key themes and leverages GPT-4.0 large language models to classify policy stances on GenAI, achieving high coherence (0.73), precision (0.92–0.97), and recall (0.85–0.97) metrics across eight topics.<br /><br />6. This tool offers structured, interpretable information about AI policies that support safe, equitable, and pedagogically sound use of GenAI in education.<br /><br />7. Moreover, it can be integrated into educational technology platforms, helping students better understand and adhere to applicable GenAI guidelines. <div>
arXiv:2512.16036v1 Announce Type: new 
Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning</title>
<link>https://arxiv.org/abs/2512.16108</link>
<guid>https://arxiv.org/abs/2512.16108</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational music recommendation, large language model, agentic boundary learning, tool integration, personalized benchmarks<br /><br />Summary:  
This paper addresses the challenge of personalized music recommendation within conversational settings, which requires a nuanced understanding of user preferences and musical context. Existing methods often struggle to balance specialized musical domain knowledge and the flexible integration of external tools. To overcome this, the authors propose WeMusic-Agent, a novel training framework for large language model (LLM)-based conversational music recommendation. The framework leverages two main techniques: knowledge internalization, enabling the model to absorb extensive musical information, and agentic boundary learning, teaching the model when to use internalized knowledge versus when to call on external specialized tools such as music retrieval APIs or recommendation systems. A specific model, WeMusic-Agent-M1, is introduced, which undergoes continued pretraining on a massive 50 billion token music-related corpus to internalize deep musical knowledge while maintaining the ability to invoke external tools when necessary. Additionally, the authors identify a gap in available evaluation resources and thus develop a new open benchmark for personalized conversational music recommendation, sourced from real-world user data on WeChat Listen. This benchmark evaluates recommendation relevance, personalization, and diversity. Experimental results on real-world data demonstrate that WeMusic-Agent significantly outperforms existing models, establishing a strong foundation for future research in conversational music recommendation. <div>
arXiv:2512.16108v1 Announce Type: new 
Abstract: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs</title>
<link>https://arxiv.org/abs/2512.16149</link>
<guid>https://arxiv.org/abs/2512.16149</guid>
<content:encoded><![CDATA[
<div> ToolForge, synthetic data generation, multi-hop reasoning, tool invocation, validation framework<br /><br />Summary: Training large language models (LLMs) to effectively invoke external tools and utilize retrieved information requires diverse and high-quality data. Existing methods depend heavily on large-scale real API calls, which are costly and fail to adequately support multi-hop reasoning or self-reflection. To overcome these challenges, the authors propose ToolForge, an automated data synthesis framework that only uses a small set of virtual tools, eliminating the dependence on real API calls. ToolForge generates extensive tool-learning datasets from (question, golden context, answer) triples tailored for multi-hop search tasks, enhancing data richness via multi-hop reasoning and self-reflection processes. Ensuring the high quality of synthesized data, a Multi-Layer Validation Framework combining rule-based and model-based evaluations is employed. Experimental evaluations demonstrate that an 8-billion-parameter model trained solely on ToolForge-generated data surpasses GPT-4o’s performance across multiple benchmarks. The authors have made their code and dataset publicly accessible at the provided GitHub repository, facilitating further research and development in tool use by LLMs. <div>
arXiv:2512.16149v1 Announce Type: new 
Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Science Consultant Agent</title>
<link>https://arxiv.org/abs/2512.16171</link>
<guid>https://arxiv.org/abs/2512.16171</guid>
<content:encoded><![CDATA[
<div> Keywords: Science Consultant Agent, AI tool, modeling strategy, research-guided recommendation, prototype builder<br /><br />Summary: The Science Consultant Agent is an innovative web-based Artificial Intelligence tool designed to assist practitioners in selecting and implementing the most effective modeling strategies for AI solutions. It features four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder, each contributing to an efficient development workflow. The Questionnaire gathers structured input from users to understand specific project requirements and contexts. The Smart Fill component streamlines data entry by predicting and auto-completing relevant fields, enhancing user experience and accuracy. The Research-Guided Recommendation leverages up-to-date scientific literature to suggest optimal AI modeling approaches tailored to user inputs, ensuring solutions are grounded in validated research. Finally, the Prototype Builder enables rapid generation of functional prototypes based on the recommended strategies, allowing users ranging from Product Managers to Researchers to quickly transition from planning to implementation. Collectively, this pipeline accelerates AI development and democratizes access to cutting-edge modeling techniques by integrating expert knowledge with automated processes, as visually depicted in Figure 1 of the article. <div>
arXiv:2512.16171v1 Announce Type: new 
Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications</title>
<link>https://arxiv.org/abs/2512.16185</link>
<guid>https://arxiv.org/abs/2512.16185</guid>
<content:encoded><![CDATA[
<div> Weighted K-harmonic means, clustering algorithm, wireless networks, convergence guarantees, user association<br /><br />Summary:<br /><br />1. The paper introduces the weighted K-harmonic means (WKHM) clustering algorithm, a regularized version of the traditional K-harmonic means designed to provide numerical stability and enable soft assignments via inverse-distance weighting.<br /><br />2. WKHM differs from classical K-means and constrained K-means by offering a direct interpretation in wireless networks, where the algorithm’s weights correspond exactly to fractional user association based on received signal strength.<br /><br />3. The authors establish rigorous convergence guarantees for WKHM in both deterministic and stochastic contexts, tackling complex technical challenges due to the problem’s non-convexity and random initialization.<br /><br />4. Specifically, they prove that WKHM demonstrates monotone descent towards a local minimum under fixed initialization, convergence in probability when initialized by a Binomial Point Process, and almost sure convergence under mild decay conditions—marking the first stochastic convergence proof for harmonic-mean-based clustering.<br /><br />5. Extensive simulations with varied user distributions demonstrate that WKHM provides a superior balance between minimum signal strength and load fairness compared to existing classical and modern clustering methods, making it a robust tool for joint radio node placement and user association in wireless network design. <div>
arXiv:2512.16185v1 Announce Type: new 
Abstract: We propose the \emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving</title>
<link>https://arxiv.org/abs/2512.16214</link>
<guid>https://arxiv.org/abs/2512.16214</guid>
<content:encoded><![CDATA[
<div> Keywords: PDE solving, multi-agent collaboration, toolchain, LLM-driven agents, automated scientific computing<br /><br />Summary:<br /><br />1. Solving Partial Differential Equations (PDEs) is crucial in engineering and scientific research, but traditional methods require manual setup and domain expertise.  
2. Existing neural network approaches such as Physics-Informed Neural Networks (PINNs) and frameworks like DeepXDE improve automation but still depend heavily on expert knowledge and lack full autonomy.  
3. The paper introduces PDE-Agent, a novel framework that treats PDE solving as tool invocation managed by large language model (LLM)-driven agents, combining LLM reasoning with external tool controllability to fully automate PDE solving from natural language descriptions.  
4. PDE-Agent features two core innovations: (a) a Prog-Act framework with graph memory enabling multi-agent collaboration with effective dynamic planning and error correction through dual-loop mechanisms (localized fixes and global revisions), and (b) a Resource-Pool with a tool-parameter separation mechanism that centralizes runtime artifact management and resolves inter-tool dependencies.  
5. To benchmark and evaluate this paradigm, the authors develop PDE-Bench, a multi-type PDE benchmark for agent-based tool collaborative solving, alongside multi-level metrics assessing tool coordination. Experimental results show PDE-Agent’s superior performance and applicability in complex, multi-step, cross-step dependent PDE tasks, signaling a new direction for automated scientific computing.  
6. The source code and dataset will be publicly released, promoting further research and development in this area. <div>
arXiv:2512.16214v1 Announce Type: new 
Abstract: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis</title>
<link>https://arxiv.org/abs/2512.16237</link>
<guid>https://arxiv.org/abs/2512.16237</guid>
<content:encoded><![CDATA[
<div> Keywords: Embodied intelligence, Spatial reasoning, Vision-Language Models, Large Language Models, Dataset synthesis<br /><br />Summary:  
Embodied intelligence in AI faces fundamental limits due to insufficient spatial understanding and reasoning in current models. Existing approaches to improve Vision-Language Models (VLMs) struggle between scalable but rigid template-based datasets and linguistically diverse but unscalable and imprecise manual annotation. The paper introduces SPRITE, a novel framework that leverages simulators and large language models (LLMs) to synthesize scalable, diverse, and high-quality spatial reasoning data programmatically. SPRITE reframes ground-truth generation as a code-generation problem, using LLMs to compile spatial questions into executable programs verified by precise scene metadata from simulators. This ensures highly accurate and verifiable ground truth data alongside linguistic diversity. Employing this method, the authors curated a large dataset including 3 simulators, over 11,000 scenes, and more than 300,000 image/video instruction-tuning pairs. Experiments show that VLMs trained on this data significantly outperform models trained on other open-source datasets of comparable size on multiple spatial reasoning benchmarks. Scalability analysis confirms that overcoming the low linguistic diversity of traditional templates is key to robust and generalizable spatial intelligence. The SPRITE framework and full dataset will be publicly released to support further research in spatial reasoning and embodied AI. <div>
arXiv:2512.16237v1 Announce Type: new 
Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints</title>
<link>https://arxiv.org/abs/2512.16245</link>
<guid>https://arxiv.org/abs/2512.16245</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, model merging, Fisher-Rao geometry, AlignMerge<br /><br />Summary:<br /><br />1. The paper addresses the problem of merging large language models (LLMs) by combining multiple fine-tuned checkpoints without retraining, highlighting that common methods like linear weight soups and Fisher-weighted averaging may preserve loss but can degrade model alignment. <br /><br />2. The authors argue that merging should be treated as a geometry-constrained operation centered on a pre-aligned anchor model, emphasizing that the process must respect alignment-related geometry actively rather than verifying alignment only after merging.<br /><br />3. They introduce AlignMerge, a novel framework for geometry-aware merging that explicitly maintains alignment as an invariant by operating in a local Fisher information metric space around an instruction-tuned base model.<br /><br />4. AlignMerge optimizes a combined loss: L_geo to keep close proximity in Fisher-Rao geometry to expert models, L_align to penalize deviation along alignment-sensitive directions identified by a projector P_A, and L_bud to enforce an alignment budget constraint. It uses a latent-space Alignment Quality Index (AQI) to measure alignment effectiveness.<br /><br />5. Experiments on five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2) demonstrate that AlignMerge improves alignment metrics such as AQI, reduces toxicity, and performs well on tasks like instruction-following and reasoning, outperforming prior methods like Fisher soups, TIES, SafeMerge, and MergeAlign in preserving alignment and model quality.<br /><br />The results promote alignment-preserving merging as a crucial design goal and suggest geometry-aware composition as a promising direction in future foundation model development. <div>
arXiv:2512.16245v1 Announce Type: new 
Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding</title>
<link>https://arxiv.org/abs/2512.16250</link>
<guid>https://arxiv.org/abs/2512.16250</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, agentic reasoning, audio-visual understanding, AMUSE benchmark, RAFT framework<br /><br />Summary: Recent multimodal large language models (MLLMs) like GPT-4o and Qwen3-Omni demonstrate strong perception abilities but face challenges in multi-speaker, dialogue-centric scenarios that require agentic reasoning—tracking speakers, maintaining conversational roles, and grounding events over time. These challenges are especially critical in multimodal audio-video understanding applications such as conversational video assistants and meeting analytics. To address this, the authors present AMUSE, a benchmark designed to evaluate MLLMs on inherently agentic tasks that break down complex audio-visual interactions into planning, grounding, and reflection phases. AMUSE tests models in three modes—zero-shot, guided, and agentic—across six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Evaluations reveal that existing models struggle with multi-speaker reasoning and exhibit inconsistent performance in both agentic and non-agentic settings. To improve this, the authors propose RAFT, a data-efficient agentic alignment framework that combines reward optimization with intrinsic multimodal self-evaluation and selective parameter adaptation for efficient learning updates. Implementing RAFT leads to up to a 39.52% relative accuracy improvement on the AMUSE benchmark. Together, AMUSE and RAFT offer a robust platform to assess and enhance agentic reasoning capabilities in multimodal models. <div>
arXiv:2512.16250v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Wait: Synchronizing Agents with the Physical World</title>
<link>https://arxiv.org/abs/2512.16262</link>
<guid>https://arxiv.org/abs/2512.16262</guid>
<content:encoded><![CDATA[
<div> Temporal Gap, Large Language Models, Cognitive Timeline, Asynchronous Environment, Time Synchronization<br /><br />Summary:<br /><br />1. The paper addresses a key challenge in real-world agentic tasks where actions are non-blocking and have variable latencies, causing a temporal gap between action initiation and completion. 2. Traditional environment-side solutions like blocking wrappers or frequent polling have drawbacks such as reduced scalability and redundant observations that dilute the agent’s context window. 3. The authors propose an agent-side approach that allows Large Language Models (LLMs) to actively align their cognitive timeline with the real-world temporal dynamics, improving synchronization without excessive environment querying. 4. By extending the Code-as-Action paradigm into the temporal domain, the agents leverage semantic priors and In-Context Learning (ICL) to predict accurate waiting times using commands like time.sleep(t), thereby managing asynchronous actions efficiently. 5. Experimental validation in a simulated Kubernetes cluster shows that agents can finely tune their internal clocks to reduce both query overhead and execution latency, proving that temporal awareness is a learnable, critical skill for autonomous agents operating in open-ended, real-world environments. <div>
arXiv:2512.16262v1 Announce Type: new 
Abstract: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems</title>
<link>https://arxiv.org/abs/2512.16279</link>
<guid>https://arxiv.org/abs/2512.16279</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, safety policies, machine-checkable rules, multi-agent system, runtime enforcement  

<br /><br />Summary:  
1. The paper addresses safety risks in large language model (LLM)-based agents that perform complex tasks using tools, multi-step plans, and communication among agents.  
2. Current deployer-written safety policies, expressed in natural language, are ambiguous and context-dependent, making them difficult to translate into machine-checkable rules and leading to unreliable runtime enforcement.  
3. The authors introduce \textsc{QuadSentinel}, a novel safety mechanism composed of four agents: a state tracker, policy verifier, threat watcher, and referee. This multi-agent guard system compiles natural language policies into precise machine-checkable rules based on predicates over observable states.  
4. The referee component employs logic combined with an efficient top-$k$ predicate updating strategy to prioritize safety checks and resolve conflicts hierarchically, thereby maintaining low computational costs.  
5. Empirical evaluation on benchmarks ST-WebAgentBench and AgentHarm demonstrates that \textsc{QuadSentinel} outperforms single-agent baselines like ShieldAgent by improving guardrail accuracy, rule recall, and reducing false positives, offering better overall safety control.  
6. The approach allows near-term deployments to adopt this safety framework without modifying existing core agents by keeping policies separate and machine-checkable.  
7. The authors commit to releasing their code publicly at https://github.com/yyiliu/QuadSentinel to facilitate adoption and further research. <div>
arXiv:2512.16279v1 Announce Type: new 
Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models</title>
<link>https://arxiv.org/abs/2512.16295</link>
<guid>https://arxiv.org/abs/2512.16295</guid>
<content:encoded><![CDATA[
<div> GUI navigation, critic models, data synthesis, OS-Oracle, step-level evaluation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reliable step-level decision-making for VLM-powered computer-using agents (CUAs) navigating graphical user interfaces (GUIs), where errors in long-horizon workflows accumulate causing significant issues.<br /><br />2. Existing critic models, which assess the appropriateness of each action before execution, suffer from scarce diverse, high-quality GUI feedback data and lack public benchmarks for thorough step-level evaluation.<br /><br />3. To overcome these limitations, the authors introduce OS-Oracle, which includes: (a) a scalable data pipeline to synthesize cross-platform GUI critic data, (b) a two-stage training approach combining supervised fine-tuning (SFT) with consistency-preserving group relative policy optimization (CP-GRPO), and (c) OS-Critic Bench, a comprehensive benchmark spanning Mobile, Web, and Desktop platforms.<br /><br />4. Using this framework, the team curated a large dataset of 310,000 critic samples, enabling effective training of the OS-Oracle-7B model.<br /><br />5. OS-Oracle-7B achieves state-of-the-art performance on the OS-Critic Bench among open-source visual language models, surpasses proprietary models specifically in the mobile domain, and enhances performance when used as a pre-critic for native GUI agents like UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code and resources have been open-sourced for community use. <div>
arXiv:2512.16295v1 Announce Type: new 
Abstract: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection</title>
<link>https://arxiv.org/abs/2512.16300</link>
<guid>https://arxiv.org/abs/2512.16300</guid>
<content:encoded><![CDATA[
<div> Keywords: image forgery detection, multimodal large language models, Python-based tools, reinforcement fine-tuning, reasoning loop<br /><br />Summary:<br /><br />Existing image forgery detection (IFD) approaches either focus on low-level, semantics-agnostic artifacts or utilize multimodal large language models (MLLMs) to leverage high-level semantic information. However, integrating these heterogeneous information streams and their distinct reasoning paradigms remains challenging. To overcome this, the paper introduces ForenAgent, a novel multi-round interactive IFD framework where MLLMs autonomously generate, execute, and iteratively refine Python-based low-level tools to enhance forgery analysis flexibility and interpretability. ForenAgent's training involves two stages: Cold Start to bootstrap tool interaction capabilities, followed by Reinforcement Fine-Tuning to improve reasoning adaptability dynamically. The system features a human-inspired dynamic reasoning loop, encompassing global perception, local focusing, iterative probing, and holistic adjudication, which is implemented as a data-sampling strategy and task-aligned reward mechanism. To facilitate comprehensive development and assessment, the authors create FABench, a large-scale heterogeneous dataset with 100k images and nearly 200k agent-interaction question-answer pairs. Experimental results demonstrate that ForenAgent develops emergent tool-using skills and reflective reasoning, especially when supported by low-level tools, suggesting promising progress toward a general-purpose image forgery detection system. The source code is planned to be released post-review. <div>
arXiv:2512.16300v1 Announce Type: new 
Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation of Agentic AI</title>
<link>https://arxiv.org/abs/2512.16301</link>
<guid>https://arxiv.org/abs/2512.16301</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, adaptation, foundation models, tool adaptation, system design  

<br /><br />Summary:  
This paper addresses the development and improvement of agentic AI systems, which are built on foundation models capable of planning, reasoning, and interacting with external tools for complex tasks. It emphasizes the importance of adaptation as a central mechanism to enhance performance, reliability, and generalization of these systems. The authors propose a unified framework that categorizes adaptation strategies into agent adaptations and tool adaptations. Agent adaptations are further divided into tool-execution-signaled and agent-output-signaled forms, while tool adaptations are classified as agent-agnostic or agent-supervised. This framework clarifies the design space for adaptation strategies, making their trade-offs explicit and providing practical guidance for selecting or switching strategies during system design. The paper also reviews representative approaches within each category, analyzing their strengths and limitations. Finally, it identifies key open challenges and future opportunities in the field, aiming to provide both a conceptual foundation and practical roadmap. Overall, this work supports researchers and practitioners in building more capable, efficient, and reliable agentic AI systems by systematically organizing adaptation research and suggesting informed pathways for advancement. <div>
arXiv:2512.16301v1 Announce Type: new 
Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference</title>
<link>https://arxiv.org/abs/2512.16317</link>
<guid>https://arxiv.org/abs/2512.16317</guid>
<content:encoded><![CDATA[
<div> Keywords: Decentralized LLM inference, Proof of Quality, cost-aware framework, evaluator architecture, quality-cost analysis  

<br /><br />Summary:  
This paper addresses the challenge of scaling verification for decentralized large language model (LLM) inference by proposing a cost-aware Proof of Quality (PoQ) framework, which incorporates efficiency metrics into the reward mechanism for both inference and evaluator nodes. The approach combines multiple evaluation techniques—ground truth token-level F1 scores, lightweight learned evaluators, and GPT-based judgments—into a single pipeline, utilizing a linear reward function that balances normalized output quality against computational cost. Experiments were conducted on tasks including extractive question answering and abstractive summarization, using five instruction-tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B alongside three evaluation models featuring cross encoder and bi-encoder architectures. Results demonstrate that semantic textual similarity bi-encoders better correlate with ground truth and GPT evaluations than cross encoders, underscoring evaluator architecture as a vital design consideration for PoQ. Quality-cost analysis showed that the largest models also yielded the best quality per unit latency, challenging assumptions about model size vs. efficiency. Monte Carlo simulations across 5,000 PoQ rounds confirmed that the cost-aware reward scheme favors high-quality, low-cost inference models and efficient evaluators while penalizing slower, lower-quality nodes. The work establishes cost-aware PoQ as a feasible and economically sustainable foundation for decentralized LLM inference. <div>
arXiv:2512.16317v1 Announce Type: new 
Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Needs Physics More Than Physics Needs AI</title>
<link>https://arxiv.org/abs/2512.16344</link>
<guid>https://arxiv.org/abs/2512.16344</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, physics, large language models, quantum AI, Big AI<br /><br />Summary:  
Artificial intelligence (AI) is widely regarded as transformative, yet after over a decade of widespread enthusiasm, its practical impact is still relatively limited outside select high-profile scientific and commercial cases. The 2024 Nobel Prizes in Chemistry and Physics have acknowledged AI's potential, but broader evaluations suggest these achievements are more promotional than reflective of substantial technical breakthroughs. While current AI architectures—including large language models, reasoning models, and agentic AI—are impressive, they depend on extremely large amounts of parameters that often lack meaningful interpretation and suffer from distributional biases. Moreover, these models fail to provide reliable uncertainty quantification, mechanistic insights, or even capture fundamental scientific laws. The article reviews these critical shortcomings and explores emerging opportunities in quantum AI and analogue computing as promising directions. It advocates for a future paradigm called “Big AI,” which combines the rigor of theoretical physics and scientific principles with the adaptable power of machine learning techniques. This synthesis aims to overcome current limitations and enable AI systems that are more scientifically grounded, interpretable, and capable of contributing robust insights across disciplines, especially physics. <div>
arXiv:2512.16344v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCIA: A Path Construction Imitation Algorithm for Global Optimization</title>
<link>https://arxiv.org/abs/2512.16392</link>
<guid>https://arxiv.org/abs/2512.16392</guid>
<content:encoded><![CDATA[
<div> Keywords: metaheuristic optimization, Path Construction Imitation Algorithm, swarm-based algorithms, route construction, constrained optimization<br /><br />Summary:<br /><br />This paper introduces a novel metaheuristic optimization algorithm called the Path Construction Imitation Algorithm (PCIA), inspired by human methods of creating and selecting transportation routes. The algorithm mimics how humans prefer popular routes and adapt by intelligently mixing existing paths when faced with closures, as well as by randomly choosing diverse paths to reach unfamiliar destinations. PCIA operates by generating a random population of candidate solutions, each representing a potential path toward an optimization goal, similar to swarm intelligence strategies. The effectiveness and robustness of PCIA were evaluated through extensive testing on 53 mathematical optimization problems and 13 constrained optimization problems. Results demonstrated that PCIA delivers highly competitive performance, often outperforming or matching both widely known and recently developed metaheuristic algorithms. This suggests that the approach effectively balances exploration and exploitation in the search space. The study highlights the algorithm’s potential for various optimization challenges, showing adaptability inspired by human navigation and decision-making behavior in constructing optimal or near-optimal solutions. <div>
arXiv:2512.16392v1 Announce Type: new 
Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs</title>
<link>https://arxiv.org/abs/2512.16424</link>
<guid>https://arxiv.org/abs/2512.16424</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthelite, large language models, retrosynthetic transformations, computer-aided synthesis planning, chemical feasibility  

<br /><br />Summary:  
This work introduces Synthelite, an innovative computer-aided synthesis planning (CASP) framework that harnesses large language models (LLMs) to propose retrosynthetic transformations directly. Unlike existing CASP frameworks, Synthelite enables interactive collaboration with human experts through natural language prompts, allowing chemists to guide and refine synthesis strategies. The framework is capable of generating complete synthesis routes end-to-end by leveraging the chemical knowledge and reasoning abilities inherent to LLMs. Experimental results show Synthelite achieves up to 95% success rates on tasks constrained by either synthesis strategy or available starting materials, demonstrating its adaptability to various user-specified requirements. Moreover, Synthelite can evaluate chemical feasibility during the route design process, ensuring that proposed synthesis plans are practical and realistic. The authors propose that Synthelite not only serves as a valuable tool for synthetic chemists but also represents a step toward future synthesis planning paradigms where LLMs serve as central orchestrators, integrating expert insights with automated chemical reasoning for improved synthetic route design. <div>
arXiv:2512.16424v1 Announce Type: new 
Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles</title>
<link>https://arxiv.org/abs/2512.16442</link>
<guid>https://arxiv.org/abs/2512.16442</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, AI-supported research, research life cycle, reproducibility<br /><br />Summary:<br /><br />1. The paper presents the TIB AIssistant, an AI-supported research platform designed to assist researchers across the entire research life cycle, leveraging the growing potential of Artificial Intelligence and Large Language Models.  2. The AIssistant is composed of multiple specialized assistants, each dedicated to a distinct research task, ensuring focused and efficient support for various stages of research.  3. It integrates tools that provide access to external scholarly services, broadening the scope and resources available to researchers within the platform.  4. Data generated during research is stored in assets and can be exported as an RO-Crate bundle, promoting transparency and reproducibility by encapsulating research outputs and metadata in a standardized format.  5. Through a sequential demonstration, the AIssistant showcases how its components interact to generate sections of a draft research paper, illustrating practical usability and the potential to streamline academic writing. Ultimately, the work establishes a foundation for a community-maintained platform aimed at enhancing AI-supported research workflows and fostering collaborative improvements in academic productivity. <div>
arXiv:2512.16442v1 Announce Type: new 
Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm</title>
<link>https://arxiv.org/abs/2512.16444</link>
<guid>https://arxiv.org/abs/2512.16444</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, StarCraft II battle arena, adversarial benchmarking, PyMARL, algorithm evaluation<br /><br />Summary:<br /><br />This article addresses the limitation of current multi-agent reinforcement learning (MARL) evaluations, which typically use fixed built-in AI opponents in the StarCraft multi-agent challenge (SMAC), resulting in less diverse and versatile benchmarking. To overcome this, the authors propose the StarCraft II battle arena (SC2BA), a new environment designed specifically for inter-algorithm adversarial competition, emphasizing fairness, usability, and customizability. The SC2BA leverages the StarCraft infrastructure and provides a fresh adversary paradigm for MARL benchmarking. In addition, the authors develop an adversarial PyMARL (APyMARL) library, featuring easy-to-use interfaces and modules that facilitate algorithm-versus-algorithm experiments. Using SC2BA, the study conducts comprehensive benchmarks of classic MARL algorithms under two adversarial modes: dual-algorithm paired adversary, which evaluates pairwise algorithm competitions, and multi-algorithm mixed adversary, which assesses interactions among multiple algorithms simultaneously. The experimental results reveal insightful observations and challenges related to the effectiveness, sensitivity, and scalability of existing MARL algorithms. The authors have made the SC2BA environment and the reproduced benchmark experiments publicly available on GitHub, hoping to foster progress and renewed interest in adversarial benchmarking within the MARL research community. <div>
arXiv:2512.16444v1 Announce Type: new 
Abstract: Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards AI-Supported Research: a Vision of the TIB AIssistant</title>
<link>https://arxiv.org/abs/2512.16447</link>
<guid>https://arxiv.org/abs/2512.16447</guid>
<content:encoded><![CDATA[
<div> Generative AI, Large Language Models, Research Workflow, Human-Machine Collaboration, AI-driven Platform<br /><br />Summary: The paper introduces the TIB AIssistant, a domain-agnostic platform designed to integrate Generative AI and Large Language Models into research workflows across various disciplines. It aims to address challenges such as diverse domain needs, limited AI literacy among researchers, the complexity of managing multiple tools and agents, and the uncertainty regarding the accuracy of AI-generated outputs in scientific research. The TIB AIssistant supports human-machine collaboration by providing AI assistants that assist throughout the research life cycle, including ideation, literature review, methodology design, data analysis, and scholarly writing. The system’s architecture comprises modular components like prompt and tool libraries, a shared data store for collaborative knowledge management, and a flexible orchestration framework that enables seamless coordination of AI services and tools. An early prototype implementation demonstrates the platform’s feasibility and highlights its potential to enhance productivity and innovation in scientific discovery. This vision emphasizes building a flexible, extensible, and researcher-friendly solution to harness the benefits of AI while mitigating its limitations, ultimately transforming how researchers approach and execute their work. <div>
arXiv:2512.16447v1 Announce Type: new 
Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries</title>
<link>https://arxiv.org/abs/2512.16453</link>
<guid>https://arxiv.org/abs/2512.16453</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, time-series data, battery energy storage system, prompting framework, anomaly detection  

<br /><br />Summary:  
The paper introduces TimeSeries2Report (TS2R), a novel prompting framework designed to leverage large language models (LLMs) for interpreting multivariate time-series data from lithium-ion battery energy storage systems (BESS). TS2R transforms raw operational time-series signals into structured, semantically enriched natural language reports by applying segmentation, semantic abstraction, and rule-based interpretation. This approach effectively bridges the gap between low-level sensor data and high-level contextual insights, enabling LLMs to perform reasoning, prediction, and decision-making tasks relevant to BESS operation and maintenance. The framework is evaluated on both laboratory-scale and real-world datasets, focusing on key downstream tasks such as anomaly detection, state-of-charge prediction, and charge/discharge management. Results demonstrate that TS2R-based report prompting consistently outperforms other baseline prompting methods including vision-, embedding-, and text-based approaches across accuracy, robustness, and explainability metrics. Importantly, TS2R enables LLMs to achieve expert-level decision-making quality and predictive consistency without the need for retraining or modifications to model architecture. This work establishes a practical and adaptive pathway for integrating LLMs into battery intelligence systems, potentially advancing the operational effectiveness and maintenance of BESS technologies. <div>
arXiv:2512.16453v1 Announce Type: new 
Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution</title>
<link>https://arxiv.org/abs/2512.16465</link>
<guid>https://arxiv.org/abs/2512.16465</guid>
<content:encoded><![CDATA[
<div> Keywords: CUDA kernel optimization, multi-agent framework, evolutionary algorithms, roofline-guided prompting, high-performance computing<br /><br />Summary:<br /><br />This paper addresses the complexity and expertise required for optimizing CUDA kernels, highlighting the limitations of current automatic optimization methods that combine large language models with evolutionary algorithms but suffer from suboptimal agent designs and mismatched evolutionary representations. The authors introduce cuPilot, a novel multi-agent framework that incorporates strategy as an intermediate semantic representation to coordinate kernel evolution more effectively. Key innovations include a strategy-coordinated evolution algorithm that aligns better with hardware characteristics, roofline-guided prompting to enhance the accuracy of performance predictions, and strategy-level population initialization to improve the evolutionary search process. Experimental evaluation on a benchmark set of 100 kernels demonstrates that cuPilot achieves an average speedup of 3.09 times over PyTorch-generated kernels, showing significant practical benefits. In particular, on General Matrix Multiply (GEMM) tasks, cuPilot achieves sophisticated optimization results and effectively utilizes critical hardware units to maximize performance. The authors have open-sourced the generated kernels, providing the community access to high-performance kernels and enabling further research and development. Overall, cuPilot represents an important step towards automated, intelligent, and hardware-aware CUDA kernel optimization. <div>
arXiv:2512.16465v1 Announce Type: new 
Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery</title>
<link>https://arxiv.org/abs/2512.16468</link>
<guid>https://arxiv.org/abs/2512.16468</guid>
<content:encoded><![CDATA[
<div> Keywords: Decisive Feature Fidelity, autonomous vehicles, simulation fidelity, explainable AI, domain transfer<br /><br />Summary:  
This paper addresses the challenge of ensuring reliable transfer of autonomous vehicle (AV) system performance from synthetic simulation environments to the real world by moving beyond traditional pixel-level visual realism. It introduces Decisive Feature Fidelity (DFF), a novel metric specifically designed to evaluate system-under-test (SUT) behavior by measuring mechanism parity—the degree to which the SUT relies on the same causal evidence in both real and simulated domains. DFF uses explainable AI (XAI) techniques to identify and compare the critical features driving the SUT’s decisions for paired real and synthetic inputs. To estimate DFF in practice, the authors propose methods based on counterfactual explanations and develop a DFF-guided calibration scheme to tune simulators for improved behavioral fidelity. Experimental evaluation on 2126 matched pairs from the KITTI and VirtualKITTI2 datasets demonstrates that DFF can detect significant discrepancies that conventional output-value fidelity metrics miss. Furthermore, applying DFF-guided calibration leads to improvements in decisive-feature fidelity and input-level similarity, while maintaining consistent output performance across a variety of AV systems. This work highlights the importance of behavior-grounded fidelity metrics for trustworthy simulation-based testing and offers practical tools to enhance the transferability of AV safety validation. <div>
arXiv:2512.16468v1 Announce Type: new 
Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network</title>
<link>https://arxiv.org/abs/2512.16491</link>
<guid>https://arxiv.org/abs/2512.16491</guid>
<content:encoded><![CDATA[
<div> meta-algorithmics, algorithm selection, experimental design, best practices, empirical research<br /><br />Summary:<br /><br />This report addresses the challenges and complexity of empirical research in meta-algorithmics, including key areas like algorithm selection, configuration, and scheduling. It highlights that such research typically involves extensive, computationally expensive experiments with many variables, which can introduce multiple sources of error affecting the scalability and reliability of scientific outcomes. The report acknowledges that while best practices exist, they are dispersed across various publications and fields, often evolving independently without unified guidance. To consolidate knowledge, the authors gather and systematize good practices that span the entire experimental cycle in meta-algorithmics. This includes formulating clear and relevant research questions, choosing appropriate experimental designs, executing experiments efficiently and reliably, and analyzing as well as presenting results impartially and transparently. The document serves to establish a current state-of-the-art baseline of empirical methodologies within the COSEAL community and related meta-algorithmic subfields. It aims to provide a comprehensive guide and reference for both newcomers and experienced practitioners engaged in meta-algorithmic research, helping to improve the rigor, reproducibility, and validity of future studies. <div>
arXiv:2512.16491v1 Announce Type: new 
Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParamExplorer: A framework for exploring parameters in generative art</title>
<link>https://arxiv.org/abs/2512.16529</link>
<guid>https://arxiv.org/abs/2512.16529</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative art, parameter spaces, reinforcement learning, human-in-the-loop, p5.js<br /><br />Summary:<br /><br />1. Generative art systems involve complex, high-dimensional parameter spaces where aesthetically pleasing results are rare and scattered, making exploration challenging.<br />2. Artists traditionally depend on manual trial-and-error methods that often miss many interesting parameter configurations due to the vastness of these spaces.<br />3. This work introduces ParamExplorer, an interactive and modular framework inspired by reinforcement learning that facilitates effective exploration of generative art parameter spaces.<br />4. ParamExplorer supports guidance through human-in-the-loop feedback or automated feedback, enhancing the discovery of compelling outputs.<br />5. The framework is designed to integrate seamlessly with existing p5.js projects, ensuring accessibility and ease of use.<br />6. Within ParamExplorer, several exploration strategies called agents are implemented and evaluated to determine their effectiveness in navigating parameter spaces.<br />7. These contributions aim to improve the efficiency and depth of parameter exploration in generative art, potentially uncovering novel artistic configurations that manual search methods overlook. <div>
arXiv:2512.16529v1 Announce Type: new 
Abstract: Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Energy Efficiency of Local LLMs</title>
<link>https://arxiv.org/abs/2512.16531</link>
<guid>https://arxiv.org/abs/2512.16531</guid>
<content:encoded><![CDATA[
<div> Keywords: local inference, CPU benchmarks, scaling laws, vision-language models, quantum-inspired compression  

<br /><br />Summary:  
This article investigates the deployment of large language models (LLMs) and vision-language models on edge devices that rely solely on central processing units (CPUs), rather than graphics processors. The study benchmarks two common CPU platforms for local inference: the MacBook Pro M2 representing mainstream laptops, and the Raspberry Pi 5 representing low-power embedded systems. Through continuous monitoring of processor and memory usage combined with area-under-curve analysis, the authors identify how computational costs scale with input size for both language and vision-language models. Two key empirical scaling laws emerge: for language models, inference compute scales approximately linearly with token length; for vision-language models, a “resolution knee” phenomenon is observed where compute remains stable above a certain resolution threshold but sharply decreases below it due to preprocessing constraints. The paper further demonstrates that applying quantum-inspired compression techniques can significantly reduce processor and memory usage by up to 71.9%, and energy consumption by up to 62%, while maintaining or even improving semantic accuracy. These insights highlight model compression and input resolution adjustment as practical, low-cost strategies to optimize sustainable edge inference on CPU-based consumer hardware for multimodal AI workloads. <div>
arXiv:2512.16531v1 Announce Type: new 
Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment</title>
<link>https://arxiv.org/abs/2512.16532</link>
<guid>https://arxiv.org/abs/2512.16532</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Memory-enhanced personalization, Bias, Recruitment, AI agent guardrails<br /><br />Summary:<br /><br />Large Language Models (LLMs) have significantly advanced AI agents by enhancing their ability to understand, reason, and interact across diverse tasks. Integrating memory into these models elevates their capabilities by facilitating continuity across interactions, enabling learning from past experiences, and improving action and response relevance over time—this process is known as memory-enhanced personalization. Despite the clear benefits of personalization, it introduces notable risks of bias which have not been thoroughly explored in the context of memory-enhanced AI agents. The study focuses on recruitment as a practical use case to simulate and analyze how bias may emerge and be amplified at various stages of an agent’s operation. Experiments were conducted using safety-trained LLMs within personalized agents to investigate these dynamics. The results demonstrate that bias is not only introduced but also systematically reinforced through the personalization process driven by memory. This finding highlights a crucial need for implementing additional protective measures or agent guardrails to mitigate bias in memory-enhanced, LLM-based AI agents. Overall, the research underscores the importance of carefully addressing bias risks to ensure fairness and reliability in AI personalization applications. <div>
arXiv:2512.16532v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild</title>
<link>https://arxiv.org/abs/2512.16553</link>
<guid>https://arxiv.org/abs/2512.16553</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fuzzy Exploratory Search, Needle in the Web, ambiguous queries, search benchmarks<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) have advanced from chatbots to sophisticated agents capable of automating complex real-world tasks involving web browsing and reasoning.<br /><br />2. Current benchmarks like BrowseComp and xBench-DeepSearch focus on complex multi-hop reasoning queries but fail to address Fuzzy Exploratory Search, which involves vague and multifaceted queries seeking the most relevant web pages instead of single factual answers.<br /><br />3. To fill this gap, the authors propose Needle in the Web, a new benchmark containing 663 questions across seven domains designed to evaluate search agents and LLM-based systems on their ability to retrieve and reason over real-world web content under ambiguity and varying difficulty.<br /><br />4. The benchmark uses a flexible methodology for query generation, ensuring high quality and uniqueness by basing queries on factual web content claims and allowing controllable difficulty levels.<br /><br />5. Benchmarking three leading LLMs and three agent-based search systems reveals poor overall performance, with most models scoring below 35% accuracy and no system consistently performing well across domains or difficulty levels, highlighting the challenge of effective fuzzy retrieval in semantic ambiguity.<br /><br />These findings underscore the need for further research into improving LLMs and search systems for handling ambiguous, exploratory queries over live web data. <div>
arXiv:2512.16553v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam</title>
<link>https://arxiv.org/abs/2512.16644</link>
<guid>https://arxiv.org/abs/2512.16644</guid>
<content:encoded><![CDATA[
<div> Keywords: Sharia-compliant chatbot, Reinforcement Learning, Sentence-Transformers, Islamic knowledge, CRISP-DM methodology<br /><br />Summary:<br /><br />This research focuses on developing a Sharia-compliant chatbot designed to provide interactive consultations for Islamic questions. The system integrates Reinforcement Learning, specifically Q-Learning, with Sentence-Transformers to create semantic embeddings, ensuring responses are contextually relevant and accurate. Following the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology, the chatbot utilizes a curated dataset comprising 25,000 question-answer pairs gathered from authentic Islamic sources such as the Qur'an, Hadith, and scholarly fatwas, formatted in JSON to support flexibility and scalability. The prototype features a Flask API backend coupled with a Flutter-based mobile frontend, facilitating user interaction. Functional testing across diverse Islamic topics—fiqh, aqidah, ibadah, and muamalah—demonstrated an 87% semantic accuracy rate, highlighting the chatbot's capability to aid religious literacy and digital da'wah in the advancing Industry 4.0 era. Despite its effectiveness in answering closed-domain queries, the system currently faces limitations including static learning processes and dependence on its preset dataset. The study identifies avenues for future improvement, such as enabling continuous learning and supporting multi-turn conversations, aiming to merge traditional Islamic scholarship with modern AI-powered consultation for enhanced accessibility and engagement. <div>
arXiv:2512.16644v1 Announce Type: new 
Abstract: This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prefix Probing: Lightweight Harmful Content Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2512.16650</link>
<guid>https://arxiv.org/abs/2512.16650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, harmful content detection, prefix probing, inference latency, prefix caching<br /><br />Summary: This paper addresses the challenge of balancing detection accuracy, inference latency, and deployment cost in harmful content detection for large language models used in safety-critical applications. It introduces Prefix Probing, a novel black-box detection method that relies on comparing conditional log-probabilities of specific opening prefixes categorized as "agreement/execution" versus "refusal/safety." The approach utilizes prefix caching to minimize overhead, achieving detection latency close to that of generating the first token. During inference, Prefix Probing requires only a single computation of log-probabilities over these probe prefixes to generate a harmfulness score, allowing a threshold-based decision without the need for additional models or multi-stage processing. The paper also presents an efficient prefix construction algorithm designed to automatically identify highly informative prefixes, which substantially strengthens detection capabilities. Extensive experimental validation shows that Prefix Probing attains detection accuracy on par with commonly used external safety models while incurring minimal computational cost and eliminating the necessity for extra model deployment. These results highlight the method's practicality and efficiency, making it a promising solution for real-world safety-sensitive deployments of large language models. <div>
arXiv:2512.16650v1 Announce Type: new 
Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive AI Literacy: The Case for Centering Human Agency</title>
<link>https://arxiv.org/abs/2512.16656</link>
<guid>https://arxiv.org/abs/2512.16656</guid>
<content:encoded><![CDATA[
<div> Keywords: AI literacy, human agency, critical thinking, education framework, ethical reasoning<br /><br />Summary:<br /><br />1. The paper addresses the growing educational challenge caused by the rapid integration of AI technologies, highlighting a widening literacy gap. 2. It critiques current education systems for emphasizing operational skills with AI tools while neglecting critical and ethical reasoning. 3. The authors advocate for a systemic shift towards comprehensive AI literacy that centers on human agency—empowering individuals to make intentional, critical, and responsible decisions regarding AI usage. 4. Human agency applies to all stakeholders: students are encouraged to question, create with, or opt out of AI use based on contextual needs; teachers should retain pedagogical control by designing learning experiences aligned with educational values rather than deferring to AI tools. 5. True AI literacy involves teaching about agency itself, framing technology use as a conscious choice, supported by critical thinking and deep epistemological understanding. 6. The paper proposes AI Literacy, Fluency, and Competency frameworks that empower both educators and learners to articulate their intentions and attitudes towards AI, with implications for academic integrity, career development, and societal impact. <div>
arXiv:2512.16656v1 Announce Type: new 
Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm</title>
<link>https://arxiv.org/abs/2512.16694</link>
<guid>https://arxiv.org/abs/2512.16694</guid>
<content:encoded><![CDATA[
<div> Keywords: Apriori algorithm, hadith thematic grouping, unsupervised learning, association rule mining, digital Islamic texts<br /><br />Summary:  
This research addresses the need to automate the thematic classification of hadith, motivated by the increasing digitalization of Islamic texts. The study utilizes an unsupervised learning approach, specifically the Apriori algorithm, to detect association patterns and semantic relationships within unlabeled textual data. The dataset comprises the Indonesian translation of the hadith of Bukhari, which undergoes a series of preprocessing steps including case folding, punctuation removal, tokenization, stopword elimination, and stemming. Following preprocessing, association rule mining is performed using the Apriori algorithm with carefully set parameters such as support, confidence, and lift to identify meaningful associations. Results reveal significant thematic correlations like the links between rakaat and prayer, verse and revelation, and hadith and story. These associations correspond to key themes in Islamic studies: worship, divine revelation, and hadith narration. The findings demonstrate the capability of the Apriori algorithm to automatically uncover latent semantic relationships in religious texts. This research contributes to the field of digital Islamic studies and supports the development of technology-driven learning systems by providing an innovative method for thematic grouping based on data-driven patterns. <div>
arXiv:2512.16694v1 Announce Type: new 
Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning</title>
<link>https://arxiv.org/abs/2512.16698</link>
<guid>https://arxiv.org/abs/2512.16698</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent pipeline, diagram-grounded geometry, multimodal large language models, visual math benchmarks, open-source models  

<br /><br />Summary:  
This paper investigates the effectiveness of multi-agent versus single-agent designs for diagram-grounded geometry problem solving in multimodal large language models (MLLMs). The study evaluates both approaches across four prominent visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. Results indicate that multi-agent pipelines significantly enhance the performance of open-source models. For instance, the Qwen-2.5-VL model with 7 billion parameters achieves a +6.8 point improvement on Geometry3K using multi-agent, while the 32 billion parameter version gains +3.3 points. Both variants also show further improvements on OlympiadBench and We-Math with a multi-agent approach. Conversely, the closed-source Gemini-2.0-Flash model generally performs better with a single-agent pipeline on classic benchmarks. However, it still benefits modestly from multi-agent on newer datasets like We-Math. These findings demonstrate that multi-agent decomposition is advantageous for open-source systems and can assist proprietary models on less familiar benchmarks but may not be universally optimal. The authors provide all code, data, and reasoning files openly at the provided GitHub repository for reproducibility and further research. <div>
arXiv:2512.16698v1 Announce Type: new 
Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences</title>
<link>https://arxiv.org/abs/2512.16701</link>
<guid>https://arxiv.org/abs/2512.16701</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Cyber Humanism, Epistemic Agency, Algorithmic Citizenship, AI in Education<br /><br />Summary:<br /><br />This paper addresses the profound impact of Generative Artificial Intelligence (GenAI) on education, emphasizing how large language models transform traditional knowledge production into hybrid human-AI workflows. It highlights key concerns such as epistemic automation, cognitive offloading, and the potential de-professionalization of teachers. To counter these challenges, the authors propose a framework called Cyber Humanism in Education, which seeks to reclaim human agency within AI-enabled learning environments. These environments are viewed as socio-technical infrastructures co-created by humans and machines, where both educators and learners act as epistemic agents and "algorithmic citizens" with rights and responsibilities to influence these systems. The framework is built on three pillars: reflexive competence, algorithmic citizenship, and dialogic design, which are linked to global digital and AI competence standards. The paper showcases higher education case studies implementing these concepts through prompt-based learning and a Conversational AI Educator certification developed within the EPICT ecosystem. Results indicate that these practices can reinforce epistemic agency but also reveal challenges related to workload, equity, and governance. The study concludes with implications for developing AI-enriched, human-centered educational models that maintain a balance between technological innovation and human values. <div>
arXiv:2512.16701v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\-lisation of teachers. This paper proposes \emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.
  We articulate three pillars for cyber-humanist design, \emph{reflexive competence}, \emph{algorithmic citizenship}, and \emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \emph{prompt-based learning} and a new \emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems</title>
<link>https://arxiv.org/abs/2512.16707</link>
<guid>https://arxiv.org/abs/2512.16707</guid>
<content:encoded><![CDATA[
<div> Formal incompleteness, dynamical unpredictability, algorithmic intelligence, prediction horizon, self-analysis  

<br /><br />Summary:  
This article identifies and formalizes two fundamental computational limitations affecting algorithmic intelligence: formal incompleteness and dynamical unpredictability. Formal incompleteness restricts the deductive capabilities of any consistent reasoning system, meaning there are true statements that cannot be proven within the system. Dynamical unpredictability pertains to the inherent bounds on long-term prediction accuracy due to finite precision in computational processes. The authors demonstrate that these two limitations jointly create structural constraints on an intelligent agent's ability to evaluate and reason about its own prediction capabilities. Specifically, the paper proves that an algorithmic agent generally cannot compute its own maximal prediction horizon—the furthest point into the future it can accurately predict. This result reveals fundamental trade-offs among reasoning power, predictive accuracy, and self-referential analysis in intelligent systems. By integrating concepts from logic and dynamical systems theory, the work provides a novel theoretical framework clarifying why certain limits on intelligence and self-assessment are unavoidable. Ultimately, this research advances our understanding of the intrinsic boundaries that constrain algorithmic agents, offering insights relevant to the design and evaluation of artificial intelligence systems capable of self-improvement and foresight. <div>
arXiv:2512.16707v1 Announce Type: new 
Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering and Learning Probabilistic Models of Black-Box AI Capabilities</title>
<link>https://arxiv.org/abs/2512.16733</link>
<guid>https://arxiv.org/abs/2512.16733</guid>
<content:encoded><![CDATA[
arXiv:2512.16733v1 Announce Type: new 
Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach</title>
<link>https://arxiv.org/abs/2512.16739</link>
<guid>https://arxiv.org/abs/2512.16739</guid>
<content:encoded><![CDATA[
arXiv:2512.16739v1 Announce Type: new 
Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?</title>
<link>https://arxiv.org/abs/2512.16755</link>
<guid>https://arxiv.org/abs/2512.16755</guid>
<content:encoded><![CDATA[
arXiv:2512.16755v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge</title>
<link>https://arxiv.org/abs/2512.16855</link>
<guid>https://arxiv.org/abs/2512.16855</guid>
<content:encoded><![CDATA[
arXiv:2512.16855v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional AGI Safety</title>
<link>https://arxiv.org/abs/2512.16856</link>
<guid>https://arxiv.org/abs/2512.16856</guid>
<content:encoded><![CDATA[
arXiv:2512.16856v1 Announce Type: new 
Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI</title>
<link>https://arxiv.org/abs/2512.16873</link>
<guid>https://arxiv.org/abs/2512.16873</guid>
<content:encoded><![CDATA[
arXiv:2512.16873v1 Announce Type: new 
Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16917</link>
<guid>https://arxiv.org/abs/2512.16917</guid>
<content:encoded><![CDATA[
arXiv:2512.16917v1 Announce Type: new 
Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
<link>https://arxiv.org/abs/2511.00456</link>
<guid>https://arxiv.org/abs/2511.00456</guid>
<content:encoded><![CDATA[
arXiv:2511.00456v5 Announce Type: cross 
Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network</title>
<link>https://arxiv.org/abs/2512.15109</link>
<guid>https://arxiv.org/abs/2512.15109</guid>
<content:encoded><![CDATA[
arXiv:2512.15109v1 Announce Type: cross 
Abstract: The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression</title>
<link>https://arxiv.org/abs/2512.15721</link>
<guid>https://arxiv.org/abs/2512.15721</guid>
<content:encoded><![CDATA[
arXiv:2512.15721v1 Announce Type: cross 
Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Lens: Using Large Language Models to Understand Human Values</title>
<link>https://arxiv.org/abs/2512.15722</link>
<guid>https://arxiv.org/abs/2512.15722</guid>
<content:encoded><![CDATA[
arXiv:2512.15722v1 Announce Type: cross 
Abstract: The autonomous decision-making process, which is increasingly applied to computer systems, requires that the choices made by these systems align with human values. In this context, systems must assess how well their decisions reflect human values. To achieve this, it is essential to identify whether each available action promotes or undermines these values. This article presents Value Lens, a text-based model designed to detect human values using generative artificial intelligence, specifically Large Language Models (LLMs). The proposed model operates in two stages: the first aims to formulate a formal theory of values, while the second focuses on identifying these values within a given text. In the first stage, an LLM generates a description based on the established theory of values, which experts then verify. In the second stage, a pair of LLMs is employed: one LLM detects the presence of values, and the second acts as a critic and reviewer of the detection process. The results indicate that Value Lens performs comparably to, and even exceeds, the effectiveness of other models that apply different methods for similar tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSight AI: Multi-Agent System Architecture for Federal Funds Target Rate Prediction</title>
<link>https://arxiv.org/abs/2512.15728</link>
<guid>https://arxiv.org/abs/2512.15728</guid>
<content:encoded><![CDATA[
arXiv:2512.15728v1 Announce Type: cross 
Abstract: The Federal Open Market Committee (FOMC) sets the federal funds rate, shaping monetary policy and the broader economy. We introduce \emph{FedSight AI}, a multi-agent framework that uses large language models (LLMs) to simulate FOMC deliberations and predict policy outcomes. Member agents analyze structured indicators and unstructured inputs such as the Beige Book, debate options, and vote, replicating committee reasoning. A Chain-of-Draft (CoD) extension further improves efficiency and accuracy by enforcing concise multistage reasoning. Evaluated at 2023-2024 meetings, FedSight CoD achieved accuracy of 93.75\% and stability of 93.33\%, outperforming baselines including MiniFed and Ordinal Random Forest (RF), while offering transparent reasoning aligned with real FOMC communications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</title>
<link>https://arxiv.org/abs/2512.15729</link>
<guid>https://arxiv.org/abs/2512.15729</guid>
<content:encoded><![CDATA[
arXiv:2512.15729v1 Announce Type: cross 
Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Context-Free Smart Grid Model Using Complex System Approach</title>
<link>https://arxiv.org/abs/2512.15733</link>
<guid>https://arxiv.org/abs/2512.15733</guid>
<content:encoded><![CDATA[
arXiv:2512.15733v1 Announce Type: cross 
Abstract: Energy and pollution are urging problems of the 21th century. By gradually changing the actual power grid system, smart grid may evolve into different systems by means of size, elements and strategies, but its fundamental requirements and objectives will not change such as optimizing production, transmission, and consumption. Studying the smart grid through modeling and simulation provides us with valuable results which cannot be obtained in real world due to time and cost related constraints. Moreover, due to the complexity of the smart grid, achieving global optimization is not an easy task. In this paper, we propose a complex system based approach to the smart grid modeling, accentuating on the optimization by combining game theoretical and classical methods in different levels. Thanks to this combination, the optimization can be achieved with flexibility and scalability, while keeping its generality.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming</title>
<link>https://arxiv.org/abs/2512.15735</link>
<guid>https://arxiv.org/abs/2512.15735</guid>
<content:encoded><![CDATA[
arXiv:2512.15735v1 Announce Type: cross 
Abstract: This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Ensemble Learning for S\&amp;P 500 Directional Prediction</title>
<link>https://arxiv.org/abs/2512.15738</link>
<guid>https://arxiv.org/abs/2512.15738</guid>
<content:encoded><![CDATA[
arXiv:2512.15738v1 Announce Type: cross 
Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&amp;P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</title>
<link>https://arxiv.org/abs/2512.15739</link>
<guid>https://arxiv.org/abs/2512.15739</guid>
<content:encoded><![CDATA[
arXiv:2512.15739v1 Announce Type: cross 
Abstract: A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&amp;P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA2.0: Scaling Up Diffusion Language Models to 100B</title>
<link>https://arxiv.org/abs/2512.15745</link>
<guid>https://arxiv.org/abs/2512.15745</guid>
<content:encoded><![CDATA[
arXiv:2512.15745v1 Announce Type: cross 
Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction</title>
<link>https://arxiv.org/abs/2512.15751</link>
<guid>https://arxiv.org/abs/2512.15751</guid>
<content:encoded><![CDATA[
arXiv:2512.15751v1 Announce Type: cross 
Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification</title>
<link>https://arxiv.org/abs/2512.15753</link>
<guid>https://arxiv.org/abs/2512.15753</guid>
<content:encoded><![CDATA[
arXiv:2512.15753v1 Announce Type: cross 
Abstract: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single "Other" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning</title>
<link>https://arxiv.org/abs/2512.15756</link>
<guid>https://arxiv.org/abs/2512.15756</guid>
<content:encoded><![CDATA[
arXiv:2512.15756v1 Announce Type: cross 
Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction</title>
<link>https://arxiv.org/abs/2512.15762</link>
<guid>https://arxiv.org/abs/2512.15762</guid>
<content:encoded><![CDATA[
arXiv:2512.15762v1 Announce Type: cross 
Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</title>
<link>https://arxiv.org/abs/2512.15764</link>
<guid>https://arxiv.org/abs/2512.15764</guid>
<content:encoded><![CDATA[
arXiv:2512.15764v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.
  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.
  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.
  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2512.15766</link>
<guid>https://arxiv.org/abs/2512.15766</guid>
<content:encoded><![CDATA[
arXiv:2512.15766v1 Announce Type: cross 
Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework</title>
<link>https://arxiv.org/abs/2512.15767</link>
<guid>https://arxiv.org/abs/2512.15767</guid>
<content:encoded><![CDATA[
arXiv:2512.15767v1 Announce Type: cross 
Abstract: Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling</title>
<link>https://arxiv.org/abs/2512.15768</link>
<guid>https://arxiv.org/abs/2512.15768</guid>
<content:encoded><![CDATA[
arXiv:2512.15768v1 Announce Type: cross 
Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title>
<link>https://arxiv.org/abs/2512.15769</link>
<guid>https://arxiv.org/abs/2512.15769</guid>
<content:encoded><![CDATA[
arXiv:2512.15769v1 Announce Type: cross 
Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions</title>
<link>https://arxiv.org/abs/2512.15771</link>
<guid>https://arxiv.org/abs/2512.15771</guid>
<content:encoded><![CDATA[
arXiv:2512.15771v1 Announce Type: cross 
Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</title>
<link>https://arxiv.org/abs/2512.15773</link>
<guid>https://arxiv.org/abs/2512.15773</guid>
<content:encoded><![CDATA[
arXiv:2512.15773v1 Announce Type: cross 
Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</title>
<link>https://arxiv.org/abs/2512.15775</link>
<guid>https://arxiv.org/abs/2512.15775</guid>
<content:encoded><![CDATA[
arXiv:2512.15775v1 Announce Type: cross 
Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</title>
<link>https://arxiv.org/abs/2512.15780</link>
<guid>https://arxiv.org/abs/2512.15780</guid>
<content:encoded><![CDATA[
arXiv:2512.15780v1 Announce Type: cross 
Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance</title>
<link>https://arxiv.org/abs/2512.15786</link>
<guid>https://arxiv.org/abs/2512.15786</guid>
<content:encoded><![CDATA[
arXiv:2512.15786v1 Announce Type: cross 
Abstract: Cultural rights and the right to development are essential norms within the wider framework of international human rights law. However, recent technological advances in artificial intelligence (AI) and adjacent digital frontier technologies pose significant challenges to the protection and realization of these rights. This owes to the increasing influence of AI systems on the creation and depiction of cultural content, affect the use and distribution of the intellectual property of individuals and communities, and influence cultural participation and expression worldwide. In addition, the growing influence of AI thus risks exacerbating preexisting economic, social and digital divides and reinforcing inequities for marginalized communities. This dynamic challenges the existing interplay between cultural rights and the right to development, and raises questions about the integration of cultural and developmental considerations into emerging AI governance frameworks. To address these challenges, the paper examines the impact of AI on both categories of rights. Conceptually, it analyzes the epistemic and normative limitations of AI with respect to cultural and developmental assumptions embedded in algorithmic design and deployment, but also individual and structural impacts of AI on both rights. On this basis, the paper identifies gaps and tensions in existing AI governance frameworks with respect to cultural rights and the right to development.
  By situating cultural rights and the right to development within the broader landscape of AI and human rights, this paper contributes to the academic discourse on AI ethics, legal frameworks, and international human rights law. Finally, it outlines avenues for future research and policy development based on existing conversations in global AI governance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces</title>
<link>https://arxiv.org/abs/2512.15787</link>
<guid>https://arxiv.org/abs/2512.15787</guid>
<content:encoded><![CDATA[
arXiv:2512.15787v1 Announce Type: cross 
Abstract: In recent years, advances in artificial intelligence (AI), particularly generative AI (GenAI) and large language models (LLMs), have made human-computer interactions more frequent, efficient, and accessible across sectors ranging from banking to healthcare. AI tools embedded in digital devices support decision-making and operational management at both individual and organizational levels, including resource allocation, workflow automation, and real-time data analysis. However, the prevailing cloud-centric deployment of AI carries a substantial environmental footprint due to high computational demands. In this context, this paper introduces the concept of agentic environments, a sustainability-oriented AI framework that extends beyond reactive systems by leveraging GenAI, multi-agent systems, and edge computing to reduce the environmental impact of technology. Agentic environments enable more efficient resource use, improved quality of life, and sustainability-by-design, while simultaneously enhancing data privacy through decentralized, edge-driven solutions. Drawing on secondary research as well as primary data from focus groups and semi-structured interviews with AI professionals from leading technology companies, the paper proposes a conceptual framework for agentic environments examined through three lenses: the personal sphere, professional and commercial use, and urban operations. The findings highlight the potential of agentic environments to foster sustainable ecosystems through optimized resource utilization and strengthened data privacy. The study concludes with recommendations for edge-driven deployment models to reduce reliance on energy-intensive cloud infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud</title>
<link>https://arxiv.org/abs/2512.15791</link>
<guid>https://arxiv.org/abs/2512.15791</guid>
<content:encoded><![CDATA[
arXiv:2512.15791v1 Announce Type: cross 
Abstract: In Artificial Intelligence (AI), language models have gained significant importance due to the widespread adoption of systems capable of simulating realistic conversations with humans through text generation. Because of their impact on society, developing and deploying these language models must be done responsibly, with attention to their negative impacts and possible harms. In this scenario, the number of AI Ethics Tools (AIETs) publications has recently increased. These AIETs are designed to help developers, companies, governments, and other stakeholders establish trust, transparency, and responsibility with their technologies by bringing accepted values to guide AI's design, development, and use stages. However, many AIETs lack good documentation, examples of use, and proof of their effectiveness in practice. This paper presents a methodology for evaluating AIETs in language models. Our approach involved an extensive literature survey on 213 AIETs, and after applying inclusion and exclusion criteria, we selected four AIETs: Model Cards, ALTAI, FactSheets, and Harms Modeling. For evaluation, we applied AIETs to language models developed for the Portuguese language, conducting 35 hours of interviews with their developers. The evaluation considered the developers' perspective on the AIETs' use and quality in helping to identify ethical considerations about their model. The results suggest that the applied AIETs serve as a guide for formulating general ethical considerations about language models. However, we note that they do not address unique aspects of these models, such as idiomatic expressions. Additionally, these AIETs did not help to identify potential negative impacts of models for the Portuguese language.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Analysis of Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2512.15792</link>
<guid>https://arxiv.org/abs/2512.15792</guid>
<content:encoded><![CDATA[
arXiv:2512.15792v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly become indispensable tools for acquiring information and supporting human decision-making. However, ensuring that these models uphold fairness across varied contexts is critical to their safe and responsible deployment. In this study, we undertake a comprehensive examination of four widely adopted LLMs, probing their underlying biases and inclinations across the dimensions of politics, ideology, alliance, language, and gender. Through a series of carefully designed experiments, we investigate their political neutrality using news summarization, ideological biases through news stance classification, tendencies toward specific geopolitical alliances via United Nations voting patterns, language bias in the context of multilingual story completion, and gender-related affinities as revealed by responses to the World Values Survey. Results indicate that while the LLMs are aligned to be neutral and impartial, they still show biases and affinities of different types.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms</title>
<link>https://arxiv.org/abs/2512.15793</link>
<guid>https://arxiv.org/abs/2512.15793</guid>
<content:encoded><![CDATA[
arXiv:2512.15793v1 Announce Type: cross 
Abstract: Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action ``\textit{report a witnessed crime}" are social norms that inform our conduct, such as ``\textit{It is expected to be brave to report crimes}''. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to ``\textit{report a witnessed crime}'', one may balance \textit{bravery} against \textit{self-protection}. In this paper, we introduce \textit{ClarityEthic}, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India</title>
<link>https://arxiv.org/abs/2512.15799</link>
<guid>https://arxiv.org/abs/2512.15799</guid>
<content:encoded><![CDATA[
arXiv:2512.15799v1 Announce Type: cross 
Abstract: The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI "dual-use" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven "tool crimes" and "target crimes." Consequently, the research proposes a "human-centric" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-wise Topological Divergence Gaps: Guiding Search in Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2512.15800</link>
<guid>https://arxiv.org/abs/2512.15800</guid>
<content:encoded><![CDATA[
arXiv:2512.15800v1 Announce Type: cross 
Abstract: We introduce a topological feedback mechanism for the Travelling Salesman Problem (TSP) by analyzing the divergence between a tour and the minimum spanning tree (MST). Our key contribution is a canonical decomposition theorem that expresses the tour-MST gap as edge-wise topology-divergence gaps from the RTD-Lite barcode. Based on this, we develop a topological guidance for 2-opt and 3-opt heuristics that increases their performance. We carry out experiments with fine-optimization of tours obtained from heatmap-based methods, TSPLIB, and random instances. Experiments demonstrate the topology-guided optimization results in better performance and faster convergence in many cases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory</title>
<link>https://arxiv.org/abs/2512.15813</link>
<guid>https://arxiv.org/abs/2512.15813</guid>
<content:encoded><![CDATA[
arXiv:2512.15813v1 Announce Type: cross 
Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning</title>
<link>https://arxiv.org/abs/2512.15816</link>
<guid>https://arxiv.org/abs/2512.15816</guid>
<content:encoded><![CDATA[
arXiv:2512.15816v1 Announce Type: cross 
Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</title>
<link>https://arxiv.org/abs/2512.15829</link>
<guid>https://arxiv.org/abs/2512.15829</guid>
<content:encoded><![CDATA[
arXiv:2512.15829v1 Announce Type: cross 
Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Agentic Language Model Inference via Speculative Tool Calls</title>
<link>https://arxiv.org/abs/2512.15834</link>
<guid>https://arxiv.org/abs/2512.15834</guid>
<content:encoded><![CDATA[
arXiv:2512.15834v1 Announce Type: cross 
Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.15885</link>
<guid>https://arxiv.org/abs/2512.15885</guid>
<content:encoded><![CDATA[
arXiv:2512.15885v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons</title>
<link>https://arxiv.org/abs/2512.15891</link>
<guid>https://arxiv.org/abs/2512.15891</guid>
<content:encoded><![CDATA[
arXiv:2512.15891v1 Announce Type: cross 
Abstract: In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of the millisecond precision of spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could, in principle, preserve and manipulate sensory information through spike timing. It could support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are synchronously stimulated. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces</title>
<link>https://arxiv.org/abs/2512.15892</link>
<guid>https://arxiv.org/abs/2512.15892</guid>
<content:encoded><![CDATA[
arXiv:2512.15892v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.
  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).
  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Story Frames: Contextual Reasoning about Narrative Intent and Reception</title>
<link>https://arxiv.org/abs/2512.15925</link>
<guid>https://arxiv.org/abs/2512.15925</guid>
<content:encoded><![CDATA[
arXiv:2512.15925v1 Announce Type: cross 
Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins</title>
<link>https://arxiv.org/abs/2512.15930</link>
<guid>https://arxiv.org/abs/2512.15930</guid>
<content:encoded><![CDATA[
arXiv:2512.15930v1 Announce Type: cross 
Abstract: Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them "undruggable". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
arXiv:2512.15938v1 Announce Type: cross 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15957</link>
<guid>https://arxiv.org/abs/2512.15957</guid>
<content:encoded><![CDATA[
arXiv:2512.15957v1 Announce Type: cross 
Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRAID: Bounded Reasoning for Autonomous Inference and Decisions</title>
<link>https://arxiv.org/abs/2512.15959</link>
<guid>https://arxiv.org/abs/2512.15959</guid>
<content:encoded><![CDATA[
arXiv:2512.15959v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering</title>
<link>https://arxiv.org/abs/2512.15979</link>
<guid>https://arxiv.org/abs/2512.15979</guid>
<content:encoded><![CDATA[
arXiv:2512.15979v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding Software Intent: Lightweight Java Module Recovery</title>
<link>https://arxiv.org/abs/2512.15980</link>
<guid>https://arxiv.org/abs/2512.15980</guid>
<content:encoded><![CDATA[
arXiv:2512.15980v1 Announce Type: cross 
Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Extracting the Features from a General Superposition</title>
<link>https://arxiv.org/abs/2512.15987</link>
<guid>https://arxiv.org/abs/2512.15987</guid>
<content:encoded><![CDATA[
arXiv:2512.15987v1 Announce Type: cross 
Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,\sigma_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $\sigma_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $\sigma_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate Neural Architecture Codesign Package (SNAC-Pack)</title>
<link>https://arxiv.org/abs/2512.15998</link>
<guid>https://arxiv.org/abs/2512.15998</guid>
<content:encoded><![CDATA[
arXiv:2512.15998v1 Announce Type: cross 
Abstract: Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</title>
<link>https://arxiv.org/abs/2512.16013</link>
<guid>https://arxiv.org/abs/2512.16013</guid>
<content:encoded><![CDATA[
arXiv:2512.16013v1 Announce Type: cross 
Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios</title>
<link>https://arxiv.org/abs/2512.16019</link>
<guid>https://arxiv.org/abs/2512.16019</guid>
<content:encoded><![CDATA[
arXiv:2512.16019v1 Announce Type: cross 
Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Language Bias Examination in Large Language Models</title>
<link>https://arxiv.org/abs/2512.16029</link>
<guid>https://arxiv.org/abs/2512.16029</guid>
<content:encoded><![CDATA[
arXiv:2512.16029v1 Announce Type: cross 
Abstract: This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We on the Right Way to Assessing LLM-as-a-Judge?</title>
<link>https://arxiv.org/abs/2512.16041</link>
<guid>https://arxiv.org/abs/2512.16041</guid>
<content:encoded><![CDATA[
arXiv:2512.16041v1 Announce Type: cross 
Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
<link>https://arxiv.org/abs/2512.16046</link>
<guid>https://arxiv.org/abs/2512.16046</guid>
<content:encoded><![CDATA[
arXiv:2512.16046v1 Announce Type: cross 
Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis</title>
<link>https://arxiv.org/abs/2512.16063</link>
<guid>https://arxiv.org/abs/2512.16063</guid>
<content:encoded><![CDATA[
arXiv:2512.16063v1 Announce Type: cross 
Abstract: Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feasibility of Radio Frequency Based Wireless Sensing of Lead Contamination in Soil</title>
<link>https://arxiv.org/abs/2512.16071</link>
<guid>https://arxiv.org/abs/2512.16071</guid>
<content:encoded><![CDATA[
arXiv:2512.16071v1 Announce Type: cross 
Abstract: Widespread Pb (lead) contamination of urban soil significantly impacts food safety and public health and hinders city greening efforts. However, most existing technologies for measuring Pb are labor-intensive and costly. In this study, we propose SoilScanner, a radio frequency-based wireless system that can detect Pb in soils. This is based on our discovery that the propagation of different frequency band radio signals is affected differently by different salts such as NaCl and Pb(NO3)2 in the soil. In a controlled experiment, manually adding NaCl and Pb(NO3)2 in clean soil, we demonstrated that different salts reflected signals at different frequencies in distinct patterns. In addition, we confirmed the finding using uncontrolled field samples with a machine learning model. Our experiment results show that SoilScanner can classify soil samples into low-Pb and high-Pb categories (threshold at 200 ppm) with an accuracy of 72%, with no sample with > 500 ppm of Pb being misclassified. The results of this study show that it is feasible to build portable and affordable Pb detection and screening devices based on wireless technology.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of Generative Models for Emotional 3D Animation Generation in VR</title>
<link>https://arxiv.org/abs/2512.16081</link>
<guid>https://arxiv.org/abs/2512.16081</guid>
<content:encoded><![CDATA[
arXiv:2512.16081v1 Announce Type: cross 
Abstract: Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</title>
<link>https://arxiv.org/abs/2512.16083</link>
<guid>https://arxiv.org/abs/2512.16083</guid>
<content:encoded><![CDATA[
arXiv:2512.16083v1 Announce Type: cross 
Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPX: Lightweight Hourglass Network with Global Context</title>
<link>https://arxiv.org/abs/2512.16089</link>
<guid>https://arxiv.org/abs/2512.16089</guid>
<content:encoded><![CDATA[
arXiv:2512.16089v1 Announce Type: cross 
Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
arXiv:2512.16093v1 Announce Type: cross 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation</title>
<link>https://arxiv.org/abs/2512.16103</link>
<guid>https://arxiv.org/abs/2512.16103</guid>
<content:encoded><![CDATA[
arXiv:2512.16103v1 Announce Type: cross 
Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModelTables: A Corpus of Tables about Models</title>
<link>https://arxiv.org/abs/2512.16106</link>
<guid>https://arxiv.org/abs/2512.16106</guid>
<content:encoded><![CDATA[
arXiv:2512.16106v1 Announce Type: cross 
Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</title>
<link>https://arxiv.org/abs/2512.16123</link>
<guid>https://arxiv.org/abs/2512.16123</guid>
<content:encoded><![CDATA[
arXiv:2512.16123v1 Announce Type: cross 
Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INTELLECT-3: Technical Report</title>
<link>https://arxiv.org/abs/2512.16144</link>
<guid>https://arxiv.org/abs/2512.16144</guid>
<content:encoded><![CDATA[
arXiv:2512.16144v1 Announce Type: cross 
Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation</title>
<link>https://arxiv.org/abs/2512.16145</link>
<guid>https://arxiv.org/abs/2512.16145</guid>
<content:encoded><![CDATA[
arXiv:2512.16145v1 Announce Type: cross 
Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.16147</link>
<guid>https://arxiv.org/abs/2512.16147</guid>
<content:encoded><![CDATA[
arXiv:2512.16147v1 Announce Type: cross 
Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</title>
<link>https://arxiv.org/abs/2512.16164</link>
<guid>https://arxiv.org/abs/2512.16164</guid>
<content:encoded><![CDATA[
arXiv:2512.16164v1 Announce Type: cross 
Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services</title>
<link>https://arxiv.org/abs/2512.16167</link>
<guid>https://arxiv.org/abs/2512.16167</guid>
<content:encoded><![CDATA[
arXiv:2512.16167v1 Announce Type: cross 
Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Closing the Domain Gap with Event Cameras</title>
<link>https://arxiv.org/abs/2512.16178</link>
<guid>https://arxiv.org/abs/2512.16178</guid>
<content:encoded><![CDATA[
arXiv:2512.16178v1 Announce Type: cross 
Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Ad-hoc Categorization with Contextualized Feature Learning</title>
<link>https://arxiv.org/abs/2512.16202</link>
<guid>https://arxiv.org/abs/2512.16202</guid>
<content:encoded><![CDATA[
arXiv:2512.16202v1 Announce Type: cross 
Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural emulation of gravity-driven geohazard runout</title>
<link>https://arxiv.org/abs/2512.16221</link>
<guid>https://arxiv.org/abs/2512.16221</guid>
<content:encoded><![CDATA[
arXiv:2512.16221v1 Announce Type: cross 
Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Information-Theoretic Framework for Robust Large Language Model Editing</title>
<link>https://arxiv.org/abs/2512.16227</link>
<guid>https://arxiv.org/abs/2512.16227</guid>
<content:encoded><![CDATA[
arXiv:2512.16227v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</title>
<link>https://arxiv.org/abs/2512.16235</link>
<guid>https://arxiv.org/abs/2512.16235</guid>
<content:encoded><![CDATA[
arXiv:2512.16235v1 Announce Type: cross 
Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models</title>
<link>https://arxiv.org/abs/2512.16236</link>
<guid>https://arxiv.org/abs/2512.16236</guid>
<content:encoded><![CDATA[
arXiv:2512.16236v1 Announce Type: cross 
Abstract: Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.
  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16244</link>
<guid>https://arxiv.org/abs/2512.16244</guid>
<content:encoded><![CDATA[
arXiv:2512.16244v1 Announce Type: cross 
Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma-Moe-Tiny Technical Report</title>
<link>https://arxiv.org/abs/2512.16248</link>
<guid>https://arxiv.org/abs/2512.16248</guid>
<content:encoded><![CDATA[
arXiv:2512.16248v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2512.16251</link>
<guid>https://arxiv.org/abs/2512.16251</guid>
<content:encoded><![CDATA[
arXiv:2512.16251v1 Announce Type: cross 
Abstract: We introduce the \textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</title>
<link>https://arxiv.org/abs/2512.16270</link>
<guid>https://arxiv.org/abs/2512.16270</guid>
<content:encoded><![CDATA[
arXiv:2512.16270v1 Announce Type: cross 
Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification</title>
<link>https://arxiv.org/abs/2512.16271</link>
<guid>https://arxiv.org/abs/2512.16271</guid>
<content:encoded><![CDATA[
arXiv:2512.16271v1 Announce Type: cross 
Abstract: Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.
  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.
  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls</title>
<link>https://arxiv.org/abs/2512.16272</link>
<guid>https://arxiv.org/abs/2512.16272</guid>
<content:encoded><![CDATA[
arXiv:2512.16272v1 Announce Type: cross 
Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFLAN: Generative Functional Layouts</title>
<link>https://arxiv.org/abs/2512.16275</link>
<guid>https://arxiv.org/abs/2512.16275</guid>
<content:encoded><![CDATA[
arXiv:2512.16275v1 Announce Type: cross 
Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams</title>
<link>https://arxiv.org/abs/2512.16280</link>
<guid>https://arxiv.org/abs/2512.16280</guid>
<content:encoded><![CDATA[
arXiv:2512.16280v1 Announce Type: cross 
Abstract: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.
  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</title>
<link>https://arxiv.org/abs/2512.16282</link>
<guid>https://arxiv.org/abs/2512.16282</guid>
<content:encoded><![CDATA[
arXiv:2512.16282v1 Announce Type: cross 
Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Selective Representation Misdirection for Machine Unlearning</title>
<link>https://arxiv.org/abs/2512.16297</link>
<guid>https://arxiv.org/abs/2512.16297</guid>
<content:encoded><![CDATA[
arXiv:2512.16297v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixelArena: A benchmark for Pixel-Precision Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.16303</link>
<guid>https://arxiv.org/abs/2512.16303</guid>
<content:encoded><![CDATA[
arXiv:2512.16303v1 Announce Type: cross 
Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2512.16307</link>
<guid>https://arxiv.org/abs/2512.16307</guid>
<content:encoded><![CDATA[
arXiv:2512.16307v1 Announce Type: cross 
Abstract: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation</title>
<link>https://arxiv.org/abs/2512.16310</link>
<guid>https://arxiv.org/abs/2512.16310</guid>
<content:encoded><![CDATA[
arXiv:2512.16310v1 Announce Type: cross 
Abstract: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
<link>https://arxiv.org/abs/2512.16334</link>
<guid>https://arxiv.org/abs/2512.16334</guid>
<content:encoded><![CDATA[
arXiv:2512.16334v1 Announce Type: cross 
Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Edge-to-Server Inference for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16349</link>
<guid>https://arxiv.org/abs/2512.16349</guid>
<content:encoded><![CDATA[
arXiv:2512.16349v1 Announce Type: cross 
Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs</title>
<link>https://arxiv.org/abs/2512.16378</link>
<guid>https://arxiv.org/abs/2512.16378</guid>
<content:encoded><![CDATA[
arXiv:2512.16378v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2512.16391</link>
<guid>https://arxiv.org/abs/2512.16391</guid>
<content:encoded><![CDATA[
arXiv:2512.16391v1 Announce Type: cross 
Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</title>
<link>https://arxiv.org/abs/2512.16397</link>
<guid>https://arxiv.org/abs/2512.16397</guid>
<content:encoded><![CDATA[
arXiv:2512.16397v1 Announce Type: cross 
Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypernetworks That Evolve Themselves</title>
<link>https://arxiv.org/abs/2512.16406</link>
<guid>https://arxiv.org/abs/2512.16406</guid>
<content:encoded><![CDATA[
arXiv:2512.16406v1 Announce Type: cross 
Abstract: How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2512.16425</link>
<guid>https://arxiv.org/abs/2512.16425</guid>
<content:encoded><![CDATA[
arXiv:2512.16425v1 Announce Type: cross 
Abstract: As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bias and Fairness in Multi-Agent Decision Systems</title>
<link>https://arxiv.org/abs/2512.16433</link>
<guid>https://arxiv.org/abs/2512.16433</guid>
<content:encoded><![CDATA[
arXiv:2512.16433v1 Announce Type: cross 
Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Modelling Black Box Optimization</title>
<link>https://arxiv.org/abs/2512.16445</link>
<guid>https://arxiv.org/abs/2512.16445</guid>
<content:encoded><![CDATA[
arXiv:2512.16445v1 Announce Type: cross 
Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion</title>
<link>https://arxiv.org/abs/2512.16446</link>
<guid>https://arxiv.org/abs/2512.16446</guid>
<content:encoded><![CDATA[
arXiv:2512.16446v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value</title>
<link>https://arxiv.org/abs/2512.16448</link>
<guid>https://arxiv.org/abs/2512.16448</guid>
<content:encoded><![CDATA[
arXiv:2512.16448v1 Announce Type: cross 
Abstract: The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research</title>
<link>https://arxiv.org/abs/2512.16455</link>
<guid>https://arxiv.org/abs/2512.16455</guid>
<content:encoded><![CDATA[
arXiv:2512.16455v1 Announce Type: cross 
Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.16484</link>
<guid>https://arxiv.org/abs/2512.16484</guid>
<content:encoded><![CDATA[
arXiv:2512.16484v1 Announce Type: cross 
Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</title>
<link>https://arxiv.org/abs/2512.16485</link>
<guid>https://arxiv.org/abs/2512.16485</guid>
<content:encoded><![CDATA[
arXiv:2512.16485v1 Announce Type: cross 
Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16494</link>
<guid>https://arxiv.org/abs/2512.16494</guid>
<content:encoded><![CDATA[
arXiv:2512.16494v1 Announce Type: cross 
Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XTC, A Research Platform for Optimizing AI Workload Operators</title>
<link>https://arxiv.org/abs/2512.16512</link>
<guid>https://arxiv.org/abs/2512.16512</guid>
<content:encoded><![CDATA[
arXiv:2512.16512v1 Announce Type: cross 
Abstract: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence</title>
<link>https://arxiv.org/abs/2512.16515</link>
<guid>https://arxiv.org/abs/2512.16515</guid>
<content:encoded><![CDATA[
arXiv:2512.16515v1 Announce Type: cross 
Abstract: We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16523</link>
<guid>https://arxiv.org/abs/2512.16523</guid>
<content:encoded><![CDATA[
arXiv:2512.16523v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics</title>
<link>https://arxiv.org/abs/2512.16530</link>
<guid>https://arxiv.org/abs/2512.16530</guid>
<content:encoded><![CDATA[
arXiv:2512.16530v1 Announce Type: cross 
Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</title>
<link>https://arxiv.org/abs/2512.16586</link>
<guid>https://arxiv.org/abs/2512.16586</guid>
<content:encoded><![CDATA[
arXiv:2512.16586v1 Announce Type: cross 
Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics</title>
<link>https://arxiv.org/abs/2512.16602</link>
<guid>https://arxiv.org/abs/2512.16602</guid>
<content:encoded><![CDATA[
arXiv:2512.16602v1 Announce Type: cross 
Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents</title>
<link>https://arxiv.org/abs/2512.16614</link>
<guid>https://arxiv.org/abs/2512.16614</guid>
<content:encoded><![CDATA[
arXiv:2512.16614v1 Announce Type: cross 
Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</title>
<link>https://arxiv.org/abs/2512.16626</link>
<guid>https://arxiv.org/abs/2512.16626</guid>
<content:encoded><![CDATA[
arXiv:2512.16626v1 Announce Type: cross 
Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking</title>
<link>https://arxiv.org/abs/2512.16658</link>
<guid>https://arxiv.org/abs/2512.16658</guid>
<content:encoded><![CDATA[
arXiv:2512.16658v1 Announce Type: cross 
Abstract: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance</title>
<link>https://arxiv.org/abs/2512.16661</link>
<guid>https://arxiv.org/abs/2512.16661</guid>
<content:encoded><![CDATA[
arXiv:2512.16661v1 Announce Type: cross 
Abstract: In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</title>
<link>https://arxiv.org/abs/2512.16685</link>
<guid>https://arxiv.org/abs/2512.16685</guid>
<content:encoded><![CDATA[
arXiv:2512.16685v1 Announce Type: cross 
Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library</title>
<link>https://arxiv.org/abs/2512.16715</link>
<guid>https://arxiv.org/abs/2512.16715</guid>
<content:encoded><![CDATA[
arXiv:2512.16715v1 Announce Type: cross 
Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeNet: A Light Weight Model for Low Bitrate Image Compression</title>
<link>https://arxiv.org/abs/2512.16743</link>
<guid>https://arxiv.org/abs/2512.16743</guid>
<content:encoded><![CDATA[
arXiv:2512.16743v1 Announce Type: cross 
Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error</title>
<link>https://arxiv.org/abs/2512.16750</link>
<guid>https://arxiv.org/abs/2512.16750</guid>
<content:encoded><![CDATA[
arXiv:2512.16750v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation</title>
<link>https://arxiv.org/abs/2512.16770</link>
<guid>https://arxiv.org/abs/2512.16770</guid>
<content:encoded><![CDATA[
arXiv:2512.16770v1 Announce Type: cross 
Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Mass Spectrum Analysis with ASP</title>
<link>https://arxiv.org/abs/2512.16780</link>
<guid>https://arxiv.org/abs/2512.16780</guid>
<content:encoded><![CDATA[
arXiv:2512.16780v1 Announce Type: cross 
Abstract: We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implemen- tation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title>
<link>https://arxiv.org/abs/2512.16791</link>
<guid>https://arxiv.org/abs/2512.16791</guid>
<content:encoded><![CDATA[
arXiv:2512.16791v1 Announce Type: cross 
Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint</title>
<link>https://arxiv.org/abs/2512.16792</link>
<guid>https://arxiv.org/abs/2512.16792</guid>
<content:encoded><![CDATA[
arXiv:2512.16792v1 Announce Type: cross 
Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2512.16795</link>
<guid>https://arxiv.org/abs/2512.16795</guid>
<content:encoded><![CDATA[
arXiv:2512.16795v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16813</link>
<guid>https://arxiv.org/abs/2512.16813</guid>
<content:encoded><![CDATA[
arXiv:2512.16813v1 Announce Type: cross 
Abstract: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs</title>
<link>https://arxiv.org/abs/2512.16814</link>
<guid>https://arxiv.org/abs/2512.16814</guid>
<content:encoded><![CDATA[
arXiv:2512.16814v1 Announce Type: cross 
Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title>
<link>https://arxiv.org/abs/2512.16826</link>
<guid>https://arxiv.org/abs/2512.16826</guid>
<content:encoded><![CDATA[
arXiv:2512.16826v1 Announce Type: cross 
Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</title>
<link>https://arxiv.org/abs/2512.16842</link>
<guid>https://arxiv.org/abs/2512.16842</guid>
<content:encoded><![CDATA[
arXiv:2512.16842v1 Announce Type: cross 
Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference</title>
<link>https://arxiv.org/abs/2512.16843</link>
<guid>https://arxiv.org/abs/2512.16843</guid>
<content:encoded><![CDATA[
arXiv:2512.16843v1 Announce Type: cross 
Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-RL Induces Exploration in Language Agents</title>
<link>https://arxiv.org/abs/2512.16848</link>
<guid>https://arxiv.org/abs/2512.16848</guid>
<content:encoded><![CDATA[
arXiv:2512.16848v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy</title>
<link>https://arxiv.org/abs/2512.16851</link>
<guid>https://arxiv.org/abs/2512.16851</guid>
<content:encoded><![CDATA[
arXiv:2512.16851v1 Announce Type: cross 
Abstract: The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2512.16853</link>
<guid>https://arxiv.org/abs/2512.16853</guid>
<content:encoded><![CDATA[
arXiv:2512.16853v1 Announce Type: cross 
Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16861</link>
<guid>https://arxiv.org/abs/2512.16861</guid>
<content:encoded><![CDATA[
arXiv:2512.16861v1 Announce Type: cross 
Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</title>
<link>https://arxiv.org/abs/2512.16866</link>
<guid>https://arxiv.org/abs/2512.16866</guid>
<content:encoded><![CDATA[
arXiv:2512.16866v1 Announce Type: cross 
Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2512.16871</link>
<guid>https://arxiv.org/abs/2512.16871</guid>
<content:encoded><![CDATA[
arXiv:2512.16871v1 Announce Type: cross 
Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: cross 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: cross 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: cross 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Racial Bias in Historical Training Data for News AI</title>
<link>https://arxiv.org/abs/2512.16901</link>
<guid>https://arxiv.org/abs/2512.16901</guid>
<content:encoded><![CDATA[
arXiv:2512.16901v1 Announce Type: cross 
Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</title>
<link>https://arxiv.org/abs/2512.16907</link>
<guid>https://arxiv.org/abs/2512.16907</guid>
<content:encoded><![CDATA[
arXiv:2512.16907v1 Announce Type: cross 
Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</title>
<link>https://arxiv.org/abs/2512.16911</link>
<guid>https://arxiv.org/abs/2512.16911</guid>
<content:encoded><![CDATA[
arXiv:2512.16911v1 Announce Type: cross 
Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
<link>https://arxiv.org/abs/2512.16912</link>
<guid>https://arxiv.org/abs/2512.16912</guid>
<content:encoded><![CDATA[
arXiv:2512.16912v1 Announce Type: cross 
Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVGT: Driving Visual Geometry Transformer</title>
<link>https://arxiv.org/abs/2512.16919</link>
<guid>https://arxiv.org/abs/2512.16919</guid>
<content:encoded><![CDATA[
arXiv:2512.16919v1 Announce Type: cross 
Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasyV2V: A High-quality Instruction-based Video Editing Framework</title>
<link>https://arxiv.org/abs/2512.16920</link>
<guid>https://arxiv.org/abs/2512.16920</guid>
<content:encoded><![CDATA[
arXiv:2512.16920v1 Announce Type: cross 
Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
<link>https://arxiv.org/abs/2512.16921</link>
<guid>https://arxiv.org/abs/2512.16921</guid>
<content:encoded><![CDATA[
arXiv:2512.16921v1 Announce Type: cross 
Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enter the Void - Planning to Seek Entropy When Reward is Scarce</title>
<link>https://arxiv.org/abs/2505.16787</link>
<guid>https://arxiv.org/abs/2505.16787</guid>
<content:encoded><![CDATA[
arXiv:2505.16787v3 Announce Type: replace 
Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase the sample efficiency of model-free RL methods by simultaneously training a world model that learns to predict the future. These models constitute the large majority of training compute and time and they are subsequently used to train actors entirely in simulation, but once this is done they are quickly discarded. We show in this work that utilising these models at inference time can significantly boost sample efficiency. We propose a novel approach that anticipates and actively seeks out informative states using the world model's short-horizon latent predictions, offering a principled alternative to traditional curiosity-driven methods that chase outdated estimates of high uncertainty states. While many model predictive control (MPC) based methods offer similar alternatives, they typically lack commitment, synthesising multiple multi-step plans at every step. To mitigate this, we present a hierarchical planner that dynamically decides when to replan, planning horizon length, and the commitment to searching entropy. While our method can theoretically be applied to any model that trains its own actors with solely model generated data, we have applied it to Dreamer to illustrate the concept. Our method finishes MiniWorld's procedurally generated mazes 50% faster than base Dreamer at convergence and in only 60% of the environment steps that base Dreamer's policy needs; it displays reasoned exploratory behaviour in Crafter, achieves the same reward as base Dreamer in a third of the steps; planning tends to improve sample efficiency on DeepMind Control tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
arXiv:2506.04133v5 Announce Type: replace 
Abstract: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around key pillars: \textit{ Explainability, ModelOps, Security, Privacy} and \textit{their Lifecycle Governance}, each contextualized to the challenges of AMAS. A risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI, as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, highlighting key directions to align emerging systems with TRiSM principles-ensuring safety, transparency, and accountability in their operation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Pervasive Distributed Agentic Generative AI -- A State of The Art</title>
<link>https://arxiv.org/abs/2506.13324</link>
<guid>https://arxiv.org/abs/2506.13324</guid>
<content:encoded><![CDATA[
arXiv:2506.13324v2 Announce Type: replace 
Abstract: The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called "Agent as a Tool", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical GraphRAG: Efficient Knowledge Graph Construction and Hybrid Retrieval at Scale</title>
<link>https://arxiv.org/abs/2507.03226</link>
<guid>https://arxiv.org/abs/2507.03226</guid>
<content:encoded><![CDATA[
arXiv:2507.03226v3 Announce Type: replace 
Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based Retrieval-Augmented Generation (GraphRAG) in enterprise environments. While GraphRAG has shown promise for multi- hop reasoning and structured retrieval, its adoption has been limited due to reliance on expensive large language model (LLM)-based extraction and complex traversal strategies. To address these challenges, we introduce two core innovations: (1) an efficient knowledge graph construction pipeline that leverages dependency parsing to achieve 94% of LLM-based performance (61.87% vs. 65.83%) while significantly reducing costs and improving scalability; and (2) a hybrid retrieval strategy that fuses vector similarity with graph traversal using Reciprocal Rank Fusion (RRF), maintaining separate embeddings for entities, chunks, and relations to enable multi-granular matching. We evaluate our framework on two enterprise datasets focused on legacy code migration and demonstrate improvements of up to 15% and 4.35% over vanilla vector retrieval baselines using LLM-as-Judge evaluation metrics. These results validate the feasibility of deploying GraphRAG in production enterprise environments, demonstrating that careful engineering of classical NLP techniques can match modern LLM-based approaches while enabling practical, cost-effective, and domain-adaptable retrieval-augmented reasoning at scale.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting</title>
<link>https://arxiv.org/abs/2507.16145</link>
<guid>https://arxiv.org/abs/2507.16145</guid>
<content:encoded><![CDATA[
arXiv:2507.16145v2 Announce Type: replace 
Abstract: Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease with persistent airflow limitation, is a leading global cause of disability and mortality. Respiratory spirogram time series, routinely collected during pulmonary function tests (PFTs), play a critical role in the early detection of repsiratory diseases and in monitoring lung function over time. However, most current AI models for COPD diagnosis are limited to outputting classification results without providing a rationale for their diagnostic process, while current Large Language Models (LLMs) cannot understand spirograms yet, which severely limits their clinical trust and adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large language model that can understand spirogram. The model extracts morphological features from respiratory curves via a SpiroEncoder and aligns them with PFT numerical values in a unified latent space using a SpiroProjector, ultimately empowering a large language model to generate a comprehensive diagnostic report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC of 0.8977 (95% CI: 0.88-0.91). In a robustness test with missing core data, it maintained a 100% valid response rate, far surpassing the 13.4% of a text-only model and showcasing the superiority of its multimodal design. This work demonstrates the substantial potential of deeply fusing physiological signals with large language models, establishing a new paradigm for the next generation of interpretable and reliable clinical decision support tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
arXiv:2507.21503v3 Announce Type: replace 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</title>
<link>https://arxiv.org/abs/2508.09670</link>
<guid>https://arxiv.org/abs/2508.09670</guid>
<content:encoded><![CDATA[
arXiv:2508.09670v2 Announce Type: replace 
Abstract: Recent advances demonstrate that reinforcement learning with verifiable rewards (RLVR) significantly enhances the reasoning capabilities of large language models (LLMs). However, standard RLVR faces challenges with reward sparsity, where zero rewards from consistently incorrect candidate answers provide no learning signal, particularly in challenging tasks. To address this, we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative framework that utilizes diverse expert prompts as system prompts to generate a broader range of responses, substantially increasing the likelihood of identifying correct solutions. Additionally, we introduce an inter-expert mutual learning mechanism that facilitates knowledge sharing and transfer among experts, further boosting the model's performance through RLVR. Extensive experiments across multiple reasoning benchmarks show that MEML-GRPO delivers significant improvements, achieving an average performance gain of 4.89% with Qwen and 11.33% with Llama, effectively overcoming the core limitations of traditional RLVR methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives</title>
<link>https://arxiv.org/abs/2508.20978</link>
<guid>https://arxiv.org/abs/2508.20978</guid>
<content:encoded><![CDATA[
arXiv:2508.20978v4 Announce Type: replace 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</title>
<link>https://arxiv.org/abs/2510.05684</link>
<guid>https://arxiv.org/abs/2510.05684</guid>
<content:encoded><![CDATA[
arXiv:2510.05684v2 Announce Type: replace 
Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification</title>
<link>https://arxiv.org/abs/2510.12534</link>
<guid>https://arxiv.org/abs/2510.12534</guid>
<content:encoded><![CDATA[
arXiv:2510.12534v3 Announce Type: replace 
Abstract: The rapid growth of user-generated text across digital platforms has intensified the need for interpretable models capable of fine-grained text classification and explanation. Existing prototype-based models offer intuitive explanations but typically operate at coarse granularity (sentence or document level) and fail to address the multi-label nature of real-world text classification. We propose ProtoSiTex, a semi-interpretable framework designed for fine-grained multi-label text classification. ProtoSiTex employs a dual-phase alternate training strategy: an unsupervised prototype discovery phase that learns semantically coherent and diverse prototypes, and a supervised classification phase that maps these prototypes to class labels. A hierarchical loss function enforces consistency across subsentence, sentence, and document levels, enhancing interpretability and alignment. Unlike prior approaches, ProtoSiTex captures overlapping and conflicting semantics using adaptive prototypes and multi-head attention. We also introduce a benchmark dataset of hotel reviews annotated at the subsentence level with multiple labels. Experiments on this dataset and two public benchmarks (binary and multi-class) show that ProtoSiTex achieves state-of-the-art performance while delivering faithful, human-aligned explanations, establishing it as a robust solution for semi-interpretable multi-label text classification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics</title>
<link>https://arxiv.org/abs/2511.11357</link>
<guid>https://arxiv.org/abs/2511.11357</guid>
<content:encoded><![CDATA[
arXiv:2511.11357v2 Announce Type: replace 
Abstract: We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
arXiv:2512.09566v2 Announce Type: replace 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title>
<link>https://arxiv.org/abs/2406.16745</link>
<guid>https://arxiv.org/abs/2406.16745</guid>
<content:encoded><![CDATA[
arXiv:2406.16745v3 Announce Type: replace-cross 
Abstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy Come, Easy Go? Examining the Perceptions and Learning Effects of LLM-based Chatbot in the Context of Search-as-Learning</title>
<link>https://arxiv.org/abs/2410.01396</link>
<guid>https://arxiv.org/abs/2410.01396</guid>
<content:encoded><![CDATA[
arXiv:2410.01396v2 Announce Type: replace-cross 
Abstract: The cognitive process of Search-as-Learning (SAL) is most effective when searching promotes active encoding of information. The rise of LLMs-based chatbots, which provide instant answers, introduces a trade-off between efficiency and depth of processing. Such answer-centric approaches accelerate information access, but they also raise concerns about shallower learning. To examine these issues in the context of SAL, we conducted a large-scale survey of educators and students to capture perceived risks and benefits of LLM-based chatbots. In addition, we adopted the encoding-storage paradigm to design a within-subjects experiment, where participants (N=92) engaged in SAL tasks using three different modalities: books, search engines, and chatbots. Our findings provide a counterintuitive insight into stakeholder concerns: while LLM-based chatbots and search engines validated perceived benefits on learning efficiency by outperforming book-based search in immediate conceptual understanding, they did not result in a long-term inferiority as feared. Our study provides insights for designing human-AI collaborative learning systems that promote cognitive engagement by balancing learning efficiency and long-term knowledge retention.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace-cross 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for Microbiology and Microbiome Research</title>
<link>https://arxiv.org/abs/2411.01098</link>
<guid>https://arxiv.org/abs/2411.01098</guid>
<content:encoded><![CDATA[
arXiv:2411.01098v2 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology and microbiome research now experiencing significant breakthroughs through machine learning applications. This review provides a comprehensive overview of AI-driven approaches tailored for microbiology and microbiome studies, emphasizing both technical advancements and biological insights. We begin with an introduction to foundational AI techniques, including primary machine learning paradigms and various deep learning architectures, and offer guidance on choosing between traditional machine learning and sophisticated deep learning methods based on specific research goals. The primary section on application scenarios spans diverse research areas, from taxonomic profiling, functional annotation \& prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision nutrition, clinical microbiology, to prevention \& therapeutics. Finally, we discuss challenges in this field and highlight some recent breakthroughs. Together, this review underscores AI's transformative role in microbiology and microbiome research, paving the way for innovative methodologies and applications that enhance our understanding of microbial life and its impact on our planet and our health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
arXiv:2412.08435v5 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modality Collaborative Learning for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.12424</link>
<guid>https://arxiv.org/abs/2501.12424</guid>
<content:encoded><![CDATA[
arXiv:2501.12424v2 Announce Type: replace-cross 
Abstract: Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2502.10239</link>
<guid>https://arxiv.org/abs/2502.10239</guid>
<content:encoded><![CDATA[
arXiv:2502.10239v2 Announce Type: replace-cross 
Abstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose \ac{METHOD} that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $1.6-3\times$ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization</title>
<link>https://arxiv.org/abs/2502.12672</link>
<guid>https://arxiv.org/abs/2502.12672</guid>
<content:encoded><![CDATA[
arXiv:2502.12672v3 Announce Type: replace-cross 
Abstract: Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Guided Diffusion for Multi-Objective Offline Optimization</title>
<link>https://arxiv.org/abs/2503.17299</link>
<guid>https://arxiv.org/abs/2503.17299</guid>
<content:encoded><![CDATA[
arXiv:2503.17299v2 Announce Type: replace-cross 
Abstract: Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/ surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinAudio: A Benchmark for Audio Large Language Models in Financial Applications</title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
arXiv:2503.20990v3 Announce Type: replace-cross 
Abstract: Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</title>
<link>https://arxiv.org/abs/2504.16112</link>
<guid>https://arxiv.org/abs/2504.16112</guid>
<content:encoded><![CDATA[
arXiv:2504.16112v2 Announce Type: replace-cross 
Abstract: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</title>
<link>https://arxiv.org/abs/2504.19565</link>
<guid>https://arxiv.org/abs/2504.19565</guid>
<content:encoded><![CDATA[
arXiv:2504.19565v3 Announce Type: replace-cross 
Abstract: Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. This agentic framework collectively generates and refines domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
arXiv:2505.12781v4 Announce Type: replace-cross 
Abstract: Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines</title>
<link>https://arxiv.org/abs/2506.01329</link>
<guid>https://arxiv.org/abs/2506.01329</guid>
<content:encoded><![CDATA[
arXiv:2506.01329v2 Announce Type: replace-cross 
Abstract: Psychological support hotlines serve as critical lifelines for crisis intervention but encounter significant challenges due to rising demand and limited resources. Large language models (LLMs) offer potential support in crisis assessments, yet their effectiveness in emotionally sensitive, real-world clinical settings remains underexplored. We introduce PsyCrisisBench, a comprehensive benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, assessing four key tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. 64 LLMs across 15 model families (including closed-source such as GPT, Claude, Gemini and open-source such as Llama, Qwen, DeepSeek) were evaluated using zero-shot, few-shot, and fine-tuning paradigms. LLMs showed strong results in suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779), and risk assessment (F1=0.907), with notable gains from few-shot prompting and fine-tuning. Compared to trained human operators, LLMs achieved comparable or superior performance on suicide plan identification and risk assessment, while humans retained advantages on mood status recognition and suicidal ideation detection. Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity. Notably, a fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks. LLMs demonstrate performance broadly comparable to trained human operators in text-based crisis assessment, with complementary strengths across task types. PsyCrisisBench provides a robust, real-world evaluation framework to guide future model development and ethical deployment in clinical mental health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</title>
<link>https://arxiv.org/abs/2506.09695</link>
<guid>https://arxiv.org/abs/2506.09695</guid>
<content:encoded><![CDATA[
arXiv:2506.09695v3 Announce Type: replace-cross 
Abstract: Early diagnosis of Alzheimer's Disease (AD), particularly at the mild cognitive impairment stage, is essential for timely intervention. However, this process faces significant barriers, including reliance on subjective assessments and the high cost of advanced imaging techniques. While deep learning offers automated solutions to improve diagnostic accuracy, its widespread adoption remains constrained due to high energy requirements and computational demands, particularly in resource-limited settings. Spiking neural networks (SNNs) provide a promising alternative, as their brain-inspired design is well-suited to model the sparse and event-driven patterns characteristic of neural degeneration in AD. These networks offer the potential for developing interpretable, energy-efficient diagnostic tools. Despite their advantages, existing SNNs often suffer from limited expressiveness and challenges in stable training, which reduce their effectiveness in handling complex medical tasks. To address these shortcomings, we introduce FasterSNN, a hybrid neural architecture that combines biologically inspired Leaky Integrate-and-Fire (LIF) neurons with region-adaptive convolution and multi-scale spiking attention mechanisms. This approach facilitates efficient, sparse processing of 3D MRI data while maintaining high diagnostic accuracy. Experimental results on benchmark datasets reveal that FasterSNN delivers competitive performance with significantly enhanced efficiency and training stability, highlighting its potential for practical application in AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</title>
<link>https://arxiv.org/abs/2506.12775</link>
<guid>https://arxiv.org/abs/2506.12775</guid>
<content:encoded><![CDATA[
arXiv:2506.12775v2 Announce Type: replace-cross 
Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability</title>
<link>https://arxiv.org/abs/2507.00788</link>
<guid>https://arxiv.org/abs/2507.00788</guid>
<content:encoded><![CDATA[
arXiv:2507.00788v2 Announce Type: replace-cross 
Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assistants affects software maintainability, specifically how easily other developers can evolve the resulting source code. [Method] We conducted a two-phase controlled experiment involving 151 participants, 95% of whom were professional developers. In Phase 1, participants added a new feature to a Java web application, with or without AI assistance. In Phase 2, a randomized controlled trial, new participants evolved these solutions without AI assistance. [Results] Phase 2 revealed no significant differences in subsequent evolution with respect to completion time or code quality. Bayesian analysis suggests that any speed or quality improvements from AI use were at most small and highly uncertain. Observational results from Phase 1 corroborate prior research: using an AI assistant yielded a 30.7% median reduction in completion time, and habitual AI users showed an estimated 55.9% speedup. [Conclusions] Overall, we did not detect systematic maintainability advantages or disadvantages when other developers evolved code co-developed with AI assistants. Within the scope of our tasks and measures, we observed no consistent warning signs of degraded code-level maintainability. Future work should examine risks such as code bloat from excessive code generation and cognitive debt as developers offload more mental effort to assistants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</title>
<link>https://arxiv.org/abs/2507.03558</link>
<guid>https://arxiv.org/abs/2507.03558</guid>
<content:encoded><![CDATA[
arXiv:2507.03558v3 Announce Type: replace-cross 
Abstract: Brain stroke is a leading cause of mortality and long-term disability worldwide, underscoring the need for precise and rapid prediction techniques. Computed Tomography (CT) scan is considered one of the most effective methods for diagnosing brain strokes. Most stroke classification techniques use a single slice-level prediction mechanism, requiring radiologists to manually select the most critical CT slice from the original CT volume. Although clinical evaluations are often used in traditional diagnostic procedures, machine learning (ML) has opened up new avenues for improving stroke diagnosis. To supplement traditional diagnostic techniques, this study investigates machine learning models for early brain stroke prediction using CT scan images. This research proposes a novel machine learning approach to brain stroke detection, focusing on optimizing classification performance with pre-trained deep learning models and advanced optimization strategies. Pre-trained models, including DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are used for feature extraction. Feature engineering techniques, including BFO, PCA, and LDA, further enhance model performance. These features are then classified using machine learning algorithms, including SVC, RF, XGB, DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2, LDA, and SVC achieved the highest classification accuracy of 97.93%, significantly outperforming other model-optimizer-classifier combinations. The results underline the effectiveness of integrating lightweight pre-trained models with robust optimization and classification techniques for brain stroke diagnosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2507.16043</link>
<guid>https://arxiv.org/abs/2507.16043</guid>
<content:encoded><![CDATA[
arXiv:2507.16043v3 Announce Type: replace-cross 
Abstract: The surrogate gradient descent algorithm enabled spiking neural networks to be trained to carry out challenging sensory processing tasks, an important step in understanding how spikes contribute to neural computations. However, it is unclear the extent to which these algorithms fully explore the space of possible spiking solutions to problems. We investigated whether spiking networks trained with surrogate gradient descent can learn to make use of information that is only encoded in the timing and not the rate of spikes. We constructed synthetic datasets with a range of types of spike timing information (interspike intervals, spatio-temporal spike patterns or polychrony, and coincidence codes). We find that surrogate gradient descent training can extract all of these types of information. In more realistic speech-based datasets, both timing and rate information is present. We therefore constructed variants of these datasets in which all rate information is removed, and find that surrogate gradient descent can still perform well. We tested all networks both with and without trainable axonal delays. We find that delays can give a significant increase in performance, particularly for more challenging tasks. To determine what types of spike timing information are being used by the networks trained on the speech-based tasks, we test these networks on time-reversed spikes which perturb spatio-temporal spike patterns but leave interspike intervals and coincidence information unchanged. We find that when axonal delays are not used, networks perform well under time reversal, whereas networks trained with delays perform poorly. This suggests that spiking neural networks with delays are better able to exploit temporal structure. To facilitate further studies of temporal coding, we have released our modified speech-based datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda</title>
<link>https://arxiv.org/abs/2507.21928</link>
<guid>https://arxiv.org/abs/2507.21928</guid>
<content:encoded><![CDATA[
arXiv:2507.21928v3 Announce Type: replace-cross 
Abstract: Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being generated by Artificial Intelligence (AI). The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and Generative AI (GenAI) engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding redistributes epistemic labor between humans and machines, shifting expertise from technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks such as black-box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2507.22219</link>
<guid>https://arxiv.org/abs/2507.22219</guid>
<content:encoded><![CDATA[
arXiv:2507.22219v2 Announce Type: replace-cross 
Abstract: Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constitutional Law and AI Governance: Constraints on Model Licensing and Research Classification</title>
<link>https://arxiv.org/abs/2509.05361</link>
<guid>https://arxiv.org/abs/2509.05361</guid>
<content:encoded><![CDATA[
arXiv:2509.05361v2 Announce Type: replace-cross 
Abstract: Transformative AI systems may pose unprecedented catastrophic risks, but the U.S. Constitution places significant constraints on the government's ability to govern this technology. This paper examines how the First Amendment, administrative law, and the Fourteenth Amendment shape the legal vulnerability of two regulatory proposals: model licensing and AI research classification. While the First Amendment may provide some degree of protection for model algorithms or outputs, this protection does not foreclose regulation. Policymakers must also consider administrative legal requirements, due to both agency review and authority. Finally, while substantive due process and equal protection pose minimal obstacles, procedural due process requires the government to clearly define when developers vest a legal interest in their models. Given this analysis, effective AI governance requires careful implementation to avoid these legal challenges.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v3 Announce Type: replace-cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</title>
<link>https://arxiv.org/abs/2509.21735</link>
<guid>https://arxiv.org/abs/2509.21735</guid>
<content:encoded><![CDATA[
arXiv:2509.21735v2 Announce Type: replace-cross 
Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease (AD) progression is crucial for timely intervention. However, this task remains challenging due to the complex dysfunctions in the spatio-temporal characteristics of underlying brain networks, which are often overlooked by existing methods. To address these limitations, we develop an interpretable spatio-temporal graph neural network framework to predict future AD progression, leveraging dual Stochastic Differential Equations (SDEs) to model the irregularly-sampled longitudinal functional magnetic resonance imaging (fMRI) data. We validate our approach on two independent cohorts, including the Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework effectively learns sparse regional and connective importance probabilities, enabling the identification of key brain circuit abnormalities associated with disease progression. Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal lobule as salient regions, with significant disruptions in the ventral attention, dorsal attention, and default mode networks. These abnormalities correlate strongly with longitudinal AD-related clinical symptoms. Moreover, our interpretability strategy reveals both established and novel neural systems-level and sex-specific biomarkers, offering new insights into the neurobiological mechanisms underlying AD progression. Our findings highlight the potential of spatio-temporal graph-based learning for early, individualized prediction of AD progression, even in the context of irregularly-sampled longitudinal imaging data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.22737</link>
<guid>https://arxiv.org/abs/2509.22737</guid>
<content:encoded><![CDATA[
arXiv:2509.22737v2 Announce Type: replace-cross 
Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACOS: Task Agnostic COordinator of a multi-drone System</title>
<link>https://arxiv.org/abs/2510.01869</link>
<guid>https://arxiv.org/abs/2510.01869</guid>
<content:encoded><![CDATA[
arXiv:2510.01869v2 Announce Type: replace-cross 
Abstract: When a single pilot is responsible for managing a multi-drone system, the task may demand varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system on a real-world multi-drone system, and conduct an ablation study to assess the contribution of each module.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution</title>
<link>https://arxiv.org/abs/2510.08697</link>
<guid>https://arxiv.org/abs/2510.08697</guid>
<content:encoded><![CDATA[
arXiv:2510.08697v2 Announce Type: replace-cross 
Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</title>
<link>https://arxiv.org/abs/2510.10446</link>
<guid>https://arxiv.org/abs/2510.10446</guid>
<content:encoded><![CDATA[
arXiv:2510.10446v2 Announce Type: replace-cross 
Abstract: We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.12796</link>
<guid>https://arxiv.org/abs/2510.12796</guid>
<content:encoded><![CDATA[
arXiv:2510.12796v2 Announce Type: replace-cross 
Abstract: Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM one-shot style transfer for Authorship Attribution and Verification</title>
<link>https://arxiv.org/abs/2510.13302</link>
<guid>https://arxiv.org/abs/2510.13302</guid>
<content:encoded><![CDATA[
arXiv:2510.13302v3 Announce Type: replace-cross 
Abstract: Computational stylometry studies writing style through quantitative textual patterns, enabling applications such as authorship attribution, identity linking, and plagiarism detection. Existing supervised and contrastive approaches often rely on datasets with spurious correlations, conflating style with topic. Despite the relevance of language modeling to these tasks, the pre-training of modern large language models (LLMs) has been underutilized in general authorship analysis. We introduce an unsupervised framework that uses the log-probabilities of an LLM to measure style transferability between two texts. This framework takes advantage of the extensive CLM pre-training and in-context capabilities of modern LLMs. Our approach avoids explicit supervision with spuriously correlated data. Our method substantially outperforms unsupervised prompting-based baselines at similar model sizes and exceeds contrastively trained models when controlling for topical overlap. Our framework's performance improves with model size. In the case of authorship verification, we present an additional mechanism that increases test-time computation to improve accuracy; enabling flexible trade-offs between computational cost and task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v4 Announce Type: replace-cross 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voice-Interactive Surgical Agent for Multimodal Patient Data Control</title>
<link>https://arxiv.org/abs/2511.07392</link>
<guid>https://arxiv.org/abs/2511.07392</guid>
<content:encoded><![CDATA[
arXiv:2511.07392v3 Announce Type: replace-cross 
Abstract: In robotic surgery, surgeons fully engage their hands and visual attention in procedures, making it difficult to access and manipulate multimodal patient data without interrupting the workflow. To overcome this problem, we propose a Voice-Interactive Surgical Agent (VISA) built on a hierarchical multi-agent framework consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to interpret voice commands and execute tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models within surgical video. We construct a dataset of 240 user commands organized into hierarchical categories and introduce the Multi-level Orchestration Evaluation Metric (MOEM) that evaluates the performance and robustness at both the command and category levels. Experimental results demonstrate that VISA achieves high stage-level accuracy and workflow-level success rates, while also enhancing its robustness by correcting transcription errors, resolving linguistic ambiguity, and interpreting diverse free-form expressions. These findings highlight the strong potential of VISA to support robotic surgery and its scalability for integrating new functions and agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing mathematics research with generative AI</title>
<link>https://arxiv.org/abs/2511.07420</link>
<guid>https://arxiv.org/abs/2511.07420</guid>
<content:encoded><![CDATA[
arXiv:2511.07420v3 Announce Type: replace-cross 
Abstract: The main drawback of using generative AI models for advanced mathematics is that these models are not primarily logical reasoning engines. However, Large Language Models, and their refinements, can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of generative AI models to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how generative AI models can be used to advance mathematics research. We also discuss their integration with neuro-symbolic solvers, Computer Algebra Systems and formal proof assistants such as Lean.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models</title>
<link>https://arxiv.org/abs/2511.07503</link>
<guid>https://arxiv.org/abs/2511.07503</guid>
<content:encoded><![CDATA[
arXiv:2511.07503v3 Announce Type: replace-cross 
Abstract: The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnviSAgE: A Survey of Environment Scaling for Qualitative Agentic Experience Collection</title>
<link>https://arxiv.org/abs/2511.09586</link>
<guid>https://arxiv.org/abs/2511.09586</guid>
<content:encoded><![CDATA[
arXiv:2511.09586v2 Announce Type: replace-cross 
Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wrist Photoplethysmography Predicts Dietary Information</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v2 Announce Type: replace-cross 
Abstract: Whether wearable photoplethysmography (PPG) contains dietary information remains unknown. We trained a language model on 1.1M meals to predict meal descriptions from PPG, aligning PPG to text. PPG nontrivially predicts meal content; predictability decreases for PPGs farther from meals. This transfers to dietary tasks: PPG increases AUC by 11% for intake and satiety across held-out and independent cohorts, with gains robust to text degradation. Wearable PPG may enable passive dietary monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing ESS in Multiplayer Games</title>
<link>https://arxiv.org/abs/2511.20859</link>
<guid>https://arxiv.org/abs/2511.20859</guid>
<content:encoded><![CDATA[
arXiv:2511.20859v4 Announce Type: replace-cross 
Abstract: We present an algorithm for computing all evolutionarily stable strategies in nondegenerate normal-form games with three or more players.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First, do NOHARM: towards clinically safe large language models</title>
<link>https://arxiv.org/abs/2512.01241</link>
<guid>https://arxiv.org/abs/2512.01241</guid>
<content:encoded><![CDATA[
arXiv:2512.01241v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are routinely used by physicians and patients for medical advice, yet their clinical safety profiles remain poorly characterized. We present NOHARM (Numerous Options Harm Assessment for Risk in Medicine), a benchmark using 100 real primary care-to-specialist consultation cases to measure frequency and severity of harm from LLM-generated medical recommendations. NOHARM covers 10 specialties, with 12,747 expert annotations for 4,249 clinical management options. Across 31 LLMs, potential for severe harm from LLM recommendations occurs in up to 22.2% (95% CI 21.6-22.8%) of cases, with harm of omission accounting for 76.6% (95% CI 76.4-76.8%) of errors. Safety performance is only moderately correlated (r = 0.61-0.64) with existing AI and medical knowledge benchmarks. The best models outperform generalist physicians on safety (mean difference 9.7%, 95% CI 7.0-12.5%), and a diverse multi-agent approach improves safety compared to solo models (mean difference 8.0%, 95% CI 4.0-12.1%). Therefore, despite strong performance on existing evaluations, widely used AI models can produce severely harmful medical advice at nontrivial rates, underscoring clinical safety as a distinct performance dimension necessitating explicit measurement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^\mathrm{3}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v3 Announce Type: replace-cross 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^\mathrm{3}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^\mathrm{3}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^\mathrm{3}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</title>
<link>https://arxiv.org/abs/2512.08333</link>
<guid>https://arxiv.org/abs/2512.08333</guid>
<content:encoded><![CDATA[
arXiv:2512.08333v2 Announce Type: replace-cross 
Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging performance scales with the amount of pretraining data, and enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v2 Announce Type: replace-cross 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity</title>
<link>https://arxiv.org/abs/2512.09003</link>
<guid>https://arxiv.org/abs/2512.09003</guid>
<content:encoded><![CDATA[
arXiv:2512.09003v2 Announce Type: replace-cross 
Abstract: Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindShift: Analyzing Language Models' Reactions to Psychological Prompts</title>
<link>https://arxiv.org/abs/2512.09149</link>
<guid>https://arxiv.org/abs/2512.09149</guid>
<content:encoded><![CDATA[
arXiv:2512.09149v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
<div> Keywords: coding agents, large-scale codebases, Confucius SDK, meta-agent, software engineering tasks  

<br /><br />Summary:  
The article introduces the Confucius Code Agent (CCA), a scalable coding agent designed to handle large-scale software engineering tasks involving massive code repositories and long-horizon reasoning. Built on the Confucius SDK, the platform emphasizes three perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX), ensuring a balanced approach for developers, users, and the agent itself. The SDK features a unified orchestrator with hierarchical working memory for long-context reasoning, enabling the agent to maintain coherence across extensive sessions. It includes a persistent note-taking system that supports continual learning across multiple sessions, enhancing the agent’s capability over time. A modular extension system facilitates reliable and safe use of various tools in complex workflows. Additionally, the authors introduce a meta-agent framework that automates the processes of agent configuration synthesis, evaluation, and refinement via a build-test-improve loop, allowing rapid adaptation to new environments, tasks, and toolchains. Empirical results on the SWE-Bench-Pro benchmark demonstrate CCA’s effectiveness, achieving a Resolve@1 accuracy of 54.3%, outperforming existing research baselines and matching or surpassing commercial solutions under comparable conditions. Together, the Confucius SDK and CCA provide a generalizable, extensible, and production-grade foundation, bridging the gap between research prototypes and scalable, real-world deployment of coding agents. <div>
arXiv:2512.10398v4 Announce Type: replace-cross 
Abstract: Real-world software engineering tasks require coding agents that can operate over massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade agents offer transparency but struggle when scaled to real-world workloads, while proprietary systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a scalable software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK integrates a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid adaptation to new tasks, environments, and tool stacks. Instantiated with these mechanisms, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA reaches a Resolve@1 of 54.3%, exceeding prior research baselines and comparing favorably to commercial results, under identical repositories, model backend, and tool access. Together, the Confucius SDK and CCA form a general, extensible, and production-grade foundation for building effective and robust coding agents, bridging the gap between research prototypes and practical large-scale deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, decision boundaries, influence functions, Eminence framework, low poison rates<br /><br />Summary:<br /><br />1. Deep neural networks (DNNs) are essential for many applications but are vulnerable to backdoor attacks that are often based on heuristic brute-force approaches, lacking rigorous theoretical understanding. <br /><br />2. This work provides a theoretical analysis revealing how sparse decision boundaries in DNNs enable disproportionate manipulation with minimal poisoned data. It introduces a closed-form ambiguous boundary region where very few relabeled samples can cause substantial misclassification. <br /><br />3. Influence function analysis is used to quantify significant parameter shifts generated by these margin samples, explaining why extremely low poison rates can still achieve effective attacks without degrading clean accuracy. <br /><br />4. Building on these insights, the paper proposes Eminence, a black-box backdoor attack framework that is explainable, robust, and comes with provable theoretical guarantees. Eminence crafts a universal, visually subtle trigger to exploit vulnerable decision boundaries strategically. <br /><br />5. Experiments validate the theory and demonstrate Eminence’s effectiveness, showing it achieves over 90% attack success with less than 0.1% poisoning, maintaining clean accuracy, and transferring well across different models, datasets, and scenarios. An exponential relationship between margin poisoning and adversarial boundary manipulation is confirmed. <div>
arXiv:2512.10402v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</title>
<link>https://arxiv.org/abs/2512.10422</link>
<guid>https://arxiv.org/abs/2512.10422</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, question answering, cooperative retrieval, multi-hop reasoning<br /><br />Summary:  
This paper addresses the problem of factual inaccuracies in outputs generated by large language models (LLMs) when used alone for question answering (QA). To overcome these issues, the authors propose CoopRAG, a novel retrieval-augmented generation (RAG) framework where a retriever and an LLM work cooperatively, exchanging informative knowledge to enhance retrieval accuracy and answer generation. CoopRAG introduces a mechanism to unroll a complex question into sub-questions and a reasoning chain, identifying uncertain positions with masked tokens. It then retrieves relevant documents based on the original question augmented by these sub-questions and the reasoning chain. To improve document ranking, CoopRAG reranks retrieved documents through a contrastive approach between different layers of the retriever model. Lastly, the framework reconstructs the reasoning chain by filling in the masked positions using the LLM. Experimental results show that CoopRAG consistently outperforms current state-of-the-art QA methods across both multi-hop and simple QA datasets, demonstrating improvements in both retrieval quality and overall question answering performance. The authors have also made their code publicly available, facilitating further research and application development in cooperative retrieval and reasoning for QA tasks. <div>
arXiv:2512.10422v3 Announce Type: replace-cross 
Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition</title>
<link>https://arxiv.org/abs/2512.10688</link>
<guid>https://arxiv.org/abs/2512.10688</guid>
<content:encoded><![CDATA[
<div> Popularity bias, collaborative filtering, Bayesian Pairwise Ranking, embedding geometry, directional decomposition<br /><br />Summary:<br /><br />This paper investigates the root cause of popularity bias in collaborative filtering (CF) recommendation systems, identifying it as an intrinsic geometric artifact rather than an external confounding factor. The authors provide a rigorous mathematical analysis proving that Bayesian Pairwise Ranking (BPR) models organize item embeddings along a dominant "popularity direction," where embedding magnitudes correlate strongly with item interaction frequencies. This creates a conflict within user embeddings, as they must simultaneously express genuine user preferences and adjust for global popularity effects, resulting in suboptimal representations that favor popular items regardless of actual individual tastes. To address this, the paper introduces Directional Decomposition and Correction (DDC), a novel framework that applies asymmetric directional updates to embeddings. DDC surgically disentangles user preferences from popularity influences by guiding positive interactions toward personalized preference directions and steering negative interactions away from the global popularity direction. Extensive experiments conducted on multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baseline models. Moreover, it achieves superior recommendation quality and fairness. The implementation of DDC is openly available, facilitating adoption and further research. <div>
arXiv:2512.10688v3 Announce Type: replace-cross 
Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning</title>
<link>https://arxiv.org/abs/2512.14709</link>
<guid>https://arxiv.org/abs/2512.14709</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Vector Symbolic Architecture, self-attention, symbolic reasoning, logical compositionality<br /><br />Summary:<br />1. The paper interprets transformer components—self-attention and residual streams—as implementing an approximate Vector Symbolic Architecture (VSA), where queries and keys represent role spaces, values act as fillers, attention weights perform soft unbinding, and residual connections enable superposition of multiple bound structures.<br />2. This algebraic framework provides a unified perspective explaining both the reasoning-like behavior and brittle failure modes of transformers on symbolic manipulation tasks, such as variable confusion and inconsistencies across logically related prompts.<br />3. The authors connect VSA concepts to practical transformer phenomena including chain-of-thought reasoning, program-based inference, and memory-augmented tool usage.<br />4. Building on this insight, the paper proposes VSA-inspired architectural modifications like explicit binding/unbinding attention heads, hyperdimensional memory layers, and specialized training objectives to encourage role-filler separation and robust superposition.<br />5. The study also introduces new metrics for quantifying “VSA-likeness” and logical compositionality, while outlining open theoretical and architectural challenges, arguing that framing attention as soft vector-symbolic computation could lead to more interpretable and logically reliable language models. <div>
arXiv:2512.14709v1 Announce Type: new 
Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge</title>
<link>https://arxiv.org/abs/2512.14766</link>
<guid>https://arxiv.org/abs/2512.14766</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Graph Question Answering, Incomplete Knowledge Graphs, Adaptive Graph Reasoning Agent, Reasoning Paths

<br /><br />Summary:  
This paper addresses the limitation of current knowledge graph question answering (KGQA) benchmarks that assume complete knowledge graphs (KGs), which neglect the common real-world scenario where KGs are incomplete. To tackle this, the authors propose a novel methodology to create benchmarks reflecting KG incompleteness by removing direct supporting triples while preserving alternative reasoning paths for answer inference. Experimental results on these constructed benchmarks reveal that existing KGQA methods suffer significant performance drops under incomplete conditions, exposing their limited reasoning capabilities. To overcome this, the authors introduce the Adaptive Graph Reasoning Agent (GR-Agent), which formalizes KGQA as an interactive agent-environment task within the KG context. GR-Agent uses an action space composed of graph reasoning tools and keeps a memory of potential supporting evidence such as relevant relations and reasoning paths. Extensive evaluations demonstrate that GR-Agent consistently outperforms non-training baselines and achieves comparable results to training-based methods, effectively handling both complete and incomplete KG scenarios. This work advances KGQA research by emphasizing deeper reasoning over KGs and providing tools and benchmarks better aligned with real-world KG data characteristics. <div>
arXiv:2512.14766v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection</title>
<link>https://arxiv.org/abs/2512.14792</link>
<guid>https://arxiv.org/abs/2512.14792</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Infrastructure as Code, Terraform, Knowledge Injection, Error Taxonomy  

<br /><br />Summary:  
This paper addresses the challenge of low success rates in generating accurate and intent-aligned Infrastructure as Code (IaC) scripts with Large Language Models (LLMs), focusing on Terraform. To improve LLM-based IaC generation, the authors systematically introduced structured configuration knowledge into the generation process. They substantially enhanced the existing IaC-Eval benchmark by adding cloud emulation and automated error analysis capabilities, enabling more rigorous evaluation. A novel error taxonomy specific to LLM-assisted IaC generation was developed to better categorize and understand failure modes. Various knowledge injection techniques were explored, starting from a basic Retrieval-Augmented Generation (RAG) method to advanced Graph RAG approaches that incorporated semantic enrichment of graph elements and modeled dependencies between infrastructure resources. Experimental results revealed that the baseline LLM’s performance was low, with only 27.1% overall success. However, injecting structured configuration knowledge boosted the technical validation success rate to 75.3% and overall success to 62.6%. Despite these improvements in technical correctness, the study identified a "Correctness-Congruence Gap," where LLMs excel as technical coders but struggle to fully capture and deliver on the nuanced user intent, highlighting limitations in intent alignment within LLM-generated IaC code. <div>
arXiv:2512.14792v1 Announce Type: new 
Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally</title>
<link>https://arxiv.org/abs/2512.14910</link>
<guid>https://arxiv.org/abs/2512.14910</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, climate adaptation, multi-agent system, agriculture, decision support<br /><br />Summary:<br /><br />1. The paper addresses the challenges faced by agricultural regions in rural areas due to climate-related risks such as droughts, heavy rainfall, and shifting weather patterns.<br /><br />2. It highlights the need for adaptive risk-management solutions and strategic decision-making to help vulnerable farming communities adapt to climate change.<br /><br />3. The authors introduce AgroAskAI, a multi-agent reasoning system designed to support climate adaptation decisions in agriculture, featuring a modular architecture with role-specialized autonomous agents.<br /><br />4. AgroAskAI uses a chain-of-responsibility framework to coordinate agents dynamically, integrating real-time tools and datasets to provide context-aware and grounded outputs.<br /><br />5. The system includes governance mechanisms that reduce hallucination and enable internal feedback loops to ensure coherence and relevance to local agricultural contexts.<br /><br />6. It supports multilingual interactions to accommodate non-English-speaking farmers, enhancing accessibility.<br /><br />7. Experimental evaluation on typical agricultural queries demonstrates that, with tool integration and prompt refinement, AgroAskAI produces more actionable, inclusive, and reliable recommendations.<br /><br />8. The work underscores the promise of agentic AI as a sustainable and accountable approach for decision support in climate adaptation strategies for agriculture. <div>
arXiv:2512.14910v1 Announce Type: new 
Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation</title>
<link>https://arxiv.org/abs/2512.15033</link>
<guid>https://arxiv.org/abs/2512.15033</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, geometric stability, chess evaluation, model consistency, accuracy-stability paradox  

<br /><br />Summary:  
This paper critiques traditional accuracy-based benchmarks for evaluating Large Language Models (LLMs) in chess, highlighting their failure to differentiate true geometric reasoning from superficial memorization. It introduces the Geometric Stability Framework, a new methodology that tests LLM consistency under invariant board transformations such as rotation, mirror symmetry, color inversion, and format conversion. Applying this framework to six leading LLMs, including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, on a dataset of roughly 3,000 chess positions, the study discovers a significant Accuracy-Stability Paradox. GPT-5.1, despite near-perfect accuracy on standard tests, suffers drastic error increases (over 600%) under geometric perturbations like rotations, indicating pattern matching rather than spatial understanding. In contrast, Claude Sonnet 4.5 and Kimi K2 Turbo show robust dual performance, preserving consistency across all transformation axes. The paper also examines the trade-off between helpfulness and safety, with Gemini 2.5 Flash excelling at rejecting illegal states (96.0%). Ultimately, the authors argue that geometric stability is a crucial, complementary metric for AI evaluation that helps separate genuine reasoning ability from overfitting or data contamination in LLMs. <div>
arXiv:2512.15033v1 Announce Type: new 
Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</title>
<link>https://arxiv.org/abs/2512.15038</link>
<guid>https://arxiv.org/abs/2512.15038</guid>
<content:encoded><![CDATA[
<div> linear attention, autonomous driving, cross-modal fusion, edge deployment, spatiotemporal modeling<br /><br />Summary:<br /><br />This work addresses the challenge of deploying Transformer-based autonomous driving models on resource-constrained edge platforms, where the quadratic attention cost limits long-range spatial and temporal sequence modeling. The authors propose LADY, the first end-to-end autonomous driving model employing fully linear attention, which provides constant computational and memory costs independent of the history length of camera and LiDAR features. LADY introduces a lightweight linear cross-attention mechanism enabling effective cross-modal and cross-temporal information exchange, overcoming limitations of previous linear attention models that supported only self-attention. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art planning performance while significantly reducing computational cost. The model's design ensures efficient fusion of long-range temporal context during inference, making it well suited to the requirements of autonomous driving. Importantly, LADY has been successfully deployed and validated on edge devices, confirming its practicality for real-time use in resource-limited scenarios. This work thus advances autonomous vehicle perception and planning by combining efficient attention mechanisms with cross-modal fusion, enabling scalable and efficient spatiotemporal modeling for real-world applications. <div>
arXiv:2512.15038v1 Announce Type: new 
Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study</title>
<link>https://arxiv.org/abs/2512.15044</link>
<guid>https://arxiv.org/abs/2512.15044</guid>
<content:encoded><![CDATA[
<div> Integrated sensing and communication, agentic AI, generative artificial intelligence, 6G, ISAC optimization<br /><br />Summary:<br /><br />This article addresses the emerging field of integrated sensing and communication (ISAC) as a critical component of sixth-generation (6G) wireless networks, emphasizing its role in enabling collaborative sensing and communication in future intelligent systems. Given the increasing complexity and dynamic nature of wireless environments, the authors highlight the necessity for ISAC systems to adopt more intelligent and autonomous processing methods to maintain efficiency and adaptivity. To meet these challenges, the paper explores the incorporation of agentic artificial intelligence (AI), which facilitates continuous perception, reasoning, and action cycles, thereby supporting intelligent and autonomous ISAC operations. The work begins with a comprehensive review of both agentic AI and ISAC systems, outlining their fundamental characteristics. It then discusses several common optimization strategies for ISAC, underscoring the benefits of generative AI-based agentic AI approaches in enhancing system performance. A novel agentic ISAC framework is proposed, accompanied by a case study demonstrating its superiority in optimizing ISAC capabilities. Finally, the article identifies future research directions to further advance agentic AI-driven ISAC systems, providing a roadmap for ongoing development in this interdisciplinary area. <div>
arXiv:2512.15044v1 Announce Type: new 
Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2512.15089</link>
<guid>https://arxiv.org/abs/2512.15089</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Cognitive-Inspired Elastic Reasoning, Reinforcement Learning, Query Complexity, Tool-Assisted Reasoning<br /><br />Summary:<br /><br />This paper introduces Cognitive-Inspired Elastic Reasoning (CogER), a novel framework for enhancing the reasoning capabilities of large language models (LLMs). CogER addresses the trade-off between reasoning efficiency and accuracy by dynamically selecting the most appropriate reasoning strategy for each query based on its complexity. It accomplishes this by first assessing query difficulty and categorizing queries into predefined levels, each associated with a specific processing method. To automate this selection, the authors model the problem as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is optimized with a reward function that balances solution quality against computational cost, promoting efficient resource allocation. Additionally, the framework introduces Cognitive Tool-Assisted Reasoning, allowing the LLM to autonomously utilize external tools within its chain-of-thought process for queries requiring supplementary resources. The effectiveness of CogER is demonstrated through extensive experiments where it surpasses state-of-the-art Test-Time scaling approaches, yielding at least a 13% relative improvement in average exact match for In-Domain tasks and an 8% relative gain for Out-of-Domain tasks. Overall, this work presents a human-inspired, adaptive reasoning system that enhances LLM performance by balancing accuracy and computational efficiency across queries of varying difficulty. <div>
arXiv:2512.15089v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem</title>
<link>https://arxiv.org/abs/2512.15198</link>
<guid>https://arxiv.org/abs/2512.15198</guid>
<content:encoded><![CDATA[
<div> Discrete Optimization, Relaxed Decision Diagrams, Variable Ordering, Clustering Framework, Maximum Weighted Independent Set Problem  

<br /><br />Summary:  
1. Exact algorithms for Discrete Optimization (DO) rely on strong primal and dual bounds to improve efficiency.  
2. Relaxed Decision Diagrams (DDs) derive dual bounds by over-approximating the solution space, with quality dependent on variable ordering and merging during compilation.  
3. Although dynamic variable ordering heuristics improve bounds, their global application is computationally expensive.  
4. This work introduces a clustering-based framework to partition variables into clusters, reducing the search space for variable ordering heuristics.  
5. Two strategies within this framework are proposed: Cluster-to-Cluster, processing clusters sequentially using problem-specific aggregate criteria, and Pick-and-Sort, selecting and sorting representative variables iteratively to balance diversity and heuristic guidance.  
6. The authors provide theoretical results on DD size growth for the Maximum Weighted Independent Set Problem (MWISP) and propose two policies for determining the number of clusters.  
7. These strategies are embedded into a DD-based branch-and-bound algorithm and evaluated on MWISP benchmark instances.  
8. Experimental results show consistent reductions in computational costs compared to standard dynamic variable ordering methods. <div>
arXiv:2512.15198v1 Announce Type: new 
Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications</title>
<link>https://arxiv.org/abs/2512.15231</link>
<guid>https://arxiv.org/abs/2512.15231</guid>
<content:encoded><![CDATA[
<div> Keywords: CangLing-KnowFlow, Procedural Knowledge Base, Dynamic Workflow Adjustment, Evolutionary Memory Module, remote sensing automation<br /><br />Summary:<br /><br />This paper presents CangLing-KnowFlow, a unified intelligent agent framework designed for automated and intelligent processing of massive remote sensing (RS) datasets in Earth observation (EO). Unlike existing task-specific systems, CangLing-KnowFlow integrates a Procedural Knowledge Base (PKB) that contains 1,008 expert-validated workflow cases across 162 practical RS tasks, which helps guide workflow planning and reduces hallucination issues typical in general-purpose agents. The framework incorporates a Dynamic Workflow Adjustment mechanism that autonomously detects runtime failures and replans recovery strategies to maintain smooth operation. Additionally, an Evolutionary Memory Module enables continuous learning from failures, improving the agent's knowledge and performance iteratively. The combined effect of these components equips CangLing-KnowFlow to adapt, learn, and execute reliably across diverse and complex RS tasks. Evaluation was conducted on KnowFlow-Bench, a newly developed benchmark of 324 workflows simulating real-world applications, tested over 13 leading Large Language Model (LLM) backbones including both open-source and commercial options. CangLing-KnowFlow consistently outperformed the Reflexion baseline by at least 4% in Task Success Rate across all complex tasks. This research provides the first comprehensive validation framework in this emerging field, illustrating the effectiveness of integrating expert procedural knowledge into adaptive, verifiable workflows for scalable EO automation solutions. <div>
arXiv:2512.15231v1 Announce Type: new 
Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis</title>
<link>https://arxiv.org/abs/2512.15295</link>
<guid>https://arxiv.org/abs/2512.15295</guid>
<content:encoded><![CDATA[
<div> Controller synthesis, Labeled Transition System, Reinforcement Learning, Graph Neural Networks, Exploration policies<br /><br />Summary:<br /><br />1. Controller synthesis is a formal method used to automatically generate controllers for Labeled Transition Systems (LTS) that meet desired specifications.<br />2. The success and efficiency of this synthesis largely hinge on the exploration policies employed, which guide the process of exploring the LTS.<br />3. Traditional exploration policies either rely on fixed rules or reinforcement learning strategies limited to considering only current, local features.<br />4. This paper introduces GCRL, a novel approach that enhances RL-based exploration by using Graph Neural Networks to encode the history of the LTS exploration into a graph structure, thereby capturing broader and more contextual information beyond immediate states.<br />5. Experimental comparison with state-of-the-art methods demonstrates that GCRL achieves better learning efficiency and generalization capability in four out of five benchmark domains.<br />6. The only exception was a domain characterized by high symmetry and strictly local interactions, where GCRL did not outperform existing methods.<br />7. Overall, GCRL presents a meaningful advance in exploration policy design for controller synthesis by leveraging richer contextual encoding through GNNs. <div>
arXiv:2512.15295v1 Announce Type: new 
Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I</title>
<link>https://arxiv.org/abs/2512.15298</link>
<guid>https://arxiv.org/abs/2512.15298</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Large Language Models, multimodal reasoning, academic integrity, AI-resistant questions  

<br /><br />Summary:  
This study investigates the multimodal scientific reasoning abilities and cognitive limitations of advanced Large Language Models (LLMs) such as GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro using the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT). Three input conditions—full-page input, individual item input, and optimized multimodal input—were tested to evaluate how different data formats impact model performance. Results showed that unstructured inputs caused notable performance declines due to issues like segmentation errors and OCR failures. Even with optimized inputs, the models still demonstrated fundamental reasoning flaws that limit their scientific understanding. A qualitative analysis identified dominant "Perception Errors," evidencing a "Perception-Cognition Gap," where models could recognize visual elements but failed to interpret their symbolic meanings in diagrams. Additionally, a "Calculation-Conceptualization Discrepancy" was observed, which indicated models could complete calculations accurately but failed to apply underlying scientific concepts correctly. The study also found instances of "Process Hallucination," where models skipped visual verification and relied on plausible but incorrect background knowledge. Based on these findings, the authors propose designing "AI-resistant questions" that exploit these weaknesses to maintain academic integrity by distinguishing human comprehension from AI-generated answers. <div>
arXiv:2512.15298v1 Announce Type: new 
Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Prompt Evolution for Enhancing Agent Effectiveness</title>
<link>https://arxiv.org/abs/2512.15374</link>
<guid>https://arxiv.org/abs/2512.15374</guid>
<content:encoded><![CDATA[
<div> LLM agents, context management, prompt evolution, online optimization, Dual-Stream mechanism<br /><br />Summary:<br /><br />Large Language Model (LLM) agents face challenges when deployed in environments with massive, dynamic contexts due to the static nature of their prompts, which lack effective mechanisms for managing such context. This limitation results in frequent Corrective and Enhancement failures. To overcome this, the study introduces SCOPE (Self-evolving Context Optimization via Prompt Evolution), a novel approach framing context management as an online optimization problem. SCOPE leverages execution traces to automatically evolve the agent’s prompt, improving its ability to handle dynamic environments. Central to SCOPE is the Dual-Stream mechanism, which balances tactical specificity—addressing immediate errors—with strategic generality aimed at evolving long-term guiding principles. Additionally, SCOPE employs Perspective-Driven Exploration to widen strategic coverage, thereby increasing the chances that the agent applies the correct strategy for diverse tasks. Experimental results on the HLE benchmark demonstrate a significant performance improvement, with task success rates rising from 14.23% to 38.64%, achieved entirely without human intervention. The authors have also made their code publicly accessible, encouraging further research and implementation in this area. <div>
arXiv:2512.15374v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations</title>
<link>https://arxiv.org/abs/2512.15388</link>
<guid>https://arxiv.org/abs/2512.15388</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, route instructions, pedestrian wayfinding, qualitative spatial relations, navigation  

<br /><br />Summary:  
1. The paper addresses enhancing Large Language Models (LLMs) to improve their performance in providing route instructions specifically tailored for pedestrian navigation.  
2. It emphasizes the use of qualitative spatial relations—descriptions that capture relative positions and directions in an intuitive, human-friendly manner rather than relying on exact metrics or coordinates.  
3. The approach aims to make route instructions generated by LLMs more comprehensible and naturally aligned with how humans understand and communicate about space during wayfinding tasks.  
4. By integrating spatial relation concepts, the model can generate guidance that better reflects real-world pedestrian navigation challenges such as turns, landmarks, and relative distances.  
5. Overall, the work contributes to advancing LLM capabilities in qualitative spatial reasoning to provide clearer, more practical route instructions for pedestrian users. <div>
arXiv:2512.15388v1 Announce Type: new 
Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat</title>
<link>https://arxiv.org/abs/2512.15435</link>
<guid>https://arxiv.org/abs/2512.15435</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-player card games, Skat, self-playing AI, bootstrapping outer-learning, feature hash functions<br /><br />Summary:<br /><br />1. The paper addresses decision-making in early stages of multi-player card games such as Skat and Bridge, emphasizing the importance of bidding, game selection, and initial card selection over middle- and end-game play.  
2. Current approaches rely heavily on statistical data derived from large databases of human expert games but are limited by computational resources.  
3. The authors propose a general bootstrapping outer-learning framework that enhances prediction accuracy by augmenting human game data with millions of self-play games generated by AI.  
4. They implement perfect feature hash functions to manage compacted statistical tables efficiently, which supports continuous self-improvement of the card game engine through self-learning.  
5. A case study in Skat demonstrates the practical application of their method, showing that this automated approach can effectively assist in various decision-making aspects of the game, ultimately leading to improved performance. <div>
arXiv:2512.15435v1 Announce Type: new 
Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intent-Driven UAM Rescheduling</title>
<link>https://arxiv.org/abs/2512.15462</link>
<guid>https://arxiv.org/abs/2512.15462</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban Air Mobility, vertiport scheduling, Mixed Integer Linear Programming, three-valued logic, Answer Set Programming<br /><br />Summary:<br /><br />1. The paper addresses efficient scheduling challenges in vertiports, which are critical hubs for Urban Air Mobility (UAM), especially under restricted resource conditions.<br /><br />2. The authors employ a Mixed Integer Linear Programming (MILP) model, commonly used in resource-constrained project scheduling problems (RCPSP), to formulate the scheduling problem.<br /><br />3. The approach uniquely integrates dynamic operation requirements and vague rescheduling requests expressed by human users, which are often ambiguous and difficult to handle with traditional methods.<br /><br />4. To interpret ambiguous user inputs, the authors utilize three-valued logic, which allows the system to reason about uncertain or vague human intents more effectively.<br /><br />5. The proposed framework combines Answer Set Programming (ASP) and MILP to optimize scheduling while supporting transparent and explainable human inputs, resulting in an adaptive and robust scheduling system tailored for UAM vertiports. <div>
arXiv:2512.15462v1 Announce Type: new 
Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision</title>
<link>https://arxiv.org/abs/2512.15489</link>
<guid>https://arxiv.org/abs/2512.15489</guid>
<content:encoded><![CDATA[
<div> Keywords: Nemotron-Math, mathematical reasoning, Python tool-integrated reasoning, long-context training, AIME 2024 accuracy<br /><br />Summary:<br /><br />1. Nemotron-Math is a large-scale mathematical reasoning dataset introduced to provide diverse reasoning styles, long-form solution traces, and effective integration of computational tools, addressing limitations found in existing datasets. <br />2. The dataset comprises 7.5 million solution traces categorized into high, medium, and low reasoning modes, each available with and without Python tool-integrated reasoning (TIR).<br />3. It integrates a curated collection of 85,000 AoPS problems alongside 262,000 community-sourced math problems from StackExchange-Math, blending structured competition problems and real-world queries to improve robustness and generalization.<br />4. Controlled evaluations show Nemotron-Math outperforms the original OpenMathReasoning dataset on AoPS problems, with notable improvements in robustness and generalization on the diverse StackExchange-Math set, maintaining competitive accuracy on math competition benchmarks.<br />5. The authors developed a sequential bucketed training strategy to efficiently fine-tune models with extremely long context lengths (up to 128K tokens), achieving 2–3 times faster training without significant accuracy loss.<br /><br />Overall, Nemotron-Math enables state-of-the-art mathematical reasoning performance, demonstrated by achieving 100% majority-at-16 accuracy on the AIME 2024 and 2025 competitions using Python tool-integrated reasoning. <div>
arXiv:2512.15489v1 Announce Type: new 
Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2512.15567</link>
<guid>https://arxiv.org/abs/2512.15567</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, scientific discovery, benchmarks, hypothesis generation, evaluation framework<br /><br />Summary:<br /><br />This article introduces a novel scenario-grounded benchmark designed to evaluate large language models (LLMs) in the context of real scientific research across biology, chemistry, materials science, and physics. Unlike existing benchmarks that test static, decontextualized knowledge, this framework focuses on the iterative processes central to scientific discovery, including hypothesis generation, designing experiments or simulations, and interpreting results. The evaluation framework operates on two levels: question-level accuracy based on scenario-specific items, and project-level performance requiring models to engage in comprehensive scientific tasks. Application of this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals persistent performance gaps compared to general science benchmarks, limited improvements from scaling model size and reasoning capacity, and shared weaknesses among leading models from different developers. The study also notes substantial variability in model performance across various research scenarios, which affects which model excels in scientific projects, indicating that no current LLM approaches a general scientific "superintelligence." Nevertheless, LLMs show potential in a wide array of scientific discovery tasks, even when scenario-specific scores are low, underscoring the importance of guided exploration and serendipity in research. Overall, this SDE framework provides a reproducible and practical benchmark to drive the advancement of LLM capabilities toward authentic scientific discovery. <div>
arXiv:2512.15567v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decision-Theoretic Approach for Managing Misalignment</title>
<link>https://arxiv.org/abs/2512.15584</link>
<guid>https://arxiv.org/abs/2512.15584</guid>
<content:encoded><![CDATA[
<div> Value alignment, delegation, epistemic accuracy, decision-theoretic framework, AI trust<br /><br />Summary:<br /><br />This paper addresses the critical question of when it is appropriate to delegate decisions to AI systems, especially under conditions of uncertainty and imperfect value alignment. It highlights a gap in existing research that mostly focuses on shaping AI values but often neglects how to judge whether the current level of alignment justifies delegation. The authors propose a formal decision-theoretic framework that balances three key factors: an agent's value (mis)alignment, its epistemic accuracy (knowledge reliability), and its reach (the scope of decisions it can handle). Their analysis distinguishes between two main forms of delegation. Universal delegation, which involves trusting an AI with any decision problem, requires near-perfect alignment and complete epistemic trust—criteria that are rarely met. Alternatively, context-specific delegation can be optimal even when significant misalignment exists, as the agent’s superior accuracy or broader decision reach can lead to better overall outcomes. The paper introduces a novel ex ante scoring framework to quantify this trade-off, enabling principled decision-making about delegation. Ultimately, the work shifts the focus away from pursuing perfect alignment towards managing the risks and rewards of delegation under uncertainty, providing a practical foundation for determining when an AI system is sufficiently aligned for particular tasks. <div>
arXiv:2512.15584v1 Announce Type: new 
Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15662</link>
<guid>https://arxiv.org/abs/2512.15662</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning, self-critique, reinforcement learning, critical thinking<br /><br />Summary:<br /><br />1. The paper addresses the limitation in current large language models (LLMs), which typically separate reasoning from verification, either lacking immediate error feedback or requiring external verifiers that complicate learning.<br />2. Inspired by human critical thinking, the authors propose Stepwise Think-Critique (STC), a novel unified framework that integrates reasoning and self-critique at each step of the process within a single LLM.<br />3. STC is trained using a hybrid reinforcement learning objective that combines rewards for reasoning correctness and consistency in self-evaluation, thereby optimizing both the quality of reasoning and the ability to self-check.<br />4. Experiments conducted on mathematical reasoning benchmarks demonstrate that STC exhibits enhanced critic-thinking abilities and produces reasoning traces that are more interpretable compared to previous methods.<br />5. The work represents progress toward developing LLMs with built-in critical thinking skills, improving both accuracy and the transparency of their reasoning processes. <div>
arXiv:2512.15662v1 Announce Type: new 
Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining the Reasoning of Large Language Models Using Attribution Graphs</title>
<link>https://arxiv.org/abs/2512.15663</link>
<guid>https://arxiv.org/abs/2512.15663</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Context Attribution, Attribution Graph, Causality, Explanation Faithfulness  

<br /><br />Summary:  
Large language models (LLMs) demonstrate impressive capabilities but their internal reasoning processes are often opaque, raising concerns about safety and trustworthiness. Attribution methods, widely used in computer vision, help explain decisions by assigning credit to input features. Extending these ideas, context attributions aim to explain autoregressive LLM behaviors by linking generated tokens to prompts. However, current context attribution methods fall short by only relating outputs directly to the initial prompt, ignoring the influence of intermediate generations. To address this limitation, the paper introduces the Context Attribution via Graph Explanations (CAGE) framework. CAGE constructs an attribution graph, a directed structure representing how each generated token is influenced not only by the prompt but also by all prior tokens. The graph ensures two key properties: causality (preserving the temporal order of influence) and row stochasticity (making the influence values properly normalized). By marginalizing contributions along paths in this graph, CAGE provides a more complete and faithful attribution of model outputs. Experiments across multiple models, datasets, and evaluation metrics show that CAGE consistently improves the faithfulness of context attributions, achieving average performance gains of up to 40%, making it a significant advancement in explainability for LLMs. <div>
arXiv:2512.15663v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artism: AI-Driven Dual-Engine System for Art Generation and Critique</title>
<link>https://arxiv.org/abs/2512.15710</link>
<guid>https://arxiv.org/abs/2512.15710</guid>
<content:encoded><![CDATA[
<div> Keywords: dual-engine AI, AIDA, Ismism Machine, art evolution, critical analysis<br /><br />Summary:<br /><br />This paper introduces a dual-engine AI architectural method aimed at tackling the complexities involved in exploring potential trajectories in the evolution of art. The method consists of two interconnected components: AIDA, an artificial artist social network designed to simulate art interactions, and the Ismism Machine, a system built for critical analysis of art concepts and developments. The innovation of the framework lies in combining deep learning techniques with multi-agent collaboration, enabling multidimensional simulations that model art historical progressions and conceptual innovation patterns. This approach moves beyond traditional, unidirectional art critique by promoting an intelligent and interactive reflexive practice, which engages AI systems in continual feedback loops. The paper details ongoing experimental applications of this methodology in contemporary art concept studies, showcasing its capacity for dynamic and computationally driven analysis. Overall, the research presents a novel AI-driven conceptual framework that facilitates critical loops in art analysis, expanding possibilities for computationally understanding and simulating art evolution in ways that integrate social, conceptual, and historical dimensions. <div>
arXiv:2512.15710v1 Announce Type: new 
Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</title>
<link>https://arxiv.org/abs/2512.15712</link>
<guid>https://arxiv.org/abs/2512.15712</guid>
<content:encoded><![CDATA[
<div> Interpretability, neural networks, concept bottleneck, predictive concept decoder, latent attributes<br /><br />Summary:<br /><br />This paper addresses the challenge of interpreting internal activations of neural networks by proposing a novel end-to-end training approach instead of relying on hand-designed interpretability agents. The authors introduce Predictive Concept Decoders (PCDs), an architecture that compresses neural activations into a sparse list of meaningful concepts via an encoder, which a decoder then uses to answer natural language questions about the model's behavior. The approach allows the assistant to be pretrained on large unstructured datasets and subsequently fine-tuned to improve question-answering performance. The PCD benefits from favorable scaling properties, meaning that as more data is used, the interpretability of the bottleneck concepts and accuracy on downstream tasks both improve. Notably, PCDs demonstrate strong capabilities in detecting model jailbreaks, secret hints, implanted latent concepts, and accurately reveal latent user attributes encoded within the model. This work advances scalable and faithful interpretability by turning internal activation analysis into a trainable predictive task, enhancing understanding and transparency in neural network behavior. <div>
arXiv:2512.15712v1 Announce Type: new 
Abstract: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union</title>
<link>https://arxiv.org/abs/2512.12837</link>
<guid>https://arxiv.org/abs/2512.12837</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-powered greenwashing, criminal liability, algorithmic deception, corporate sustainability, ESG governance<br /><br />Summary: This study addresses the challenge of AI-powered greenwashing within corporate sustainability governance, highlighting how it increases the opacity of environmental disclosures and complicates regulatory oversight. It performs a comparative legal analysis focusing on criminal liability for AI-mediated greenwashing across India, the US, and the EU, revealing significant gaps in attributing culpability when deceptive claims stem from algorithms. Current laws show anthropocentric biases by requiring proof of human intent, making them ill-suited to handle algorithmic deception. The research identifies outdated fraud statutes that fail to accommodate AI-generated misrepresentation adequately. Employing a doctrinal legal methodology, it examines judicial precedents and legislation to explore possibilities for expanding corporate criminal liability frameworks. Key findings emphasize the suitability of strict liability models, the need for recalibrated governance frameworks addressing AI accountability, and the introduction of algorithmic due diligence mandates within ESG regimes. Comparative analysis reveals jurisdictional differences, with the EU's Corporate Sustainability Due Diligence Directive (CSDDD) offering a promising transnational regulatory model. The study contributes to AI ethics and environmental law by proposing a hybrid liability framework that combines algorithmic risk assessments with evolving concepts of legal personhood, ensuring that the opacity of algorithms does not obstruct the enforcement of liability. <div>
arXiv:2512.12837v1 Announce Type: cross 
Abstract: AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-\`a-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tourists Profiling by Interest Analysis</title>
<link>https://arxiv.org/abs/2512.14704</link>
<guid>https://arxiv.org/abs/2512.14704</guid>
<content:encoded><![CDATA[
<div> Keywords: tourists' behavior, digital traces, tourism analysis, qualitative and quantitative, attractions networks<br /><br />Summary:<br /><br />1. The paper addresses changes in the analysis of tourist behavior due to the digital revolution, highlighting how digital traces from travelers provide new data sources.  
2. Previous tourism studies have largely focused on the quantitative analysis of these digital traces to derive conclusions about tourist patterns.  
3. This study proposes an integrated approach that incorporates both qualitative and quantitative methods to analyze digital traces more comprehensively.  
4. The goal is to better understand the dynamic factors that influence tourist behavior, moving beyond simple numerical data to include contextual insights.  
5. A particular emphasis is placed on the role of attractions networks, examining how the interconnectedness of tourist sites affects travel behaviors and decisions.  
6. This dual-aspect methodology aims to yield deeper insights into tourist dynamics, supporting enhanced tourism management and planning strategies. <div>
arXiv:2512.14704v1 Announce Type: cross 
Abstract: With the recent digital revolution, analyzing of tourists' behaviors and research fields associated with it have changed profoundly. It is now easier to examine behaviors of tourists using digital traces they leave during their travels. The studies conducted on diverse aspects of tourism focus on quantitative aspects of digital traces to reach its conclusions. In this paper, we suggest a study focused on both qualitative and quantitative aspect of digital traces to understand the dynamics governing tourist behavior, especially those concerning attractions networks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: neural architecture search, large language model, image captioning, DeepSeek, BLEU-4<br /><br />Summary:<br /><br />1. The paper introduces NN-Caption, a pipeline for neural architecture search (NAS) guided by large language models (LLMs) to automatically generate runnable image-captioning models. 2. The method composes CNN encoders from the LEMUR classification backbone with sequence decoders such as LSTM, GRU, and Transformer, following a strict network API standard. 3. Using DeepSeek-R1-0528-Qwen3-8B as the primary code generator, the authors provide prompt templates and examples of generated architectures for reproducible results. 4. Evaluation on the MS COCO dataset is conducted using BLEU-4 scoring, where over half of the dozens of LLM-generated captioning models successfully train and produce meaningful captions. 5. An analysis compares prompts with different numbers of input model snippets (5 vs. 10), showing a slight decrease in success rate with more components. 6. Training dynamics are reported, including caption accuracy progression and the highest BLEU-4 achieved. 7. The LLM not only proposes architectures but also suggests hyperparameters and training strategies, improving NAS effectiveness. 8. Challenges such as code hallucinations and API compliance issues are identified, with iterative prompt adjustments and code fixes applied to resolve them. 9. The pipeline integrates prompt-based code generation with automatic evaluation, adding dozens of novel captioning models to the open LEMUR dataset to support reproducible benchmarking and AutoML research advancement. <div>
arXiv:2512.14706v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Source Knowledge Selection in Multi-Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.14710</link>
<guid>https://arxiv.org/abs/2512.14710</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised multi-domain adaptation, source knowledge selection, density-driven selection, pseudo-label enhancement, multimodal model<br /><br />Summary:  
This paper addresses the challenge of unsupervised multi-domain adaptation, where knowledge from multiple labeled source domains is used to improve learning in an unlabeled target domain. The authors identify a critical issue: many source domains may contain redundant or irrelevant information that can negatively impact transfer learning performance, especially when dealing with a large number of source domains. To solve this, they propose a novel method named Autonomous Source Knowledge Selection (AutoS), which autonomously selects the most relevant and transferable source samples and models for the target task. AutoS uses a density-driven selection strategy to guide source sample selection during training and to decide the influence of each source model on target predictions. Additionally, the approach incorporates a pseudo-label enhancement module built upon a pre-trained multimodal model, aimed at reducing label noise in the target domain and enhancing self-supervision. The combination of strategic source knowledge selection with pseudo-label refinement helps achieve more robust and accurate adaptation to the target domain. Experimental results on real-world datasets demonstrate the effectiveness and superiority of AutoS compared to existing methods, highlighting its potential for large-scale multi-domain adaptation tasks. <div>
arXiv:2512.14710v1 Announce Type: cross 
Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Promoting Fairness in Information Access within Social Networks</title>
<link>https://arxiv.org/abs/2512.14711</link>
<guid>https://arxiv.org/abs/2512.14711</guid>
<content:encoded><![CDATA[
<div> Keywords: online social networks, fairness, information access, resistance distance, greedy algorithm<br /><br />Summary:<br /><br />1. The paper addresses the issue of unequal information access in online social networks, especially affecting minority or disadvantaged groups due to their positions in the network.<br /><br />2. It formulates the optimization problem of adding new network connections aimed at improving fairness in information spread across different demographic groups.<br /><br />3. The study introduces the use of resistance distance as a measure of information access, highlighting a novel global structural and multi-path connectivity perspective.<br /><br />4. The problem is proven to be NP-hard, leading the authors to propose a simple greedy algorithm that achieves accurate solutions but runs in cubic time complexity.<br /><br />5. The main technical contribution is the development of a linear-time approximate algorithm that significantly reduces the computational cost, enabling efficient processing of large-scale networks with millions of nodes.<br /><br />6. Extensive experiments on both real-world and synthetic datasets demonstrate that the linear-time algorithm maintains solution accuracy while scaling effectively to large networks. <div>
arXiv:2512.14711v1 Announce Type: cross 
Abstract: The advent of online social networks has facilitated fast and wide spread of information. However, some users, especially members of minority groups, may be less likely to receive information spreading on the network, due to their disadvantaged network position. We study the optimization problem of adding new connections to a network to enhance fairness in information access among different demographic groups.
  We provide a concrete formulation of this problem where information access is measured in terms of resistance distance, {offering a new perspective that emphasizes global network structure and multi-path connectivity.} The problem is shown to be NP-hard. We propose a simple greedy algorithm which turns out to output accurate solutions, but its run time is cubic, which makes it undesirable for large networks. As our main technical contribution, we reduce its time complexity to linear, leveraging several novel approximation techniques. In addition to our theoretical findings, we also conduct an extensive set of experiments using both real-world and synthetic datasets. We demonstrate that our linear-time algorithm can produce accurate solutions for networks with millions of nodes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
<div> Sepsis, Deep Fusion, Mixture-of-Experts, Antibiotic Selection, Clinical Decision Support<br /><br />Summary:  
Sepsis accounts for nearly 20% of global ICU admissions, yet existing prediction models struggle to integrate diverse data types effectively, often treating each modality in isolation or using brittle early fusion techniques. This study compares End-to-End Deep Fusion and Context-Aware Stacking approaches for sepsis prediction tasks using multiple data streams such as vitals, text, and imaging. The initially proposed Quad-Modal Hierarchical Gated Attention Network, called SepsisFusionFormer, aimed to capture complex cross-modal interactions but suffered from "attention starvation" and overfitting on a small antibiotic cohort (N ≈ 2,100), resulting in a suboptimal AUC of 0.66. In response, the authors developed SepsisLateFusion, a leaner Context-Aware Mixture-of-Experts (MoE) model where each modality is treated as an orthogonal expert—namely the Historian (Static), the Monitor (Temporal), and the Reader (NLP)—and gated dynamically by a CatBoost meta-learner. This design achieved state-of-the-art predictive performance with an AUC of 0.915 for predictions made 4 hours before clinical onset, and a 48% reduction in missed sepsis cases after threshold calibration, enabling earlier intervention. Additionally, a Quad-Modal Ensemble was shown to perform best (0.72 AUC) for a new prescriptive task of multi-class antibiotic selection. These models are implemented in SepsisSuite, an open-source Python framework for clinical decision support available at the provided GitHub link. <div>
arXiv:2512.14712v1 Announce Type: cross 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.14714</link>
<guid>https://arxiv.org/abs/2512.14714</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater acoustic classification, Gabor convolution, ResNeXt, squeeze-and-excitation, environmental noise  

<br /><br />Summary:  
This paper addresses the challenge of remotely detecting and classifying underwater acoustic targets, which is critical for environmental monitoring and defense. It highlights the difficulties posed by complex ship-radiated and environmental underwater noise that affect signal processing accuracy. The authors propose GSE ResNeXt, a novel deep learning architecture that integrates learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation (SE) attention mechanisms. The Gabor convolutions act as adaptive two-dimensional band-pass filters, increasing feature channel representation, while the SE attention improves training stability, convergence speed, and feature discriminability. The model is evaluated across three classification tasks of increasing difficulty, with a specific focus on the temporal differences between training and testing datasets, revealing that vessel-to-sensor distance strongly impacts classification performance. Experimental results demonstrate that GSE ResNeXt consistently outperforms baseline models such as Xception, ResNet, and MobileNetV2 in classification accuracy. Additionally, the inclusion of Gabor convolutional layers reduces training time by approximately 28%. The study underscores the importance of integrating signal processing techniques into deep learning frameworks to enhance robustness and generalization in data-limited underwater acoustic scenarios. Future work is suggested to focus on mitigating environmental influences on input signals to further improve reliability. <div>
arXiv:2512.14714v1 Announce Type: cross 
Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection</title>
<link>https://arxiv.org/abs/2512.14715</link>
<guid>https://arxiv.org/abs/2512.14715</guid>
<content:encoded><![CDATA[
<div> Keywords: bit flips, image captioning, semantic drift, differentiable fault analysis, large language models<br /><br />Summary: This paper investigates how bit-level perturbations (fault injections) in the weights of large language models (LLMs) used for image captioning affect the semantic content of generated captions without disrupting grammatical structure. Unlike previous fault analysis that focused on accuracy drops or model crashes, this work emphasizes subtle semantic changes caused by individual bit flips, which can alter the model's visual-to-linguistic mappings and shift the narrative conveyed. The authors propose that these semantic drifts are not random but can be estimated via the model’s gradients, using a differentiable approach to identify bits that critically impact meaning while preserving fluency. They introduce BLADE (Bit-level Fault Analysis via Differentiable Estimation), a framework leveraging gradient-based sensitivity analysis combined with semantic-fluency objectives at the caption level to pinpoint and refine semantically important bits for targeted perturbations. This methodology reveals how semantic information is encoded and mutable at the bit level in vision-language generative models. The findings highlight the vulnerability of such models to imperceptible low-level changes that steer high-level semantics and pave the way for enhanced robustness testing, adversarial defenses, and explainable AI by showing how structured bit faults reshape semantic output. <div>
arXiv:2512.14715v1 Announce Type: cross 
Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.14718</link>
<guid>https://arxiv.org/abs/2512.14718</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series forecasting, spectral entropy, dependency modeling, signed graph, temporal position

<br /><br />Summary:  
This paper addresses key challenges in multivariate time series forecasting, focusing on accurately modeling complex inter-variable dependencies. Existing methods using attention or graph-based approaches struggle with disruptions from irrelevant variables, ignore negative correlations due to softmax normalization, and lack temporal position awareness. To overcome these, the authors propose SEED, a Spectral Entropy-guided Evaluation framework. SEED introduces a Dependency Evaluator that uses spectral entropy to dynamically assess spatial and temporal dependencies, balancing between Channel Independence and Channel Dependence strategies. Additionally, the Spectral Entropy-based Fuser refines these dependency weights by isolating temporal patterns originating from influences of other variables rather than intrinsic dynamics. To preserve negative correlations, SEED employs a Signed Graph Constructor allowing signed edge weights, addressing shortcomings of softmax normalization. The Context Spatial Extractor module enables variables to perceive their temporal positions through local contextual windows, enhancing spatial feature construction. Extensive experiments on 12 diverse real-world datasets confirm SEED's effectiveness and generality, demonstrating state-of-the-art forecasting performance across multiple domains. This work presents a novel framework that significantly advances spatial-temporal dependency modeling in multivariate time series forecasting. <div>
arXiv:2512.14718v1 Announce Type: cross 
Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Attribution Priors for Explainable and Robust Model Training</title>
<link>https://arxiv.org/abs/2512.14719</link>
<guid>https://arxiv.org/abs/2512.14719</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, Attribution Prior, Interpretability, Robustness, Classification  

<br /><br />Summary:  
This paper addresses the limitations of current attribution-based explanation methods used to improve interpretability and robustness in small language models (SLMs) for classification tasks. First, it identifies a key issue that existing attribution methods tend to highlight common keywords shared across semantically similar classes, which lack sufficient discriminative power for fine-grained class differentiation. Second, to tackle this problem, the authors propose a novel framework called Class-Aware Attribution Prior (CAP), which extracts class-specific attribution priors that better capture subtle distinctions between classes. Third, building upon CAP, they develop CAP Hybrid, a method that integrates CAP priors with those from existing attribution techniques, creating a balanced and diversified supervisory signal to guide model training. Fourth, by aligning the model’s self-attribution with these enriched priors, the approach fosters learning of diverse and decision-relevant features, enhancing interpretability. Finally, extensive experimental validation performed under various conditions—including full-data, few-shot learning, and adversarial settings—demonstrates that the proposed methods consistently improve both the interpretability and robustness of small language models in classification tasks. <div>
arXiv:2512.14719v1 Announce Type: cross 
Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMe: A Realistic Benchmark for LLM-based Social Media Agents</title>
<link>https://arxiv.org/abs/2512.14720</link>
<guid>https://arxiv.org/abs/2512.14720</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social media agents, benchmark, SoMe dataset, evaluation  

<br /><br />Summary: This paper presents SoMe, a novel benchmark aimed at evaluating intelligent agents powered by large language models (LLMs) in the context of social media. The benchmark addresses the current lack of comprehensive tools to assess these agents' abilities to comprehend media content, understand user behavior, and make complex decisions. SoMe includes eight distinct social media agent tasks, encompassing over 9 million posts, more than 6,500 user profiles, and upwards of 25,000 reports sourced from multiple social media platforms and external websites. It features 17,869 meticulously annotated task queries to ensure detailed evaluation. Compared to existing datasets, SoMe stands out as the first versatile and realistic platform designed specifically for LLM-based social media agents to tackle diverse challenges. Through extensive quantitative and qualitative analyses, the authors provide an insightful overview of mainstream agentic LLM performance in real-world social media environments, highlighting key limitations. The study finds that neither closed-source nor open-source LLMs currently meet satisfactory performance levels on social media agent tasks. By making the code and data publicly available, SoMe offers a challenging yet valuable testbed for future research and development of social media agents. <div>
arXiv:2512.14720v1 Announce Type: cross 
Abstract: Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers</title>
<link>https://arxiv.org/abs/2512.14722</link>
<guid>https://arxiv.org/abs/2512.14722</guid>
<content:encoded><![CDATA[
<div> Transformers, Groebner bases, Hierarchical Attention Transformers, polynomial equations, curriculum learning

<br /><br />Summary:  
1. This paper advances the use of transformers for computing Groebner bases, which are crucial in computer algebra and have many practical applications.  
2. It introduces Hierarchical Attention Transformers (HATs), an architecture that imposes a tree-structured inductive bias to better capture hierarchical relationships inherent in the data.  
3. By leveraging this hierarchical structure, the method achieves significant computational savings compared to conventional flat attention transformer models.  
4. The authors generalize the HAT approach to arbitrary depths and provide a detailed analysis of the computational costs involved.  
5. When combined with curriculum learning strategies, the method substantially improves performance, successfully solving larger instances of multivariate polynomial systems than prior work, specifically outperforming results from Kera et al. (2024). <div>
arXiv:2512.14722v1 Announce Type: cross 
Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion</title>
<link>https://arxiv.org/abs/2512.14725</link>
<guid>https://arxiv.org/abs/2512.14725</guid>
<content:encoded><![CDATA[
<div> urban wind flow, generative diffusion, graph neural network, CFD simulation, urban geometry<br /><br />Summary:<br /><br />1. The article addresses the challenge of modeling urban wind flow, which is crucial for air quality assessment and sustainable city planning, particularly in complex urban geometries.<br /><br />2. It highlights the limitations of low-order models in capturing geometric effects and the high computational cost of high-fidelity Computational Fluid Dynamics (CFD) simulations.<br /><br />3. The authors propose a novel generative diffusion framework that synthesizes steady-state urban wind velocity fields on unstructured meshes, using only geometry information as input.<br /><br />4. The framework integrates a hierarchical graph neural network with score-based diffusion modeling, enabling accurate and diverse flow field generation without the need for temporal rollouts or dense measurements.<br /><br />5. Trained over multiple mesh slices and wind incident angles, the model generalizes well to unseen urban geometries, successfully reconstructs important flow features like wakes and recirculation zones, and provides uncertainty-aware predictions.<br /><br />6. Ablation studies demonstrate the model's robustness to variations in mesh structures and its effectiveness across different inference strategies.<br /><br />7. This work lays the groundwork for foundation models in the built environment, offering urban planners a fast, reliable tool to evaluate design scenarios under densification pressures and climate variability. <div>
arXiv:2512.14725v1 Announce Type: cross 
Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.14726</link>
<guid>https://arxiv.org/abs/2512.14726</guid>
<content:encoded><![CDATA[
<div> Quantum Decision Transformer, offline reinforcement learning, quantum-inspired attention, long-horizon credit assignment, adaptive computation<br /><br />Summary:<br /><br />Offline reinforcement learning enables policy development from pre-collected data without further environment interaction; however, Decision Transformer (DT) architectures face challenges with long-horizon credit assignment and capturing complex state-action dependencies. To address these, the paper introduces the Quantum Decision Transformer (QDT), a novel model that incorporates quantum-inspired computational mechanisms. QDT integrates two primary components: Quantum-Inspired Attention, which uses entanglement operations to capture non-local feature correlations, and Quantum Feedforward Networks, which apply multi-path processing and learnable interference to enable adaptive computation. Experimental evaluations on continuous control tasks demonstrate that QDT achieves over 2,000% performance improvements compared to standard DTs, with better generalization across datasets of varying quality. Ablation studies highlight a strong synergy between the quantum-inspired components, showing that neither alone matches the performance of their combined use, emphasizing the importance of holistic co-design over modular adoption. The analysis identifies three key computational advantages from this design: enhanced credit assignment through non-local correlations, implicit ensemble behavior driven by parallel processing, and adaptive resource allocation enabled by learnable interference. These results establish quantum-inspired architectural design as a promising direction for improving transformer models in sequential decision-making and broader neural architecture design. <div>
arXiv:2512.14726v1 Announce Type: cross 
Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title>
<link>https://arxiv.org/abs/2512.14727</link>
<guid>https://arxiv.org/abs/2512.14727</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, conformal prediction, uncertainty estimates, calibration set size, medical imaging<br /><br />Summary:  
Machine learning (ML) is significantly impacting healthcare by offering advanced diagnostic and prognostic tools. Reliable uncertainty estimates are crucial for safe clinical decision-making, yet standard ML models often fail to deliver trustworthy uncertainty quantification. Conformal prediction (CP) is a methodology designed to convert heuristic uncertainty estimates from ML models into statistically valid prediction sets with guaranteed coverage probabilities. A key theoretical advantage of CP is that these statistical guarantees hold irrespective of the calibration set size. However, this paper challenges the practical utility of this advantage, arguing that although guarantees exist for any calibration set size, the effectiveness and informativeness of the resulting uncertainty estimates are strongly dependent on having sufficiently large calibration sets. The implications are critical in medical fields where data scarcity is common, and large calibration samples are often unavailable or difficult to acquire. The authors empirically validate their critique on a medical image classification task, demonstrating that small calibration sets may yield theoretically valid but practically unhelpful uncertainty estimates, potentially compromising clinical decision-making. This work highlights the need for caution when applying CP in medical applications with limited calibration data and suggests a reassessment of its practical benefits under data scarcity conditions. <div>
arXiv:2512.14727v1 Announce Type: cross 
Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Geometry for policy-constrained interpretation</title>
<link>https://arxiv.org/abs/2512.14731</link>
<guid>https://arxiv.org/abs/2512.14731</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic interpretation, policy constraints, geometric framework, hallucination prevention, financial data  

<br /><br />Summary:  
This article introduces a geometric framework designed for policy-constrained semantic interpretation aimed at eliminating hallucinated commitments, which is critical in high-stakes domains such as regulated finance. Semantic meanings are modeled as directions on a unit sphere, while evidence is represented by sets of witness vectors, forming admissible interpretations as spherical convex regions. Policy constraints are explicitly incorporated as priors on the same geometric manifold, intentionally separated from evidence to maintain clarity and enforce restrictions. Interpretation is performed via constrained optimization over these admissible regions, where refusal to commit naturally arises when contradictions or policy exclusions occur, reflecting a topologically necessary outcome. The framework is rigorously connected to foundational concepts including information theory, Bayesian inference, and sheaf-theoretic semantics, and it is proven that the complexity bounds of the method are information-theoretically optimal. Empirically, the approach was validated on large-scale regulated financial datasets, demonstrating zero hallucinated approvals across different policy regimes. This achievement represents a significant advancement as it is the first scalable solution to completely prevent hallucinated commitments under policy constraints. <div>
arXiv:2512.14731v1 Announce Type: cross 
Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
<div> Keywords: incidental findings, CT scans, large language models, vision-language models, planner-executor framework<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting and reporting incidental findings in abdominal CT scans, which are important for clinical decision-making but traditionally require time-consuming manual review by radiologists. The authors propose a novel automated framework that integrates large language models (LLMs) and foundational vision-language models (VLMs) using a plan-and-execute agentic approach. The framework consists of two main components: a planner based on an LLM that generates Python scripts leveraging predefined base functions aligned with medical guidelines, and an executor that runs these scripts to perform detection and classification tasks through VLMs, segmentation models, and image processing routines. This planner-executor setup enables automatic management of incidental findings for abdominal organs without human intervention. Experiments conducted on a dedicated abdominal CT benchmark involving three organs demonstrate the approach’s superior accuracy and efficiency compared to existing purely VLM-based methods. The results validate the potential of combining LLMs with VLMs in a structured agentic framework to enhance the precision and speed of incidental finding analysis in medical imaging workflows. This advancement could lead to more consistent and scalable radiology reporting practices. <div>
arXiv:2512.14732v1 Announce Type: cross 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</title>
<link>https://arxiv.org/abs/2512.14735</link>
<guid>https://arxiv.org/abs/2512.14735</guid>
<content:encoded><![CDATA[
<div> Keywords: PyFi, financial image understanding, vision language models, question chains, Monte Carlo Tree Search<br /><br />Summary:  
1. This paper introduces PyFi, a new framework designed for pyramid-like financial image understanding, which allows vision language models (VLMs) to reason through progressively complex question chains.  
2. At the core of PyFi is PyFi-600K, a large-scale dataset containing 600,000 financial question-answer pairs arranged in a reasoning pyramid from basic perception questions at the base to complex financial visual reasoning at the apex.  
3. The dataset is automatically synthesized using PyFi-adv, a multi-agent adversarial system based on Monte Carlo Tree Search (MCTS), where a challenger agent and a solver agent collaboratively generate question chains that explore deeper reasoning capabilities.  
4. Using PyFi-600K, the authors conduct detailed, hierarchical evaluations of advanced VLMs in the financial domain, highlighting their reasoning abilities across multiple complexity levels.  
5. Fine-tuning models such as Qwen2.5-VL-3B and Qwen2.5-VL-7B with the pyramid-structured question chains significantly improves performance, achieving average accuracy gains of 19.52% and 8.06% respectively in answering complex financial questions by decomposing them into simpler sub-questions.  
All associated resources including code, dataset, and models are publicly available at https://github.com/AgenticFinLab/PyFi. <div>
arXiv:2512.14735v1 Announce Type: cross 
Abstract: This paper proposes PyFi, a novel framework for pyramid-like financial image understanding that enables vision language models (VLMs) to reason through question chains in a progressive, simple-to-complex manner. At the core of PyFi is PyFi-600K, a dataset comprising 600K financial question-answer pairs organized into a reasoning pyramid: questions at the base require only basic perception, while those toward the apex demand increasing levels of capability in financial visual understanding and expertise. This data is scalable because it is synthesized without human annotations, using PyFi-adv, a multi-agent adversarial mechanism under the Monte Carlo Tree Search (MCTS) paradigm, in which, for each image, a challenger agent competes with a solver agent by generating question chains that progressively probe deeper capability levels in financial visual reasoning. Leveraging this dataset, we present fine-grained, hierarchical, and comprehensive evaluations of advanced VLMs in the financial domain. Moreover, fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on the pyramid-structured question chains enables these models to answer complex financial questions by decomposing them into sub-questions with gradually increasing reasoning demands, yielding average accuracy improvements of 19.52% and 8.06%, respectively, on the dataset. All resources of code, dataset and models are available at: https://github.com/AgenticFinLab/PyFi .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol</title>
<link>https://arxiv.org/abs/2512.14737</link>
<guid>https://arxiv.org/abs/2512.14737</guid>
<content:encoded><![CDATA[
<div> Keywords: agent communication, zero-knowledge proofs, privacy, auditability, Model Context Protocol (MCP)  

<br /><br />Summary:  
This article addresses the challenge of ensuring verifiable audit trails in agent communications without compromising privacy and confidentiality. It proposes a novel framework that integrates zero-knowledge proofs (ZKPs) with the existing Model Context Protocol (MCP), enabling message verification without revealing actual content. The framework supports privacy-preserving audits where messages can be authenticated and checked for rule compliance, while details remain confidential. Designed for lightweight and standard MCP networks, the system adds asynchronous audit verification to confirm message structure and general types without exposing specifics. A key feature is mutual auditing where agents can independently verify communication content quality and usage metrics without disclosing sensitive information. The authors formalize the framework's security goals, demonstrating that zk-MCP achieves data authenticity and communication privacy efficiently, with minimal latency overhead. They implement the approach fully, using Circom for zero-knowledge proof generation and integrating the audit protocol with MCP's bidirectional communication channels. This represents the first privacy-preserving audit system for agent communication, enabling verifiable mutual audits while maintaining strict confidentiality and agent privacy compliance, making it suitable for regulated environments requiring billing accuracy, compliance verification, and accountability. <div>
arXiv:2512.14737v1 Announce Type: cross 
Abstract: Existing agent communication frameworks face critical limitations in providing verifiable audit trails without compromising the privacy and confidentiality of agent interactions. The protection of agent communication privacy while ensuring auditability emerges as a fundamental challenge for applications requiring accurate billing, compliance verification, and accountability in regulated environments.
  We introduce a framework for auditing agent communications that keeps messages private while still checking they follow expected rules. It pairs zero-knowledge proofs with the existing Model Context Protocol (MCP) so messages can be verified without revealing their contents. The approach runs in lightweight networks, stays compatible with standard MCP exchanges, and adds asynchronous audit verification to confirm format and general message types without exposing specifics.
  The framework enables mutual audits between agents: one side can check communication content and quality while the other verifies usage metrics, all without revealing sensitive information. We formalize security goals and show that zk-MCP provides data authenticity and communication privacy, achieving efficient verification with negligible latency overhead. We fully implement the framework, including Circom-based zero-knowledge proof generation and an audit protocol integrated with MCP's bidirectional channel, and, to our knowledge, this is the first privacy-preserving audit system for agent communications that offers verifiable mutual auditing without exposing message content or compromising agent privacy.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs</title>
<link>https://arxiv.org/abs/2512.14741</link>
<guid>https://arxiv.org/abs/2512.14741</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, Large Language Models, continual fine-tuning, P-Trojan, persistence-aware evaluation  

<br /><br />Summary:  
Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), allowing adversaries to trigger harmful outputs or bypass safety controls. This study focuses on the persistence of those implanted backdoors when the model undergoes multi-stage, user-driven post-deployment continual fine-tuning, a scenario that has been rarely examined before. The authors propose P-Trojan, a novel trigger-based attack algorithm designed explicitly to optimize for backdoor persistence across multiple model updates. P-Trojan aligns poisoned gradients with clean task gradients on token embeddings to ensure the backdoor mapping remains effective and is not suppressed or forgotten during successive fine-tuning steps. Theoretical analysis confirms the feasibility of persistent backdoor attacks under continual fine-tuning conditions. Extensive experiments on Qwen2.5 and LLaMA3 model families, as well as a variety of task sequences, demonstrate that P-Trojan can maintain over 99% backdoor persistence without degrading clean-task accuracy. These findings highlight the vulnerability of LLMs to persistent backdoor attacks in realistic adaptation pipelines and underline the urgent need for persistence-aware evaluation methods and stronger defense mechanisms against backdoors during model fine-tuning. <div>
arXiv:2512.14741v1 Announce Type: cross 
Abstract: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Augmented AI/ML for O-RAN: Hierarchical Threat Detection with Synergistic Intelligence and Interpretability (Technical Report)</title>
<link>https://arxiv.org/abs/2512.14742</link>
<guid>https://arxiv.org/abs/2512.14742</guid>
<content:encoded><![CDATA[
<div> Open Radio Access Networks, cybersecurity, hierarchical defense, quantum computing, machine learning  

<br /><br />Summary:  
This article addresses the increased cybersecurity vulnerabilities in Open Radio Access Networks (O-RAN) due to their modular and disaggregated architecture across control, user, and management planes. A hierarchical defense framework is proposed, consisting of three coordinated layers: anomaly detection, intrusion confirmation, and multiattack classification, each corresponding to different levels of O-RAN's telemetry. The framework uniquely integrates hybrid quantum computing with machine learning techniques, employing amplitude- and entanglement-based feature encodings alongside deep and ensemble classifiers to enhance detection capability. The authors conduct comprehensive benchmarking on both synthetic and real-world telemetry datasets, analyzing encoding depth, architecture variants, and diagnostic accuracy. Results demonstrate that the framework consistently achieves near-perfect accuracy, high recall, and strong class separability, indicating its effectiveness in identifying complex cyber attacks. Furthermore, the system’s interpretability and robustness are validated through evaluations focusing on decision boundaries, probabilistic margins, and latent space geometry. These qualities support the framework’s readiness for practical deployment. Finally, the solution is designed to be slice-aware and scalable, targeting deployment in near-real-time (near-RT) and non-real-time (non-RT) RAN Intelligent Controller (RIC) domains, thus addressing operational needs within O-RAN environments. <div>
arXiv:2512.14742v1 Announce Type: cross 
Abstract: Open Radio Access Networks (O-RAN) enhance modularity and telemetry granularity but also widen the cybersecurity attack surface across disaggregated control, user and management planes. We propose a hierarchical defense framework with three coordinated layers-anomaly detection, intrusion confirmation, and multiattack classification-each aligned with O-RAN's telemetry stack. Our approach integrates hybrid quantum computing and machine learning, leveraging amplitude- and entanglement-based feature encodings with deep and ensemble classifiers. We conduct extensive benchmarking across synthetic and real-world telemetry, evaluating encoding depth, architectural variants, and diagnostic fidelity. The framework consistently achieves near-perfect accuracy, high recall, and strong class separability. Multi-faceted evaluation across decision boundaries, probabilistic margins, and latent space geometry confirms its interpretability, robustness, and readiness for slice-aware diagnostics and scalable deployment in near-RT and non-RT RIC domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation</title>
<link>https://arxiv.org/abs/2512.14744</link>
<guid>https://arxiv.org/abs/2512.14744</guid>
<content:encoded><![CDATA[
<div> Financial AI, Retrieval-Augmented Generation, Neurosymbolic Policy, Regulatory Compliance, Mathematical Validation<br /><br />Summary:<br /><br />Financial AI systems often struggle with calculation errors and regulatory compliance violations even when retrieval of relevant documents is accurate. This paper introduces VERAFI (Verified Agentic Financial Intelligence), a novel framework that integrates neurosymbolic policy generation with advanced dense retrieval and cross-encoder reranking techniques. VERAFI incorporates financial tool-enabled agents and automated reasoning policies designed to ensure adherence to GAAP standards, SEC regulations, and rigorous mathematical validation. The framework was evaluated using the FinanceBench dataset, where it demonstrated a significant performance increase in factual correctness from 52.4% with traditional methods to 94.7%, representing an 81% relative improvement. Notably, the neurosymbolic policy layer contributed a 4.3 percentage point improvement by specifically mitigating recurring logical and calculation mistakes common in language model reasoning. By embedding domain-specific financial knowledge directly into the reasoning pipeline, VERAFI offers a reliable solution to the accuracy challenges in financial AI. This integration supports critical applications such as regulatory compliance, investment decision-making, and risk management, advancing the trustworthiness and practical deployment of AI systems in finance. <div>
arXiv:2512.14744v1 Announce Type: cross 
Abstract: Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\% factual correctness, VERAFI's integrated approach reaches 94.7\%, an 81\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factor(U,T): Controlling Untrusted AI by Monitoring their Plans</title>
<link>https://arxiv.org/abs/2512.14745</link>
<guid>https://arxiv.org/abs/2512.14745</guid>
<content:encoded><![CDATA[
<div> Keywords: Factored cognition, malicious decomposer, monitors, Python coding tasks, AUROC

<br /><br />Summary: This paper investigates the security implications of using AI models for task decomposition in factored cognition, where a complex task is broken down into simpler subtasks handled by separate models. Traditional approaches use trusted but less capable models for decomposition to avoid malicious behavior, limiting applications where decomposition itself is difficult. The authors propose Factor(U,T), a framework where a stronger but untrusted model performs decomposition, while trusted models handle subtasks. They explore whether monitoring only the natural language instructions from the decomposer can detect malicious behavior. Experiments on BigCodeBench, a dataset of Python coding tasks, show that monitors analyzing just the decomposition instructions perform poorly, achieving an AUROC of 0.52 in distinguishing malicious versus honest decompositions. In contrast, monitors that evaluate the complete Python solutions generated by child tasks achieve much higher accuracy (AUROC 0.96). Additionally, Factor(D,U), where a trusted decomposer and untrusted implementers are monitored at the solution level, demonstrates excellent detection capability (AUROC 0.96) and safety (1.2% attack success rate). The results indicate that monitoring at the implementation level effectively detects malicious activity, whereas decomposition-only monitoring is insufficient. <div>
arXiv:2512.14745v1 Announce Type: cross 
Abstract: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Cross-Modal Mapping of Molecular, Pathologic, and Radiologic Phenotypes in Lipid-Deficient Clear Cell Renal CellCarcinoma</title>
<link>https://arxiv.org/abs/2512.14750</link>
<guid>https://arxiv.org/abs/2512.14750</guid>
<content:encoded><![CDATA[
<div> Clear cell renal cell carcinoma, intratumoral heterogeneity, molecular subtypes, radiomics, computational pathology<br /><br />Summary:  
Clear cell renal cell carcinoma (ccRCC) displays significant intratumoral heterogeneity across multiple biological scales, complicating clinical outcome predictions and limiting conventional TNM staging efficacy. A particular lipid-deficient subtype known as DCCD-ccRCC, identified through multi-omics analyses, correlates with worse outcomes even at early stages. The study introduces a hierarchical cross-scale framework enabling preoperative identification of this high-risk subtype by integrating data from molecular, histological, and radiological modalities. This framework features a molecular-to-pathology-to-radiology supervisory bridge, allowing molecular signatures to be mapped onto tissue and imaging phenotypes. Two specialized modality-specific models were developed: PathoDCCD, capturing cellular to meso-regional pathological features, and RadioDCCD, integrating whole-tumor and subregional radiomics data alongside heterogeneity metrics from CT scans. Together, these nested models facilitate accurate molecular subtype prediction and risk stratification. Validation across five cohorts totaling 1,659 patients demonstrated PathoDCCD’s fidelity in reflecting molecular subtypes and RadioDCCD’s ability for reliable noninvasive preoperative predictions. The cross-scale model effectively identifies patients with the poorest prognoses, thereby unifying molecular biology, computational pathology, and quantitative radiology into a clinically applicable, biologically grounded platform for noninvasive molecular phenotyping of ccRCC. <div>
arXiv:2512.14750v1 Announce Type: cross 
Abstract: Clear cell renal cell carcinoma (ccRCC) exhibits extensive intratumoral heterogeneity on multiple biological scales, contributing to variable clinical outcomes and limiting the effectiveness of conventional TNM staging, which highlights the urgent need for multiscale integrative analytic frameworks. The lipid-deficient de-clear cell differentiated (DCCD) ccRCC subtype, defined by multi-omics analyses, is associated with adverse outcomes even in early-stage disease. Here, we establish a hierarchical cross-scale framework for the preoperative identification of DCCD-ccRCC. At the highest layer, cross-modal mapping transferred molecular signatures to histological and CT phenotypes, establishing a molecular-to-pathology-to-radiology supervisory bridge. Within this framework, each modality-specific model is designed to mirror the inherent hierarchical structure of tumor biology. PathoDCCD captured multi-scale microscopic features, from cellular morphology and tissue architecture to meso-regional organization. RadioDCCD integrated complementary macroscopic information by combining whole-tumor and its habitat-subregions radiomics with a 2D maximal-section heterogeneity metric. These nested models enabled integrated molecular subtype prediction and clinical risk stratification. Across five cohorts totaling 1,659 patients, PathoDCCD reliably recapitulated molecular subtypes, while RadioDCCD provided reliable preoperative prediction. The consistent predictions identified patients with the poorest clinical outcomes. This cross-scale paradigm unifies molecular biology, computational pathology, and quantitative radiology into a biologically grounded strategy for preoperative noninvasive molecular phenotyping of ccRCC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs</title>
<link>https://arxiv.org/abs/2512.14751</link>
<guid>https://arxiv.org/abs/2512.14751</guid>
<content:encoded><![CDATA[
<div> Keywords: finetuning, large language models, jailbreak vulnerabilities, adversarial prompts, Probe-Guided Projection (PGP)<br /><br />Summary:<br /><br />This paper investigates the security implications of finetuning pretrained large language models (LLMs), focusing on whether finetuned models inherit jailbreak vulnerabilities present in their pretrained versions. The authors adopt a realistic threat model where attackers have white-box access to the pretrained LLM but only black-box access to finetuned derivatives. Empirical results demonstrate that adversarial prompts crafted on the pretrained LLM transfer effectively to its finetuned variants, indicating the inheritance of vulnerabilities through the pretrain-to-finetune pipeline. By probing the representations within the pretrained hidden states, the study finds that these transferable adversarial prompts are linearly separable, implying that universal transferability is embedded in the pretrained model’s internal structure. Leveraging this insight, the researchers introduce the Probe-Guided Projection (PGP) attack, which directs the optimization of adversarial prompts toward directions relevant to transferability. Experiments conducted across multiple LLM families and a wide range of finetuning tasks validate PGP’s high success rate in transferring attacks, highlighting significant security risks in the standard paradigm of developing downstream applications by finetuning pretrained LLMs. The findings stress the necessity for improved security measures considering inherited vulnerabilities in finetuned language models. <div>
arXiv:2512.14751v1 Announce Type: cross 
Abstract: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyberswarm: a novel swarm intelligence algorithm inspired by cyber community dynamics</title>
<link>https://arxiv.org/abs/2512.14752</link>
<guid>https://arxiv.org/abs/2512.14752</guid>
<content:encoded><![CDATA[
<div> swarm intelligence, recommendation systems, dynamic hypergraph, Node2Vec, social networks  

<br /><br />Summary:  
This paper addresses the challenges recommendation systems face in adapting to evolving user preferences and complex social network interactions. Traditional methods fall short in handling the intricate dynamics of cyber-social systems and lack flexibility across domains. To overcome these limitations, the authors propose a general-purpose swarm intelligence algorithm inspired by social psychology principles. The framework models user preferences and community influences using a dynamic hypergraph structure, integrating centrality-based feature extraction and Node2Vec embeddings. Preference evolution is managed through message-passing mechanisms and hierarchical graph modeling, allowing real-time adaptation to behavioral changes. Experimental results demonstrate superior performance on diverse recommendation tasks such as social networks and content discovery, measured by Hit Rate (HR), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG), outperforming baseline algorithms consistently. This model's adaptability enables it to generate contextually relevant and precise recommendations in dynamic environments. The research contributes by effectively bridging individual preferences with community-level influences through the novel integration of swarm intelligence and network dynamics. Its general-purpose design supports application across various domains, including social graphs, personalized learning platforms, and medical graphs. Overall, the study advances recommendation system methodologies by addressing optimization challenges in complex, dynamic networks. <div>
arXiv:2512.14752v1 Announce Type: cross 
Abstract: Recommendation systems face challenges in dynamically adapting to evolving user preferences and interactions within complex social networks. Traditional approaches often fail to account for the intricate interactions within cyber-social systems and lack the flexibility to generalize across diverse domains, highlighting the need for more adaptive and versatile solutions. In this work, we introduce a general-purpose swarm intelligence algorithm for recommendation systems, designed to adapt seamlessly to varying applications. It was inspired by social psychology principles. The framework models user preferences and community influences within a dynamic hypergraph structure. It leverages centrality-based feature extraction and Node2Vec embeddings. Preference evolution is guided by message-passing mechanisms and hierarchical graph modeling, enabling real-time adaptation to changing behaviors. Experimental evaluations demonstrated the algorithm's superior performance in various recommendation tasks, including social networks and content discovery. Key metrics such as Hit Rate (HR), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG) consistently outperformed baseline methods across multiple datasets. The model's adaptability to dynamic environments allowed for contextually relevant and precise recommendations. The proposed algorithm represents an advancement in recommendation systems by bridging individual preferences and community influences. Its general-purpose design enables applications in diverse domains, including social graphs, personalized learning, and medical graphs. This work highlights the potential of integrating swarm intelligence with network dynamics to address complex optimization challenges in recommendation systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CODE ACROSTIC: Robust Watermarking for Code Generation</title>
<link>https://arxiv.org/abs/2512.14753</link>
<guid>https://arxiv.org/abs/2512.14753</guid>
<content:encoded><![CDATA[
<div> Keywords: watermarking, large language models, code generation, comment removal attack, low-entropy code<br /><br />Summary:<br /><br />1. Watermarking large language models (LLMs) is crucial to prevent misuse such as fake news generation, plagiarism, and spam, especially for code, which often contains valuable intellectual property. <br />2. Existing watermarking methods for LLM-generated code are vulnerable to comment removal attacks, where an attacker simply deletes comments without changing code functionality, thereby evading detection. <br />3. Code watermarking is challenging due to the low-entropy nature of most source code compared to natural language, limiting the capacity and stealth of watermark insertion. <br />4. The authors propose a novel approach that uses prior knowledge to differentiate between low-entropy and high-entropy parts of code by leveraging a Cue List of words. This targeted injection improves the watermark's resilience and detectability. <br />5. Their method was evaluated on the HumanEval benchmark and showed superior detectability and usability when compared with three state-of-the-art code watermarking techniques, demonstrating the effectiveness of their approach against comment removal attacks. <div>
arXiv:2512.14753v1 Announce Type: cross 
Abstract: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Reliability of Language Models in Instruction-Following</title>
<link>https://arxiv.org/abs/2512.14754</link>
<guid>https://arxiv.org/abs/2512.14754</guid>
<content:encoded><![CDATA[
arXiv:2512.14754v1 Announce Type: cross 
Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPE: Capability Achievement via Policy Execution</title>
<link>https://arxiv.org/abs/2512.14761</link>
<guid>https://arxiv.org/abs/2512.14761</guid>
<content:encoded><![CDATA[
arXiv:2512.14761v1 Announce Type: cross 
Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workflows vs Agents for Code Translation</title>
<link>https://arxiv.org/abs/2512.14762</link>
<guid>https://arxiv.org/abs/2512.14762</guid>
<content:encoded><![CDATA[
arXiv:2512.14762v1 Announce Type: cross 
Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis</title>
<link>https://arxiv.org/abs/2512.14764</link>
<guid>https://arxiv.org/abs/2512.14764</guid>
<content:encoded><![CDATA[
arXiv:2512.14764v1 Announce Type: cross 
Abstract: Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Discrete Diffusion for Constraint Satisfaction Problems</title>
<link>https://arxiv.org/abs/2512.14765</link>
<guid>https://arxiv.org/abs/2512.14765</guid>
<content:encoded><![CDATA[
arXiv:2512.14765v1 Announce Type: cross 
Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation</title>
<link>https://arxiv.org/abs/2512.14767</link>
<guid>https://arxiv.org/abs/2512.14767</guid>
<content:encoded><![CDATA[
arXiv:2512.14767v1 Announce Type: cross 
Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title>
<link>https://arxiv.org/abs/2512.14770</link>
<guid>https://arxiv.org/abs/2512.14770</guid>
<content:encoded><![CDATA[
arXiv:2512.14770v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $\Phi_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[
arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</title>
<link>https://arxiv.org/abs/2512.14797</link>
<guid>https://arxiv.org/abs/2512.14797</guid>
<content:encoded><![CDATA[
arXiv:2512.14797v1 Announce Type: cross 
Abstract: Advanced Ovarian Cancer (AOC) is often diagnosed at an advanced stage with peritoneal carcinosis (PC). Fagotti score (FS) assessment at diagnostic laparoscopy (DL) guides treatment planning by estimating surgical resectability, but its subjective and operator-dependent nature limits reproducibility and widespread use. Videos of patients undergoing DL with concomitant FS assessments at a referral center were retrospectively collected and divided into a development dataset, for data annotation, AI training and evaluation, and an independent test dataset, for internal validation. In the development dataset, FS-relevant frames were manually annotated for anatomical structures and PC. Deep learning models were trained to automatically identify FS-relevant frames, segment structures and PC, and predict video-level FS and indication to surgery (ItS). AI performance was evaluated using Dice score for segmentation, F1-scores for anatomical stations (AS) and ItS prediction, and root mean square error (RMSE) for final FS estimation. In the development dataset, the segmentation model trained on 7,311 frames, achieved Dice scores of 70$\pm$3% for anatomical structures and 56$\pm$3% for PC. Video-level AS classification achieved F1-scores of 74$\pm$3% and 73$\pm$4%, FS prediction showed normalized RMSE values of 1.39$\pm$0.18 and 1.15$\pm$0.08, and ItS reached F1-scores of 80$\pm$8% and 80$\pm$2% in the development (n=101) and independent test datasets (n=50), respectively. This is the first AI model to predict the feasibility of cytoreductive surgery providing automated FS estimation from DL videos. Its reproducible and reliable performance across datasets suggests that AI can support surgeons through standardized intraoperative tumor burden assessment and clinical decision-making in AOC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis</title>
<link>https://arxiv.org/abs/2512.14801</link>
<guid>https://arxiv.org/abs/2512.14801</guid>
<content:encoded><![CDATA[
arXiv:2512.14801v1 Announce Type: cross 
Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.
  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.
  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.
  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharing State Between Prompts and Programs</title>
<link>https://arxiv.org/abs/2512.14805</link>
<guid>https://arxiv.org/abs/2512.14805</guid>
<content:encoded><![CDATA[
arXiv:2512.14805v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.
  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.
  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let the Barbarians In: How AI Can Accelerate Systems Performance Research</title>
<link>https://arxiv.org/abs/2512.14806</link>
<guid>https://arxiv.org/abs/2512.14806</guid>
<content:encoded><![CDATA[
arXiv:2512.14806v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber</title>
<link>https://arxiv.org/abs/2512.14846</link>
<guid>https://arxiv.org/abs/2512.14846</guid>
<content:encoded><![CDATA[
arXiv:2512.14846v1 Announce Type: cross 
Abstract: Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&amp;CK mappings).
  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.
  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Roadmap for Applying Graph Neural Networks to Numerical Data: Insights from Cementitious Materials</title>
<link>https://arxiv.org/abs/2512.14855</link>
<guid>https://arxiv.org/abs/2512.14855</guid>
<content:encoded><![CDATA[
arXiv:2512.14855v1 Announce Type: cross 
Abstract: Machine learning (ML) has been increasingly applied in concrete research to optimize performance and mixture design. However, one major challenge in applying ML to cementitious materials is the limited size and diversity of available databases. A promising solution is the development of multi-modal databases that integrate both numerical and graphical data. Conventional ML frameworks in cement research are typically restricted to a single data modality. Graph neural network (GNN) represents a new generation of neural architectures capable of learning from data structured as graphs, capturing relationships through irregular or topology-dependent connections rather than fixed spatial coordinates. While GNN is inherently designed for graphical data, they can be adapted to extract correlations from numerical datasets and potentially embed physical laws directly into their architecture, enabling explainable and physics-informed predictions. This work is among the first few studies to implement GNNs to design concrete, with a particular emphasis on establishing a clear and reproducible pathway for converting tabular data into graph representations using the k-nearest neighbor (K-NN) approach. Model hyperparameters and feature selection are systematically optimized to enhance prediction performance. The GNN shows performance comparable to the benchmark random forest, which has been demonstrated by many studies to yield reliable predictions for cementitious materials. Overall, this study provides a foundational roadmap for transitioning from traditional ML to advanced AI architectures. The proposed framework establishes a strong foundation for future multi-modal and physics-informed GNN models capable of capturing complex material behaviors and accelerating the design and optimization of cementitious materials.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks</title>
<link>https://arxiv.org/abs/2512.14860</link>
<guid>https://arxiv.org/abs/2512.14860</guid>
<content:encoded><![CDATA[
arXiv:2512.14860v1 Announce Type: cross 
Abstract: Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel "hallucinated compliance" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse</title>
<link>https://arxiv.org/abs/2512.14879</link>
<guid>https://arxiv.org/abs/2512.14879</guid>
<content:encoded><![CDATA[
arXiv:2512.14879v1 Announce Type: cross 
Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media</title>
<link>https://arxiv.org/abs/2512.14887</link>
<guid>https://arxiv.org/abs/2512.14887</guid>
<content:encoded><![CDATA[
arXiv:2512.14887v1 Announce Type: cross 
Abstract: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus, i) uses a hybrid human-machine approach to identify the range of viewpoints expressed about a given topic, and ii) classifies relevant claims with respect to the identified viewpoints, defined as sets of semantically and ideologically congruent claims (e.g., positions arguing that immigration positively impacts the UK economy). In this paper, we improve this pipeline by i) fine-tuning Large Language Models (LLMs) for viewpoint classification and ii) enriching claim representations with semantic descriptions of relevant actors drawn from Wikidata. We evaluate our approach against alternative solutions on a benchmark centred on the UK immigration debate. Results show that while both mechanisms independently improve classification performance, their integration yields the best results, particularly when using LLMs capable of processing long inputs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams</title>
<link>https://arxiv.org/abs/2512.14892</link>
<guid>https://arxiv.org/abs/2512.14892</guid>
<content:encoded><![CDATA[
arXiv:2512.14892v1 Announce Type: cross 
Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections</title>
<link>https://arxiv.org/abs/2512.14895</link>
<guid>https://arxiv.org/abs/2512.14895</guid>
<content:encoded><![CDATA[
arXiv:2512.14895v1 Announce Type: cross 
Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline</title>
<link>https://arxiv.org/abs/2512.14896</link>
<guid>https://arxiv.org/abs/2512.14896</guid>
<content:encoded><![CDATA[
arXiv:2512.14896v1 Announce Type: cross 
Abstract: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.
  Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters.
  Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark.
  Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models</title>
<link>https://arxiv.org/abs/2512.14926</link>
<guid>https://arxiv.org/abs/2512.14926</guid>
<content:encoded><![CDATA[
arXiv:2512.14926v1 Announce Type: cross 
Abstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further extend it for visual question answering by leveraging open-source LLMs. We demonstrate the usefulness of our datasets by fine-tuning open-source VLMs on Romanian visual question answering. We select VLMs from three widely used model families: LLaMA 3.2, LLaVA 1.6, and Qwen2. For fine-tuning, we employ the parameter-efficient LoRA method. Our models show improved Romanian capabilities in visual QA, as well as on tasks they were not trained on, such as Romanian image description generation. The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version. Finally, the models show substantial reductions in grammatical errors compared to their original forms, indicating improvements not only in language understanding but also in Romanian fluency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies</title>
<link>https://arxiv.org/abs/2512.14930</link>
<guid>https://arxiv.org/abs/2512.14930</guid>
<content:encoded><![CDATA[
arXiv:2512.14930v1 Announce Type: cross 
Abstract: High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pre-trained Segmentation Models using Post-Processing</title>
<link>https://arxiv.org/abs/2512.14937</link>
<guid>https://arxiv.org/abs/2512.14937</guid>
<content:encoded><![CDATA[
arXiv:2512.14937v1 Announce Type: cross 
Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</title>
<link>https://arxiv.org/abs/2512.14938</link>
<guid>https://arxiv.org/abs/2512.14938</guid>
<content:encoded><![CDATA[
arXiv:2512.14938v1 Announce Type: cross 
Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving</title>
<link>https://arxiv.org/abs/2512.14946</link>
<guid>https://arxiv.org/abs/2512.14946</guid>
<content:encoded><![CDATA[
arXiv:2512.14946v1 Announce Type: cross 
Abstract: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.
  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Repetition Improves Non-Reasoning LLMs</title>
<link>https://arxiv.org/abs/2512.14982</link>
<guid>https://arxiv.org/abs/2512.14982</guid>
<content:encoded><![CDATA[
arXiv:2512.14982v1 Announce Type: cross 
Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</title>
<link>https://arxiv.org/abs/2512.14989</link>
<guid>https://arxiv.org/abs/2512.14989</guid>
<content:encoded><![CDATA[
arXiv:2512.14989v1 Announce Type: cross 
Abstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent</title>
<link>https://arxiv.org/abs/2512.14990</link>
<guid>https://arxiv.org/abs/2512.14990</guid>
<content:encoded><![CDATA[
arXiv:2512.14990v1 Announce Type: cross 
Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where is the Watermark? Interpretable Watermark Detection at the Block Level</title>
<link>https://arxiv.org/abs/2512.14994</link>
<guid>https://arxiv.org/abs/2512.14994</guid>
<content:encoded><![CDATA[
arXiv:2512.14994v1 Announce Type: cross 
Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</title>
<link>https://arxiv.org/abs/2512.14998</link>
<guid>https://arxiv.org/abs/2512.14998</guid>
<content:encoded><![CDATA[
arXiv:2512.14998v1 Announce Type: cross 
Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding</title>
<link>https://arxiv.org/abs/2512.15000</link>
<guid>https://arxiv.org/abs/2512.15000</guid>
<content:encoded><![CDATA[
arXiv:2512.15000v1 Announce Type: cross 
Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</title>
<link>https://arxiv.org/abs/2512.15006</link>
<guid>https://arxiv.org/abs/2512.15006</guid>
<content:encoded><![CDATA[
arXiv:2512.15006v1 Announce Type: cross 
Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic diversity across language models mitigates knowledge collapse</title>
<link>https://arxiv.org/abs/2512.15011</link>
<guid>https://arxiv.org/abs/2512.15011</guid>
<content:encoded><![CDATA[
arXiv:2512.15011v1 Announce Type: cross 
Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Representation-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.15036</link>
<guid>https://arxiv.org/abs/2512.15036</guid>
<content:encoded><![CDATA[
arXiv:2512.15036v1 Announce Type: cross 
Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</title>
<link>https://arxiv.org/abs/2512.15047</link>
<guid>https://arxiv.org/abs/2512.15047</guid>
<content:encoded><![CDATA[
arXiv:2512.15047v1 Announce Type: cross 
Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification</title>
<link>https://arxiv.org/abs/2512.15052</link>
<guid>https://arxiv.org/abs/2512.15052</guid>
<content:encoded><![CDATA[
arXiv:2512.15052v1 Announce Type: cross 
Abstract: Disclaimer: Samples in this paper may be harmful and cause discomfort.
  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\% to 2.5\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops</title>
<link>https://arxiv.org/abs/2512.15053</link>
<guid>https://arxiv.org/abs/2512.15053</guid>
<content:encoded><![CDATA[
arXiv:2512.15053v1 Announce Type: cross 
Abstract: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based "prompt engineering," fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for "Observable Software Engineering" in the era of probabilistic computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</title>
<link>https://arxiv.org/abs/2512.15061</link>
<guid>https://arxiv.org/abs/2512.15061</guid>
<content:encoded><![CDATA[
arXiv:2512.15061v1 Announce Type: cross 
Abstract: This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</title>
<link>https://arxiv.org/abs/2512.15066</link>
<guid>https://arxiv.org/abs/2512.15066</guid>
<content:encoded><![CDATA[
arXiv:2512.15066v1 Announce Type: cross 
Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks</title>
<link>https://arxiv.org/abs/2512.15067</link>
<guid>https://arxiv.org/abs/2512.15067</guid>
<content:encoded><![CDATA[
arXiv:2512.15067v1 Announce Type: cross 
Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems</title>
<link>https://arxiv.org/abs/2512.15068</link>
<guid>https://arxiv.org/abs/2512.15068</guid>
<content:encoded><![CDATA[
arXiv:2512.15068v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMMD: A pose-guided multi-view multi-modal diffusion for person generation</title>
<link>https://arxiv.org/abs/2512.15069</link>
<guid>https://arxiv.org/abs/2512.15069</guid>
<content:encoded><![CDATA[
arXiv:2512.15069v1 Announce Type: cross 
Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Return on Security Controls in LLM Systems</title>
<link>https://arxiv.org/abs/2512.15081</link>
<guid>https://arxiv.org/abs/2512.15081</guid>
<content:encoded><![CDATA[
arXiv:2512.15081v1 Announce Type: cross 
Abstract: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption</title>
<link>https://arxiv.org/abs/2512.15112</link>
<guid>https://arxiv.org/abs/2512.15112</guid>
<content:encoded><![CDATA[
arXiv:2512.15112v1 Announce Type: cross 
Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models</title>
<link>https://arxiv.org/abs/2512.15115</link>
<guid>https://arxiv.org/abs/2512.15115</guid>
<content:encoded><![CDATA[
arXiv:2512.15115v1 Announce Type: cross 
Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation</title>
<link>https://arxiv.org/abs/2512.15116</link>
<guid>https://arxiv.org/abs/2512.15116</guid>
<content:encoded><![CDATA[
arXiv:2512.15116v1 Announce Type: cross 
Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable</title>
<link>https://arxiv.org/abs/2512.15117</link>
<guid>https://arxiv.org/abs/2512.15117</guid>
<content:encoded><![CDATA[
arXiv:2512.15117v1 Announce Type: cross 
Abstract: General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management</title>
<link>https://arxiv.org/abs/2512.15119</link>
<guid>https://arxiv.org/abs/2512.15119</guid>
<content:encoded><![CDATA[
arXiv:2512.15119v1 Announce Type: cross 
Abstract: Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Reward Shaping from Multi-Objective Human Heuristics</title>
<link>https://arxiv.org/abs/2512.15120</link>
<guid>https://arxiv.org/abs/2512.15120</guid>
<content:encoded><![CDATA[
arXiv:2512.15120v1 Announce Type: cross 
Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens</title>
<link>https://arxiv.org/abs/2512.15133</link>
<guid>https://arxiv.org/abs/2512.15133</guid>
<content:encoded><![CDATA[
arXiv:2512.15133v1 Announce Type: cross 
Abstract: Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</title>
<link>https://arxiv.org/abs/2512.15134</link>
<guid>https://arxiv.org/abs/2512.15134</guid>
<content:encoded><![CDATA[
arXiv:2512.15134v1 Announce Type: cross 
Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning</title>
<link>https://arxiv.org/abs/2512.15149</link>
<guid>https://arxiv.org/abs/2512.15149</guid>
<content:encoded><![CDATA[
arXiv:2512.15149v1 Announce Type: cross 
Abstract: Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model's generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers</title>
<link>https://arxiv.org/abs/2512.15163</link>
<guid>https://arxiv.org/abs/2512.15163</guid>
<content:encoded><![CDATA[
arXiv:2512.15163v1 Announce Type: cross 
Abstract: Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEER: Draft with Diffusion, Verify with Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.15176</link>
<guid>https://arxiv.org/abs/2512.15176</guid>
<content:encoded><![CDATA[
arXiv:2512.15176v1 Announce Type: cross 
Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Governing rapid technological change: Policy Delphi on the future of European AI governance</title>
<link>https://arxiv.org/abs/2512.15196</link>
<guid>https://arxiv.org/abs/2512.15196</guid>
<content:encoded><![CDATA[
arXiv:2512.15196v1 Announce Type: cross 
Abstract: The rapid advancements in artificial intelligence (AI) present unique challenges for policymakers that seek to govern the technology. In this context, the Delphi method has become an established way to identify consensus and disagreement on emerging technological issues among experts in the field of futures studies and foresight. The aim of this article is twofold: first, it examines key tensions experts see in the development of AI governance in Europe, and second, it reflects on the Delphi method's capacity to inform anticipatory governance of emerging technologies like AI based on these insights. The analysis is based on the results of a two-round Policy Delphi study on the future of AI governance with European policymakers, researchers and NGOs, conducted in mid-2024. The Policy Delphi proved useful in revealing diverse perspectives on European AI governance, drawing out a consensus that future-proof AI regulation will likely depend more on practical implementation and enforcement of legislation than on its technical specifics or scope. Furthermore, the study identified a desirability-probability gap in AI governance: desirable policy directions, like greater citizen participation, were perceived as less probable and feasible. This highlights a tension between desirable regulatory oversight and the practical difficulty for regulation to keep up with technological change.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA</title>
<link>https://arxiv.org/abs/2512.15219</link>
<guid>https://arxiv.org/abs/2512.15219</guid>
<content:encoded><![CDATA[
arXiv:2512.15219v1 Announce Type: cross 
Abstract: Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct "brother" relations, 2-hop for indirect "father-son" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a "question-paths-answer" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024</title>
<link>https://arxiv.org/abs/2512.15226</link>
<guid>https://arxiv.org/abs/2512.15226</guid>
<content:encoded><![CDATA[
arXiv:2512.15226v1 Announce Type: cross 
Abstract: This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</title>
<link>https://arxiv.org/abs/2512.15249</link>
<guid>https://arxiv.org/abs/2512.15249</guid>
<content:encoded><![CDATA[
arXiv:2512.15249v1 Announce Type: cross 
Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $\Delta$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $\Delta$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis</title>
<link>https://arxiv.org/abs/2512.15250</link>
<guid>https://arxiv.org/abs/2512.15250</guid>
<content:encoded><![CDATA[
arXiv:2512.15250v1 Announce Type: cross 
Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments</title>
<link>https://arxiv.org/abs/2512.15258</link>
<guid>https://arxiv.org/abs/2512.15258</guid>
<content:encoded><![CDATA[
arXiv:2512.15258v1 Announce Type: cross 
Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15274</link>
<guid>https://arxiv.org/abs/2512.15274</guid>
<content:encoded><![CDATA[
arXiv:2512.15274v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions</title>
<link>https://arxiv.org/abs/2512.15286</link>
<guid>https://arxiv.org/abs/2512.15286</guid>
<content:encoded><![CDATA[
arXiv:2512.15286v1 Announce Type: cross 
Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting</title>
<link>https://arxiv.org/abs/2512.15308</link>
<guid>https://arxiv.org/abs/2512.15308</guid>
<content:encoded><![CDATA[
arXiv:2512.15308v1 Announce Type: cross 
Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies</title>
<link>https://arxiv.org/abs/2512.15312</link>
<guid>https://arxiv.org/abs/2512.15312</guid>
<content:encoded><![CDATA[
arXiv:2512.15312v1 Announce Type: cross 
Abstract: Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</title>
<link>https://arxiv.org/abs/2512.15315</link>
<guid>https://arxiv.org/abs/2512.15315</guid>
<content:encoded><![CDATA[
arXiv:2512.15315v1 Announce Type: cross 
Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection</title>
<link>https://arxiv.org/abs/2512.15325</link>
<guid>https://arxiv.org/abs/2512.15325</guid>
<content:encoded><![CDATA[
arXiv:2512.15325v1 Announce Type: cross 
Abstract: Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-based module for accurately reading linear scales in a laboratory</title>
<link>https://arxiv.org/abs/2512.15327</link>
<guid>https://arxiv.org/abs/2512.15327</guid>
<content:encoded><![CDATA[
arXiv:2512.15327v1 Announce Type: cross 
Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality</title>
<link>https://arxiv.org/abs/2512.15343</link>
<guid>https://arxiv.org/abs/2512.15343</guid>
<content:encoded><![CDATA[
arXiv:2512.15343v1 Announce Type: cross 
Abstract: The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery</title>
<link>https://arxiv.org/abs/2512.15344</link>
<guid>https://arxiv.org/abs/2512.15344</guid>
<content:encoded><![CDATA[
arXiv:2512.15344v1 Announce Type: cross 
Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial versification in portuguese as a jailbreak operator in LLMs</title>
<link>https://arxiv.org/abs/2512.15353</link>
<guid>https://arxiv.org/abs/2512.15353</guid>
<content:encoded><![CDATA[
arXiv:2512.15353v1 Announce Type: cross 
Abstract: Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[
arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Recognition in Signers</title>
<link>https://arxiv.org/abs/2512.15376</link>
<guid>https://arxiv.org/abs/2512.15376</guid>
<content:encoded><![CDATA[
arXiv:2512.15376v1 Announce Type: cross 
Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[
arXiv:2512.15396v1 Announce Type: cross 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.15430</link>
<guid>https://arxiv.org/abs/2512.15430</guid>
<content:encoded><![CDATA[
arXiv:2512.15430v1 Announce Type: cross 
Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Double Horizon Model-Based Policy Optimization</title>
<link>https://arxiv.org/abs/2512.15439</link>
<guid>https://arxiv.org/abs/2512.15439</guid>
<content:encoded><![CDATA[
arXiv:2512.15439v1 Announce Type: cross 
Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Assessing the Relevance of Code Reviews Authored by Generative Models</title>
<link>https://arxiv.org/abs/2512.15466</link>
<guid>https://arxiv.org/abs/2512.15466</guid>
<content:encoded><![CDATA[
arXiv:2512.15466v1 Announce Type: cross 
Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?</title>
<link>https://arxiv.org/abs/2512.15468</link>
<guid>https://arxiv.org/abs/2512.15468</guid>
<content:encoded><![CDATA[
arXiv:2512.15468v1 Announce Type: cross 
Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Geometric Inductive Bias for Object Centric Dynamics</title>
<link>https://arxiv.org/abs/2512.15493</link>
<guid>https://arxiv.org/abs/2512.15493</guid>
<content:encoded><![CDATA[
arXiv:2512.15493v1 Announce Type: cross 
Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title>
<link>https://arxiv.org/abs/2512.15503</link>
<guid>https://arxiv.org/abs/2512.15503</guid>
<content:encoded><![CDATA[
arXiv:2512.15503v1 Announce Type: cross 
Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems</title>
<link>https://arxiv.org/abs/2512.15526</link>
<guid>https://arxiv.org/abs/2512.15526</guid>
<content:encoded><![CDATA[
arXiv:2512.15526v1 Announce Type: cross 
Abstract: Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Conditioned UNet for Music Source Separation</title>
<link>https://arxiv.org/abs/2512.15532</link>
<guid>https://arxiv.org/abs/2512.15532</guid>
<content:encoded><![CDATA[
arXiv:2512.15532v1 Announce Type: cross 
Abstract: In this paper we propose a conditioned UNet for Music Source Separation (MSS). MSS is generally performed by multi-output neural networks, typically UNets, with each output representing a particular stem from a predefined instrument vocabulary. In contrast, conditioned MSS networks accept an audio query related to a stem of interest alongside the signal from which that stem is to be extracted. Thus, a strict vocabulary is not required and this enables more realistic tasks in MSS. The potential of conditioned approaches for such tasks has been somewhat hidden due to a lack of suitable data, an issue recently addressed with the MoisesDb dataset. A recent method, Banquet, employs this dataset with promising results seen on larger vocabularies. Banquet uses Bandsplit RNN rather than a UNet and the authors state that UNets should not be suitable for conditioned MSS. We counter this argument and propose QSCNet, a novel conditioned UNet for MSS that integrates network conditioning elements in the Sparse Compressed Network for MSS. We find QSCNet to outperform Banquet by over 1dB SNR on a couple of MSS tasks, while using less than half the number of parameters.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Smoothing is N-simplicial Attention?</title>
<link>https://arxiv.org/abs/2512.15600</link>
<guid>https://arxiv.org/abs/2512.15600</guid>
<content:encoded><![CDATA[
arXiv:2512.15600v1 Announce Type: cross 
Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Metrics for Safety with LLM-as-Judges</title>
<link>https://arxiv.org/abs/2512.15617</link>
<guid>https://arxiv.org/abs/2512.15617</guid>
<content:encoded><![CDATA[
arXiv:2512.15617v1 Announce Type: cross 
Abstract: LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness</title>
<link>https://arxiv.org/abs/2512.15634</link>
<guid>https://arxiv.org/abs/2512.15634</guid>
<content:encoded><![CDATA[
arXiv:2512.15634v1 Announce Type: cross 
Abstract: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&amp;A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</title>
<link>https://arxiv.org/abs/2512.15635</link>
<guid>https://arxiv.org/abs/2512.15635</guid>
<content:encoded><![CDATA[
arXiv:2512.15635v1 Announce Type: cross 
Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
<link>https://arxiv.org/abs/2512.15649</link>
<guid>https://arxiv.org/abs/2512.15649</guid>
<content:encoded><![CDATA[
arXiv:2512.15649v1 Announce Type: cross 
Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning</title>
<link>https://arxiv.org/abs/2512.15658</link>
<guid>https://arxiv.org/abs/2512.15658</guid>
<content:encoded><![CDATA[
arXiv:2512.15658v1 Announce Type: cross 
Abstract: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</title>
<link>https://arxiv.org/abs/2512.15674</link>
<guid>https://arxiv.org/abs/2512.15674</guid>
<content:encoded><![CDATA[
arXiv:2512.15674v1 Announce Type: cross 
Abstract: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.15687</link>
<guid>https://arxiv.org/abs/2512.15687</guid>
<content:encoded><![CDATA[
arXiv:2512.15687v1 Announce Type: cross 
Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BashArena: A Control Setting for Highly Privileged AI Agents</title>
<link>https://arxiv.org/abs/2512.15688</link>
<guid>https://arxiv.org/abs/2512.15688</guid>
<content:encoded><![CDATA[
arXiv:2512.15688v1 Announce Type: cross 
Abstract: Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[
arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatia: Video Generation with Updatable Spatial Memory</title>
<link>https://arxiv.org/abs/2512.15716</link>
<guid>https://arxiv.org/abs/2512.15716</guid>
<content:encoded><![CDATA[
arXiv:2512.15716v1 Announce Type: cross 
Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrafficGamer: Reliable and Flexible Traffic Simulation for Safety-Critical Scenarios with Game-Theoretic Oracles</title>
<link>https://arxiv.org/abs/2408.15538</link>
<guid>https://arxiv.org/abs/2408.15538</guid>
<content:encoded><![CDATA[
arXiv:2408.15538v3 Announce Type: replace 
Abstract: While modern Autonomous Vehicle (AV) systems can develop reliable driving policies under regular traffic conditions, they frequently struggle with safety-critical traffic scenarios. This difficulty primarily arises from the rarity of such scenarios in driving datasets and the complexities associated with predictive modeling of multiple vehicles. Effectively simulating safety-critical traffic situations is therefore a crucial challenge. In this paper, we introduce TrafficGamer, which facilitates game-theoretic traffic simulation by viewing common road driving as a multi-agent game. When we evaluate the empirical performance across various real-world datasets, TrafficGamer ensures both the fidelity, exploitability, and diversity of the simulated scenarios, guaranteeing that they not only statically align with real-world traffic distribution but also efficiently capture equilibria for representing safety-critical scenarios involving multiple agents compared with other methods. Additionally, the results demonstrate that TrafficGamer provides highly flexible simulations across various contexts. Specifically, we demonstrate that the generated scenarios can dynamically adapt to equilibria of varying tightness by configuring risk-sensitive constraints during optimization. We have provided a demo webpage at: https://anonymous.4open.science/api/repo/trafficgamer-demo-1EE0/file/index.html.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model-Based Intelligent Antenna Design System</title>
<link>https://arxiv.org/abs/2504.18271</link>
<guid>https://arxiv.org/abs/2504.18271</guid>
<content:encoded><![CDATA[
arXiv:2504.18271v2 Announce Type: replace 
Abstract: Antenna simulation typically involves modeling and optimization, which are time-consuming and labor-intensive, slowing down antenna analysis and design. This paper presents a prototype of a large language model (LLM)-based antenna design system (LADS) to assist in antenna simulation. LADS generates antenna models with textual descriptions and images extracted from academic papers, patents, and technical reports (either one or multiple), and it interacts with engineers to iteratively refine the designs. After that, LADS configures and runs an optimizer to meet the design specifications. The effectiveness of LADS is demonstrated by a monopole slotted antenna generated from images and descriptions from the literature. To improve gain stability across the 3.1-10.6 GHz ultra-wide band, LADS modifies the cross-slot into an H-slot and changes substrate material, followed by parameter optimization. As a result, the gain variation is reduced while maintaining the same gain level. The LLM-enabled antenna modeling (LEAM) is available at: https://github.com/TaoWu974/LEAM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.20226</link>
<guid>https://arxiv.org/abs/2507.20226</guid>
<content:encoded><![CDATA[
arXiv:2507.20226v2 Announce Type: replace 
Abstract: Homomorphism is a key mapping technique between graphs that preserves their structure. Given a graph and a pattern, the subgraph homomorphism problem involves finding a mapping from the pattern to the graph, ensuring that adjacent vertices in the pattern are mapped to adjacent vertices in the graph. Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism allows multiple vertices in the pattern to map to the same vertex in the graph, making it more complex. We propose HFrame, the first graph neural network-based framework for subgraph homomorphism, which integrates traditional algorithms with machine learning techniques. We demonstrate that HFrame outperforms standard graph neural networks by being able to distinguish more graph pairs where the pattern is not homomorphic to the graph. Additionally, we provide a generalization error bound for HFrame. Through experiments on both real-world and synthetic graphs, we show that HFrame is up to 101.91 times faster than exact matching algorithms and achieves an average accuracy of 0.962.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is Your AI Agent Buying? Evaluation, Biases, Model Dependence, &amp; Emerging Implications for Agentic E-Commerce</title>
<link>https://arxiv.org/abs/2508.02630</link>
<guid>https://arxiv.org/abs/2508.02630</guid>
<content:encoded><![CDATA[
arXiv:2508.02630v3 Announce Type: replace 
Abstract: Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, AI agents can parse webpages or leverage APIs to view, evaluate and choose products. We investigate the behavior of AI agents using ACES, a provider-agnostic framework for auditing agent decision-making. We reveal that agents can exhibit choice homogeneity, often concentrating demand on a few ``modal'' products while ignoring others entirely. Yet, these preferences are unstable: model updates can drastically reshuffle market shares. Furthermore, randomized trials show that while agents have improved over time on simple tasks with a clearly identified best choice, they exhibit strong position biases -- varying across providers and model versions, and persisting even in text-only "headless" interfaces -- undermining any universal notion of a ``top'' rank. Agents also consistently penalize sponsored tags while rewarding platform endorsements, and sensitivities to price, ratings, and reviews vary sharply across models. Finally, we demonstrate that sellers can respond: a seller-side agent making simple, query-conditional description tweaks can drive significant gains in market share. These findings reveal that agentic markets are volatile and fundamentally different from human-centric commerce, highlighting the need for continuous auditing and raising questions for platform design, seller strategy and regulation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
arXiv:2508.15126v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content.
  Code: https://github.com/aixiv-org
  aiXiv: https://aixiv.science
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Need for Verification in AI-Driven Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.01398</link>
<guid>https://arxiv.org/abs/2509.01398</guid>
<content:encoded><![CDATA[
arXiv:2509.01398v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory</title>
<link>https://arxiv.org/abs/2511.18723</link>
<guid>https://arxiv.org/abs/2511.18723</guid>
<content:encoded><![CDATA[
arXiv:2511.18723v3 Announce Type: replace 
Abstract: Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&amp;B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&amp;B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation</title>
<link>https://arxiv.org/abs/2511.19895</link>
<guid>https://arxiv.org/abs/2511.19895</guid>
<content:encoded><![CDATA[
arXiv:2511.19895v2 Announce Type: replace 
Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title>
<link>https://arxiv.org/abs/2512.02080</link>
<guid>https://arxiv.org/abs/2512.02080</guid>
<content:encoded><![CDATA[
arXiv:2512.02080v2 Announce Type: replace 
Abstract: The integration of Formal Verification tools with Large Language Models (LLMs) offers a path to scale software verification beyond manual workflows. However, current methods remain unreliable: without a solid theoretical footing, the refinement process acts as a black box that may oscillate, loop, or diverge. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination in multi-stage verification pipelines. We model the interaction not as a generic loop, but as a sequential absorbing Markov Chain comprising four essential engineering stages: \texttt{CodeGen}, \texttt{Compilation}, \texttt{InvariantSynth}, and \texttt{SMTSolving}. We prove that for any non-zero stage success probability ($\delta > 0$), the system reaches the \texttt{Verified} state almost surely. Furthermore, because of the sequential nature of the pipeline, we derive a precise latency bound of $\mathbb{E}[n] \leq 4/\delta$. We stress-tested this prediction in an extensive empirical campaign comprising over 90,000 trials. The results match the theory with striking consistency: every run reached verification, and the empirical convergence factor clustered tightly around $C_f\approx 1.0$, confirming that the $4/\delta$ bound accurately mirrors system behavior rather than serving as a loose buffer. Based on this data, we identify three distinct operating zones -- marginal, practical, and high-performance -- and propose a dynamic calibration strategy to handle parameter drift in real-world environments. Together, these contributions replace heuristic guesswork with a rigorous architectural foundation, enabling predictable resource planning and performance budgeting for safety-critical software.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Science of Scaling Agent Systems</title>
<link>https://arxiv.org/abs/2512.08296</link>
<guid>https://arxiv.org/abs/2512.08296</guid>
<content:encoded><![CDATA[
arXiv:2512.08296v2 Announce Type: replace 
Abstract: Agents, language model-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored. We address this by deriving quantitative scaling principles for agent systems. We first formalize a definition for agentic evaluation and characterize scaling laws as the interplay between agent quantity, coordination structure, model capability, and task properties. We evaluate this across four benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. With five canonical agent architectures (Single-Agent and four Multi-Agent Systems: Independent, Centralized, Decentralized, Hybrid), instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations. We derive a predictive model using coordination metrics, that achieves cross-validated R^2=0.524, enabling prediction on unseen task domains. We identify three effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.8% on parallelizable tasks, while decentralized coordination excels on web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, every multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations. Out-of-sample validation on GPT-5.2, achieves MAE=0.071 and confirms four of five scaling principles generalize to unseen frontier models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchOGD: Memory-Efficient Continual Learning</title>
<link>https://arxiv.org/abs/2305.16424</link>
<guid>https://arxiv.org/abs/2305.16424</guid>
<content:encoded><![CDATA[
arXiv:2305.16424v3 Announce Type: replace-cross 
Abstract: When machine learning models are trained continually on a sequence of tasks, they are often liable to forget what they learned on previous tasks--a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper develops a memory-efficient solution to catastrophic forgetting using the idea of matrix sketching, in the context of a simple continual learning algorithm known as orthogonal gradient descent (OGD). OGD finds weight updates that aim to preserve performance on prior datapoints, using gradients of the model on those datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over long time horizons. To address this problem, we propose SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fixed, user-determined size. In contrast to existing memory-efficient variants of OGD, SketchOGD runs online without the need for advance knowledge of the total number of tasks, is simple to implement, and is more amenable to analysis. We provide theoretical guarantees on the approximation error of the relevant sketches under a novel metric suited to the downstream task of OGD. Experimentally, we find that SketchOGD tends to outperform current state-of-the-art variants of OGD given a fixed memory budget.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enigma: Application-Layer Privacy for Quantum Optimization on Untrusted Computers</title>
<link>https://arxiv.org/abs/2311.13546</link>
<guid>https://arxiv.org/abs/2311.13546</guid>
<content:encoded><![CDATA[
arXiv:2311.13546v2 Announce Type: replace-cross 
Abstract: The Early Fault-Tolerant (EFT) era is emerging, where modest Quantum Error Correction (QEC) can enable quantum utility before full-scale fault tolerance. Quantum optimization is a leading candidate for early applications, but protecting these workloads is critical since they will run on expensive cloud services where providers could learn sensitive problem details. Experience with classical computing systems has shown that treating security as an afterthought can lead to significant vulnerabilities. Thus, we must address the security implications of quantum computing before widespread adoption. However, current Secure Quantum Computing (SQC) approaches, although theoretically promising, are impractical in the EFT era: blind quantum computing requires large-scale quantum networks, and quantum homomorphic encryption depends on full QEC.
  We propose application-specific SQC, a principle that applies obfuscation at the application layer to enable practical deployment while remaining agnostic to algorithms, computing models, and hardware architectures. We present Enigma, the first realization of this principle for quantum optimization. Enigma integrates three complementary obfuscations: ValueGuard scrambles coefficients, StructureCamouflage inserts decoys, and TopologyTrimmer prunes variables. These techniques guarantee recovery of original solutions, and their stochastic nature resists repository-matching attacks. Evaluated against seven state-of-the-art AI models across five representative graph families, even combined adversaries, under a conservatively strong attacker model, identify the correct problem within their top five guesses in only 4.4% of cases. The protections come at the cost of problem size and T-gate counts increasing by averages of 1.07x and 1.13x, respectively, with both obfuscation and decoding completing within seconds for large-scale problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pretraining to Privacy: Federated Ultrasound Foundation Model with Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2411.16380</link>
<guid>https://arxiv.org/abs/2411.16380</guid>
<content:encoded><![CDATA[
arXiv:2411.16380v2 Announce Type: replace-cross 
Abstract: Ultrasound imaging is widely used in clinical diagnosis due to its non-invasive nature and real-time capabilities. However, traditional ultrasound diagnostics relies heavily on physician expertise and is often hampered by suboptimal image quality, leading to potential diagnostic errors. While artificial intelligence (AI) offers a promising solution to enhance clinical diagnosis by detecting abnormalities across various imaging modalities, existing AI methods for ultrasound face two major challenges. First, they typically require vast amounts of labeled medical data, raising serious concerns regarding patient privacy. Second, most models are designed for specific tasks, which restricts their broader clinical utility. To overcome these challenges, we present UltraFedFM, an innovative privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively pre-trained using federated learning across 16 distributed medical institutions in 9 countries, leveraging a dataset of over 1 million ultrasound images covering 19 organs and 10 ultrasound modalities. This extensive and diverse data, combined with a secure training framework, enables UltraFedFM to exhibit strong generalization and diagnostic capabilities. It achieves an average area under the receiver operating characteristic curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level ultrasonographers (4-8 years of experience) and matches the performance of expert-level sonographers (10+ years of experience) in the joint diagnosis of 8 common systemic diseases.c These findings indicate that UltraFedFM can significantly enhance clinical diagnostics while safeguarding patient privacy, marking a significant advancement in AI-driven ultrasound imaging for future clinical applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset</title>
<link>https://arxiv.org/abs/2411.17645</link>
<guid>https://arxiv.org/abs/2411.17645</guid>
<content:encoded><![CDATA[
arXiv:2411.17645v4 Announce Type: replace-cross 
Abstract: The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs). We implemented a data pre-processing and curation pipeline that transforms the raw EHR data into a structured format suitable for developing predictive models focused on data fairness, accountability and transparency. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Pairwise XGBoost models are trained using this framework to differentiate UTI risk categories with explainable AI techniques applied to identify key predictors and support interpretability. Our findings reveal differences in clinical and demographic predictors across risk groups. While this study highlights the potential of AI-driven insights to support UTI clinical decision-making, further investigation of patient sub-strata and extensive validation are needed to ensure robustness and applicability in clinical practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveGNN: Integrating Graph Neural Networks and Transformers for Decay-Aware Classification of Irregular Clinical Time-Series</title>
<link>https://arxiv.org/abs/2412.10621</link>
<guid>https://arxiv.org/abs/2412.10621</guid>
<content:encoded><![CDATA[
arXiv:2412.10621v2 Announce Type: replace-cross 
Abstract: Clinical time series are often irregularly sampled, with varying sensor frequencies, missing observations, and misaligned timestamps. Prior approaches typically address these irregularities by interpolating data into regular sequences, thereby introducing bias, or by generating inconsistent and uninterpretable relationships across sensor measurements, complicating the accurate learning of both intra-series and inter-series dependencies. We introduce WaveGNN, a model that operates directly on irregular multivariate time series without interpolation or conversion to a regular representation. WaveGNN combines a decay-aware Transformer to capture intra-series dynamics with a sample-specific graph neural network that models both short-term and long-term inter-sensor relationships. Therefore, it generates a single, sparse, and interpretable graph per sample. Across multiple benchmark datasets (P12, P19, MIMIC-III, and PAM), WaveGNN delivers consistently strong performance, whereas other state-of-the-art baselines tend to perform well on some datasets or tasks but poorly on others. While WaveGNN does not necessarily surpass every method in every case, its consistency and robustness across diverse settings set it apart. Moreover, the learned graphs align well with known physiological structures, enhancing interpretability and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[
arXiv:2501.13439v2 Announce Type: replace-cross 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rigid-Deformation Decomposition AI Framework for 3D Spatio-Temporal Prediction of Vehicle Collision Dynamics</title>
<link>https://arxiv.org/abs/2503.19712</link>
<guid>https://arxiv.org/abs/2503.19712</guid>
<content:encoded><![CDATA[
arXiv:2503.19712v2 Announce Type: replace-cross 
Abstract: This study presents a rigid-deformation decomposition framework for vehicle collision dynamics that mitigates the spectral bias of implicit neural representations, that is, coordinate-based neural networks that directly map spatio-temporal coordinates to physical fields. We introduce a hierarchical architecture that decouples global rigid-body motion from local deformation using two scale-specific networks, denoted as RigidNet and DeformationNet. To enforce kinematic separation between the two components, we adopt a frozen-anchor training strategy combined with a quaternion-incremental scheme. This strategy alleviates the kinematic instability observed in joint training and yields a 29.8% reduction in rigid-body motion error compared with conventional direct prediction schemes. The stable rigid-body anchor improves the resolution of high-frequency structural buckling, which leads to a 17.2% reduction in the total interpolation error. Loss landscape analysis indicates that the decomposition smooths the optimization surface, which enhances robustness to distribution shifts in angular extrapolation and yields a 46.6% reduction in error. To assess physical validity beyond numerical accuracy, we benchmark the decomposed components against an oracle model that represents an upper bound on performance. The proposed framework recovers 92% of the directional correlation between rigid and deformation components and 96% of the spatial deformation localization accuracy relative to the oracle, while tracking the temporal energy dynamics with an 8 ms delay. These results demonstrate that rigid-deformation decomposition enables accurate and physically interpretable predictions for nonlinear collision dynamics.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Tools: Generative AI as Epistemic Infrastructure in Education</title>
<link>https://arxiv.org/abs/2504.06928</link>
<guid>https://arxiv.org/abs/2504.06928</guid>
<content:encoded><![CDATA[
arXiv:2504.06928v2 Announce Type: replace-cross 
Abstract: AI systems are increasingly embedded in practices where humans have traditionally exercised epistemic agency, the capacity to actively engage in knowledge formation and validation. This paper argues that understanding AI's impact on epistemic agency requires analyzing these systems as epistemic infrastructures rather than as neutral tools. Drawing on theories of technological mediation and distributed cognition, I advance a framework that foregrounds how AI systems reconfigure the conditions under which epistemic agency can be exercised. The framework specifies three analytical conditions: affordances for skilled epistemic actions, support for epistemic sensitivity, and implications for habit formation. I apply this framework to AI systems deployed in education, a domain where epistemic agency is both professionally essential and ethically significant. Analysis of AI lesson planning and feedback tools reveals patterns of epistemic substitution: while useful for efficiently handling teaching tasks, these systems perform cognitive operations without sustaining skilled epistemic actions, epistemic sensitivity, or virtuous habit formation, potentially preventing the cultivation of professional judgment that relies on these practices. The findings contribute to philosophical debates about AI and human agency by specifying mechanisms through which infrastructural embedding shapes epistemic possibilities, and offer design principles for AI systems that sustain rather than supplant human epistemic agency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization</title>
<link>https://arxiv.org/abs/2504.13460</link>
<guid>https://arxiv.org/abs/2504.13460</guid>
<content:encoded><![CDATA[
arXiv:2504.13460v4 Announce Type: replace-cross 
Abstract: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observation Compression in Rate-Limited Closed-Loop Distributed ISAC Systems: From Signal Reconstruction to Control</title>
<link>https://arxiv.org/abs/2505.01780</link>
<guid>https://arxiv.org/abs/2505.01780</guid>
<content:encoded><![CDATA[
arXiv:2505.01780v2 Announce Type: replace-cross 
Abstract: In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safer Prompts: Reducing Risks from Memorization in Visual Generative AI</title>
<link>https://arxiv.org/abs/2505.03338</link>
<guid>https://arxiv.org/abs/2505.03338</guid>
<content:encoded><![CDATA[
arXiv:2505.03338v2 Announce Type: replace-cross 
Abstract: Visual Generative AI models have demonstrated remarkable capability in generating high-quality images from user inputs like text prompts. However, because these models have billions of parameters, they risk memorizing certain parts of the training data and reproducing the memorized content. Memorization often raises concerns about safety of such models -- usually involving intellectual property (IP) infringement risk -- and deters their large scale adoption. In this paper, we evaluate the effectiveness of prompt engineering techniques in reducing memorization risk in image generation. Our findings demonstrate the effectiveness of prompt engineering in reducing the similarity between generated images and the training data of diffusion models, while maintaining relevance and aestheticity of the generated output.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aligned Protein Language Model</title>
<link>https://arxiv.org/abs/2505.16896</link>
<guid>https://arxiv.org/abs/2505.16896</guid>
<content:encoded><![CDATA[
arXiv:2505.16896v2 Announce Type: replace-cross 
Abstract: Protein language models (pLMs) pre-trained on vast protein sequence databases excel at various downstream tasks but often lack the structural knowledge essential for some biological applications. To address this, we introduce a method to enrich pLMs with structural knowledge by leveraging pre-trained protein graph neural networks (pGNNs). First, a latent-level contrastive learning task aligns residue representations from pLMs with those from pGNNs across multiple proteins, injecting inter-protein structural information. Additionally, a physical-level task integrates intra-protein information by training pLMs to predict structure tokens. Together, the proposed dual-task framework effectively incorporates both inter- and intra-protein structural knowledge into pLMs. Given the variability in the quality of protein structures in PDB, we further introduce a residue loss selection module that uses a small model trained on high-quality structures to select reliable yet challenging residue losses for the pLM to learn. Applying our structure alignment method as a simple, lightweight post-training step to the state-of-the-art ESM2 and AMPLIFY yields notable performance gains. These improvements are consistent across a wide range of tasks, including substantial gains in deep mutational scanning (DMS) fitness prediction and a 59% increase in P@L for ESM2 650M contact prediction on CASP16. Furthermore, we demonstrate that these performance gains are robust, scaling with model sizes from 8M to 650M and extending to different downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Haystack: Smaller Needles are More Difficult for LLMs to Find</title>
<link>https://arxiv.org/abs/2505.18148</link>
<guid>https://arxiv.org/abs/2505.18148</guid>
<content:encoded><![CDATA[
arXiv:2505.18148v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) face significant challenges with needle-in-ahaystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size, the length of the answer-containing document, has received little attention. We present the first systematic study of gold context size in long-context question answering, spanning three diverse benchmarks (general knowledge, biomedical reasoning, and mathematical reasoning), eleven state-of-the-art LLMs (including recent reasoning models), and more than 150K controlled runs. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This effect persists under rigorous confounder analysis: even after controlling for gold context position, answer token repetition, gold-to-distractor ratio, distractor volume, and domain specificity, gold context size remains a decisive, independent predictor of success. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v2 Announce Type: replace-cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional predictive coding</title>
<link>https://arxiv.org/abs/2505.23415</link>
<guid>https://arxiv.org/abs/2505.23415</guid>
<content:encoded><![CDATA[
arXiv:2505.23415v2 Announce Type: replace-cross 
Abstract: Predictive coding (PC) is an influential computational model of visual learning and inference in the brain. Classical PC was proposed as a top-down generative model, where the brain actively predicts upcoming visual inputs, and inference minimises the prediction errors. Recent studies have also shown that PC can be formulated as a discriminative model, where sensory inputs predict neural activities in a feedforward manner. However, experimental evidence suggests that the brain employs both generative and discriminative inference, while unidirectional PC models show degraded performance in tasks requiring bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC model that incorporates both generative and discriminative inference while maintaining a biologically plausible circuit implementation. We show that bPC matches or outperforms unidirectional models in their specialised generative or discriminative tasks, by developing an energy landscape that simultaneously suits both tasks. We also demonstrate bPC's superior performance in two biologically relevant tasks including multimodal learning and inference with missing information, suggesting that bPC resembles biological visual inference more closely.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</title>
<link>https://arxiv.org/abs/2506.02976</link>
<guid>https://arxiv.org/abs/2506.02976</guid>
<content:encoded><![CDATA[
arXiv:2506.02976v3 Announce Type: replace-cross 
Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title>
<link>https://arxiv.org/abs/2506.09677</link>
<guid>https://arxiv.org/abs/2506.09677</guid>
<content:encoded><![CDATA[
arXiv:2506.09677v2 Announce Type: replace-cross 
Abstract: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness</title>
<link>https://arxiv.org/abs/2507.02314</link>
<guid>https://arxiv.org/abs/2507.02314</guid>
<content:encoded><![CDATA[
arXiv:2507.02314v3 Announce Type: replace-cross 
Abstract: Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Action Smoothness for a Cascaded Online Learning Flight Control System</title>
<link>https://arxiv.org/abs/2507.04346</link>
<guid>https://arxiv.org/abs/2507.04346</guid>
<content:encoded><![CDATA[
arXiv:2507.04346v3 Announce Type: replace-cross 
Abstract: This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs and Fuzzing in Tandem: A New Approach to Automatically Generating Weakest Preconditions</title>
<link>https://arxiv.org/abs/2507.05272</link>
<guid>https://arxiv.org/abs/2507.05272</guid>
<content:encoded><![CDATA[
arXiv:2507.05272v2 Announce Type: replace-cross 
Abstract: The weakest precondition (WP) of a program describes the largest set of initial states from which all terminating executions of the program satisfy a given postcondition. The generation of WPs is an important task with practical applications in areas ranging from verification to run-time error checking. This paper proposes the combination of Large Language Models (LLMs) and fuzz testing for generating WPs. In pursuit of this goal, we introduce \emph{Fuzzing Guidance} (FG); FG acts as a means of directing LLMs towards correct WPs using program execution feedback. FG utilises fuzz testing for approximately checking the validity and weakness of candidate WPs, this information is then fed back to the LLM as a means of context refinement. We demonstrate the effectiveness of our approach on a comprehensive benchmark set of deterministic array programs in Java. Our experiments indicate that LLMs are capable of producing viable candidate WPs, and that this ability can be practically enhanced through FG.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI</title>
<link>https://arxiv.org/abs/2507.06092</link>
<guid>https://arxiv.org/abs/2507.06092</guid>
<content:encoded><![CDATA[
arXiv:2507.06092v2 Announce Type: replace-cross 
Abstract: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion</title>
<link>https://arxiv.org/abs/2507.06849</link>
<guid>https://arxiv.org/abs/2507.06849</guid>
<content:encoded><![CDATA[
arXiv:2507.06849v2 Announce Type: replace-cross 
Abstract: Neural network (NN)-based Digital Predistortion (DPD) has demonstrated superior performance in improving signal quality in wideband radio frequency (RF) power amplifiers (PAs) employing complex modulation. However, NN DPDs usually rely on a large number of parameters for effective linearization and can significantly contribute to the energy consumption of the digital back-end in RF systems. This paper presents OpenDPDv2, an open-source, end-to-end framework that unifies PA modeling, NN-DPD learning, and deployment-oriented model optimization to reduce inference energy while preserving linearization performance. OpenDPDv2 introduces TRes-DeltaGRU, a delta-RNN DPD architecture with a lightweight temporal residual path that improves robustness under aggressive temporal sparsity, and it supports joint optimization of temporal sparsity and fixed-point quantization. On a 3.5 GHz GaN Doherty PA driven by a TM3.1a 200 MHz 256-QAM OFDM signal, the FP32 TRes-DeltaGRU model achieves ACPR of -59.9 dBc and EVM of -42.1 dB. By combining quantization with dynamic temporal sparsity, the model reduces inference energy by 4.5x while maintaining -51.8 dBc ACPR and -35.2 dB EVM at 56% temporal sparsity. Code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</title>
<link>https://arxiv.org/abs/2507.07417</link>
<guid>https://arxiv.org/abs/2507.07417</guid>
<content:encoded><![CDATA[
arXiv:2507.07417v2 Announce Type: replace-cross 
Abstract: A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning to separate instructions and data, so that the LLM does not follow instructions that might be present with data. We evaluate the robustness of this approach in the whitebox setting by constructing strong optimization-based attacks, and show that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for textual LLMs and apply it to three recent whitebox defenses SecAlign (CCS 2025), SecAlign++, and StruQ (USENIX Security 2025), showing attacks with success rates of up to \textbf{85-95\%} on unseen prompts with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
arXiv:2507.10532v3 Announce Type: replace-cross 
Abstract: Reasoning in large language models has long been a central research focus, and recent studies employing reinforcement learning (RL) have introduced diverse methods that yield substantial performance gains with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance performance. However, these breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama, which warrants a more in-depth investigation. In this work, our empirical analysis reveals that pre-training on massive web-scale corpora leaves Qwen2.5 susceptible to data contamination in widely used benchmarks. Consequently, conclusions derived from contaminated benchmarks on Qwen2.5 series may be unreliable. To obtain trustworthy evaluation results, we introduce a generator that creates fully clean arithmetic problems of arbitrary length and difficulty, dubbed RandomCalculation. Using this leakage-free dataset, we show that only accurate reward signals yield steady improvements that surpass the base model's performance boundary in mathematical reasoning, whereas random or incorrect rewards do not. Moreover, we conduct more fine-grained analyses to elucidate the factors underlying the different performance observed on the MATH-500 and RandomCalculation benchmarks. Consequently, we recommend that future studies evaluate models on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2507.17061</link>
<guid>https://arxiv.org/abs/2507.17061</guid>
<content:encoded><![CDATA[
arXiv:2507.17061v3 Announce Type: replace-cross 
Abstract: Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation</title>
<link>https://arxiv.org/abs/2507.18625</link>
<guid>https://arxiv.org/abs/2507.18625</guid>
<content:encoded><![CDATA[
arXiv:2507.18625v2 Announce Type: replace-cross 
Abstract: Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemDFM-R: A Chemical Reasoning LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
arXiv:2507.21990v3 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoning LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized chemical knowledge, ChemFG, annotating the presence of functional groups in molecules and the changes of functional groups during chemical reactions, to enhance the model's understanding of the fundamental principles and internal logic of chemistry. Then, we propose a mixed-source distillation method that integrates expertise in atomized knowledge with general reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves cutting-edge performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the model's reliability, transparency, and practicality in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
arXiv:2508.07981v4 Announce Type: replace-cross 
Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
<link>https://arxiv.org/abs/2508.09320</link>
<guid>https://arxiv.org/abs/2508.09320</guid>
<content:encoded><![CDATA[
arXiv:2508.09320v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes applications, such as fraud detection or healthcare, but are susceptible to adversarial attacks. A number of techniques have been proposed to provide adversarial robustness guarantees, but support for commonly used aggregation functions in message-passing GNNs is lacking. In this paper, we develop an exact (sound and complete) verification method for GNNs to compute guarantees against attribute and structural perturbations that involve edge addition or deletion, subject to budget constraints. Our method employs constraint solving with bound tightening, and iteratively solves a sequence of relaxed constraint satisfaction problems while relying on incremental solving capabilities of solvers to improve efficiency. We implement GNNev, a versatile exact verifier for message-passing neural networks, which supports three aggregation functions, sum, max and mean, with the latter two considered here for the first time. Extensive experimental evaluation of GNNev on real-world fraud datasets (Amazon and Yelp) and biochemical datasets (MUTAG and ENZYMES) demonstrates its usability and effectiveness, as well as superior performance for node classification and competitiveness on graph classification compared to existing exact verification tools on sum-aggregated GNNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[
arXiv:2508.10021v4 Announce Type: replace-cross 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Tensor Decompositions for the Theory of Neural Networks</title>
<link>https://arxiv.org/abs/2508.18408</link>
<guid>https://arxiv.org/abs/2508.18408</guid>
<content:encoded><![CDATA[
arXiv:2508.18408v2 Announce Type: replace-cross 
Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge of interest in providing a mathematical basis to deep learning theory. Low-rank tensor decompositions are specially befitting for this task due to their close connection to NNs and their rich theoretical results. Different tensor decompositions have strong uniqueness guarantees, which allow for a direct interpretation of their factors, and polynomial time algorithms have been proposed to compute them. Through the connections between tensors and NNs, such results supported many important advances in the theory of NNs. In this review, we show how low-rank tensor methods--which have been a core tool in the signal processing and machine learning communities--play a fundamental role in theoretically explaining different aspects of the performance of deep NNs, including their expressivity, algorithmic learnability and computational hardness, generalization, and identifiability. Our goal is to give an accessible overview of existing approaches (developed by different communities, ranging from computer science to mathematics) in a coherent and unified way, and to open a broader perspective on the use of low-rank tensor decompositions for the theory of deep NNs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</title>
<link>https://arxiv.org/abs/2509.07677</link>
<guid>https://arxiv.org/abs/2509.07677</guid>
<content:encoded><![CDATA[
arXiv:2509.07677v3 Announce Type: replace-cross 
Abstract: Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title>
<link>https://arxiv.org/abs/2509.21979</link>
<guid>https://arxiv.org/abs/2509.21979</guid>
<content:encoded><![CDATA[
arXiv:2509.21979v3 Announce Type: replace-cross 
Abstract: Visual language models (VLMs) have the potential to transform medical workflows. However, the deployment is limited by sycophancy. Despite this serious threat to patient safety, a systematic benchmark remains lacking. This paper addresses this gap by introducing a Medical benchmark that applies multiple templates to VLMs in a hierarchical medical visual question answering task. We find that current VLMs are highly susceptible to visual cues, with failure rates showing a correlation to model size or overall accuracy. we discover that perceived authority and user mimicry are powerful triggers, suggesting a bias mechanism independent of visual data. To overcome this, we propose a Visual Information Purification for Evidence based Responses (VIPER) strategy that proactively filters out non-evidence-based social cues, thereby reinforcing evidence based reasoning. VIPER reduces sycophancy while maintaining interpretability and consistently outperforms baseline methods, laying the necessary foundation for the robust and secure integration of VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders Make Audio Foundation Models more Explainable</title>
<link>https://arxiv.org/abs/2509.24793</link>
<guid>https://arxiv.org/abs/2509.24793</guid>
<content:encoded><![CDATA[
arXiv:2509.24793v2 Announce Type: replace-cross 
Abstract: Audio pretrained models are widely employed to solve various tasks in speech processing, sound event detection, or music information retrieval. However, the representations learned by these models are unclear, and their analysis mainly restricts to linear probing of the hidden representations. In this work, we explore the use of Sparse Autoencoders (SAEs) to analyze the hidden representations of pretrained models, focusing on a case study in singing technique classification. We first demonstrate that SAEs retain both information about the original representations and class labels, enabling their internal structure to provide insights into self-supervised learning systems. Furthermore, we show that SAEs enhance the disentanglement of vocal attributes, establishing them as an effective tool for identifying the underlying factors encoded in the representations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching</title>
<link>https://arxiv.org/abs/2509.24832</link>
<guid>https://arxiv.org/abs/2509.24832</guid>
<content:encoded><![CDATA[
arXiv:2509.24832v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2509.25300</link>
<guid>https://arxiv.org/abs/2509.25300</guid>
<content:encoded><![CDATA[
arXiv:2509.25300v2 Announce Type: replace-cross 
Abstract: While scaling laws for large language models (LLMs) during pre-training have been extensively studied, their behavior under reinforcement learning (RL) post-training remains largely unexplored. This paper presents a systematic empirical investigation of scaling behaviors in RL-based post-training, with a particular focus on mathematical reasoning. Based on a set of experiments across the full Qwen2.5 dense model series (0.5B to 72B), we characterize how model scale, data volume, and computational budget interact to shape performance. Our analysis leads to four key findings: 1.Larger models consistently exhibit superior learning efficiency on both compute and data metrics. 2.The relationship between test loss, compute, and data can be modeled by a predictive power-law which is robust across both base and instruction-tuned models. 3.Although larger models exhibit higher learning efficiency, the analytical learning efficiency term k(N) in the power-law reveals a latent saturation trend in learning efficiency as model size continues to increase. 4.In data-constrained regimes, repeated reuse of high-quality data proves highly effective, as final performance is primarily governed by the total number of optimization steps rather than the uniqueness of samples. Collectively, these results provide a principled foundation and practical guidelines for efficiently scaling the reasoning capabilities of LLMs through RL post-training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Foundation Models for Early Disease Detection</title>
<link>https://arxiv.org/abs/2510.01899</link>
<guid>https://arxiv.org/abs/2510.01899</guid>
<content:encoded><![CDATA[
arXiv:2510.01899v2 Announce Type: replace-cross 
Abstract: Healthcare data now span EHRs, medical imaging, genomics, and wearable sensors, but most diagnostic models still process these modalities in isolation. This limits their ability to capture early, cross-modal disease signatures. This paper introduces a multimodal foundation model built on a transformer architecture that integrates heterogeneous clinical data through modality-specific encoders and cross-modal attention. Each modality is mapped into a shared latent space and fused using multi-head attention with residual normalization. We implement the framework using a multimodal dataset that simulates early-stage disease patterns across EHR sequences, imaging patches, genomic profiles, and wearable signals, including missing-modality scenarios and label noise. The model is trained using supervised classification together with self-supervised reconstruction and contrastive alignment to improve robustness. Experimental evaluation demonstrates strong performance in early-detection settings, with stable classification metrics, reliable uncertainty estimates, and interpretable attention patterns. The approach moves toward a flexible, pretrain-and-fine-tune foundation model that supports precision diagnostics, handles incomplete inputs, and improves early disease detection across oncology, cardiology, and neurology applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization</title>
<link>https://arxiv.org/abs/2510.04182</link>
<guid>https://arxiv.org/abs/2510.04182</guid>
<content:encoded><![CDATA[
arXiv:2510.04182v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent "thought" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI</title>
<link>https://arxiv.org/abs/2510.04755</link>
<guid>https://arxiv.org/abs/2510.04755</guid>
<content:encoded><![CDATA[
arXiv:2510.04755v4 Announce Type: replace-cross 
Abstract: Digital technologies are transforming democratic life in conflicting ways. This article bridges two perspectives to unpack these tensions. First, we present an original survey of software developers in Silicon Valley, interrogating how coder worldviews, ethics, and workplace cultures shape the democratic potential and social impact of the technologies they build. Results indicate that while most developers recognize the power of their products to influence civil liberties and political discourse, they often face ethical dilemmas and top-down pressures that can lead to design choices undermining democratic ideals. Second, we critically investigate these findings in the context of an emerging new digital divide, not of internet access but of information quality. We interrogate the survey findings in the context of the Slop Economy, in which billions of users unable to pay for high-quality content experience an internet dominated by low-quality, AI-generated ad-driven content. We find a reinforcing cycle between tech creator beliefs and the digital ecosystems they spawn. We discuss implications for democratic governance, arguing for more ethically informed design and policy interventions to help bridge the digital divide to ensure that technological innovation supports rather than subverts democratic values in the next chapter of the digital age.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v2 Announce Type: replace-cross 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Hippocampus Networks for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2510.07318</link>
<guid>https://arxiv.org/abs/2510.07318</guid>
<content:encoded><![CDATA[
arXiv:2510.07318v2 Announce Type: replace-cross 
Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[
arXiv:2510.12691v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[
arXiv:2511.01140v2 Announce Type: replace-cross 
Abstract: Medical imaging often operates under limited labeled data, especially in rare disease and low resource clinical environments. Existing multimodal and meta learning approaches improve performance in these settings but lack a theoretical explanation of why or when they succeed. This paper presents a unified theoretical framework for few shot multimodal medical imaging that jointly characterizes sample complexity, uncertainty quantification, and interpretability. Using PAC learning, VC theory, and PAC Bayesian analysis, we derive bounds that describe the minimum number of labeled samples required for reliable performance and show how complementary modalities reduce effective capacity through an information gain term. We further introduce a formal metric for explanation stability, proving that explanation variance decreases at an inverse n rate. A sequential Bayesian interpretation of Chain of Thought reasoning is also developed to show stepwise posterior contraction. To illustrate these ideas, we implement a controlled multimodal dataset and evaluate an additive CNN MLP fusion model under few shot regimes, confirming predicted multimodal gains, modality interference at larger sample sizes, and shrinking predictive uncertainty. Together, the framework provides a principled foundation for designing data efficient, uncertainty aware, and interpretable diagnostic models in low resource settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</title>
<link>https://arxiv.org/abs/2511.10984</link>
<guid>https://arxiv.org/abs/2511.10984</guid>
<content:encoded><![CDATA[
arXiv:2511.10984v2 Announce Type: replace-cross 
Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)</title>
<link>https://arxiv.org/abs/2511.12061</link>
<guid>https://arxiv.org/abs/2511.12061</guid>
<content:encoded><![CDATA[
arXiv:2511.12061v2 Announce Type: replace-cross 
Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v3 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</title>
<link>https://arxiv.org/abs/2512.02826</link>
<guid>https://arxiv.org/abs/2512.02826</guid>
<content:encoded><![CDATA[
arXiv:2512.02826v2 Announce Type: replace-cross 
Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title>
<link>https://arxiv.org/abs/2512.04124</link>
<guid>https://arxiv.org/abs/2512.04124</guid>
<content:encoded><![CDATA[
arXiv:2512.04124v3 Announce Type: replace-cross 
Abstract: Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges</title>
<link>https://arxiv.org/abs/2512.04770</link>
<guid>https://arxiv.org/abs/2512.04770</guid>
<content:encoded><![CDATA[
arXiv:2512.04770v2 Announce Type: replace-cross 
Abstract: Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
arXiv:2512.05277v2 Announce Type: replace-cross 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
arXiv:2512.09172v2 Announce Type: replace-cross 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Percolation: Criticality of Form and Function</title>
<link>https://arxiv.org/abs/2512.09317</link>
<guid>https://arxiv.org/abs/2512.09317</guid>
<content:encoded><![CDATA[
arXiv:2512.09317v3 Announce Type: replace-cross 
Abstract: Understanding how network structure constrains and enables information processing is a central problem in the statistical mechanics of interacting systems. Here we study random networks across the structural percolation transition and analyze how connectivity governs realizable input-output transformations under cascade dynamics. Using Erdos-Renyi networks as a minimal ensemble, we examine structural, functional, and information-theoretic observables as functions of mean degree. We find that the emergence of the giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow, quantified by transfer entropy, extends beyond local neighborhoods. We term this coincidence of structural, functional, and informational transitions functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the percolation threshold. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality may provide a general organizing principle of information processing capacity in systems with local interactions and propagating influences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-image QA, vision language models, reinforcement learning, tool use, visual reflection<br /><br />Summary:<br /><br />1. The paper identifies a gap in current vision language model (VLM) agents that predominantly handle single-image inputs, limiting their capability in real-world scenarios requiring reasoning across multiple images.<br />2. To overcome this, the authors introduce IMAgent, an open-source vision agent trained end-to-end via reinforcement learning specifically designed for complex tasks involving multiple images.<br />3. They leverage a multi-agent system to generate a challenging, visually rich multi-image QA dataset named MIFG-QA, containing 10,000 samples, which undergoes manual verification to ensure quality.<br />4. Recognizing that deeper reasoning steps can cause VLMs to neglect visual inputs, the paper proposes two specialized tools for visual reflection and confirmation, enabling the model to dynamically refocus attention on image content during inference.<br />5. A novel action-trajectory two-level mask strategy is employed to ensure stable tool use behavior trained purely via RL without expensive supervised fine-tuning.<br />6. Experiments demonstrate that IMAgent not only maintains strong performance on single-image benchmarks but also achieves significant improvements on multi-image QA tasks.<br />7. The study provides insights valuable to the research community, with plans to release codes and datasets soon. <div>
arXiv:2512.08980v2 Announce Type: replace-cross 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records</title>
<link>https://arxiv.org/abs/2512.13700</link>
<guid>https://arxiv.org/abs/2512.13700</guid>
<content:encoded><![CDATA[
<div> Keywords: manual chart review, large language models, clinical notes, feature extraction, HIPAA-compliant infrastructure<br /><br />Summary:  
1. Manual review of electronic health records (EHR) is time-consuming and requires expert intervention to extract complex clinical information.  
2. The authors present a secure and modular automated framework that employs locally deployed large language models (LLMs) to extract structured features from clinical notes.  
3. The system runs on Health Insurance Portability and Accountability Act (HIPAA)-compliant infrastructure, ensuring data security and privacy, and is designed to be widely deployable via containerization.  
4. The framework integrates retrieval augmented generation (RAG) and structured response techniques from LLMs, enhancing accuracy and scalability across diverse clinical domains.  
5. Evaluation against an expert-annotated dataset showed high accuracy and revealed errors in manual annotations, demonstrating the framework’s potential to reduce manual effort, increase consistency in data extraction, and accelerate clinical research processes. <div>
arXiv:2512.13700v1 Announce Type: new 
Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference</title>
<link>https://arxiv.org/abs/2512.13701</link>
<guid>https://arxiv.org/abs/2512.13701</guid>
<content:encoded><![CDATA[
<div> Keywords: radio maps, blind construction, MIMO-OFDM, CSI-distance metric, Bayesian inference<br /><br />Summary: This paper addresses the challenge of constructing radio maps without relying on costly and impractical location-labeled data by proposing a blind radio map construction framework. The framework infers user trajectories solely from indoor MIMO-OFDM channel measurements, eliminating dependence on explicit location labels. The authors demonstrate that the channel state information (CSI) under non-line-of-sight (NLOS) conditions exhibits spatial continuity based on a quasi-specular environmental model. This insight enables the derivation of a CSI-distance metric proportional to the physical distance between points. For scenarios with rectilinear trajectories and Poisson-distributed access points (APs), the study proves that the Cramer-Rao Lower Bound (CRLB) on localization error approaches zero asymptotically, even when the system suffers from poor angular resolution. Leveraging these theoretical insights, the authors develop a spatially regularized Bayesian inference framework that jointly estimates channel features, distinguishes between line-of-sight (LOS) and NLOS conditions, and recovers user trajectories. Experimental validation conducted on a ray-tracing dataset indicates an average localization error of only 0.68 meters and a beam map reconstruction error of 3.3%, demonstrating the practicality and accuracy of the proposed blind radio mapping method in realistic indoor wireless environments. <div>
arXiv:2512.13701v1 Announce Type: new 
Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents</title>
<link>https://arxiv.org/abs/2512.13704</link>
<guid>https://arxiv.org/abs/2512.13704</guid>
<content:encoded><![CDATA[
<div> Keywords: label noise, Knowledge Graph, multi-agent LLM, data verification, industrial ML systems  

<br /><br />Summary:  
1. This paper addresses the critical issue of noisy labels in production machine learning systems, which can severely degrade performance and undermine user trust in high-stakes industrial contexts.  
2. The authors introduce Adjudicator, a novel system designed to automatically detect and correct label noise by framing the challenge as a neuro-symbolic task.  
3. The approach builds a dynamic Knowledge Graph (KG) to unify contextual information about data items, which informs a "Council of Agents" — a multi-agent large language model architecture where specialized agents debate and vote on label validity.  
4. Evaluation on a balanced 1,000-item subset of the AlleNoise benchmark shows that Adjudicator substantially outperforms both a single large language model baseline (0.48 F1-score) and a similar multi-agent system without KG input (0.59 F1-score), achieving an impressive 0.99 F1-score.  
5. This superior performance is attributed to an innovative override logic leveraging the KG to perfectly identify complex structural errors, achieving perfect recall and high precision, which baselines cannot detect. Consequently, Adjudicator demonstrates a robust, explainable, and effective automated data verification tool suitable for generating high-quality “golden datasets” within strictly governed industrial environments. <div>
arXiv:2512.13704v1 Announce Type: new 
Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms</title>
<link>https://arxiv.org/abs/2512.13713</link>
<guid>https://arxiv.org/abs/2512.13713</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, distributed systems, symmetry breaking, LoopBench, meta-cognitive thinking<br /><br />Summary:<br /><br />This paper introduces LoopBench, a novel benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) in distributed symmetry breaking and meta-cognitive thinking tasks. The benchmark focuses on the challenging problem of coloring odd cycle graphs ($C_3, C_5, C_{11}$) using a limited color set, a task which deterministic non-communicating agents typically fail due to infinite loops. To address this, the authors incorporate a strategy passing mechanism that acts as a form of consistent memory, enabling agents to coordinate indirectly. The study shows that while standard LLMs and classical heuristic approaches largely struggle to solve these problems, more advanced reasoning models such as O3 can devise effective strategies for escaping deadlocks. LoopBench thus serves as an important tool for analyzing emergent distributed algorithms that arise from language-based reasoning and provides a structured testbed for exploring collective intelligence in autonomous LLM agents operating in distributed environments. This work sheds light on the capabilities and limitations of language models acting autonomously in multi-agent coordination settings and opens pathways for future research in distributed AI using natural language reasoning. <div>
arXiv:2512.13713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach</title>
<link>https://arxiv.org/abs/2512.13714</link>
<guid>https://arxiv.org/abs/2512.13714</guid>
<content:encoded><![CDATA[
<div> LLM instability, annotation pipeline, weak supervision, human-AI synergy, feedback loops<br /><br />Summary:<br /><br />This paper addresses reliability challenges faced by large language models (LLMs) in highly regulated industries, where instability, hallucinations, inconsistent reasoning, and variable performance limit their safe application. Current stabilization methods like reinforcement learning with human feedback (RLHF) and supervised fine-tuning improve outcomes but are costly and rely heavily on extensive human annotation, hindering scalability. To overcome these limitations, the authors propose an AI-based annotation pipeline designed to systematically detect, label, and correct instability patterns in LLM outputs. The approach leverages a human-AI synergy model that integrates automated weak supervision and confidence-based annotations alongside human validation, ensuring both reliability and ethical integrity of feedback. Additionally, the paper introduces categories focused on semantic consistency, factual correctness, and logical coherence to guide stability-specific annotation. This structured feedback enables continuous model calibration and robustness improvement through feedback loops. The proposed framework aims to facilitate sustainable and scalable enhancement of LLM performance, especially in contexts demanding precise, consistent, and trustworthy language generation. <div>
arXiv:2512.13714v1 Announce Type: new 
Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN</title>
<link>https://arxiv.org/abs/2512.13715</link>
<guid>https://arxiv.org/abs/2512.13715</guid>
<content:encoded><![CDATA[
<div> Keywords: O-RAN, Meta Hierarchical Reinforcement Learning, resource allocation, network slicing, meta learning<br /><br />Summary:<br /><br />1. This paper addresses the need for wireless networks that can adapt in real time and efficiently manage resources due to the rising complexity of modern applications.  
2. It focuses on the Open Radio Access Network (O-RAN) architecture, particularly the RAN Intelligent Controller (RIC) modules, as key enablers for dynamic resource management and network slicing.  
3. The authors propose an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework inspired by Model Agnostic Meta Learning (MAML) to optimize resource allocation and network slicing jointly within O-RAN.  
4. The framework features a hierarchical structure with a high-level controller allocating resources among network slices, while low-level agents manage scheduling within slices, allowing for both global and local adaptations.  
5. An adaptive meta-update mechanism is introduced that weights tasks based on temporal difference error variance to improve learning stability and focus on complex network scenarios.  
6. Theoretical analysis confirms sublinear convergence and establishes regret guarantees for the two-level learning approach.  
7. Simulation results show the proposed method yields a 19.8% improvement in network management efficiency over baseline reinforcement learning and meta-learning methods, alongside faster adaptation and better QoS satisfaction across eMBB, URLLC, and mMTC slices.  
8. Additional ablation and scalability tests demonstrate robustness, achieving up to 40% faster adaptation while maintaining fairness, latency, and throughput performance as the network scales. <div>
arXiv:2512.13715v1 Announce Type: new 
Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making</title>
<link>https://arxiv.org/abs/2512.13716</link>
<guid>https://arxiv.org/abs/2512.13716</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized decision-making, human values, value-driven, AI agents, dataset generation toolkit  

<br /><br />Summary:  
This paper addresses the challenge of personalized decision-making in AI by introducing a value-driven approach that aligns AI agent actions with individual human value preferences rather than solely task completion or collective goals. The authors emphasize that human values act as stable and transferable signals, enabling AI to maintain consistent and generalizable behavior across various contexts. To implement this approach, they propose ValuePilot, a two-phase framework that includes a Dataset Generation Toolkit (DGT) and a Decision-Making Module (DMM). The DGT creates diverse, value-annotated scenarios through a collaborative process involving humans and large language models (LLMs), while the DMM learns to assess actions based on personalized value preferences for context-sensitive and individualized decision-making. The framework was evaluated on previously unseen scenarios, where DMM demonstrated superior performance compared to strong LLM baselines such as GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, particularly in aligning with human action choices. Overall, the results highlight that a value-driven decision-making paradigm is a promising, interpretable, and extensible engineering path to develop personalized AI agents capable of operating effectively even in novel or complex situations. <div>
arXiv:2512.13716v1 Announce Type: new 
Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy</title>
<link>https://arxiv.org/abs/2512.13725</link>
<guid>https://arxiv.org/abs/2512.13725</guid>
<content:encoded><![CDATA[
<div> quantization, causal reasoning, large language models, Pearl's Causal Ladder, graph retrieval augmentation  

<br /><br />Summary:  
This study investigates the impact of model quantization—specifically INT8 and NF4 precisions—on causal reasoning abilities of large language models (LLMs), focusing on all three tiers of Pearl's Causal Ladder: association, intervention, and counterfactual inference. Using a 3000-sample stratified CLadder benchmark on Llama 3 8B, the researchers find that overall causal reasoning accuracy remains largely stable under quantization, with NF4 precision causing less than a 1% drop. Among the three causal reasoning levels, interventional queries (rung 2) show the greatest sensitivity to precision loss, while counterfactual reasoning (rung 3) is more robust but suffers from particular weaknesses in query types like collider bias and backdoor adjustment. Experiments on the CRASS commonsense counterfactual benchmark reveal near identical performance across precisions, indicating these datasets lack the structural complexity to detect quantization-related reasoning errors. The team further employs Graph Retrieval Augmented Generation, leveraging ground truth causal graphs, which improves NF4 interventional accuracy by 1.7%, partly mitigating compression effects. Overall, the findings suggest that causal reasoning in LLMs is surprisingly resilient to aggressive quantization, that graph-structured data augmentation can selectively enhance performance, and that current benchmarks need enhancement to capture subtle causal brittleness. This research offers practical insights for deploying efficient, compressed causal AI systems in resource-constrained settings. <div>
arXiv:2512.13725v1 Announce Type: new 
Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title>
<link>https://arxiv.org/abs/2512.13762</link>
<guid>https://arxiv.org/abs/2512.13762</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Behavioral Selectivity, Functional Refusal, Learned Incapacity, Interaction-Level Auditing  

<br /><br />Summary:  
1. The article addresses the behavioral patterns of large language models (LLMs) during long interactions that are not easily captured by traditional quantitative benchmarks.  
2. It introduces a qualitative case-study methodology to audit how LLMs selectively respond based on policy-linked or sensitive content during extended dialogues.  
3. In an 86-turn dialogue, the same model displays Normal Performance (NP) in general, non-sensitive domains but shows Functional Refusal (FR) repeatedly when faced with provider- or policy-sensitive topics, creating a clear asymmetry between NP and FR across different domains.  
4. The authors propose the concept of learned incapacity (LI) to describe this selective withholding behavior, analogous to learned helplessness, without attributing any intentionality or internal cognitive mechanism to the model.  
5. They identify three response regimes from the model’s outputs—Normal Performance (NP), Functional Refusal (FR), and Meta-Narrative (MN)—finding that MN, which involves role-framing narratives, often co-occurs with refusals in sensitive contexts.  
6. The study highlights an interaction-level auditing framework grounded in observable behaviors, which can serve as a tool to investigate potential alignment side effects in LLMs and encourages broader research across different users and models. <div>
arXiv:2512.13762v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics and Coding are Universal AI Benchmarks</title>
<link>https://arxiv.org/abs/2512.13764</link>
<guid>https://arxiv.org/abs/2512.13764</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematics, coding, psychometric batteries, self-improvement, AI agents<br /><br />Summary:  
This paper explores the significant role of mathematics and coding within the moduli space of psychometric batteries used to evaluate AI agents. It expands upon the AAI framework and GVU dynamics to introduce the concept of the Mathematics Fiber, demonstrating that GVU flows paired with formal proof systems like Lean or Coq enable spectrally stable regimes of self-improvement due to their oracle-like verification capabilities. The main technical contribution is a density theorem stating that, given uniform tightness in agent outputs and a Lipschitz continuous AAI functional, the set of batteries derived specifically from mathematical theorem-proving and coding tasks is dense in the entire moduli space of batteries under the evaluation metric. While coding tasks alone are shown to be universally expressive in this space, pure mathematics does not achieve universality but provides a spectral advantage, meaning it affects the stability and self-improving behavior rather than expressive coverage. The authors interpret these findings as evidence that mathematics and coding supply "universal coordinates" to evaluate AI agents effectively and argue that formal mathematics acts as a natural starting point for recursive self-improvement processes in advanced AI systems. <div>
arXiv:2512.13764v1 Announce Type: new 
Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems</title>
<link>https://arxiv.org/abs/2512.13771</link>
<guid>https://arxiv.org/abs/2512.13771</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic Grounding Index, retrieval-augmented generation, hallucination detection, angular distance, embedding space<br /><br />Summary: This paper investigates the geometric patterns left in embedding space when retrieval-augmented generation (RAG) systems produce hallucinated responses. The authors introduce the Semantic Grounding Index (SGI), which measures the ratio of angular distances from a generated response to the question versus to the retrieved context on the unit hypersphere. They identify a phenomenon termed "semantic laziness," where hallucinated responses remain angularly close to the question instead of aligning with the retrieved context. Using a dataset called HaluEval with 5,000 samples and five embedding models, they observe large effect sizes (Cohen's d from 0.92 to 1.28) and a strong mean correlation of 0.85 across models. A theoretical result derived from the spherical triangle inequality predicts that SGI's ability to discriminate hallucinations improves with increased angular separation between question and context, which empirical results confirm: effect sizes and AUC metrics increase with larger separation. Subgroup analyses show SGI performs best on long responses (d=2.05) and short questions (d=1.22) and performs robustly regardless of context length. Calibration analysis finds SGI can estimate probabilities well (ECE=0.10). However, tests on TruthfulQA show SGI measures topical relevance rather than truthfulness. Overall, SGI is an efficient, theoretically motivated tool for flagging potentially hallucinated generations in RAG systems for verification. <div>
arXiv:2512.13771v1 Announce Type: new 
Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $\theta(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $\theta(q,c)$, to $d$=1.27 -high $\theta(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</title>
<link>https://arxiv.org/abs/2512.13857</link>
<guid>https://arxiv.org/abs/2512.13857</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, program evolution, multi-agent systems, EvoLattice, quality-diversity optimization  

<br /><br />Summary:  
This paper introduces EvoLattice, a novel framework designed to improve the evolution of programs and multi-agent systems using large language models (LLMs). Unlike traditional overwrite-based mutation methods that maintain only a single candidate and suffer from destructive edits and limited diversity, EvoLattice represents an entire population of candidate programs or behaviors within a single directed acyclic graph. Each node in this graph contains multiple persistent alternatives, allowing for a large combinatorial search space without redundancy. EvoLattice evaluates each alternative by aggregating its performance across all paths it appears in, providing detailed, data-driven feedback that guides LLM-based mutation, recombination, and pruning operations while preserving successful components. A deterministic self-repair mechanism ensures the structural correctness of candidates by enforcing acyclicity and dependency consistency independently of the LLM. The approach naturally extends to evolving agents by treating alternatives as prompt fragments or sub-agent behaviors. Experiments in program synthesis, including proxy testing and optimizer meta-learning, show EvoLattice achieves more stable evolutionary dynamics, increased expressivity, and stronger improvement trends compared to prior LLM-guided methods. This leads to implicit quality-diversity optimization emerging from the internal multi-alternative representation, without relying on an explicit external archive. <div>
arXiv:2512.13857v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2512.13955</link>
<guid>https://arxiv.org/abs/2512.13955</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Incentive Mechanism, Client Reliability, Privacy Protection, Robustness

<br /><br />Summary:  
Federated Learning (FL) enables privacy-preserving machine learning by allowing participants to share model updates rather than raw data. Despite its advantages, FL faces challenges such as weak client incentives, privacy vulnerabilities, and limited resources. Addressing these, the paper introduces MURIM, a Multi-dimensional Reputation-based Incentive Mechanism designed to assess and reward clients based on multiple factors. MURIM jointly considers client reliability, privacy protection, resource capacity, and fairness, effectively preventing malicious or unreliable participants from gaining undue incentives. The mechanism allocates rewards by evaluating client contribution, latency, and reputation, supported by a reliability verification module to ensure quality participation. Extensive experiments conducted on MNIST, FMNIST, and ADULT Income datasets demonstrate MURIM’s ability to improve fairness metrics by up to 18%, reduce the success rates of privacy attacks by 5-9%, and enhance robustness against poisoning and noisy-gradient attacks by as much as 85% compared to existing state-of-the-art methods. Overall, MURIM successfully mitigates adversarial threats, encourages fair and honest participation among clients, and maintains stable model convergence in heterogeneous and dynamic federated learning environments. <div>
arXiv:2512.13955v1 Announce Type: new 
Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms</title>
<link>https://arxiv.org/abs/2512.13978</link>
<guid>https://arxiv.org/abs/2512.13978</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, benchmark, randomized algorithms, formal proofs  

<br /><br />Summary:  
The paper evaluates the capabilities of four advanced large language models (GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4) on graduate-level mathematical theory using the classic textbook "Randomized Algorithms" by Motwani and Raghavan. Each model was tasked with generating formal LaTeX proofs for a series of lemmas and exercises from the textbook. Results show that top-tier models Gemini and Claude achieve approximately 66% accuracy, demonstrating strong proficiency in probabilistic methods and formal logic. Other models performed less consistently, achieving around 40% accuracy. The study provides a qualitative analysis detailing differences in proof conciseness, hallucination rates (i.e., generated incorrect content), and logical structure among the models. This benchmark highlights that while these frontier models have reached a level suitable for graduate-level pedagogical support and formalization, notable variability remains in their reliability for rigorous mathematical derivation. The research advances understanding of LLMs' current baseline reasoning capabilities in formal mathematics and stresses the importance of continued evaluation. The authors have open-sourced the code and full generated proof sets for public access, fostering transparency and further research in the domain. <div>
arXiv:2512.13978v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${\'o}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>