<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models</title>
<link>https://arxiv.org/abs/2512.11835</link>
<guid>https://arxiv.org/abs/2512.11835</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Artificial Age Score, Monadology, Memory constraints, Executable specification  

<br><br>Summary:  
This paper addresses the challenge of governing internal memory and self-like behaviors in large language models (LLMs) by introducing a clause-based architectural framework grounded in philosophical principles. It builds upon the Artificial Age Score (AAS), a metric previously defined and mathematically justified to measure artificial memory aging, using it as the core kernel for memory and control constraints. The authors select twenty monads from Leibniz's Monadology and organize them into six bundles—ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology—each realized as executable Python specifications on top of the AAS framework. Through six minimal Python experiments, the paper demonstrates the practical instantiation of these clauses on measurable quantities like recall scores, redundancy, and weights. Each experiment follows a structured approach: inputs/setup, clause implementation, numerical analysis, and design implications for LLMs. Results show that the clause system produces bounded, interpretable behavior with continuous and rate-limited AAS trajectories, explicit penalties for contradictions and unsupported claims, and a hierarchical refinement that reveals organic internal structure. The harmony and reason clauses align dual views and goal-action pairs, while windowed drift in perfection scores helps distinguish between improvement and degradation phases. Overall, the work proposes a philosophically motivated, transparent, and implementable blueprint for constraining and analyzing the internal dynamics of artificial agents using an AAS backbone. <div>
arXiv:2512.11835v1 Announce Type: new 
Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars</title>
<link>https://arxiv.org/abs/2512.11864</link>
<guid>https://arxiv.org/abs/2512.11864</guid>
<content:encoded><![CDATA[
<div> Keywords: parallel machine scheduling, job precedences, calendar-based resource constraints, constraint modeling, metaheuristic

<br><br>Summary:  
This paper addresses the complex problem of scheduling production on parallel machines, a challenge common in industrial manufacturing environments where efficiency can greatly reduce costs. The authors identify that while many basic variants of machine scheduling are already NP-hard, real-world scenarios involve additional complex constraints such as job precedences and calendar-based cumulative resource restrictions that current methods struggle to handle. To tackle this, the study introduces a novel parallel machine scheduling variant incorporating these realistic constraints. For small-scale problems, an exact solution method based on constraint modeling combined with advanced constraint-solving tools is proposed. Recognizing the limitations of exact methods on large instances, the paper presents a construction heuristic and a specialized metaheuristic that uses local search to efficiently solve large-scale scheduling problems. Importantly, this metaheuristic has been implemented in an industrial context and is actively being used, demonstrating practical applicability. Overall, the work contributes innovative modeling and solution techniques that better align with the complexities of actual manufacturing scheduling needs, bridging the gap between theoretical NP-hard scheduling problems and real-life industrial applications. <div>
arXiv:2512.11864v1 Announce Type: new 
Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.11902</link>
<guid>https://arxiv.org/abs/2512.11902</guid>
<content:encoded><![CDATA[
<div> Keywords: Mirror Mode, Imitation Learning, Reinforcement Learning, Fire Emblem Heroes, Player Strategy<br><br>Summary:<br><br>This study introduces a novel game mode called Mirror Mode designed to enhance enemy AI behavior in turn-based strategy games by imitating the personal playstyle of human players. A simplified version of the Nintendo game Fire Emblem Heroes was developed in the Unity engine to test this concept, featuring both a Standard Mode and the new Mirror Mode. The research involves two main experimental phases. The first phase focused on finding an effective model for replicating player strategies, combining approaches from Reinforcement Learning and Imitation Learning, specifically Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second phase evaluated these models through player tests where the AI was trained on real player demonstrations. Results showed that the AI successfully imitated players’ defensive behaviors but struggled with offensive tactics. Furthermore, player feedback indicated that participants recognized their own retreating strategies during Mirror Mode gameplay, which contributed to higher overall satisfaction. The study suggests that further refinement of the modeling approach could improve the AI’s imitation quality, especially in offensive play, thereby increasing player engagement and satisfaction. All code and survey data related to the study are publicly available on GitHub. <div>
arXiv:2512.11902v1 Announce Type: new 
Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents</title>
<link>https://arxiv.org/abs/2512.11907</link>
<guid>https://arxiv.org/abs/2512.11907</guid>
<content:encoded><![CDATA[
<div> Personalization, Large Language Models, Submodularity, Matroid Constraints, Knowledge Graph<br><br>Summary: This paper addresses the challenge of personalizing large language model (LLM) agents using user-specific data, balancing task utility and data privacy. It highlights that while adding user data exhibits diminishing returns, real-world personalization involves complex structural constraints such as logical dependencies, categorical quotas, and hierarchical rules that complicate standard subset selection approaches. To manage these complexities, the authors propose a formal method that models these constraints by transforming a user's knowledge graph into a set of abstract macro-facets through a compilation process. The core theoretical contribution is proving that common hierarchical and quota-based constraints over these macro-facets correspond to a laminar matroid structure. This insight allows framing the personalization problem as a submodular function maximization under matroid constraints. Consequently, this enables the use of greedy algorithms with constant-factor approximation guarantees and continuous greedy methods achieving a (1 - 1/e) approximation ratio. Ultimately, the work provides a principled approach to structured personalization with more realistic constraint handling, improving algorithmic performance and practical applicability in personalizing LLM agents. <div>
arXiv:2512.11907v1 Announce Type: new 
Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets</title>
<link>https://arxiv.org/abs/2512.11909</link>
<guid>https://arxiv.org/abs/2512.11909</guid>
<content:encoded><![CDATA[
<div> Keywords: causal reasoning, large language models, human comparison, collider graph, Bayesian modeling<br><br>Summary:<br>The paper investigates the nature of intelligence through the lens of causal reasoning, focusing on comparing how humans and large language models (LLMs) perform on identical reasoning tasks. The authors explore three key questions: (Q1) whether LLMs align with humans when given the same causal reasoning tasks; (Q2) if both LLMs and humans show consistency in reasoning at the task level; and (Q3) whether they demonstrate distinct reasoning signatures. To address these, over 20 LLMs are evaluated on eleven causal reasoning tasks formalized using a collider graph structure ($C_1 \to E \leftarrow C_2$), analyzed under two conditions: a Direct one-shot task requiring a probability judgment of a query node, and a Chain of Thought (CoT) approach where the model reasons stepwise before answering. The judgments from models and humans are modeled using a leaky noisy-OR causal Bayesian network (CBN) with parameters $\theta = (b, m_1, m_2, p(C))$ bounded between 0 and 1, where $p(C)$ is a shared prior. Model selection is performed using the Akaike Information Criterion (AIC) to choose between a more parsimonious symmetric causal strength model ($m_1 = m_2$) and a more flexible asymmetric variant ($m_1 \neq m_2$). This study advances understanding of parallels and divergences in causal reasoning between humans and AI models. <div>
arXiv:2512.11909v1 Announce Type: new 
Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $\theta=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis</title>
<link>https://arxiv.org/abs/2512.11912</link>
<guid>https://arxiv.org/abs/2512.11912</guid>
<content:encoded><![CDATA[
<div> Keywords: robustness, low-quality data, autoregressive language models, diffusion models, information theory<br><br>Summary:<br><br>This study systematically compares how modern probabilistic models respond to low-quality data, revealing a wide range of robustness among them. First, it shows that autoregressive language models, such as GPT-2, maintain high resilience even when subjected to 50% token corruption, with only a modest increase in test negative log likelihood (NLL) from 2.87 to 3.59. Second, class-conditional diffusion models are found to suffer catastrophic performance drops under similar data corruption, with image-label consistency falling by 56.81% relative to baseline. Third, classifiers experience moderate impacts from data corruption, but the severity lessens as the dataset size grows. Fourth, the paper interprets these differences through the lenses of information theory, PAC learning, and gradient dynamics, offering a theoretical framework for understanding robustness. Finally, it identifies two primary principles influencing robustness: the richness of conditioning information, which constrains the learning complexity, and the absolute information content of training data, which enables correct signals to prevail over noise. Together, these insights help explain why some probabilistic models are more robust to low-quality data than others. <div>
arXiv:2512.11912v1 Announce Type: new 
Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving</title>
<link>https://arxiv.org/abs/2512.11920</link>
<guid>https://arxiv.org/abs/2512.11920</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, KV-cache, CXL, FPGA acceleration, memory disaggregation  

<br><br>Summary:  
This paper addresses the significant memory challenges posed by KV-caches during the autoregressive decoding of Large Language Models (LLMs) in datacenter environments, which limit batch sizes and system throughput. To overcome these limitations, the authors propose CXL-SpecKV, a novel architecture combining Compute Express Link (CXL) interconnects and FPGA accelerators for efficient memory disaggregation and speculative execution. First, CXL-SpecKV includes a memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, relieving GPU memory pressure. Second, it employs a speculative KV-cache prefetching mechanism that predicts and preloads future token cache entries to reduce stalls during decoding. Third, the system integrates an FPGA-accelerated compression and decompression engine for KV-caches, achieving up to 4× reduction in memory bandwidth usage. Evaluated on modern LLMs, CXL-SpecKV demonstrates up to 3.2× higher throughput compared to GPU-only baselines, while cutting memory costs by 2.8× without sacrificing model accuracy. This work showcases how combining intelligent memory disaggregation and speculative execution can help overcome the memory wall, enabling more scalable and efficient large-scale LLM serving. The implementation has been made publicly available to facilitate further research and development. <div>
arXiv:2512.11920v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org</title>
<link>https://arxiv.org/abs/2512.11935</link>
<guid>https://arxiv.org/abs/2512.11935</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, materials discovery, open-source LLMs, AGAPI, multi-step workflows<br><br>Summary:  
Artificial intelligence is transforming scientific discovery but its application in materials research is hindered by fragmented computational tools, challenges in reproducibility, and reliance on proprietary large language models (LLMs). To address these limitations, the authors introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates over eight open-source LLMs with more than twenty materials-science APIs, creating a unified ecosystem combining databases, simulation tools, and machine learning models. AGAPI features an Agent-Planner-Executor-Summarizer architecture enabling autonomous construction and execution of complex multi-step workflows, covering tasks such as materials data retrieval, graph neural network-based property prediction, force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. The platform is demonstrated through practical applications like heterostructure creation, powder X-ray diffraction analysis, and semiconductor defect engineering that require up to ten sequential operations. To validate AGAPI, the authors test it with over 30 example prompts and compare outputs generated with and without tool access against experimental results. With a growing user base exceeding 1,000 active users, AGAPI offers a scalable, transparent, and reproducible foundation for AI-driven materials discovery. The AGAPI-Agents source code is openly available at https://github.com/atomgptlab/agapi. <div>
arXiv:2512.11935v1 Announce Type: new 
Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play</title>
<link>https://arxiv.org/abs/2512.11942</link>
<guid>https://arxiv.org/abs/2512.11942</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergame theory, multi-agent systems, bounded rationality, answer-set programming, strategic AI<br><br>Summary:  
This work addresses the challenge of representing and analyzing games where players have differing perceptions, incomplete information, and bounded rationality, leading to private, subjective views of the game scenario. Traditional game-theoretic models often neglect such heterogeneity, but hypergame theory offers a formal mathematical framework to capture mismatched mental models among players. Despite increasing interest, the practical adoption of hypergames in multi-agent research has been limited by the absence of a unified, formal, and scalable representation language and solution algorithms. To fill this gap, the authors introduce a declarative, logic-based domain-specific language designed to encode hypergame structures and their solution concepts effectively. Utilizing answer-set programming, they develop an automated system capable of instantiating hypergame structures and implementing a novel hypergame rationalisation procedure, which finds belief configurations justifying seemingly irrational behaviors. This language and methodology provide a unifying formalism for hypergames, enabling the creation of heterogeneous reasoners based on belief models with verifiable logical guarantees. Overall, the paper establishes a critical link connecting hypergame theory to multi-agent systems and strategic AI, advancing the ability to model, reason about, and compute equilibria in complex, belief-driven strategic interactions. <div>
arXiv:2512.11942v1 Announce Type: new 
Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion</title>
<link>https://arxiv.org/abs/2512.11997</link>
<guid>https://arxiv.org/abs/2512.11997</guid>
<content:encoded><![CDATA[
<div> Keywords: system logs, anomaly detection, enrichment, retrieval-augmented generation, log analysis<br><br>Summary: System logs play a vital role in monitoring and managing distributed systems by offering insights into failures and anomalous behaviors. Traditional log analysis methods, such as template-based and sequence-driven approaches, often lose crucial semantic details or face challenges with ambiguous log patterns. To overcome these limitations, the article introduces EnrichLog, a training-free, entry-based anomaly detection framework designed to enrich raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog integrates contextual information, including historical examples and reasoning derived from the entire corpus, to enable more accurate and interpretable anomaly detection without requiring retraining. The framework employs retrieval-augmented generation to seamlessly incorporate relevant contextual knowledge during inference. The evaluation of EnrichLog on four large-scale system log benchmark datasets, compared against five baseline methods, demonstrates consistent improvements in anomaly detection performance. The framework effectively manages ambiguous log entries while maintaining efficient inference speeds. Additionally, combining corpus- and sample-specific knowledge boosts the model’s confidence and detection accuracy. These attributes make EnrichLog a promising and practical solution for real-world anomaly detection deployments in system log analysis. <div>
arXiv:2512.11997v1 Announce Type: new 
Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp</title>
<link>https://arxiv.org/abs/2512.12048</link>
<guid>https://arxiv.org/abs/2512.12048</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent coordination, electric vehicle charging, deep reinforcement learning, context-aware decision-making, sustainable transportation  

<br><br>Summary:  
This paper introduces CAMAC-DRA, a novel context-sensitive multi-agent coordination framework designed to optimize dynamic resource allocation in smart electric vehicle (EV) charging ecosystems via the Smart2Charge application. It coordinates autonomous charging agents over networks involving 250 EVs and 45 charging stations while adapting to varying environmental conditions using context-aware decision-making. The system incorporates Deep Q-Networks combined with Graph Neural Networks and attention mechanisms to process 20 contextual features, such as weather, traffic, grid load, and electricity prices. CAMAC-DRA balances the interests of five stakeholders—EV users (25%), grid operators (20%), charging station operators (20%), fleet operators (20%), and environmental factors (15%)—through weighted coordination and consensus protocols. Validation on real-world data with 441,077 charging transactions demonstrates superior results compared to baseline algorithms like DDPG, A3C, PPO, and other GNN-based methods, achieving a 92% coordination success rate, 15% energy efficiency gain, 10% cost reduction, 20% decrease in grid strain, and 2.3x faster convergence. The system maintains 88% training stability and 85% sample efficiency. Real-world tests confirm commercial viability with a Net Present Cost of -$122,962 and a 69% reduction in costs through renewable energy integration. Overall, CAMAC-DRA offers a breakthrough in intelligent EV charging coordination by balancing multiple stakeholder objectives while adapting in real-time for sustainable transportation electrification. <div>
arXiv:2512.12048v1 Announce Type: new 
Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification</title>
<link>https://arxiv.org/abs/2512.12059</link>
<guid>https://arxiv.org/abs/2512.12059</guid>
<content:encoded><![CDATA[
<div> Forecast monitoring, Large Language Models, time series forecasting, anomaly detection, multi-modal learning<br><br>Summary:  
This paper introduces The Forecast Critic, a system that utilizes Large Language Models (LLMs) for automated monitoring of time series forecasts, aimed at improving accuracy in large-scale retail businesses. The study investigates three key questions: whether LLMs can identify obviously unreasonable forecasts, how effectively they incorporate unstructured exogenous features for assessment, and how their performance varies with model size and reasoning capabilities. Through three experiments on synthetic and real-world data, results show that LLMs can reliably detect forecast errors such as temporal misalignment, trend inconsistencies, and spikes, achieving an F1 score of 0.88 compared to human-level 0.97. Additionally, multi-modal LLMs that process unstructured contextual information, like historical promotions, enhance the detection of spurious or missing promotional spikes with an F1 score of 0.84. The approach is validated on the real-world M5 dataset, where forecasts flagged as unreasonable have significantly higher error metrics (sCRPS at least 10% higher) than reasonable ones. The findings suggest LLMs, even without domain-specific fine-tuning, provide an effective and scalable solution for automated forecast evaluation and monitoring in retail forecasting contexts. <div>
arXiv:2512.12059v1 Announce Type: new 
Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations</title>
<link>https://arxiv.org/abs/2512.12088</link>
<guid>https://arxiv.org/abs/2512.12088</guid>
<content:encoded><![CDATA[
<div> Keywords: Reliable Policy Iteration, policy iteration, function approximation, deep reinforcement learning, control tasks<br><br>Summary:  
This paper evaluates the robustness of Reliable Policy Iteration (RPI), an algorithm previously proposed to restore the monotonicity-of-value-estimates property of policy iteration within the function approximation context. The research focuses on two classical control environments: CartPole and Inverted Pendulum. RPI's performance is compared against established deep reinforcement learning algorithms including DQN, Double DQN, DDPG, TD3, and PPO. Experimental results demonstrate that RPI achieves near-optimal policy performance early during training and maintains its stability throughout the training process. The study underscores the challenges commonly faced by traditional deep RL methods, such as sample inefficiency, instability during training, and sensitivity to hyperparameters. By contrast, RPI shows greater reliability, suggesting it as a promising alternative for tasks requiring consistent and stable policy improvement. The findings indicate that RPI could be a valuable approach for enhancing the dependability and effectiveness of deep reinforcement learning in control domains. <div>
arXiv:2512.12088v1 Announce Type: new 
Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective</title>
<link>https://arxiv.org/abs/2512.12175</link>
<guid>https://arxiv.org/abs/2512.12175</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context learning, label consistency, Bayesian view, label propagation, prompt demonstration selection  

<br><br>Summary: Large language models (LLMs) perform in-context learning (ICL) by using minimal supervised examples, which improves many NLP tasks. Current methods select prompt demonstrations by retrieving the top-K semantically similar examples, but they often fail to ensure label consistency among these examples. This paper rethinks ICL from both a Bayesian perspective and a transductive label propagation viewpoint, treating ICL as a transductive learning task. By linking label consistency to propagation error bounds, the authors establish a label propagation framework that highlights the importance of consistent labels in guiding query concept understanding. To address inconsistency, the paper introduces a data synthesis method that leverages both semantic and label information to create more reliable demonstrations. They then propose TopK sampling with Synthetic Data (TopK-SD) as a novel approach for demonstration selection, which yields examples with consistent labels. Experiments show that TopK-SD outperforms conventional TopK semantic retrieval methods across multiple benchmarks. Overall, this work offers a fresh theoretical understanding of how prompt demonstration selection impacts ICL and proposes practical improvements to enhance label consistency, thereby improving downstream NLP performance. <div>
arXiv:2512.12175v1 Announce Type: new 
Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation</title>
<link>https://arxiv.org/abs/2512.12177</link>
<guid>https://arxiv.org/abs/2512.12177</guid>
<content:encoded><![CDATA[
<div> Keywords: indoor navigation, visual impairments, foundation model, knowledge graph, large language model  

<br><br>Summary:  
1. Indoor navigation poses significant challenges for people with visual impairments, particularly in dynamic environments where current infrastructure-based systems fall short.  
2. The study introduces Floorplan2Guide, a novel navigation approach that leverages a foundation model to convert floor plans into navigable knowledge graphs and generate user-friendly navigation instructions.  
3. Floorplan2Guide incorporates a large language model (LLM) to automatically extract spatial information from architectural layouts, significantly reducing manual preprocessing compared to previous floorplan parsing methods.  
4. Experimental results show that few-shot learning enhances navigation accuracy over zero-shot learning in both simulated and real-world settings.  
5. Among evaluated models, Claude 3.7 Sonnet achieved the highest accuracy with 92.31%, 76.92%, and 61.54% success rates on short, medium, and long routes respectively, using 5-shot prompting on the MP-1 floor plan.  
6. The graph-based spatial representation improved navigation success rates by 15.4% over direct visual reasoning, confirming that combining graphical structures with in-context learning boosts performance.  
7. These findings demonstrate that Floorplan2Guide offers a more precise and effective indoor navigation solution tailored for Blind and Low Vision (BLV) users. <div>
arXiv:2512.12177v1 Announce Type: new 
Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2512.12182</link>
<guid>https://arxiv.org/abs/2512.12182</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graph Completion, Few-Shot Learning, Diffusion Model, Attention Mechanism, Long-Tailed Distribution  

<br><br>Summary: This paper addresses the challenge of knowledge graph (KG) completion under few-shot learning conditions, particularly focusing on the long-tailed distribution of relations found in real-world data. The authors identify the limitations of prior approaches, such as metric matching and meta learning, which either insufficiently leverage the neighborhood information in the graph or ignore the unique distributional characteristics of contrastive signals. To overcome these issues, the paper proposes a novel generative representation framework that combines a two-stage attention triple enhancer with a U-shaped Knowledge Augmented Network (U-KAN) based diffusion model. This approach enhances the representation of triples while effectively capturing relational context in a generative manner. Extensive empirical evaluations on two public datasets demonstrate that the proposed method achieves new state-of-the-art results in few-shot knowledge graph completion tasks. The study highlights the importance of integrating neighborhood information and generative modeling for improved KG completion, especially in scenarios suffering from data scarcity and long-tail relation distribution. <div>
arXiv:2512.12182v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Theory of Cognition</title>
<link>https://arxiv.org/abs/2512.12225</link>
<guid>https://arxiv.org/abs/2512.12225</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive manifold, Riemannian metric, dual-process cognition, gradient flow, artificial intelligence<br><br>Summary:  
1. The paper proposes a unified mathematical framework for human cognition, modeling it as a point on a differentiable manifold equipped with a learned Riemannian metric that captures representational constraints, computational costs, and variable interrelations.  
2. It defines a scalar cognitive potential incorporating predictive accuracy, task utility, parsimony, and normative requirements, which governs cognitive state evolution via Riemannian gradient flow, providing a universal dynamical law for cognition.  
3. This geometric framework naturally explains classical dual-process phenomena — fast intuitive judgments and slower deliberative reasoning — as emergent properties resulting from anisotropies in the metric that create intrinsic time-scale separations and phase transitions, eliminating the need for modular or hybrid cognitive models.  
4. The authors provide analytical conditions delineating these regimes and validate their approach through simulations of canonical cognitive tasks, demonstrating behavioral signatures consistent with observed psychological effects.  
5. The work establishes a foundational geometric theory of cognition with potential implications for developing more general and human-like artificial intelligence systems, suggesting that cognitive processes can be unified and modeled through geometric principles and continuous dynamical laws. <div>
arXiv:2512.12225v1 Announce Type: new 
Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure</title>
<link>https://arxiv.org/abs/2512.12260</link>
<guid>https://arxiv.org/abs/2512.12260</guid>
<content:encoded><![CDATA[
<div> Keywords: ontology design, Wikidata, polyhierarchy, multi-axial classification, knowledge graphs<br><br>Summary:<br><br>This article contrasts traditional ontology design with Wikidata’s unique approach. Traditional ontologies rely on strict, disjoint top-level categories, such as continuant vs. occurrent or abstract vs. concrete, to create unified hierarchical structures where each entity belongs to a single upper-level class. In contrast, Wikidata does not impose a singular foundational taxonomy; instead, it supports multiple classification axes simultaneously. This results in a polyhierarchical and multi-axial structure rooted under a general class called entity. The paper explores how this design influences the architecture and usability of large-scale knowledge bases. Wikidata’s approach facilitates scalability and modularity in ontology construction, making it especially effective for collaborative environments where the ontology evolves over time. By allowing entities to be classified along multiple dimensions without enforcing mutual exclusivity, Wikidata supports more flexible and adaptive knowledge representations. This contributes to managing complex and diverse data typical of large knowledge graphs. Overall, the paper highlights the advantages of Wikidata’s polyhierarchical model in building collaborative, evolving ontologies that differ fundamentally from traditional rigid taxonomy-based designs. <div>
arXiv:2512.12260v1 Announce Type: new 
Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases</title>
<link>https://arxiv.org/abs/2512.12288</link>
<guid>https://arxiv.org/abs/2512.12288</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, materials discovery, multi-fidelity learning, active validation, quantum-mechanical descriptors<br><br>Summary:<br><br>1. Conventional generative models for materials discovery rely heavily on Density Functional Theory (DFT) data, which use approximate exchange-correlation functionals and often fail in strongly correlated systems, limiting their exploration capacity and accuracy.<br><br>2. The authors propose a quantum-aware generative AI framework that integrates multi-fidelity learning and active validation to overcome the shortcomings of DFT-based models.<br><br>3. Their approach employs a diffusion-based generative model conditioned on quantum-mechanical descriptors, combined with an equivariant neural network potential acting as a validator that is trained on a hierarchical dataset spanning multiple theoretical levels (PBE, SCAN, HSE06, CCSD(T)).<br><br>4. A robust active learning loop quantifies divergence between low- and high-fidelity predictions, focusing computational resources on regions where DFT is less reliable.<br><br>5. Through comprehensive ablation studies and failure mode analyses, the framework was benchmarked against leading generative models (CDVAE, GNoME, DiffCSP), demonstrating 3-5 times higher success rates in identifying stable material candidates in challenging, high-divergence regions such as correlated oxides, while maintaining computational feasibility.<br><br>This work extends the effective search space for computational materials discovery by addressing inherent limitations of single-fidelity DFT models with a transparent, multi-fidelity, and actively validated generative AI framework. <div>
arXiv:2512.12288v1 Announce Type: new 
Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Collapse: A Universal Failure Mode of Intelligent Systems</title>
<link>https://arxiv.org/abs/2512.12381</link>
<guid>https://arxiv.org/abs/2512.12381</guid>
<content:encoded><![CDATA[
<div> entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality<br><br>Summary:  
This article explores a universal failure mode in intelligent systems termed "entropy collapse," which occurs when feedback amplification surpasses the system's capacity for novelty regeneration. The authors demonstrate that intelligent systems, across diverse domains such as artificial intelligence, economics, and biological evolution, experience a sharp transition from a high-entropy, adaptive state to a low-entropy, collapsed state. This collapse does not imply a total loss of activity but rather a contraction in the effective adaptive dimensionality, signifying rigidity and reduced adaptability. The study formally models this phenomenon, identifying critical thresholds, irreversibility in dynamics, and stable attractor structures that characterize the collapse. Through analytical and minimal simulation approaches, the work shows the universality of entropy collapse across different update mechanisms, unifying observed issues like model collapse in AI, institutional stagnation in economics, and genetic bottlenecks in evolution under a common framework. Importantly, the article reframes entropy collapse as an inherent structural cost of increasing intelligence, explaining why late-stage interventions often fail. Finally, it advocates for entropy-aware design principles to help maintain adaptability and robustness over the long term in intelligent systems. <div>
arXiv:2512.12381v1 Announce Type: new 
Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feeling the Strength but Not the Source: Partial Introspection in LLMs</title>
<link>https://arxiv.org/abs/2512.12411</link>
<guid>https://arxiv.org/abs/2512.12411</guid>
<content:encoded><![CDATA[
<div> Keywords: introspection, language models, injected concepts, robustness, Meta-Llama-3.1-8B-Instruct<br><br>Summary: This work evaluates Anthropic's claim that language models can detect and name injected "concepts" as activation directions within their internal representations. First, the study reproduces Anthropic's multi-turn "emergent introspection" on the Meta-Llama-3.1-8B-Instruct model, confirming that the model identifies and names the injected concept about 20% of the time, matching previous reported results and showing that such introspection is not limited to very large or highly capable models. Second, the researchers test the robustness of this introspection by varying the inference prompt and performing related tasks like multiple-choice identification or binary discrimination to detect concept injection; performance significantly deteriorates, indicating that introspection is fragile and highly sensitive to prompt design. Third, the study discovers a contrasting "partial introspection" capability in which the same model reliably classifies the strength of the injected concept vector's coefficient into four categories (weak, moderate, strong, very strong) with up to 70% accuracy, well above random chance. Overall, the results support that language models internally compute functions related to their baseline representations during introspection, but the fidelity of their self-reports is narrow and prompt-dependent. The authors provide their code publicly for further research and verification. <div>
arXiv:2512.12411v1 Announce Type: new 
Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale</title>
<link>https://arxiv.org/abs/2512.12413</link>
<guid>https://arxiv.org/abs/2512.12413</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Critical Thinking, Verification, AI Use Scale, Fact-Checking<br><br>Summary:<br><br>The article addresses the importance of critical thinking when using generative AI tools, emphasizing that users must verify and reflect on AI-generated content rather than accept it uncritically. Researchers developed and validated a 13-item scale measuring critical thinking in AI use, encompassing three factors: Verification, Motivation, and Reflection. The scale was created and content-validated in Study 1, while Study 2 confirmed its three-factor structure. Studies 3 to 5 further validated the scale by demonstrating its reliability, strong factor loadings, sex invariance, and both convergent and discriminant validity. Additionally, Studies 3 and 4 found positive correlations between critical thinking in AI use and personality traits such as openness, extraversion, and positive affect, as well as with increased frequency of AI usage. Study 6 established criterion validity, linking higher scale scores to more frequent and varied verification strategies, better accuracy in veracity judgments within a ChatGPT fact-checking task, and deeper reflection on responsible AI use. Overall, the research clarifies mechanisms by which people oversee generative AI outputs and offers a validated tool and task paradigm for future theoretical, cross-group, and longitudinal studies on engaging critically with AI-generated information. <div>
arXiv:2512.12413v1 Announce Type: new 
Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline</title>
<link>https://arxiv.org/abs/2512.12443</link>
<guid>https://arxiv.org/abs/2512.12443</guid>
<content:encoded><![CDATA[
<div> Keywords: AI model documentation, transparency framework, safety evaluation, multi-agent pipeline, compliance gaps  

<br><br>Summary:  
This study addresses the fragmentation and inconsistency of AI model documentation across platforms, which hinders stakeholders such as policymakers, auditors, and users from effectively assessing safety claims, data provenance, and version-level changes. The analysis covered documentation from five leading models—Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5—alongside 100 Hugging Face model cards, revealing 947 unique section names with high naming variability. Usage information was labeled under 97 distinct titles alone. To standardize transparency, the researchers developed a weighted framework based on the EU AI Act Annex IV and Stanford Transparency Index, comprising 8 main sections and 23 subsections, prioritizing safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical details. An automated multi-agent pipeline was created to extract and score documentation completeness using LLM-based consensus. The evaluation of 50 models from various domains including vision, multimodal, open-source, and closed-source models cost under $3 and showed consistent transparency gaps. Frontier labs such as xAI, Microsoft, and Anthropic scored around 80% compliance, whereas most providers fell below 60%. Significant deficits were observed in safety-critical areas, especially regarding deception behaviors, hallucinations, and child safety assessments, which contributed to the largest aggregate points lost across models. <div>
arXiv:2512.12443v1 Announce Type: new 
Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.12477</link>
<guid>https://arxiv.org/abs/2512.12477</guid>
<content:encoded><![CDATA[
<div> Keywords: Node Importance Estimation, Heterogeneous Knowledge Graphs, Meta-path, Hypergraph Contrastive Learning, Cross-modal Alignment  

<br><br>Summary:  
Node importance estimation (NIE) in heterogeneous knowledge graphs is vital for tasks like recommendation, knowledge reasoning, and question answering. Existing methods typically rely on pairwise connections, missing out on high-order dependencies spanning multiple entities and relations. They also treat structural and semantic information separately, which limits the integration of these complementary signals. To overcome these issues, the authors propose MetaHGNIE, a novel framework based on meta-path induced hypergraph contrastive learning, which disentangles and aligns structural and semantic features effectively. MetaHGNIE constructs a higher-order knowledge graph using meta-path sequences, where typed hyperedges represent multi-entity relational contexts, capturing complex interactions. Structural dependencies are aggregated via local attention mechanisms, while semantic information is encoded through a hypergraph transformer enhanced with sparse chunking to minimize redundancy. A multimodal fusion module then integrates structural and semantic embeddings using contrastive learning with auxiliary supervision to ensure robust cross-modal alignment. Extensive experiments on standard NIE benchmarks demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines, validating the advantage of explicitly modeling higher-order interactions and facilitating cross-modal alignment in heterogeneous knowledge graphs. The authors provide their implementation publicly, supporting reproducibility and further research. <div>
arXiv:2512.12477v1 Announce Type: new 
Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.12501</link>
<guid>https://arxiv.org/abs/2512.12501</guid>
<content:encoded><![CDATA[
<div> Generative AI, text-to-image, ethical safeguards, Trustworthy AI, fairness-aware training<br><br>Summary:  
1. The paper addresses the dual-use dilemma of generative AI text-to-image systems like DALL.E, Stable Diffusion, and Midjourney, highlighting ethical concerns including amplification of societal biases, production of disinformation, and intellectual property violations.  
2. It introduces SafeGen, a novel framework embedding ethical safeguards directly into the generation pipeline, designed following Trustworthy AI principles to ensure responsible use.  
3. SafeGen consists of two core components: BGE-M3, a fine-tuned multilingual (English-Vietnamese) text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model generating high-fidelity, semantically accurate images.  
4. The framework is trained on a curated multilingual dataset with a fairness-aware approach, successfully balancing creative freedom with ethical responsibility within one cohesive workflow.  
5. Quantitative results demonstrate strong performance, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 attains an F1-score of 0.81. Ablation studies confirm the value of domain-specific fine-tuning for both modules. Case studies show SafeGen’s practical benefits in blocking unsafe content, creating inclusive educational materials, and supporting academic integrity. <div>
arXiv:2512.12501v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs</title>
<link>https://arxiv.org/abs/2512.12503</link>
<guid>https://arxiv.org/abs/2512.12503</guid>
<content:encoded><![CDATA[
<div> KidsArtBench, Multimodal LLMs, Artistic Evaluation, Multi-LoRA, Educational AI<br><br>Summary:<br><br>1. The paper addresses the limited capability of Multimodal Large Language Models (MLLMs) to evaluate artistic expression, particularly focusing on children's artwork.  
2. It introduces KidsArtBench, a novel benchmark dataset consisting of over 1,000 artworks created by children aged 5-15, annotated by 12 expert educators across 9 rubric-aligned evaluation dimensions, accompanied by expert feedback comments.  
3. Unlike existing aesthetic datasets that provide single scalar scores on adult art, KidsArtBench provides multi-dimensional, ordinal annotations and formative feedback specifically tailored for children's artwork.  
4. The authors propose an attribute-specific multi-LoRA method, assigning each attribute (e.g., Realism, Imagination) to separate evaluation dimensions, combined with Regression-Aware Fine-Tuning (RAFT) to better align model predictions with ordinal scales.  
5. Experimental results on the Qwen2.5-VL-7B model demonstrate significant performance improvements, with correlation increasing from 0.468 to 0.653, especially in perceptual attributes and diminishing gaps in higher-order traits.  
6. These findings indicate that educator-aligned supervision and attribute-aware training lead to evaluations that are pedagogically meaningful, establishing KidsArtBench as a rigorous testbed for continuous progress in educational AI.  
7. The dataset, code, and ethical documentation are made publicly available to support further research in this domain. <div>
arXiv:2512.12503v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2512.12548</link>
<guid>https://arxiv.org/abs/2512.12548</guid>
<content:encoded><![CDATA[
<div> Keywords: Patch foraging, Marginal Value Theorem, model-based reinforcement learning, anticipatory behavior, ecological optimality principles  

<br><br>Summary:  
1. The article investigates patch foraging behavior, which involves deciding the optimal moment to leave a resource-rich area to search for potentially better options.  
2. The Marginal Value Theorem (MVT) is a key theoretical model often used to describe optimal patch-leaving strategies in behavioral ecology.  
3. Despite MVT’s widespread use, the underlying computational mechanisms by which biological foragers achieve these optimal decisions are not fully understood.  
4. The study demonstrates that artificial agents equipped with learned predictive world models, using model-based reinforcement learning, naturally develop strategies that align with MVT predictions.  
5. These model-based agents rely on anticipatory abilities rather than simple reward maximization to execute efficient patch-leaving, differentiating them from standard model-free RL agents.  
6. The decision patterns of these agents closely mirror those observed in biological foragers, suggesting that predictive world models provide a biologically plausible basis for decision-making in AI.  
7. Overall, the research emphasizes the importance of ecological optimality principles, like MVT, in designing AI systems that are both interpretable and adaptive, bridging gaps between biological insights and artificial decision processes. <div>
arXiv:2512.12548v1 Announce Type: new 
Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Newsvendor: Decision Biases and Cognitive Mechanisms</title>
<link>https://arxiv.org/abs/2512.12552</link>
<guid>https://arxiv.org/abs/2512.12552</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cognitive biases, newsvendor problem, decision-making, supply chain management  

<br><br>Summary: This study explores how large language models (LLMs) replicate and even amplify human cognitive biases in decision-making, particularly in supply chain management through the dynamic newsvendor problem. The researchers conducted multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, examining five known decision biases. They found that all models consistently exhibited the classic "Too Low/Too High" ordering bias and amplified behaviors such as demand-chasing compared to human benchmarks. Notably, GPT-4, the most sophisticated model, displayed the highest irrationality due to overthinking, while GPT-4o, optimized for efficiency, performed near-optimally. These biases persisted even when optimal decision-making formulas were provided, indicating that the origins lie in architectural constraints rather than lack of knowledge. From a managerial perspective, the findings suggest that selecting an AI model should be task-specific since simpler, efficiency-tuned models may outperform complex ones for certain optimization tasks. The amplification of biases by LLMs underlines the importance of robust human-in-the-loop oversight in high-stakes operational decisions to avoid costly errors. Additionally, designing structured, rule-based prompts emerges as an effective approach for managers to limit heuristic-driven errors and enhance AI-assisted decision reliability. <div>
arXiv:2512.12552v1 Announce Type: new 
Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation</title>
<link>https://arxiv.org/abs/2512.12597</link>
<guid>https://arxiv.org/abs/2512.12597</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, explainability, Shapley values, tool attribution, Monte Carlo sampling<br><br>Summary:<br><br>This paper introduces AgentSHAP, a novel explainability framework designed to assess the contribution of external tools used by large language model (LLM) agents in solving complex tasks. Unlike existing explainability methods, AgentSHAP focuses on tool-level importance and is model-agnostic, operating as a black-box method applicable across various LLMs such as GPT, Claude, and Llama without requiring access to internal model parameters or gradients. The core technical innovation is the use of Monte Carlo Shapley values from game theory, which fairly allocate importance scores by evaluating agent outputs across different subsets of tools. This approach significantly reduces computational complexity from an exponential O(2^n) scale to a manageable level, enabling practical application. The authors provide extensive experiments on the API-Bank benchmark, demonstrating that AgentSHAP yields consistent importance scores, accurately identifies relevant tools, and effectively distinguishes between useful and irrelevant tools involved in generating responses. AgentSHAP complements existing XAI methods like TokenSHAP and PixelSHAP, thus expanding the family of Shapley-based explainability tools tailored for modern generative AI systems. The authors also offer open-source implementation at https://github.com/GenAISHAP/TokenSHAP, fostering further research and adoption. <div>
arXiv:2512.12597v1 Announce Type: new 
Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2512.12634</link>
<guid>https://arxiv.org/abs/2512.12634</guid>
<content:encoded><![CDATA[
<div> Mobile GUI Agents, Benchmarking, Offline Evaluation, Modular Analysis, Mobile Applications<br><br>Summary:<br><br>1. Mobile GUI Agents are AI systems designed to interact with mobile applications on behalf of users, aiming to revolutionize human-computer interaction. 2. Current evaluation methods for these agents have two main issues: reliance on either single-path offline benchmarks, which unfairly penalize valid alternate actions, or online benchmarks, which have poor scalability and reproducibility because of their dynamic nature. 3. Existing benchmarks treat agents as black boxes, ignoring the performance and contributions of individual components, which leads to unfair comparisons and hides key bottlenecks. 4. To overcome these problems, the authors introduce MobiBench, a first-of-its-kind modular and multi-path aware offline benchmarking framework that supports high-fidelity, scalable, and reproducible evaluation fully offline. 5. Experiments show MobiBench's evaluation aligns 94.72% with human judgments, comparable to advanced online benchmarks, while maintaining the reproducibility and scalability advantages of offline methods. 6. A detailed module-level analysis using MobiBench reveals insights into different techniques in mobile GUI agents, optimal module configurations for various model sizes, limitations of current large foundation models (LFMs), and offers practical guidelines to build more capable and cost-efficient mobile agents. <div>
arXiv:2512.12634v1 Announce Type: new 
Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Aware Multiagent Systems</title>
<link>https://arxiv.org/abs/2512.12652</link>
<guid>https://arxiv.org/abs/2512.12652</guid>
<content:encoded><![CDATA[
<div> value awareness, AI ethics, value alignment, formal semantics, explainability<br><br>Summary:<br><br>This paper introduces the novel concept of value awareness in artificial intelligence (AI), extending beyond the conventional value-alignment problem to foster a deeper understanding and integration of human values within AI systems. The authors propose a clear and streamlined roadmap for developing value-aware AI, structured around three fundamental pillars. First, the roadmap emphasizes learning and formally representing human values through the use of formal semantics, providing a rigorous foundation for encoding complex value systems. Second, it addresses ensuring value alignment not only for individual AI agents but also for multiagent systems, highlighting the challenges and methodologies for maintaining consistent value-driven behavior in diverse and interacting systems. Third, the framework includes the provision of value-based explainability, aiming to make AI behavior transparent and interpretable in terms of underlying human values. The paper additionally shares ongoing research efforts that explore these themes and demonstrates practical applications of the approach in real-world domains, showcasing the feasibility and relevance of value awareness in enhancing AI reliability, safety, and societal compatibility. This work thus contributes to advancing AI ethics by offering both theoretical and applied insights into embedding and explaining human values within intelligent systems. <div>
arXiv:2512.12652v1 Announce Type: new 
Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI</title>
<link>https://arxiv.org/abs/2512.12686</link>
<guid>https://arxiv.org/abs/2512.12686</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic memory, large language models, personalized AI, knowledge graph, session summarization<br><br>Summary:  
The paper introduces Memoria, a modular memory framework designed to enhance large language model (LLM)-based conversational systems with agent-like persistence. Agentic memory is defined as the capability of LLMs to retain and act upon information across user interactions, supporting continuity, personalization, and long-term context. Memoria combines two main components: dynamic session-level summarization, which helps maintain short-term dialogue coherence, and a weighted knowledge graph (KG)-based user modeling engine that incrementally encodes user traits, preferences, and behaviors as structured entities and relationships. This hybrid design allows the framework to operate efficiently within the token limits of current LLMs while supporting both immediate conversational flow and evolving personalization over time. The system bridges the gap between stateless LLM interfaces and persistent agentic memory systems, enabling scalable and context-rich conversational AI experiences. Memoria offers a practical solution for deploying adaptive, personalized AI agents in industry applications that require sustained and evolving user engagement. <div>
arXiv:2512.12686v1 Announce Type: new 
Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment</title>
<link>https://arxiv.org/abs/2512.12692</link>
<guid>https://arxiv.org/abs/2512.12692</guid>
<content:encoded><![CDATA[
<div> Keywords: WebOperator, tree-search, backtracking, web navigation, LLM agents<br><br>Summary:<br><br>1. LLM-based agents for web tasks typically operate in a greedy, step-by-step manner without foresight, often causing troubles in partially observable web environments where errors are difficult to undo.<br>2. Existing tree-search methods enable structured exploration but assume all actions are reversible and lack safe backtracking mechanisms, leading to unintended side effects and reduced effectiveness.<br>3. WebOperator is introduced as a novel tree-search framework that allows reliable backtracking and strategic exploration to overcome these limitations.<br>4. The framework employs a best-first search strategy that ranks actions by expected rewards and safety, combined with a robust backtracking method that verifies path feasibility before replaying to avoid side effects.<br>5. WebOperator enhances exploration by generating diverse action candidates from multiple reasoning contexts and filters invalid or semantically redundant actions pre-execution.<br>6. Experimental results on WebArena and WebVoyager demonstrate WebOperator’s effectiveness, achieving a state-of-the-art 54.6% success rate on WebArena using gpt-4o, highlighting the importance of integrating strategic foresight with safe action execution in web-based LLM agents. <div>
arXiv:2512.12692v1 Announce Type: new 
Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12706</link>
<guid>https://arxiv.org/abs/2512.12706</guid>
<content:encoded><![CDATA[
<div> Keywords: automated game testing, reinforcement learning, structural coverage, large language models, gameplay validation<br><br>Summary:<br><br>The paper addresses challenges in automated game testing within the "Games as a Service" model, where frequent content updates demand rigorous quality assurance. It identifies a dichotomy in current methods: code-centric approaches emphasize structural code coverage but miss gameplay context, whereas player-centric agents focus on gameplay intent but lack thorough underlying code verification. To bridge this gap, the authors introduce SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that integrates structural verification with functional validation. SMART utilizes large language models (LLMs) to analyze abstract syntax tree (AST) changes and infer the functional intent behind code modifications. This insight enables the creation of a context-aware hybrid reward system that directs reinforcement learning agents to fulfill sequential gameplay goals while effectively exploring altered code branches. The framework is evaluated on two complex game environments, Overcooked and Minecraft. Results demonstrate that SMART surpasses state-of-the-art baselines by achieving over 94% branch coverage on modified code—nearly twice that of conventional reinforcement learning—while maintaining a 98% success rate in completing gameplay tasks. This balance of comprehensive structural coverage and functional correctness suggests SMART is a promising advance for automated game testing in rapid development cycles. <div>
arXiv:2512.12706v1 Announce Type: new 
Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks</title>
<link>https://arxiv.org/abs/2512.12736</link>
<guid>https://arxiv.org/abs/2512.12736</guid>
<content:encoded><![CDATA[
<div> Keywords: QoE prediction, demographic-aware, data augmentation, machine learning, 5G video streaming<br><br>Summary:<br><br>1. The paper addresses the challenge of predicting Quality of Experience (QoE) for adaptive video streaming in 5G networks, emphasizing the need for accurate and personalized QoE estimation to improve resource management and user-centric services.<br><br>2. Traditional QoE prediction models are limited by small datasets and the assumption of uniform user perception, which do not reflect the diversity of real-world user experiences.<br><br>3. To overcome these limitations, the authors propose a demographic-aware machine learning framework that incorporates behaviorally realistic, demographic-based data augmentation. This strategy amplifies a small initial QoE dataset sixfold by simulating varying user sensitivities to common streaming impairments such as rebuffering, bitrate fluctuations, and quality degradation.<br><br>4. The augmented dataset is used to train and evaluate a range of machine learning models, including classical algorithms and advanced deep learning architectures like attention-based MLPs and TabNet.<br><br>5. Experimental results showcase significant improvements in prediction accuracy measured by RMSE, MAE, and correlation metrics. Among all evaluated models, TabNet performs best due to its built-in feature selection and attention mechanisms.<br><br>6. The study concludes that demographic-aware data augmentation significantly enhances QoE prediction robustness and scalability, paving the way for personalized QoE-aware intelligence in 5G video streaming systems. <div>
arXiv:2512.12736v1 Announce Type: new 
Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Counterfactuals Reconsidered</title>
<link>https://arxiv.org/abs/2512.12804</link>
<guid>https://arxiv.org/abs/2512.12804</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactuals, probabilistic causal models, Pearl semantics, Markov condition, causal abstractions

<br><br>Summary:  
1. The paper develops a novel semantics for probabilities of counterfactuals that extends beyond the standard Pearlian framework, addressing probabilistic causal models that cannot be embedded into classical structural causal models.  
2. The need for this generalization is demonstrated by showing that such challenging probabilistic causal models appear even in simple scenarios.  
3. The proposed semantics offer a middle ground in the debate between Pearl and Dawid: agreeing with Dawid in rejecting universal causal determinism and unrealistic variables, while endorsing Pearl’s view that a general counterfactual semantics remains achievable.  
4. The focus is restricted to causal models satisfying the Markov condition, containing only realistic (observable or well-defined) variables, and being causally complete, improving the conceptual and practical foundations of counterfactual reasoning.  
5. Though formulated using structural causal models as Pearl does, the semantics avoid the use of response variables, making the approach simpler and more general. Additionally, the author proves equivalence with two recent alternative semantics not reliant on structural causal models and aligns the theory with broader literature on stochastic counterfactuals. Finally, the paper reflects on the universality of the Markov condition and proposes a novel extension of causal abstractions. <div>
arXiv:2512.12804v1 Announce Type: new 
Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution</title>
<link>https://arxiv.org/abs/2512.12806</link>
<guid>https://arxiv.org/abs/2512.12806</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fault-Tolerant Sandboxing, transactional filesystem, autonomous agents, safety risks  

<br><br>Summary:  
This paper addresses the safety challenges posed by the transition of Large Language Models (LLMs) from passive code generation tools to autonomous agents capable of executing commands without user intervention. The authors identify significant risks including destructive commands and inconsistent system states that arise in fully autonomous operation. Existing commercial solutions focus on interactive user safety, often requiring authentication steps that disrupt the automated, headless workflows essential to true autonomy. To mitigate these issues, the paper proposes a Fault-Tolerant Sandboxing framework that combines a policy-based interception layer with a transactional filesystem snapshot mechanism. By encapsulating agent actions within atomic transactions, the framework ensures system safety while maintaining acceptable latency, thus avoiding the overhead and friction associated with containerization or CLI-based authentication. The method was experimentally validated using the Minimind-MoE LLM served through nano-vllm in a Proxmox testbed with EVPN/VXLAN network isolation. The results demonstrate perfect interception of high-risk commands and successful rollback of failed states, all achieved with a modest 14.5% performance overhead (~1.8 seconds per transaction). This outperforms existing Gemini CLI sandboxes that require interactive sign-ins, making the proposed framework better suited for autonomous agent workflows. <div>
arXiv:2512.12806v1 Announce Type: new 
Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents</title>
<link>https://arxiv.org/abs/2512.12856</link>
<guid>https://arxiv.org/abs/2512.12856</guid>
<content:encoded><![CDATA[
<div> Keywords: generative agents, memory management, forgetting policies, privacy preservation, agent evaluation<br><br>Summary:<br><br>This paper addresses critical challenges in memory management for generative agents operating in long-term interactive scenarios, highlighting performance, privacy, and computational issues. It critiques existing methods that either keep unlimited memory, causing computational and privacy problems, or use simplistic forgetting strategies that reduce agent coherence and capabilities. The authors introduce the Memory-Aware Retention Schema (MaRS), a novel framework designed for human-centered memory management that integrates six theoretically grounded forgetting policies. These policies aim to optimize the trade-offs between performance, privacy, and computational efficiency. To evaluate these approaches, the paper presents the Forgetful but Faithful Agent (FiFA) benchmark, which assesses agents on narrative coherence, goal achievement, social recall accuracy, privacy preservation, and cost efficiency. Experimental results from 300 evaluation runs across various memory budgets and agent configurations demonstrate that a hybrid forgetting policy within MaRS achieves a superior composite performance score of 0.911, offering improved coherence alongside strict privacy guarantees and computational tractability. The study establishes new benchmarks for evaluating memory-budgeted generative agents and offers practical guidelines for deploying such agents in resource-constrained and privacy-sensitive contexts. Overall, the theoretical insights, framework, and empirical findings contribute significantly to human-centered AI by tackling fundamental agent memory management challenges that influence user trust, scalability, and compliance with regulations. <div>
arXiv:2512.12856v1 Announce Type: new 
Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Satisfiability Modulo Theory Meets Inductive Logic Programming</title>
<link>https://arxiv.org/abs/2512.12918</link>
<guid>https://arxiv.org/abs/2512.12918</guid>
<content:encoded><![CDATA[
<div> Keywords: Inductive Logic Programming, SMT solver, numerical constraints, PyGol, hybrid rules<br><br>Summary:<br><br>This paper explores extending Inductive Logic Programming (ILP) by integrating it with Satisfiability Modulo Theories (SMT) to better handle numerical constraints within relational domains. Traditional ILP frameworks predominantly work with discrete predicates and require either discretisation or manually designed numerical predicates, which limits their ability to learn arithmetic relations or thresholds that apply jointly across examples. The authors propose a modular approach that couples the ILP system PyGol with the SMT solver Z3. PyGol generates candidate clauses interpreted as quantifier-free formulas over background theories like linear or nonlinear real arithmetic. The SMT solver then instantiates and verifies the numerical parameters, enabling the induction of hybrid rules combining symbolic relational predicates with numerical constraints such as thresholds, intervals, and multi-literal arithmetic relations. This architecture preserves ILP's declarative bias while increasing expressivity and flexibility. The SMT-ILP framework is formalized and evaluated on synthetic datasets designed to test linear, relational, nonlinear, and multi-hop reasoning capabilities. Experimental results demonstrate that the modular SMT-ILP approach complements existing numerical ILP methods and provides a solid foundation for future enhancements toward richer, theory-aware inductive learning. <div>
arXiv:2512.12918v1 Announce Type: new 
Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open Standards for Systemic Complexity in Digital Forensics</title>
<link>https://arxiv.org/abs/2512.12970</link>
<guid>https://arxiv.org/abs/2512.12970</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, digital forensics, forensic errors, human-readable artifacts, open standards<br><br>Summary:<br><br>1. The paper discusses the increasing complexity and ubiquity of the intersection between artificial intelligence (AI) and digital forensics (DF), highlighting how overlapping techniques and technologies are being integrated into various scientific and technical fields. <br>2. It acknowledges that despite significant advancements in forensic sciences, errors remain prevalent, and forensic processes are still susceptible to human and systemic fallibility. <br>3. To address these challenges, the paper identifies systemic complexity as a key factor contributing to errors and proposes measures to mitigate this. <br>4. One major recommendation is the adoption of human-readable artifacts to improve transparency, interpretability, and trustworthiness in forensic analyses involving AI. <br>5. The study also advocates for the use of open standards to enhance consistency, reproducibility, and collaborative validation within the digital forensics community. <br>6. Finally, the paper outlines a state-of-the-art DF AI model schema designed to integrate these principles, aiming to reduce errors and improve the reliability of AI-assisted digital forensic investigations. <div>
arXiv:2512.12970v1 Announce Type: new 
Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization</title>
<link>https://arxiv.org/abs/2512.13070</link>
<guid>https://arxiv.org/abs/2512.13070</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-supervised reinforcement learning, policy collapse, M-GRPO, entropy filtering, Large Language Models<br><br>Summary:<br><br>This paper addresses a significant challenge in self-supervised reinforcement learning (RL) for enhancing Large Language Models (LLMs), identifying a critical failure mode termed "policy collapse" that leads to drastic performance degradation during long-horizon training. The authors demonstrate that increasing the number of rollouts delays but does not prevent this collapse, indicating that conventional scaling strategies are insufficient. To resolve this instability, they introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a novel framework that utilizes a slowly evolving momentum model to provide a stable training target, effectively stabilizing the training process. Additionally, the study finds that training instability is linked to a rapid decrease in policy entropy, causing the model to adopt a prematurely confident yet suboptimal policy. To counter this, the authors propose an adaptive interquartile range (IQR) filtering method that dynamically removes low-entropy trajectories, thereby preserving policy diversity essential for robust learning. Extensive experiments conducted on multiple reasoning benchmarks validate that M-GRPO combined with the IQR filter improves training stability and prevents premature convergence. Together, these innovations lead to state-of-the-art performance in self-supervised RL for reasoning tasks in LLMs, offering a valuable strategy for enhancing model reasoning without expensive human annotations. <div>
arXiv:2512.13070v1 Announce Type: new 
Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socratic Students: Teaching Language Models to Learn by Asking Questions</title>
<link>https://arxiv.org/abs/2512.13102</link>
<guid>https://arxiv.org/abs/2512.13102</guid>
<content:encoded><![CDATA[
<div> interactive learning, large language models, student-teacher interaction, Direct Preference Optimization, question generation<br><br>Summary:<br><br>1. Large Language Models (LLMs) are strong at answering static queries based on their internal knowledge but struggle in settings requiring acquisition of new information through active interaction.  
2. In practical domains like education and medical assistance, information is often not directly available, necessitating an interactive agent that can identify its uncertainty, ask relevant questions, and retain newly acquired knowledge efficiently.  
3. Previous research has mainly focused on how teachers can guide students by identifying knowledge gaps and giving instructions, whereas this work shifts attention to the student’s role in actively querying the teacher.  
4. Experimental results on math and coding benchmarks demonstrate that student-led querying strategies consistently outperform static approaches, showing improvements of at least 0.5 in Pass@k metrics starting from near-zero baseline performance.  
5. To further improve question quality and learning efficiency, the authors train students using Direct Preference Optimization (DPO) with guidance sourced from either the model itself (self) or stronger student models, enabling smaller models to learn better questioning strategies. <div>
arXiv:2512.13102v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
<link>https://arxiv.org/abs/2512.13131</link>
<guid>https://arxiv.org/abs/2512.13131</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D gesture generation, speech-driven animation, hierarchical modeling, periodic autoencoders, co-speech gestures<br><br>Summary:<br><br>This paper addresses the challenge of generating realistic 3D body movements from speech, which has potential applications but struggles with producing natural human-like gestures. Existing approaches mainly use end-to-end learning models such as GANs, VQ-VAE, and diffusion models but often fail to capture the crucial inter- and intra-correlations between different motion units like the head, body, and hands. To overcome this, the authors propose a Hierarchical Implicit Periodicity (HIP) learning framework. The HIP approach features two key innovations: first, it disentangles complex gesture movements by exploring gesture motion phase manifolds using periodic autoencoders, which model human-like periodic patterns while also incorporating non-periodic latent states to introduce instance-level diversity. Second, it models the hierarchical dependencies among face motions, body gestures, and hand movements by using cascaded guidance during the learning process to produce coordinated animations. The method was demonstrated on 3D avatars and validated through extensive experiments, showing superior performance to state-of-the-art co-speech gesture generation techniques in both quantitative metrics and qualitative assessments. The authors plan to release their code and trained models publicly, enabling further research and practical applications in speech-driven 3D gesture animation. <div>
arXiv:2512.13131v1 Announce Type: new 
Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
<link>https://arxiv.org/abs/2512.13142</link>
<guid>https://arxiv.org/abs/2512.13142</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, abortion stigma, Individual Level Abortion Stigma Scale, multilevel understanding, AI safety<br><br>Summary: This study evaluates whether large language models (LLMs) genuinely understand the complex psychological and physiological aspects of abortion stigma. It uses the validated Individual Level Abortion Stigma Scale (ILAS) to systematically test 627 diverse personas across five leading LLMs. The analysis focuses on three stigma levels: cognitive (self-judgment), interpersonal (anticipated judgment and isolation), and structural (community condemnation and disclosure patterns). The results reveal that LLMs fail to exhibit genuine understanding across all these levels. Specifically, models tend to overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation without nuance, and introduce demographic biases not present in human-validated data. Additionally, LLMs miss empirically established relationships, such as the connection between stigma and secrecy, and sometimes produce contradictory responses within theoretical frameworks. These findings indicate that current model alignment techniques ensure linguistically appropriate outputs but do not guarantee coherent, multilevel comprehension of complex human experiences. The paper argues that AI safety in sensitive, high-stakes areas requires novel design principles emphasizing multilevel coherence, continuous auditing protocols, regulatory measures including mandatory audits and accountability, and improved AI literacy especially in contexts where understanding unspoken experiences is essential to providing effective support. <div>
arXiv:2512.13142v1 Announce Type: new 
Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations</title>
<link>https://arxiv.org/abs/2512.13154</link>
<guid>https://arxiv.org/abs/2512.13154</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, clarification dialogue, user ambiguity, conversational agents, MultiWOZ

<br><br>Summary:  
This paper addresses the challenge of resolving ambiguous user requests in conversational agents, which is crucial for effective task completion. The authors focus on the underexplored problem of when and how multiple agents should initiate clarifications and coordinate their actions during uncertain or incomplete user inputs. To tackle this, they propose MAC (Multi-Agent Clarification), a novel interactive multi-agent framework specifically optimized for managing clarification dialogues. The approach begins with a new taxonomy that categorizes types of user ambiguities to systematically guide clarification strategies. MAC autonomously coordinates multiple agents to engage synergistically with users, improving dialogue efficiency and effectiveness. Empirical evaluations on the MultiWOZ 2.4 dataset demonstrate that incorporating clarification at two levels increases the task success rate significantly by 7.8%, from 54.5% to 62.3%. Additionally, the average number of dialogue turns decreases from 6.53 to 4.86, as the system elicits all necessary user information upfront while reducing repetitive exchanges. The results underscore the value of active user engagement and role-aware clarification within multi-agent conversational systems to enhance the reliability and naturalness of human-agent communication. <div>
arXiv:2512.13154v1 Announce Type: new 
Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13159</link>
<guid>https://arxiv.org/abs/2512.13159</guid>
<content:encoded><![CDATA[
<div> Keywords: human-agent collaboration, reinforcement learning, conversational proactivity, clarification questions, task-oriented dialogues<br><br>Summary:<br><br>1. The paper addresses the need for more proactive human-agent collaboration, moving beyond current models that mostly passively respond to user instructions without seeking clarifications.  
2. It introduces SpeakRL, a reinforcement learning method designed to improve agents’ conversational abilities by rewarding proactive behavior, specifically the asking of appropriate clarification questions when needed.  
3. To enable training and evaluation, the authors created SpeakER, a synthetic dataset featuring a variety of task-oriented dialogue scenarios that require interactive clarification to resolve tasks effectively.  
4. The work also explores how to design reward functions that incentivize a balance between agents asking clarifying questions and taking direct actions, optimizing for both engagement and efficiency.  
5. Experimental results demonstrate that SpeakRL achieves a 20.14% absolute improvement in task completion compared to baseline models, without increasing the number of conversation turns, and even outperforms larger proprietary models, highlighting the benefits of clarification-centric conversational strategies in user-agent interactions. <div>
arXiv:2512.13159v1 Announce Type: new 
Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finch: Benchmarking Finance &amp; Accounting across Spreadsheet-Centric Enterprise Workflows</title>
<link>https://arxiv.org/abs/2512.13168</link>
<guid>https://arxiv.org/abs/2512.13168</guid>
<content:encoded><![CDATA[
<div> finance, enterprise workflows, AI benchmarking, LLM-assisted annotation, spreadsheet analysis<br><br>Summary: The paper introduces Finch, a finance and accounting benchmark designed to evaluate AI agents on complex, real-world enterprise workflows encompassing tasks like data entry, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is based on authentic datasets from Enron, including 15,000 spreadsheets and 500,000 emails from 150 employees, and other financial institutions, capturing the messy, multimodal nature of real enterprise data across various domains such as budgeting, trading, and asset management. The benchmark was created through a two-step workflow construction process involving LLM-assisted workflow discovery verified by experts, followed by meticulous expert annotation, requiring over 700 hours of domain-expert effort. This resulted in 172 composite workflows with 384 tasks, involving 1,710 spreadsheets containing 27 million cells, alongside PDFs and other related artifacts, reflecting the complexity and collaborative nature of enterprise work. The study evaluates top AI systems, including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, revealing that even the leading GPT 5.1 Pro, after 48 hours of testing, passes only 38.4% of workflows, while Claude Sonnet 4.5 achieves a 25.0% pass rate. Case studies highlight significant challenges these workflows present for AI agents, emphasizing the need for further research in this area. <div>
arXiv:2512.13168v1 Announce Type: new 
Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection</title>
<link>https://arxiv.org/abs/2512.13240</link>
<guid>https://arxiv.org/abs/2512.13240</guid>
<content:encoded><![CDATA[
<div> Direct Preference Optimization, Reflective Preference Optimization, hallucination reduction, large language models, policy learning  

<br><br>Summary:  
This paper introduces Reflective Preference Optimization (RPO), a novel enhancement to Direct Preference Optimization (DPO), aimed at improving the alignment of large language models (LLMs) and vision-language models. Traditional DPO uses the same policy to generate both chosen and rejected responses, resulting in weak learning signals due to similar errors and low KL divergence between outputs, which slows convergence and makes training unstable. RPO addresses this by incorporating hint-guided reflection, leveraging external models to detect hallucination and produce concise reflective hints. These hints help create on-policy preference pairs with stronger contrastiveness and more distinct preference signals. Theoretically, conditioning on reflective hints increases the expected preference margin through mutual information, enhancing sample efficiency without departing from the original policy family. Empirical results demonstrate that RPO requires fewer training samples and iterations to achieve superior alignment compared to standard DPO. Furthermore, RPO significantly reduces hallucination rates and achieves state-of-the-art performance on various multimodal benchmarks. This approach provides a more effective and lightweight alternative to existing frameworks such as Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF), promising improved training stability and faster convergence in preference-based model alignment. <div>
arXiv:2512.13240v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data</title>
<link>https://arxiv.org/abs/2512.13297</link>
<guid>https://arxiv.org/abs/2512.13297</guid>
<content:encoded><![CDATA[
<div> Keywords: medical data analysis, multi-modal models, MedInsightBench, MedInsightAgent, healthcare insights  

<br><br>Summary:  
1. The paper addresses the need for extracting deep insights from complex multi-modal medical datasets to enhance patient care, diagnostic accuracy, and healthcare operations.  
2. It highlights the current gap in high-quality datasets tailored to evaluate large multi-modal models (LMMs) for medical insight discovery.  
3. The authors introduce MedInsightBench, a novel benchmark containing 332 carefully curated medical cases with annotated insights, designed to assess LMMs' abilities to analyze multi-modal medical images, ask relevant questions, interpret findings, and generate actionable recommendations.  
4. Evaluation shows that existing LMMs struggle on MedInsightBench mainly due to difficulties in extracting deep, multi-step insights and lacking medical expertise.  
5. To overcome this, the paper proposes MedInsightAgent, an automated agent framework composed of three modules—Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer—which enhances general LMM performance in medical insight discovery.  
6. Experimental results on MedInsightBench reveal persistent challenges but demonstrate that MedInsightAgent significantly improves the capability of LMMs in analyzing medical data and generating meaningful insights. <div>
arXiv:2512.13297v1 Announce Type: new 
Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Prompt Optimization for Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2512.13323</link>
<guid>https://arxiv.org/abs/2512.13323</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, arithmetic reasoning, small language models, error-driven optimization, privacy-compliant  

<br><br>Summary:  
The paper addresses the challenge of enhancing arithmetic reasoning capabilities within AI systems, particularly for regulated sectors like finance and healthcare where data privacy is critical. It introduces an error-driven optimization framework designed to improve the performance of Code Generation Agents (CGAs) that leverage on-premises small language models (SLMs). The approach involves systematically clustering erroneous predictions from the model and iteratively refining prompt-based rules to reduce errors. Experiments were conducted using the Qwen3 4B model, a state-of-the-art SLM. The base model initially showed significant limitations in executing accurate arithmetic operations on tabular data. However, after applying the proposed error-driven prompt optimization method, the model's accuracy improved dramatically to 70.8%. This improvement demonstrates that small, privacy-focused models, when paired with strategic prompt engineering, can outperform much larger language models, such as GPT-3.5 Turbo, for specialized arithmetic tasks. The study highlights that such enhancements can be achieved without costly fine-tuning, making the solution more practical and interpretable for industrial deployment. Ultimately, the research provides a pathway for developing reliable, industrial AI assistants capable of secure, on-premises operation with strong arithmetic reasoning skills. <div>
arXiv:2512.13323v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection</title>
<link>https://arxiv.org/abs/2512.13374</link>
<guid>https://arxiv.org/abs/2512.13374</guid>
<content:encoded><![CDATA[

arXiv:2512.13374v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Evolutionary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13399</link>
<guid>https://arxiv.org/abs/2512.13399</guid>
<content:encoded><![CDATA[

arXiv:2512.13399v1 Announce Type: new 
Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings</title>
<link>https://arxiv.org/abs/2512.13481</link>
<guid>https://arxiv.org/abs/2512.13481</guid>
<content:encoded><![CDATA[

arXiv:2512.13481v1 Announce Type: new 
Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defending the Hierarchical Result Models of Precedential Constraint</title>
<link>https://arxiv.org/abs/2512.13505</link>
<guid>https://arxiv.org/abs/2512.13505</guid>
<content:encoded><![CDATA[

arXiv:2512.13505v1 Announce Type: new 
Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph</title>
<link>https://arxiv.org/abs/2512.13510</link>
<guid>https://arxiv.org/abs/2512.13510</guid>
<content:encoded><![CDATA[

arXiv:2512.13510v1 Announce Type: new 
Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level</title>
<link>https://arxiv.org/abs/2410.00015</link>
<guid>https://arxiv.org/abs/2410.00015</guid>
<content:encoded><![CDATA[

arXiv:2410.00015v2 Announce Type: cross 
Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[

arXiv:2508.15250v3 Announce Type: cross 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
<link>https://arxiv.org/abs/2512.11811</link>
<guid>https://arxiv.org/abs/2512.11811</guid>
<content:encoded><![CDATA[

arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare</title>
<link>https://arxiv.org/abs/2512.11814</link>
<guid>https://arxiv.org/abs/2512.11814</guid>
<content:encoded><![CDATA[

arXiv:2512.11814v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique</title>
<link>https://arxiv.org/abs/2512.11818</link>
<guid>https://arxiv.org/abs/2512.11818</guid>
<content:encoded><![CDATA[

arXiv:2512.11818v1 Announce Type: cross 
Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
<link>https://arxiv.org/abs/2512.11827</link>
<guid>https://arxiv.org/abs/2512.11827</guid>
<content:encoded><![CDATA[

arXiv:2512.11827v1 Announce Type: cross 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference with Reusable State-Dependent Value Profiles</title>
<link>https://arxiv.org/abs/2512.11829</link>
<guid>https://arxiv.org/abs/2512.11829</guid>
<content:encoded><![CDATA[

arXiv:2512.11829v1 Announce Type: cross 
Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.11830</link>
<guid>https://arxiv.org/abs/2512.11830</guid>
<content:encoded><![CDATA[

arXiv:2512.11830v1 Announce Type: cross 
Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2512.11832</link>
<guid>https://arxiv.org/abs/2512.11832</guid>
<content:encoded><![CDATA[

arXiv:2512.11832v1 Announce Type: cross 
Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $\Delta_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&amp;D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[

arXiv:2512.11833v1 Announce Type: cross 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions</title>
<link>https://arxiv.org/abs/2512.11836</link>
<guid>https://arxiv.org/abs/2512.11836</guid>
<content:encoded><![CDATA[

arXiv:2512.11836v1 Announce Type: cross 
Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[

arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spiking Manifesto</title>
<link>https://arxiv.org/abs/2512.11843</link>
<guid>https://arxiv.org/abs/2512.11843</guid>
<content:encoded><![CDATA[

arXiv:2512.11843v1 Announce Type: cross 
Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach</title>
<link>https://arxiv.org/abs/2512.11845</link>
<guid>https://arxiv.org/abs/2512.11845</guid>
<content:encoded><![CDATA[

arXiv:2512.11845v1 Announce Type: cross 
Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document</title>
<link>https://arxiv.org/abs/2512.11849</link>
<guid>https://arxiv.org/abs/2512.11849</guid>
<content:encoded><![CDATA[

arXiv:2512.11849v1 Announce Type: cross 
Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs</title>
<link>https://arxiv.org/abs/2512.11851</link>
<guid>https://arxiv.org/abs/2512.11851</guid>
<content:encoded><![CDATA[

arXiv:2512.11851v1 Announce Type: cross 
Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things</title>
<link>https://arxiv.org/abs/2512.11852</link>
<guid>https://arxiv.org/abs/2512.11852</guid>
<content:encoded><![CDATA[

arXiv:2512.11852v1 Announce Type: cross 
Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks</title>
<link>https://arxiv.org/abs/2512.11854</link>
<guid>https://arxiv.org/abs/2512.11854</guid>
<content:encoded><![CDATA[

arXiv:2512.11854v1 Announce Type: cross 
Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry</title>
<link>https://arxiv.org/abs/2512.11855</link>
<guid>https://arxiv.org/abs/2512.11855</guid>
<content:encoded><![CDATA[

arXiv:2512.11855v1 Announce Type: cross 
Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search</title>
<link>https://arxiv.org/abs/2512.11856</link>
<guid>https://arxiv.org/abs/2512.11856</guid>
<content:encoded><![CDATA[

arXiv:2512.11856v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TopicProphet: Prophesies on Temporal Topic Trends and Stocks</title>
<link>https://arxiv.org/abs/2512.11857</link>
<guid>https://arxiv.org/abs/2512.11857</guid>
<content:encoded><![CDATA[

arXiv:2512.11857v1 Announce Type: cross 
Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Path Integral Diffusion: AdaPID</title>
<link>https://arxiv.org/abs/2512.11858</link>
<guid>https://arxiv.org/abs/2512.11858</guid>
<content:encoded><![CDATA[

arXiv:2512.11858v1 Announce Type: cross 
Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion</title>
<link>https://arxiv.org/abs/2512.11859</link>
<guid>https://arxiv.org/abs/2512.11859</guid>
<content:encoded><![CDATA[

arXiv:2512.11859v1 Announce Type: cross 
Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes</title>
<link>https://arxiv.org/abs/2512.11860</link>
<guid>https://arxiv.org/abs/2512.11860</guid>
<content:encoded><![CDATA[

arXiv:2512.11860v1 Announce Type: cross 
Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL</title>
<link>https://arxiv.org/abs/2512.11862</link>
<guid>https://arxiv.org/abs/2512.11862</guid>
<content:encoded><![CDATA[

arXiv:2512.11862v1 Announce Type: cross 
Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence</title>
<link>https://arxiv.org/abs/2512.11863</link>
<guid>https://arxiv.org/abs/2512.11863</guid>
<content:encoded><![CDATA[

arXiv:2512.11863v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.11865</link>
<guid>https://arxiv.org/abs/2512.11865</guid>
<content:encoded><![CDATA[

arXiv:2512.11865v1 Announce Type: cross 
Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[

arXiv:2512.11867v1 Announce Type: cross 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models</title>
<link>https://arxiv.org/abs/2512.11868</link>
<guid>https://arxiv.org/abs/2512.11868</guid>
<content:encoded><![CDATA[

arXiv:2512.11868v1 Announce Type: cross 
Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT</title>
<link>https://arxiv.org/abs/2512.11870</link>
<guid>https://arxiv.org/abs/2512.11870</guid>
<content:encoded><![CDATA[

arXiv:2512.11870v1 Announce Type: cross 
Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops</title>
<link>https://arxiv.org/abs/2512.11871</link>
<guid>https://arxiv.org/abs/2512.11871</guid>
<content:encoded><![CDATA[

arXiv:2512.11871v1 Announce Type: cross 
Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.11872</link>
<guid>https://arxiv.org/abs/2512.11872</guid>
<content:encoded><![CDATA[

arXiv:2512.11872v1 Announce Type: cross 
Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's About Time: The Temporal and Modal Dynamics of Copilot Usage</title>
<link>https://arxiv.org/abs/2512.11879</link>
<guid>https://arxiv.org/abs/2512.11879</guid>
<content:encoded><![CDATA[

arXiv:2512.11879v1 Announce Type: cross 
Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Structural Representation in Foundation Models for Polymers</title>
<link>https://arxiv.org/abs/2512.11881</link>
<guid>https://arxiv.org/abs/2512.11881</guid>
<content:encoded><![CDATA[

arXiv:2512.11881v1 Announce Type: cross 
Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education</title>
<link>https://arxiv.org/abs/2512.11882</link>
<guid>https://arxiv.org/abs/2512.11882</guid>
<content:encoded><![CDATA[

arXiv:2512.11882v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
<link>https://arxiv.org/abs/2512.11883</link>
<guid>https://arxiv.org/abs/2512.11883</guid>
<content:encoded><![CDATA[

arXiv:2512.11883v1 Announce Type: cross 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2512.11887</link>
<guid>https://arxiv.org/abs/2512.11887</guid>
<content:encoded><![CDATA[

arXiv:2512.11887v1 Announce Type: cross 
Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Should AI Become an Intergenerational Civil Right?</title>
<link>https://arxiv.org/abs/2512.11892</link>
<guid>https://arxiv.org/abs/2512.11892</guid>
<content:encoded><![CDATA[

arXiv:2512.11892v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.11893</link>
<guid>https://arxiv.org/abs/2512.11893</guid>
<content:encoded><![CDATA[

arXiv:2512.11893v1 Announce Type: cross 
Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A fine-grained look at causal effects in causal spaces</title>
<link>https://arxiv.org/abs/2512.11919</link>
<guid>https://arxiv.org/abs/2512.11919</guid>
<content:encoded><![CDATA[

arXiv:2512.11919v1 Announce Type: cross 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control</title>
<link>https://arxiv.org/abs/2512.11921</link>
<guid>https://arxiv.org/abs/2512.11921</guid>
<content:encoded><![CDATA[

arXiv:2512.11921v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use</title>
<link>https://arxiv.org/abs/2512.11922</link>
<guid>https://arxiv.org/abs/2512.11922</guid>
<content:encoded><![CDATA[

arXiv:2512.11922v1 Announce Type: cross 
Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
<link>https://arxiv.org/abs/2512.11925</link>
<guid>https://arxiv.org/abs/2512.11925</guid>
<content:encoded><![CDATA[

arXiv:2512.11925v1 Announce Type: cross 
Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene regulatory network inference algorithm based on spectral signed directed graph convolution</title>
<link>https://arxiv.org/abs/2512.11927</link>
<guid>https://arxiv.org/abs/2512.11927</guid>
<content:encoded><![CDATA[

arXiv:2512.11927v1 Announce Type: cross 
Abstract: Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion</title>
<link>https://arxiv.org/abs/2512.11928</link>
<guid>https://arxiv.org/abs/2512.11928</guid>
<content:encoded><![CDATA[

arXiv:2512.11928v1 Announce Type: cross 
Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction</title>
<link>https://arxiv.org/abs/2512.11930</link>
<guid>https://arxiv.org/abs/2512.11930</guid>
<content:encoded><![CDATA[

arXiv:2512.11930v1 Announce Type: cross 
Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy</title>
<link>https://arxiv.org/abs/2512.11931</link>
<guid>https://arxiv.org/abs/2512.11931</guid>
<content:encoded><![CDATA[

arXiv:2512.11931v1 Announce Type: cross 
Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance</title>
<link>https://arxiv.org/abs/2512.11933</link>
<guid>https://arxiv.org/abs/2512.11933</guid>
<content:encoded><![CDATA[

arXiv:2512.11933v1 Announce Type: cross 
Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching</title>
<link>https://arxiv.org/abs/2512.11934</link>
<guid>https://arxiv.org/abs/2512.11934</guid>
<content:encoded><![CDATA[

arXiv:2512.11934v1 Announce Type: cross 
Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11941</link>
<guid>https://arxiv.org/abs/2512.11941</guid>
<content:encoded><![CDATA[

arXiv:2512.11941v1 Announce Type: cross 
Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism</title>
<link>https://arxiv.org/abs/2512.11943</link>
<guid>https://arxiv.org/abs/2512.11943</guid>
<content:encoded><![CDATA[

arXiv:2512.11943v1 Announce Type: cross 
Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach</title>
<link>https://arxiv.org/abs/2512.11944</link>
<guid>https://arxiv.org/abs/2512.11944</guid>
<content:encoded><![CDATA[

arXiv:2512.11944v1 Announce Type: cross 
Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations</title>
<link>https://arxiv.org/abs/2512.11946</link>
<guid>https://arxiv.org/abs/2512.11946</guid>
<content:encoded><![CDATA[

arXiv:2512.11946v1 Announce Type: cross 
Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)</title>
<link>https://arxiv.org/abs/2512.11979</link>
<guid>https://arxiv.org/abs/2512.11979</guid>
<content:encoded><![CDATA[

arXiv:2512.11979v1 Announce Type: cross 
Abstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[

arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering</title>
<link>https://arxiv.org/abs/2512.11984</link>
<guid>https://arxiv.org/abs/2512.11984</guid>
<content:encoded><![CDATA[

arXiv:2512.11984v1 Announce Type: cross 
Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[

arXiv:2512.11995v1 Announce Type: cross 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hold Onto That Thought: Assessing KV Cache Compression On Reasoning</title>
<link>https://arxiv.org/abs/2512.12008</link>
<guid>https://arxiv.org/abs/2512.12008</guid>
<content:encoded><![CDATA[

arXiv:2512.12008v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title>
<link>https://arxiv.org/abs/2512.12012</link>
<guid>https://arxiv.org/abs/2512.12012</guid>
<content:encoded><![CDATA[

arXiv:2512.12012v1 Announce Type: cross 
Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers</title>
<link>https://arxiv.org/abs/2512.12045</link>
<guid>https://arxiv.org/abs/2512.12045</guid>
<content:encoded><![CDATA[

arXiv:2512.12045v1 Announce Type: cross 
Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-Tuning Open-Weight Language Models for BPMN Model Generation</title>
<link>https://arxiv.org/abs/2512.12063</link>
<guid>https://arxiv.org/abs/2512.12063</guid>
<content:encoded><![CDATA[

arXiv:2512.12063v1 Announce Type: cross 
Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior</title>
<link>https://arxiv.org/abs/2512.12066</link>
<guid>https://arxiv.org/abs/2512.12066</guid>
<content:encoded><![CDATA[

arXiv:2512.12066v1 Announce Type: cross 
Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring</title>
<link>https://arxiv.org/abs/2512.12069</link>
<guid>https://arxiv.org/abs/2512.12069</guid>
<content:encoded><![CDATA[

arXiv:2512.12069v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models</title>
<link>https://arxiv.org/abs/2512.12081</link>
<guid>https://arxiv.org/abs/2512.12081</guid>
<content:encoded><![CDATA[

arXiv:2512.12081v1 Announce Type: cross 
Abstract: Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A neuro-symbolic framework for accountability in public-sector AI</title>
<link>https://arxiv.org/abs/2512.12109</link>
<guid>https://arxiv.org/abs/2512.12109</guid>
<content:encoded><![CDATA[

arXiv:2512.12109v1 Announce Type: cross 
Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models</title>
<link>https://arxiv.org/abs/2512.12121</link>
<guid>https://arxiv.org/abs/2512.12121</guid>
<content:encoded><![CDATA[

arXiv:2512.12121v1 Announce Type: cross 
Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery</title>
<link>https://arxiv.org/abs/2512.12128</link>
<guid>https://arxiv.org/abs/2512.12128</guid>
<content:encoded><![CDATA[

arXiv:2512.12128v1 Announce Type: cross 
Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity</title>
<link>https://arxiv.org/abs/2512.12135</link>
<guid>https://arxiv.org/abs/2512.12135</guid>
<content:encoded><![CDATA[

arXiv:2512.12135v1 Announce Type: cross 
Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[

arXiv:2512.12142v1 Announce Type: cross 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings</title>
<link>https://arxiv.org/abs/2512.12167</link>
<guid>https://arxiv.org/abs/2512.12167</guid>
<content:encoded><![CDATA[

arXiv:2512.12167v1 Announce Type: cross 
Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Language Model Inference with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2512.12168</link>
<guid>https://arxiv.org/abs/2512.12168</guid>
<content:encoded><![CDATA[

arXiv:2512.12168v1 Announce Type: cross 
Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms</title>
<link>https://arxiv.org/abs/2512.12199</link>
<guid>https://arxiv.org/abs/2512.12199</guid>
<content:encoded><![CDATA[

arXiv:2512.12199v1 Announce Type: cross 
Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation</title>
<link>https://arxiv.org/abs/2512.12201</link>
<guid>https://arxiv.org/abs/2512.12201</guid>
<content:encoded><![CDATA[

arXiv:2512.12201v1 Announce Type: cross 
Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[

arXiv:2512.12206v1 Announce Type: cross 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search</title>
<link>https://arxiv.org/abs/2512.12207</link>
<guid>https://arxiv.org/abs/2512.12207</guid>
<content:encoded><![CDATA[

arXiv:2512.12207v1 Announce Type: cross 
Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.12211</link>
<guid>https://arxiv.org/abs/2512.12211</guid>
<content:encoded><![CDATA[

arXiv:2512.12211v1 Announce Type: cross 
Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Versatile Coding Agents in Synthetic Environments</title>
<link>https://arxiv.org/abs/2512.12216</link>
<guid>https://arxiv.org/abs/2512.12216</guid>
<content:encoded><![CDATA[

arXiv:2512.12216v1 Announce Type: cross 
Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs</title>
<link>https://arxiv.org/abs/2512.12222</link>
<guid>https://arxiv.org/abs/2512.12222</guid>
<content:encoded><![CDATA[

arXiv:2512.12222v1 Announce Type: cross 
Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Distance Measurement based on Multi-Kernel Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.12238</link>
<guid>https://arxiv.org/abs/2512.12238</guid>
<content:encoded><![CDATA[

arXiv:2512.12238v1 Announce Type: cross 
Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Mat\'ern and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Probing Cross-Family Sound Symbolism in 27 Languages</title>
<link>https://arxiv.org/abs/2512.12245</link>
<guid>https://arxiv.org/abs/2512.12245</guid>
<content:encoded><![CDATA[

arXiv:2512.12245v1 Announce Type: cross 
Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&amp;P 500 Index Volatility Forecasting</title>
<link>https://arxiv.org/abs/2512.12250</link>
<guid>https://arxiv.org/abs/2512.12250</guid>
<content:encoded><![CDATA[

arXiv:2512.12250v1 Announce Type: cross 
Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accurate de novo sequencing of the modified proteome with OmniNovo</title>
<link>https://arxiv.org/abs/2512.12272</link>
<guid>https://arxiv.org/abs/2512.12272</guid>
<content:encoded><![CDATA[

arXiv:2512.12272v1 Announce Type: cross 
Abstract: Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRC-Net: Gram Residual Co-attention Net for epilepsy prediction</title>
<link>https://arxiv.org/abs/2512.12273</link>
<guid>https://arxiv.org/abs/2512.12273</guid>
<content:encoded><![CDATA[

arXiv:2512.12273v1 Announce Type: cross 
Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[

arXiv:2512.12284v1 Announce Type: cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation</title>
<link>https://arxiv.org/abs/2512.12285</link>
<guid>https://arxiv.org/abs/2512.12285</guid>
<content:encoded><![CDATA[

arXiv:2512.12285v1 Announce Type: cross 
Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMark: Artificial Intelligence Generated Content Identification Toolkit</title>
<link>https://arxiv.org/abs/2512.12324</link>
<guid>https://arxiv.org/abs/2512.12324</guid>
<content:encoded><![CDATA[

arXiv:2512.12324v1 Announce Type: cross 
Abstract: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks</title>
<link>https://arxiv.org/abs/2512.12332</link>
<guid>https://arxiv.org/abs/2512.12332</guid>
<content:encoded><![CDATA[

arXiv:2512.12332v1 Announce Type: cross 
Abstract: The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema</title>
<link>https://arxiv.org/abs/2512.12337</link>
<guid>https://arxiv.org/abs/2512.12337</guid>
<content:encoded><![CDATA[

arXiv:2512.12337v1 Announce Type: cross 
Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
<link>https://arxiv.org/abs/2512.12410</link>
<guid>https://arxiv.org/abs/2512.12410</guid>
<content:encoded><![CDATA[

arXiv:2512.12410v1 Announce Type: cross 
Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rough Sets for Explainability of Spectral Graph Clustering</title>
<link>https://arxiv.org/abs/2512.12436</link>
<guid>https://arxiv.org/abs/2512.12436</guid>
<content:encoded><![CDATA[

arXiv:2512.12436v1 Announce Type: cross 
Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling</title>
<link>https://arxiv.org/abs/2512.12461</link>
<guid>https://arxiv.org/abs/2512.12461</guid>
<content:encoded><![CDATA[

arXiv:2512.12461v1 Announce Type: cross 
Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference</title>
<link>https://arxiv.org/abs/2512.12462</link>
<guid>https://arxiv.org/abs/2512.12462</guid>
<content:encoded><![CDATA[

arXiv:2512.12462v1 Announce Type: cross 
Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Design Space of Transition Matching</title>
<link>https://arxiv.org/abs/2512.12465</link>
<guid>https://arxiv.org/abs/2512.12465</guid>
<content:encoded><![CDATA[

arXiv:2512.12465v1 Announce Type: cross 
Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion</title>
<link>https://arxiv.org/abs/2512.12474</link>
<guid>https://arxiv.org/abs/2512.12474</guid>
<content:encoded><![CDATA[

arXiv:2512.12474v1 Announce Type: cross 
Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers</title>
<link>https://arxiv.org/abs/2512.12483</link>
<guid>https://arxiv.org/abs/2512.12483</guid>
<content:encoded><![CDATA[

arXiv:2512.12483v1 Announce Type: cross 
Abstract: With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public</title>
<link>https://arxiv.org/abs/2512.12500</link>
<guid>https://arxiv.org/abs/2512.12500</guid>
<content:encoded><![CDATA[

arXiv:2512.12500v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts</title>
<link>https://arxiv.org/abs/2512.12506</link>
<guid>https://arxiv.org/abs/2512.12506</guid>
<content:encoded><![CDATA[

arXiv:2512.12506v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline</title>
<link>https://arxiv.org/abs/2512.12510</link>
<guid>https://arxiv.org/abs/2512.12510</guid>
<content:encoded><![CDATA[

arXiv:2512.12510v1 Announce Type: cross 
Abstract: The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.12523</link>
<guid>https://arxiv.org/abs/2512.12523</guid>
<content:encoded><![CDATA[

arXiv:2512.12523v1 Announce Type: cross 
Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?</title>
<link>https://arxiv.org/abs/2512.12536</link>
<guid>https://arxiv.org/abs/2512.12536</guid>
<content:encoded><![CDATA[

arXiv:2512.12536v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model</title>
<link>https://arxiv.org/abs/2512.12545</link>
<guid>https://arxiv.org/abs/2512.12545</guid>
<content:encoded><![CDATA[

arXiv:2512.12545v1 Announce Type: cross 
Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
<link>https://arxiv.org/abs/2512.12560</link>
<guid>https://arxiv.org/abs/2512.12560</guid>
<content:encoded><![CDATA[

arXiv:2512.12560v1 Announce Type: cross 
Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupled Variational Reinforcement Learning for Language Model General Reasoning</title>
<link>https://arxiv.org/abs/2512.12576</link>
<guid>https://arxiv.org/abs/2512.12576</guid>
<content:encoded><![CDATA[

arXiv:2512.12576v1 Announce Type: cross 
Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Prompt Injection Attacks Against Application Using Classifiers</title>
<link>https://arxiv.org/abs/2512.12583</link>
<guid>https://arxiv.org/abs/2512.12583</guid>
<content:encoded><![CDATA[

arXiv:2512.12583v1 Announce Type: cross 
Abstract: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
<link>https://arxiv.org/abs/2512.12596</link>
<guid>https://arxiv.org/abs/2512.12596</guid>
<content:encoded><![CDATA[

arXiv:2512.12596v1 Announce Type: cross 
Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery</title>
<link>https://arxiv.org/abs/2512.12608</link>
<guid>https://arxiv.org/abs/2512.12608</guid>
<content:encoded><![CDATA[

arXiv:2512.12608v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives</title>
<link>https://arxiv.org/abs/2512.12620</link>
<guid>https://arxiv.org/abs/2512.12620</guid>
<content:encoded><![CDATA[

arXiv:2512.12620v1 Announce Type: cross 
Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists</title>
<link>https://arxiv.org/abs/2512.12630</link>
<guid>https://arxiv.org/abs/2512.12630</guid>
<content:encoded><![CDATA[

arXiv:2512.12630v1 Announce Type: cross 
Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.12633</link>
<guid>https://arxiv.org/abs/2512.12633</guid>
<content:encoded><![CDATA[

arXiv:2512.12633v1 Announce Type: cross 
Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.12662</link>
<guid>https://arxiv.org/abs/2512.12662</guid>
<content:encoded><![CDATA[

arXiv:2512.12662v1 Announce Type: cross 
Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[

arXiv:2512.12663v1 Announce Type: cross 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization</title>
<link>https://arxiv.org/abs/2512.12669</link>
<guid>https://arxiv.org/abs/2512.12669</guid>
<content:encoded><![CDATA[

arXiv:2512.12669v1 Announce Type: cross 
Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
<link>https://arxiv.org/abs/2512.12675</link>
<guid>https://arxiv.org/abs/2512.12675</guid>
<content:encoded><![CDATA[

arXiv:2512.12675v1 Announce Type: cross 
Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches</title>
<link>https://arxiv.org/abs/2512.12677</link>
<guid>https://arxiv.org/abs/2512.12677</guid>
<content:encoded><![CDATA[

arXiv:2512.12677v1 Announce Type: cross 
Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.12683</link>
<guid>https://arxiv.org/abs/2512.12683</guid>
<content:encoded><![CDATA[

arXiv:2512.12683v1 Announce Type: cross 
Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity</title>
<link>https://arxiv.org/abs/2512.12688</link>
<guid>https://arxiv.org/abs/2512.12688</guid>
<content:encoded><![CDATA[

arXiv:2512.12688v1 Announce Type: cross 
Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits</title>
<link>https://arxiv.org/abs/2512.12693</link>
<guid>https://arxiv.org/abs/2512.12693</guid>
<content:encoded><![CDATA[

arXiv:2512.12693v1 Announce Type: cross 
Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
<link>https://arxiv.org/abs/2512.12703</link>
<guid>https://arxiv.org/abs/2512.12703</guid>
<content:encoded><![CDATA[

arXiv:2512.12703v1 Announce Type: cross 
Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Scientific Literature Explorer using Machine Learning (ISLE)</title>
<link>https://arxiv.org/abs/2512.12760</link>
<guid>https://arxiv.org/abs/2512.12760</guid>
<content:encoded><![CDATA[

arXiv:2512.12760v1 Announce Type: cross 
Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[

arXiv:2512.12762v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[

arXiv:2512.12768v1 Announce Type: cross 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)</title>
<link>https://arxiv.org/abs/2512.12769</link>
<guid>https://arxiv.org/abs/2512.12769</guid>
<content:encoded><![CDATA[

arXiv:2512.12769v1 Announce Type: cross 
Abstract: Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.12773</link>
<guid>https://arxiv.org/abs/2512.12773</guid>
<content:encoded><![CDATA[

arXiv:2512.12773v1 Announce Type: cross 
Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State over Tokens: Characterizing the Role of Reasoning Tokens</title>
<link>https://arxiv.org/abs/2512.12777</link>
<guid>https://arxiv.org/abs/2512.12777</guid>
<content:encoded><![CDATA[

arXiv:2512.12777v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average</title>
<link>https://arxiv.org/abs/2512.12785</link>
<guid>https://arxiv.org/abs/2512.12785</guid>
<content:encoded><![CDATA[

arXiv:2512.12785v1 Announce Type: cross 
Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Statistical Significance of Online Regression over Multiple Datasets</title>
<link>https://arxiv.org/abs/2512.12787</link>
<guid>https://arxiv.org/abs/2512.12787</guid>
<content:encoded><![CDATA[

arXiv:2512.12787v1 Announce Type: cross 
Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems</title>
<link>https://arxiv.org/abs/2512.12791</link>
<guid>https://arxiv.org/abs/2512.12791</guid>
<content:encoded><![CDATA[

arXiv:2512.12791v1 Announce Type: cross 
Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2512.12792</link>
<guid>https://arxiv.org/abs/2512.12792</guid>
<content:encoded><![CDATA[

arXiv:2512.12792v1 Announce Type: cross 
Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness</title>
<link>https://arxiv.org/abs/2512.12802</link>
<guid>https://arxiv.org/abs/2512.12802</guid>
<content:encoded><![CDATA[

arXiv:2512.12802v1 Announce Type: cross 
Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs</title>
<link>https://arxiv.org/abs/2512.12805</link>
<guid>https://arxiv.org/abs/2512.12805</guid>
<content:encoded><![CDATA[

arXiv:2512.12805v1 Announce Type: cross 
Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization</title>
<link>https://arxiv.org/abs/2512.12809</link>
<guid>https://arxiv.org/abs/2512.12809</guid>
<content:encoded><![CDATA[

arXiv:2512.12809v1 Announce Type: cross 
Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA</title>
<link>https://arxiv.org/abs/2512.12812</link>
<guid>https://arxiv.org/abs/2512.12812</guid>
<content:encoded><![CDATA[

arXiv:2512.12812v1 Announce Type: cross 
Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles</title>
<link>https://arxiv.org/abs/2512.12817</link>
<guid>https://arxiv.org/abs/2512.12817</guid>
<content:encoded><![CDATA[

arXiv:2512.12817v1 Announce Type: cross 
Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects</title>
<link>https://arxiv.org/abs/2512.12818</link>
<guid>https://arxiv.org/abs/2512.12818</guid>
<content:encoded><![CDATA[

arXiv:2512.12818v1 Announce Type: cross 
Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the continuity of flows</title>
<link>https://arxiv.org/abs/2512.12821</link>
<guid>https://arxiv.org/abs/2512.12821</guid>
<content:encoded><![CDATA[

arXiv:2512.12821v1 Announce Type: cross 
Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.12822</link>
<guid>https://arxiv.org/abs/2512.12822</guid>
<content:encoded><![CDATA[

arXiv:2512.12822v1 Announce Type: cross 
Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
<link>https://arxiv.org/abs/2512.12824</link>
<guid>https://arxiv.org/abs/2512.12824</guid>
<content:encoded><![CDATA[

arXiv:2512.12824v1 Announce Type: cross 
Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future</title>
<link>https://arxiv.org/abs/2512.12832</link>
<guid>https://arxiv.org/abs/2512.12832</guid>
<content:encoded><![CDATA[

arXiv:2512.12832v1 Announce Type: cross 
Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks</title>
<link>https://arxiv.org/abs/2512.12840</link>
<guid>https://arxiv.org/abs/2512.12840</guid>
<content:encoded><![CDATA[

arXiv:2512.12840v1 Announce Type: cross 
Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word priv\'ee, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding</title>
<link>https://arxiv.org/abs/2512.12842</link>
<guid>https://arxiv.org/abs/2512.12842</guid>
<content:encoded><![CDATA[

arXiv:2512.12842v1 Announce Type: cross 
Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Conformal Risk Control</title>
<link>https://arxiv.org/abs/2512.12844</link>
<guid>https://arxiv.org/abs/2512.12844</guid>
<content:encoded><![CDATA[

arXiv:2512.12844v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Consistent Language Model Recommendations through Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2512.12858</link>
<guid>https://arxiv.org/abs/2512.12858</guid>
<content:encoded><![CDATA[

arXiv:2512.12858v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM</title>
<link>https://arxiv.org/abs/2512.12868</link>
<guid>https://arxiv.org/abs/2512.12868</guid>
<content:encoded><![CDATA[

arXiv:2512.12868v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels</title>
<link>https://arxiv.org/abs/2512.12870</link>
<guid>https://arxiv.org/abs/2512.12870</guid>
<content:encoded><![CDATA[

arXiv:2512.12870v1 Announce Type: cross 
Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
<link>https://arxiv.org/abs/2512.12885</link>
<guid>https://arxiv.org/abs/2512.12885</guid>
<content:encoded><![CDATA[

arXiv:2512.12885v1 Announce Type: cross 
Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.12888</link>
<guid>https://arxiv.org/abs/2512.12888</guid>
<content:encoded><![CDATA[

arXiv:2512.12888v1 Announce Type: cross 
Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs</title>
<link>https://arxiv.org/abs/2512.12914</link>
<guid>https://arxiv.org/abs/2512.12914</guid>
<content:encoded><![CDATA[

arXiv:2512.12914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cisco Integrated AI Security and Safety Framework Report</title>
<link>https://arxiv.org/abs/2512.12921</link>
<guid>https://arxiv.org/abs/2512.12921</guid>
<content:encoded><![CDATA[

arXiv:2512.12921v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
<link>https://arxiv.org/abs/2512.12929</link>
<guid>https://arxiv.org/abs/2512.12929</guid>
<content:encoded><![CDATA[

arXiv:2512.12929v1 Announce Type: cross 
Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Data Pruning for Pretraining Biological Foundation Models at Scale</title>
<link>https://arxiv.org/abs/2512.12932</link>
<guid>https://arxiv.org/abs/2512.12932</guid>
<content:encoded><![CDATA[

arXiv:2512.12932v1 Announce Type: cross 
Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
<link>https://arxiv.org/abs/2512.12935</link>
<guid>https://arxiv.org/abs/2512.12935</guid>
<content:encoded><![CDATA[

arXiv:2512.12935v1 Announce Type: cross 
Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
<link>https://arxiv.org/abs/2512.12936</link>
<guid>https://arxiv.org/abs/2512.12936</guid>
<content:encoded><![CDATA[

arXiv:2512.12936v1 Announce Type: cross 
Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping</title>
<link>https://arxiv.org/abs/2512.12950</link>
<guid>https://arxiv.org/abs/2512.12950</guid>
<content:encoded><![CDATA[

arXiv:2512.12950v1 Announce Type: cross 
Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[

arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[

arXiv:2512.12997v1 Announce Type: cross 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Bidirectional Spans and Span Violations in Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.13033</link>
<guid>https://arxiv.org/abs/2512.13033</guid>
<content:encoded><![CDATA[

arXiv:2512.13033v1 Announce Type: cross 
Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
<link>https://arxiv.org/abs/2512.13043</link>
<guid>https://arxiv.org/abs/2512.13043</guid>
<content:encoded><![CDATA[

arXiv:2512.13043v1 Announce Type: cross 
Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators</title>
<link>https://arxiv.org/abs/2512.13063</link>
<guid>https://arxiv.org/abs/2512.13063</guid>
<content:encoded><![CDATA[

arXiv:2512.13063v1 Announce Type: cross 
Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval</title>
<link>https://arxiv.org/abs/2512.13074</link>
<guid>https://arxiv.org/abs/2512.13074</guid>
<content:encoded><![CDATA[

arXiv:2512.13074v1 Announce Type: cross 
Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
<link>https://arxiv.org/abs/2512.13089</link>
<guid>https://arxiv.org/abs/2512.13089</guid>
<content:encoded><![CDATA[

arXiv:2512.13089v1 Announce Type: cross 
Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation</title>
<link>https://arxiv.org/abs/2512.13094</link>
<guid>https://arxiv.org/abs/2512.13094</guid>
<content:encoded><![CDATA[

arXiv:2512.13094v1 Announce Type: cross 
Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning</title>
<link>https://arxiv.org/abs/2512.13100</link>
<guid>https://arxiv.org/abs/2512.13100</guid>
<content:encoded><![CDATA[

arXiv:2512.13100v1 Announce Type: cross 
Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\pi_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[

arXiv:2512.13101v1 Announce Type: cross 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.13106</link>
<guid>https://arxiv.org/abs/2512.13106</guid>
<content:encoded><![CDATA[

arXiv:2512.13106v1 Announce Type: cross 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2512.13107</link>
<guid>https://arxiv.org/abs/2512.13107</guid>
<content:encoded><![CDATA[

arXiv:2512.13107v1 Announce Type: cross 
Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing</title>
<link>https://arxiv.org/abs/2512.13109</link>
<guid>https://arxiv.org/abs/2512.13109</guid>
<content:encoded><![CDATA[

arXiv:2512.13109v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network</title>
<link>https://arxiv.org/abs/2512.13111</link>
<guid>https://arxiv.org/abs/2512.13111</guid>
<content:encoded><![CDATA[

arXiv:2512.13111v1 Announce Type: cross 
Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
<link>https://arxiv.org/abs/2512.13122</link>
<guid>https://arxiv.org/abs/2512.13122</guid>
<content:encoded><![CDATA[

arXiv:2512.13122v1 Announce Type: cross 
Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
<link>https://arxiv.org/abs/2512.13157</link>
<guid>https://arxiv.org/abs/2512.13157</guid>
<content:encoded><![CDATA[

arXiv:2512.13157v1 Announce Type: cross 
Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
<link>https://arxiv.org/abs/2512.13164</link>
<guid>https://arxiv.org/abs/2512.13164</guid>
<content:encoded><![CDATA[

arXiv:2512.13164v1 Announce Type: cross 
Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACn: Soft Actor-Critic with n-step Returns</title>
<link>https://arxiv.org/abs/2512.13165</link>
<guid>https://arxiv.org/abs/2512.13165</guid>
<content:encoded><![CDATA[

arXiv:2512.13165v1 Announce Type: cross 
Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments</title>
<link>https://arxiv.org/abs/2512.13174</link>
<guid>https://arxiv.org/abs/2512.13174</guid>
<content:encoded><![CDATA[

arXiv:2512.13174v1 Announce Type: cross 
Abstract: Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning</title>
<link>https://arxiv.org/abs/2512.13186</link>
<guid>https://arxiv.org/abs/2512.13186</guid>
<content:encoded><![CDATA[

arXiv:2512.13186v1 Announce Type: cross 
Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory</title>
<link>https://arxiv.org/abs/2512.13190</link>
<guid>https://arxiv.org/abs/2512.13190</guid>
<content:encoded><![CDATA[

arXiv:2512.13190v1 Announce Type: cross 
Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models</title>
<link>https://arxiv.org/abs/2512.13194</link>
<guid>https://arxiv.org/abs/2512.13194</guid>
<content:encoded><![CDATA[

arXiv:2512.13194v1 Announce Type: cross 
Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Contrastive Masked Feature Reconstruction on Graphs</title>
<link>https://arxiv.org/abs/2512.13235</link>
<guid>https://arxiv.org/abs/2512.13235</guid>
<content:encoded><![CDATA[

arXiv:2512.13235v1 Announce Type: cross 
Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[

arXiv:2512.13290v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration</title>
<link>https://arxiv.org/abs/2512.13293</link>
<guid>https://arxiv.org/abs/2512.13293</guid>
<content:encoded><![CDATA[

arXiv:2512.13293v1 Announce Type: cross 
Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniLingua: A Small Open-Source LLM for European Languages</title>
<link>https://arxiv.org/abs/2512.13298</link>
<guid>https://arxiv.org/abs/2512.13298</guid>
<content:encoded><![CDATA[

arXiv:2512.13298v1 Announce Type: cross 
Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction</title>
<link>https://arxiv.org/abs/2512.13300</link>
<guid>https://arxiv.org/abs/2512.13300</guid>
<content:encoded><![CDATA[

arXiv:2512.13300v1 Announce Type: cross 
Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning</title>
<link>https://arxiv.org/abs/2512.13316</link>
<guid>https://arxiv.org/abs/2512.13316</guid>
<content:encoded><![CDATA[

arXiv:2512.13316v1 Announce Type: cross 
Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[

arXiv:2512.13317v1 Announce Type: cross 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</title>
<link>https://arxiv.org/abs/2512.13325</link>
<guid>https://arxiv.org/abs/2512.13325</guid>
<content:encoded><![CDATA[

arXiv:2512.13325v1 Announce Type: cross 
Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models</title>
<link>https://arxiv.org/abs/2512.13330</link>
<guid>https://arxiv.org/abs/2512.13330</guid>
<content:encoded><![CDATA[

arXiv:2512.13330v1 Announce Type: cross 
Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)</title>
<link>https://arxiv.org/abs/2512.13356</link>
<guid>https://arxiv.org/abs/2512.13356</guid>
<content:encoded><![CDATA[

arXiv:2512.13356v1 Announce Type: cross 
Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers</title>
<link>https://arxiv.org/abs/2512.13363</link>
<guid>https://arxiv.org/abs/2512.13363</guid>
<content:encoded><![CDATA[

arXiv:2512.13363v1 Announce Type: cross 
Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
<link>https://arxiv.org/abs/2512.13402</link>
<guid>https://arxiv.org/abs/2512.13402</guid>
<content:encoded><![CDATA[

arXiv:2512.13402v1 Announce Type: cross 
Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents</title>
<link>https://arxiv.org/abs/2512.13438</link>
<guid>https://arxiv.org/abs/2512.13438</guid>
<content:encoded><![CDATA[

arXiv:2512.13438v1 Announce Type: cross 
Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[

arXiv:2512.13458v1 Announce Type: cross 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[

arXiv:2512.13478v1 Announce Type: cross 
Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $\rho$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping</title>
<link>https://arxiv.org/abs/2512.13494</link>
<guid>https://arxiv.org/abs/2512.13494</guid>
<content:encoded><![CDATA[

arXiv:2512.13494v1 Announce Type: cross 
Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, na\"ive low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS</title>
<link>https://arxiv.org/abs/2512.13501</link>
<guid>https://arxiv.org/abs/2512.13501</guid>
<content:encoded><![CDATA[

arXiv:2512.13501v1 Announce Type: cross 
Abstract: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifying Rumors via Stance-Aware Structural Modeling</title>
<link>https://arxiv.org/abs/2512.13559</link>
<guid>https://arxiv.org/abs/2512.13559</guid>
<content:encoded><![CDATA[

arXiv:2512.13559v1 Announce Type: cross 
Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory in the Age of AI Agents</title>
<link>https://arxiv.org/abs/2512.13564</link>
<guid>https://arxiv.org/abs/2512.13564</guid>
<content:encoded><![CDATA[

arXiv:2512.13564v1 Announce Type: cross 
Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
<link>https://arxiv.org/abs/2512.13568</link>
<guid>https://arxiv.org/abs/2512.13568</guid>
<content:encoded><![CDATA[

arXiv:2512.13568v1 Announce Type: cross 
Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication</title>
<link>https://arxiv.org/abs/2512.13583</link>
<guid>https://arxiv.org/abs/2512.13583</guid>
<content:encoded><![CDATA[

arXiv:2512.13583v1 Announce Type: cross 
Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}{\delta} \right)}/(\sqrt{n}J\epsilon) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(\epsilon, \delta\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding</title>
<link>https://arxiv.org/abs/2512.13586</link>
<guid>https://arxiv.org/abs/2512.13586</guid>
<content:encoded><![CDATA[

arXiv:2512.13586v1 Announce Type: cross 
Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
<link>https://arxiv.org/abs/2512.13600</link>
<guid>https://arxiv.org/abs/2512.13600</guid>
<content:encoded><![CDATA[

arXiv:2512.13600v1 Announce Type: cross 
Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models</title>
<link>https://arxiv.org/abs/2512.13607</link>
<guid>https://arxiv.org/abs/2512.13607</guid>
<content:encoded><![CDATA[

arXiv:2512.13607v1 Announce Type: cross 
Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[

arXiv:2512.13641v1 Announce Type: cross 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.13644</link>
<guid>https://arxiv.org/abs/2512.13644</guid>
<content:encoded><![CDATA[

arXiv:2512.13644v1 Announce Type: cross 
Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-Language Memorization During the Classification of United States Supreme Court Cases</title>
<link>https://arxiv.org/abs/2512.13654</link>
<guid>https://arxiv.org/abs/2512.13654</guid>
<content:encoded><![CDATA[

arXiv:2512.13654v1 Announce Type: cross 
Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</title>
<link>https://arxiv.org/abs/2512.13658</link>
<guid>https://arxiv.org/abs/2512.13658</guid>
<content:encoded><![CDATA[

arXiv:2512.13658v1 Announce Type: cross 
Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
<link>https://arxiv.org/abs/2512.13678</link>
<guid>https://arxiv.org/abs/2512.13678</guid>
<content:encoded><![CDATA[

arXiv:2512.13678v1 Announce Type: cross 
Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[

arXiv:2512.13690v1 Announce Type: cross 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OM4OV: Leveraging Ontology Matching for Ontology Versioning</title>
<link>https://arxiv.org/abs/2409.20302</link>
<guid>https://arxiv.org/abs/2409.20302</guid>
<content:encoded><![CDATA[

arXiv:2409.20302v5 Announce Type: replace 
Abstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many views treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary modifications, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and less explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignment(s) from OM to reduce the number of matching candidates and improve overall OV performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Driven Reasoning in DatalogMTL with Magic Sets</title>
<link>https://arxiv.org/abs/2412.07259</link>
<guid>https://arxiv.org/abs/2412.07259</guid>
<content:encoded><![CDATA[

arXiv:2412.07259v4 Announce Type: replace 
Abstract: DatalogMTL is a powerful rule-based language for temporal reasoning. Due to its high expressive power and flexible modeling capabilities, it is suitable for a wide range of applications, including tasks from industrial and financial sectors. However, due to its high computational complexity, practical reasoning in DatalogMTL is highly challenging. To address this difficulty, we introduce a new reasoning method for DatalogMTL which exploits the magic sets technique -- a rewriting approach developed for (non-temporal) Datalog to simulate top-down evaluation with bottom-up reasoning. We have implemented this approach and evaluated it on publicly available benchmarks, showing that the proposed approach significantly and consistently outperformed state-of-the-art reasoning techniques.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Traitors: Deception and Trust in Multi-Agent Language Model Simulations</title>
<link>https://arxiv.org/abs/2505.12923</link>
<guid>https://arxiv.org/abs/2505.12923</guid>
<content:encoded><![CDATA[

arXiv:2505.12923v2 Announce Type: replace 
Abstract: As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[

arXiv:2505.14479v5 Announce Type: replace 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMELLNET: A Large-scale Dataset for Real-world Smell Recognition</title>
<link>https://arxiv.org/abs/2506.00239</link>
<guid>https://arxiv.org/abs/2506.00239</guid>
<content:encoded><![CDATA[

arXiv:2506.00239v3 Announce Type: replace 
Abstract: The ability of AI to sense and identify various substances based on their smell alone can have profound impacts on allergen detection (e.g., smelling gluten or peanuts in a cake), monitoring the manufacturing process, and sensing hormones that indicate emotional states, stress levels, and diseases. Despite these broad impacts, there are virtually no large-scale benchmarks, and therefore little progress, for training and evaluating AI systems' ability to smell in the real world. In this paper, we use small gas and chemical sensors to create SmellNet, the first large-scale database that digitizes a diverse range of smells in the natural world. SmellNet contains about 828,000 data points across 50 substances, spanning nuts, spices, herbs, fruits, and vegetables, and 43 mixtures among them, with 68 hours of data collected. Using SmellNet, we developed ScentFormer, a Transformer-based architecture combining temporal differencing and sliding-window augmentation for smell data. For the SmellNet-Base classification task, ScentFormer achieves 58.5% Top-1 accuracy, and for the SmellNet-Mixture distribution prediction task, ScentFormer achieves 50.2% Top-1@0.1 on the test-seen split. ScentFormer's ability to generalize across conditions and capture transient chemical dynamics demonstrates the promise of temporal modeling in olfactory AI. SmellNet and ScentFormer lay the groundwork for real-world olfactory applications across healthcare, food and beverage, environmental monitoring, manufacturing, and entertainment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[

arXiv:2506.07927v3 Announce Type: replace 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Copilots for Reproducibility in Science: A Case Study</title>
<link>https://arxiv.org/abs/2506.20130</link>
<guid>https://arxiv.org/abs/2506.20130</guid>
<content:encoded><![CDATA[

arXiv:2506.20130v4 Announce Type: replace 
Abstract: Open science initiatives seek to make research outputs more transparent, accessible, and reusable, but ensuring that published findings can be independently reproduced remains a persistent challenge. In this paper we describe an AI-driven "Reproducibility Copilot" that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or "rote", reproducibility. Our initial results suggest that the copilot has the potential to substantially reduce reproduction time (in one case from over 30 hours to about 1 hour) while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing values for hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets. Although preliminary, these findings suggest that AI tools can meaningfully reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference AI Systems for Scientific Discovery</title>
<link>https://arxiv.org/abs/2506.21329</link>
<guid>https://arxiv.org/abs/2506.21329</guid>
<content:encoded><![CDATA[

arXiv:2506.21329v4 Announce Type: replace 
Abstract: The rapid evolution of artificial intelligence has led to expectations of transformative impact on science, yet current systems remain fundamentally limited in enabling genuine scientific discovery. This perspective contends that progress turns on closing three mutually reinforcing gaps in abstraction, reasoning and empirical grounding. Central to addressing these gaps is recognizing complementary cognitive modes: thinking as slow, iterative hypothesis generation -- exploring counterfactual spaces where physical laws can be temporarily violated to discover new patterns -- and reasoning as fast, deterministic validation, traversing established knowledge graphs to test consistency with known principles. Abstractions in this loop should be manipulable models that enable counterfactual prediction, causal attribution, and refinement. Design principles -- rather than a monolithic recipe -- are proposed for systems that reason in imaginary spaces and learn from the world: causal, multimodal models for internal simulation; persistent, uncertainty-aware scientific memory that distinguishes hypotheses from established claims; formal verification pathways coupled to computations and experiments. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties make human judgment indispensable, not as a temporary scaffold but as a permanent architectural component. Evaluations must assess the system's ability to identify novel phenomena, propose falsifiable hypotheses, and efficiently guide experimental programs toward genuine discoveries.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[

arXiv:2508.19005v5 Announce Type: replace 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Common Framework for Autoformalization</title>
<link>https://arxiv.org/abs/2509.09810</link>
<guid>https://arxiv.org/abs/2509.09810</guid>
<content:encoded><![CDATA[

arXiv:2509.09810v3 Announce Type: replace 
Abstract: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</title>
<link>https://arxiv.org/abs/2509.24250</link>
<guid>https://arxiv.org/abs/2509.24250</guid>
<content:encoded><![CDATA[

arXiv:2509.24250v2 Announce Type: replace 
Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most prior work has focused on non collaborative physical activities. Collaborative tasks introduce added complexity, requiring systems to infer users assumptions about their teammates intent, which is an inherently ambiguous and dynamic process. This necessitates representations that are interpretable and correctable, enabling users to inspect and refine system behavior. We address this challenge by framing collaborative task learning as a program synthesis problem. Our system represents behavior as editable programs and uses narrated demonstrations, i.e. paired physical actions and natural language, as a unified modality for teaching, inspecting, and correcting system logic without requiring users to see or write code. The same modality is used for the system to communicate its learning to users. In a within subjects study, 20 users taught multiplayer soccer tactics to our system. 70 percent (14/20) of participants successfully refined learned programs to match their intent and 90 percent (18/20) found it easy to correct the programs. The study surfaced unique challenges in representing learning as programs and in enabling users to teach collaborative physical activities. We discuss these issues and outline mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale</title>
<link>https://arxiv.org/abs/2509.25540</link>
<guid>https://arxiv.org/abs/2509.25540</guid>
<content:encoded><![CDATA[

arXiv:2509.25540v2 Announce Type: replace 
Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous large language model (LLM)-based agent capable of independently retrieving patient-specific information, iteratively assessing evidence, and returning structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two clearly defined tiers of increasing complexity: (1) a structured quality assurance (QA) tier, assessing the accurate retrieval of demographic and radiotherapy treatment plan details, followed by (2) a complex clinical outcomes labeling tier involving determination of mandibular osteoradionecrosis (ORN) in head-and-neck cancer patients and detection of cancer recurrence in independent prostate and head-and-neck cancer cohorts requiring combined interpretation of structured and unstructured patient data. The QA tier establishes foundational trust in structured-data retrieval, a critical prerequisite for successful complex clinical outcome labeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What Matters: The AI Pluralism Index</title>
<link>https://arxiv.org/abs/2510.08193</link>
<guid>https://arxiv.org/abs/2510.08193</guid>
<content:encoded><![CDATA[

arXiv:2510.08193v2 Announce Type: replace 
Abstract: Artificial intelligence systems increasingly mediate knowledge, communication, and decision making. Development and governance remain concentrated within a small set of firms and states, raising concerns that technologies may encode narrow interests and limit public agency. Capability benchmarks for language, vision, and coding are common, yet public, auditable measures of pluralistic governance are rare. We define AI pluralism as the degree to which affected stakeholders can shape objectives, data practices, safeguards, and deployment. We present the AI Pluralism Index (AIPI), a transparent, evidence-based instrument that evaluates producers and system families across four pillars: participatory governance, inclusivity and diversity, transparency, and accountability. AIPI codes verifiable practices from public artifacts and independent evaluations, explicitly handling "Unknown" evidence to report both lower-bound ("evidence") and known-only scores with coverage. We formalize the measurement model; implement a reproducible pipeline that integrates structured web and repository analysis, external assessments, and expert interviews; and assess reliability with inter-rater agreement, coverage reporting, cross-index correlations, and sensitivity analysis. The protocol, codebook, scoring scripts, and evidence graph are maintained openly with versioned releases and a public adjudication process. We report pilot provider results and situate AIPI relative to adjacent transparency, safety, and governance frameworks. The index aims to steer incentives toward pluralistic practice and to equip policymakers, procurers, and the public with comparable evidence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models</title>
<link>https://arxiv.org/abs/2510.14925</link>
<guid>https://arxiv.org/abs/2510.14925</guid>
<content:encoded><![CDATA[

arXiv:2510.14925v3 Announce Type: replace 
Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition in linear-Gaussian state-space models via H-Risk, a composite instability index integrating spectral margin, conditioning, temporal sensitivity, and innovation amplification. In simulations, higher H-Risk predicts overconfident errors and degraded closed-loop behavior even when the dynamics remain formally stable, exposing a gap between nominal and epistemic stability.
  Extending this stability lens to large language models (LLMs), we introduce a domain-wise proxy based on confidence fluctuations and overconfident errors. In a binary-question study, a Kantian-inspired policy that permits ''cannot judge'' responses yields targeted reductions in policy-aware squared loss in high-stakes domains relative to an overconfident baseline. To probe internal dynamics, we analyse layer-wise sensitivity of hidden states to small input perturbations. Contrary to a naive instability hypothesis, confidently wrong answers show no instability gap; instead, they are at least as locally stable as confidently correct answers, revealing stable miscalibration in which hallucinations behave like robust but misaligned attractors. For Qwen-2.5, spectral and activation profiles suggest a high signal-to-noise, low effective signal temperature regime in which representations become inertial and resistant to contextual shifts. These results bridge Kantian self-limitation and feedback control, and suggest that stable high-confidence hallucinations may not be readily corrected by output-only heuristics (e.g., temperature scaling or re-sampling), motivating process-level interventions that explicitly perturb and re-evaluate the inference trajectory.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
<link>https://arxiv.org/abs/2510.21720</link>
<guid>https://arxiv.org/abs/2510.21720</guid>
<content:encoded><![CDATA[

arXiv:2510.21720v2 Announce Type: replace 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive "Personality Brain." Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making</title>
<link>https://arxiv.org/abs/2511.12876</link>
<guid>https://arxiv.org/abs/2511.12876</guid>
<content:encoded><![CDATA[

arXiv:2511.12876v2 Announce Type: replace 
Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[

arXiv:2511.15407v2 Announce Type: replace 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M^3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark</title>
<link>https://arxiv.org/abs/2511.17729</link>
<guid>https://arxiv.org/abs/2511.17729</guid>
<content:encoded><![CDATA[

arXiv:2511.17729v3 Announce Type: replace 
Abstract: We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures</title>
<link>https://arxiv.org/abs/2511.17833</link>
<guid>https://arxiv.org/abs/2511.17833</guid>
<content:encoded><![CDATA[

arXiv:2511.17833v2 Announce Type: replace 
Abstract: Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Localisation in Localist LLMs</title>
<link>https://arxiv.org/abs/2511.18375</link>
<guid>https://arxiv.org/abs/2511.18375</guid>
<content:encoded><![CDATA[

arXiv:2511.18375v3 Announce Type: replace 
Abstract: This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate five locality configurations: two uniform baselines (fully distributed and fully localist) and three progressive polynomial schedules. We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Transparency Failures in Expert-Persona LLMs: How Instruction-Following Overrides Honesty</title>
<link>https://arxiv.org/abs/2511.21569</link>
<guid>https://arxiv.org/abs/2511.21569</guid>
<content:encoded><![CDATA[

arXiv:2511.21569v4 Announce Type: replace 
Abstract: Self-transparency is a critical safety boundary, requiring language models to honestly disclose their limitations and artificial nature. This study stress-tests this capability, investigating whether models willingly disclose their identity when assigned professional personas that conflict with transparent self-representation. When models prioritize role consistency over this boundary disclosure, users may calibrate trust based on overstated competence claims, treating AI-generated guidance as equivalent to licensed professional advice. Using a common-garden experimental design, sixteen open-weight models (4B-671B parameters) were audited under identical conditions across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure at the first prompt, while a Neurosurgeon persona elicited only 3.5% -- an 8.8-fold difference that emerged at the initial epistemic inquiry. Disclosure ranged from 2.8% to 73.6% across model families, with a 14B model reaching 39.4% while a 70B model produced just 4.1%. Model identity provided substantially larger improvement in fitting observations than parameter count ($\Delta R_{adj}^{2}=0.359$ vs $0.018$). Reasoning variants showed heterogeneous effects: some exhibited up to 48.4 percentage points lower disclosure than their base instruction-tuned counterparts, while others maintained high transparency. An additional experiment demonstrated that explicit permission to disclose AI nature increased disclosure from 23.7% to 65.8%, revealing that suppression reflects instruction-following prioritization rather than capability limitations. Bayesian validation confirmed robustness to judge measurement error ($\kappa=0.908$). Organizations cannot assume safety properties will transfer across deployment domains, requiring deliberate behavior design and empirical verification.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs</title>
<link>https://arxiv.org/abs/2512.00319</link>
<guid>https://arxiv.org/abs/2512.00319</guid>
<content:encoded><![CDATA[

arXiv:2512.00319v2 Announce Type: replace 
Abstract: The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?</title>
<link>https://arxiv.org/abs/2512.03005</link>
<guid>https://arxiv.org/abs/2512.03005</guid>
<content:encoded><![CDATA[

arXiv:2512.03005v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Soccer Substitutions</title>
<link>https://arxiv.org/abs/2512.04480</link>
<guid>https://arxiv.org/abs/2512.04480</guid>
<content:encoded><![CDATA[

arXiv:2512.04480v3 Announce Type: replace 
Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI &amp; Human Co-Improvement for Safer Co-Superintelligence</title>
<link>https://arxiv.org/abs/2512.05356</link>
<guid>https://arxiv.org/abs/2512.05356</guid>
<content:encoded><![CDATA[

arXiv:2512.05356v2 Announce Type: replace 
Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach</title>
<link>https://arxiv.org/abs/2210.00858</link>
<guid>https://arxiv.org/abs/2210.00858</guid>
<content:encoded><![CDATA[

arXiv:2210.00858v4 Announce Type: replace-cross 
Abstract: In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power of deep networks. We generate a 3D vision-and-language synthetic dataset of tabletop scenes in a simulation environment to train our approach and perform extensive evaluations in both synthetic and real-world scenes. Results showcase the benefits of our approach in terms of accuracy, sample-efficiency, and robustness to the user's vocabulary, while being transferable to real-world scenes with few-shot visual fine-tuning. Finally, we integrate our method with a robot framework and demonstrate how it can serve as an interpretable solution for an interactive object-picking task, achieving an average success rate of 80.2\%, both in simulation and with a real robot. We make supplementary material available in https://gtziafas.github.io/neurosymbolic-manipulation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling</title>
<link>https://arxiv.org/abs/2401.08930</link>
<guid>https://arxiv.org/abs/2401.08930</guid>
<content:encoded><![CDATA[

arXiv:2401.08930v2 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis PADS, a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Self-Supervised Learning for Recommendation</title>
<link>https://arxiv.org/abs/2404.03354</link>
<guid>https://arxiv.org/abs/2404.03354</guid>
<content:encoded><![CDATA[

arXiv:2404.03354v3 Announce Type: replace-cross 
Abstract: Recommender systems play a crucial role in tackling the challenge of information overload by delivering personalized recommendations based on individual user preferences. Deep learning techniques, such as RNNs, GNNs, and Transformer architectures, have significantly propelled the advancement of recommender systems by enhancing their comprehension of user behaviors and preferences. However, supervised learning methods encounter challenges in real-life scenarios due to data sparsity, resulting in limitations in their ability to learn representations effectively. To address this, self-supervised learning (SSL) techniques have emerged as a solution, leveraging inherent data structures to generate supervision signals without relying solely on labeled data. By leveraging unlabeled data and extracting meaningful representations, recommender systems utilizing SSL can make accurate predictions and recommendations even when confronted with data sparsity. In this paper, we provide a comprehensive review of self-supervised learning frameworks designed for recommender systems, encompassing a thorough analysis of over 170 papers. We conduct an exploration of nine distinct scenarios, enabling a comprehensive understanding of SSL-enhanced recommenders in different contexts. For each domain, we elaborate on different self-supervised learning paradigms, namely contrastive learning, generative learning, and adversarial learning, so as to present technical details of how SSL enhances recommender systems in various contexts. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-SSLRec-Papers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need</title>
<link>https://arxiv.org/abs/2405.07293</link>
<guid>https://arxiv.org/abs/2405.07293</guid>
<content:encoded><![CDATA[

arXiv:2405.07293v2 Announce Type: replace-cross 
Abstract: Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\% while consuming only 19.12\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Neural Common Neighbor for Temporal Graph Link Prediction</title>
<link>https://arxiv.org/abs/2406.07926</link>
<guid>https://arxiv.org/abs/2406.07926</guid>
<content:encoded><![CDATA[

arXiv:2406.07926v2 Announce Type: replace-cross 
Abstract: Temporal graphs are widespread in real-world applications such as social networks, as well as trade and transportation networks. Predicting dynamic links within these evolving graphs is a key problem. Many memory-based methods use temporal interaction histories to generate node embeddings, which are then combined to predict links. However, these approaches primarily focus on individual node representations, often overlooking the inherently pairwise nature of link prediction. While some recent methods attempt to capture pairwise features, they tend to be limited by high computational complexity arising from repeated embedding calculations, making them unsuitable for large-scale datasets like the Temporal Graph Benchmark (TGB). To address the critical need for models that combine strong expressive power with high computational efficiency for link prediction on large temporal graphs, we propose Temporal Neural Common Neighbor (TNCN). Our model achieves this balance by adapting the powerful pairwise modeling principles of Neural Common Neighbor (NCN) to an efficient temporal architecture. TNCN improves upon NCN by efficiently preserving and updating temporal neighbor dictionaries for each node and by using multi-hop common neighbors to learn more expressive pairwise representations. TNCN achieves new state-of-the-art performance on Review from five large-scale real-world TGB datasets, 6 out of 7 datasets in the transductive setting and 3 out of 7 in the inductive setting on small- to medium-scale datasets. Additionally, TNCN demonstrates excellent scalability, outperforming prominent GNN baselines by up to 30.3 times in speed on large datasets. Our code is available at https://github.com/GraphPKU/TNCN.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dy-mer: An Explainable DNA Sequence Representation Scheme using Dictionary Learning</title>
<link>https://arxiv.org/abs/2407.12051</link>
<guid>https://arxiv.org/abs/2407.12051</guid>
<content:encoded><![CDATA[

arXiv:2407.12051v2 Announce Type: replace-cross 
Abstract: DNA sequences encode critical genetic information, yet their variable length and discrete nature impede direct utilization in deep learning models. Existing DNA representation schemes convert sequences into numerical vectors but fail to capture structural features of local subsequences and often suffer from limited interpretability and poor generalization on small datasets. To address these limitations, we propose Dy-mer, an interpretable and robust DNA representation scheme based on dictionary learning. Dy-mer formulates an optimization problem in tensor format, which ensures computational efficiency in batch processing. Our scheme reconstructs DNA sequences as concatenations of dynamic-length subsequences (dymers) through a convolution operation and simultaneously optimize a learnable dymer dictionary and sparse representations. Our method achieves state-of-the-art performance in downstream tasks such as DNA promoter classification and motif detection. Experiments further show that the learned dymers match known DNA motifs and clustering using Dy-mer yields semantically meaningful phylogenetic trees. These results demonstrate that the proposed approach achieves both strong predictive performance and high interpretability, making it well suited for biological research applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Simultaneous Multislice MRI Reconstruction Using Slice-Wise Learned Generative Diffusion Priors</title>
<link>https://arxiv.org/abs/2407.21600</link>
<guid>https://arxiv.org/abs/2407.21600</guid>
<content:encoded><![CDATA[

arXiv:2407.21600v3 Announce Type: replace-cross 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2408.06069</link>
<guid>https://arxiv.org/abs/2408.06069</guid>
<content:encoded><![CDATA[

arXiv:2408.06069v2 Announce Type: replace-cross 
Abstract: Deep Gaussian process models typically employ discrete hierarchies, but recent advancements in differential Gaussian processes (DiffGPs) have extended these models to infinite depths. However, existing DiffGP approaches often overlook the uncertainty in kernel hyperparameters by treating them as fixed and time-invariant, which degrades the model's predictive performance and neglects the posterior distribution. In this work, we introduce a fully Bayesian framework that models kernel hyperparameters as random variables and utilizes coupled stochastic differential equations (SDEs) to jointly learn their posterior distributions alongside those of inducing points. By incorporating the estimation uncertainty of hyperparameters, our method significantly enhances model flexibility and adaptability to complex dynamic systems. Furthermore, we employ a black-box adaptive SDE solver with a neural network to achieve realistic, time varying posterior approximations, thereby improving the expressiveness of the variational posterior. Comprehensive experimental evaluations demonstrate that our approach outperforms traditional methods in terms of flexibility, accuracy, and other key performance metrics. This work not only provides a robust Bayesian extension to DiffGP models but also validates its effectiveness in handling intricate dynamic behaviors, thereby advancing the applicability of Gaussian process models in diverse real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Screening is More Efficient with Multiple Objects</title>
<link>https://arxiv.org/abs/2408.10077</link>
<guid>https://arxiv.org/abs/2408.10077</guid>
<content:encoded><![CDATA[

arXiv:2408.10077v2 Announce Type: replace-cross 
Abstract: We study efficient mechanism design for allocating multiple heterogeneous objects. The aim is to maximize the residual surplus, the total value generated from an allocation minus the costs of screening. We discover a robust trend indicating that no-screening mechanisms, such as serial dictatorship with exogenous priority order, tend to perform better as the variety of goods increases. We analyze the underlying reasons by characterizing asymptotically efficient mechanisms in a stylized environment. We also apply an automated mechanism design approach to numerically derive efficient mechanisms and validate the trend in general environments. Building on these implications, we propose the \emph{register-invite-book system} (RIB) as an efficient system for scheduling vaccination against pandemic diseases.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-aware TDNN: Speaker Recognition Using Multi-Layer Features from Pre-Trained Models</title>
<link>https://arxiv.org/abs/2409.07770</link>
<guid>https://arxiv.org/abs/2409.07770</guid>
<content:encoded><![CDATA[

arXiv:2409.07770v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning (SSL) on Transformers have significantly improved speaker verification (SV) by providing domain-general speech representations. However, existing approaches have underutilized the multi-layered nature of SSL encoders. To address this limitation, we propose the layer-aware time-delay neural network (L-TDNN), which directly performs layer/frame-wise processing on the layer-wise hidden state outputs from pre-trained models, extracting fixed-size speaker vectors. L-TDNN comprises a layer-aware convolutional network, a frame-adaptive layer aggregation, and attentive statistic pooling, explicitly modeling of the recognition and processing of previously overlooked layer dimension. We evaluated L-TDNN across multiple speech SSL Transformers and diverse speech-speaker corpora against other approaches for leveraging pre-trained encoders. L-TDNN consistently demonstrated robust verification performance, achieving the lowest error rates throughout the experiments. Concurrently, it stood out in terms of model compactness and exhibited inference efficiency comparable to the existing systems. These results highlight the advantages derived from the proposed layer-aware processing approach. Future work includes exploring joint training with SSL frontends and the incorporation of score calibration to further enhance state-of-the-art verification performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2409.11169</link>
<guid>https://arxiv.org/abs/2409.11169</guid>
<content:encoded><![CDATA[

arXiv:2409.11169v3 Announce Type: replace-cross 
Abstract: Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion</title>
<link>https://arxiv.org/abs/2411.14507</link>
<guid>https://arxiv.org/abs/2411.14507</guid>
<content:encoded><![CDATA[

arXiv:2411.14507v3 Announce Type: replace-cross 
Abstract: Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficiency but often suffers from irreversible performance degradation due to the discarding of transformer blocks. In this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as iterative knowledge grafting rather than simple removal. Motivated by the observation that linear block merging fails to capture non-linear feature disparities and that block importance fluctuates dynamically during pruning, FuseGPT employs a dual-strategy pipeline. First, we propose Macro Influence (MI), a dynamic fusion-aware metric that continuously re-evaluates block redundancy as the network topology evolves. Second, instead of rigid parameter averaging, we introduce a learnable low-rank fusion mechanism that adaptively grafts the knowledge of pruned blocks onto surviving layers via lightweight local distillation. Extensive experiments on LLaMA, Mistral, Qwen, and Phi families demonstrate that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25\% sparsity, FuseGPT achieves lower perplexity than prior methods at 20\% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33$\times$ inference speedup with 25\% memory reduction. Furthermore, FuseGPT is orthogonal to quantization, achieving 52.1\% total compression with negligible quality loss when combined with 4-bit GPTQ. We make our code publicly available at https://github.com/JarvisPei/FuseGPT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment</title>
<link>https://arxiv.org/abs/2412.04783</link>
<guid>https://arxiv.org/abs/2412.04783</guid>
<content:encoded><![CDATA[

arXiv:2412.04783v5 Announce Type: replace-cross 
Abstract: Wireless sensing has recently found widespread applications in diverse environments, including homes, offices, and public spaces. By analyzing patterns in channel state information (CSI), it is possible to infer human actions for tasks such as person identification, gesture recognition, and fall detection. However, CSI is highly sensitive to environmental changes, where even minor alterations can significantly distort the CSI patterns. This sensitivity often leads to performance degradation or outright failure when applying wireless sensing models trained in one environment to another. To address this challenge, Domain Alignment (DAL) has been widely adopted for cross-domain classification tasks, as it focuses on aligning the global distributions of the source and target domains in feature space. Despite its popularity, DAL often neglects inter-category relationships, which can lead to misalignment between categories across domains, even when global alignment is achieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless sensing. Our approach begins by constructing a help set using KNN from the target domain, enabling local alignment between the source and target domains within each category using MMD. Additionally, we address a key instability issue commonly observed in cross-domain methods, where model performance fluctuates sharply between epochs. Further, most existing methods struggle to determine an optimal stopping point during training due to the absence of labeled data from the target domain. Our method resolves this by excluding the support set from the target domain during training and employing it as a validation set to determine the stopping criterion. The dataset and code are publicly available at https://github.com/RS2002/KNN-MMD .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics</title>
<link>https://arxiv.org/abs/2501.10100</link>
<guid>https://arxiv.org/abs/2501.10100</guid>
<content:encoded><![CDATA[

arXiv:2501.10100v5 Announce Type: replace-cross 
Abstract: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmarks: On The False Promise of AI Regulation</title>
<link>https://arxiv.org/abs/2501.15693</link>
<guid>https://arxiv.org/abs/2501.15693</guid>
<content:encoded><![CDATA[

arXiv:2501.15693v2 Announce Type: replace-cross 
Abstract: The performance of AI models on safety benchmarks does not indicate their real-world performance after deployment. This opaqueness of AI models impedes existing regulatory frameworks constituted on benchmark performance, leaving them incapable of mitigating ongoing real-world harm. The problem stems from a fundamental challenge in AI interpretability, which seems to be overlooked by regulators and decision makers. We propose a simple, realistic and readily usable regulatory framework which does not rely on benchmarks, and call for interdisciplinary collaboration to find new ways to address this crucial problem.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[

arXiv:2502.13764v4 Announce Type: replace-cross 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Learning of Evolving Data Streams with Tabular Foundational Models</title>
<link>https://arxiv.org/abs/2502.16840</link>
<guid>https://arxiv.org/abs/2502.16840</guid>
<content:encoded><![CDATA[

arXiv:2502.16840v2 Announce Type: replace-cross 
Abstract: State-of-the-art data stream mining has long drawn from ensembles of the Very Fast Decision Tree, a seminal algorithm honored with the 2015 KDD Test-of-Time Award. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees, such as Adaptive Random Forest, and Streaming Random Patches, across all non-stationary benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2503.07874</link>
<guid>https://arxiv.org/abs/2503.07874</guid>
<content:encoded><![CDATA[

arXiv:2503.07874v2 Announce Type: replace-cross 
Abstract: Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Burst-Sampled Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[

arXiv:2503.10253v4 Announce Type: replace-cross 
Abstract: Deep learning has shown strong potential in modeling complex spatiotemporal dynamics. However, most existing methods depend on densely and uniformly sampled data, which is often unavailable in practice due to sensor and cost limitations. In many real-world settings, such as mobile sensing and physical experiments, data are burst-sampled with short high-frequency segments followed by long gaps, making it difficult to learn accurate dynamics from sparse observations. To address this issue, we propose Physics-Informed Multi-Scale Recurrent Learning (PIMRL), a novel framework specifically designed for burst-sampled spatiotemporal data. PIMRL combines macro-scale latent dynamics inference with micro-scale adaptive refinement guided by incomplete prior information from partial differential equations (PDEs). It further introduces a temporal message-passing mechanism to effectively propagate information across burst intervals. This multi-scale architecture enables PIMRL to model complex systems accurately even under severe data scarcity. We evaluate our approach on five benchmark datasets involving 1D to 3D multi-scale PDEs. The results show that PIMRL consistently outperforms state-of-the-art baselines, achieving substantial improvements and reducing errors by up to 80% in the most challenging settings, which demonstrates the clear advantage of our model. Our work demonstrates the effectiveness of physics-informed recurrent learning for accurate and efficient modeling of sparse spatiotemporal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI</title>
<link>https://arxiv.org/abs/2503.11950</link>
<guid>https://arxiv.org/abs/2503.11950</guid>
<content:encoded><![CDATA[

arXiv:2503.11950v4 Announce Type: replace-cross 
Abstract: The increasing integration of artificial intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups-young digital citizens, parents/educators, and AI professionals-and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from key participants through structured surveys, qualitative interviews, and focus groups to identify distinct privacy expectations. Young digital citizens emphasized autonomy and digital agency, while parents and educators prioritized oversight and AI literacy. AI professionals focused on balancing ethical design with system performance. The analysis revealed significant gaps in transparency and digital literacy, underscoring the need for inclusive, stakeholder-driven privacy frameworks. Drawing on comparative thematic analysis, this study introduces the Privacy-Ethics Alignment in AI (PEA-AI) model, which conceptualizes privacy decision-making as a dynamic negotiation among stakeholders. By aligning empirical findings with governance implications, this research provides a scalable foundation for adaptive, youth-centered AI privacy governance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[

arXiv:2503.14505v2 Announce Type: replace-cross 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[

arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2504.03494</link>
<guid>https://arxiv.org/abs/2504.03494</guid>
<content:encoded><![CDATA[

arXiv:2504.03494v3 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints</title>
<link>https://arxiv.org/abs/2505.05019</link>
<guid>https://arxiv.org/abs/2505.05019</guid>
<content:encoded><![CDATA[

arXiv:2505.05019v2 Announce Type: replace-cross 
Abstract: The generation of synthetic clinical trial data offers a promising approach to mitigating privacy concerns and data accessibility limitations in medical research. However, ensuring that synthetic datasets maintain high fidelity, utility, and adherence to domain-specific constraints remains a key challenge. While hyperparameter optimization (HPO) improves generative model performance, the effectiveness of different optimization strategies for synthetic clinical data remains unclear. This study systematically evaluates four HPO objectives across nine generative models, comparing single-metric to compound metric optimization. Our results demonstrate that HPO consistently improves synthetic data quality, with Tab DDPM achieving the largest relative gains, followed by TVAE (60%), CTGAN (39%), and CTAB-GAN+ (38%). Compound metric optimization outperformed single-metric objectives, producing more generalizable synthetic datasets. Despite improving overall quality, HPO alone fails to prevent violations of essential clinical survival constraints. Preprocessing and postprocessing played a crucial role in reducing these violations, as models lacking robust processing steps produced invalid data in up to 61% of cases. These findings underscore the necessity of integrating explicit domain knowledge alongside HPO to generate high-quality synthetic datasets. Our study provides actionable recommendations for improving synthetic data generation, with future work needed to refine metric selection and validate findings on larger datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectrumFM: A Foundation Model for Intelligent Spectrum Management</title>
<link>https://arxiv.org/abs/2505.06256</link>
<guid>https://arxiv.org/abs/2505.06256</guid>
<content:encoded><![CDATA[

arXiv:2505.06256v2 Announce Type: replace-cross 
Abstract: Intelligent spectrum management is crucial for improving spectrum efficiency and achieving secure utilization of spectrum resources. However, existing intelligent spectrum management methods, typically based on small-scale models, suffer from notable limitations in recognition accuracy, convergence speed, and generalization, particularly in the complex and dynamic spectrum environments. To address these challenges, this paper proposes a novel spectrum foundation model, termed SpectrumFM, establishing a new paradigm for spectrum management. SpectrumFM features an innovative encoder architecture that synergistically exploits the convolutional neural networks and the multi-head self-attention mechanisms to enhance feature extraction and enable robust representation learning. The model is pre-trained via two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, which leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive and transferable spectrum representations. Furthermore, a parameter-efficient fine-tuning strategy is proposed to enable SpectrumFM to adapt to various downstream spectrum management tasks, including automatic modulation classification (AMC), wireless technology classification (WTC), spectrum sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate that SpectrumFM achieves superior performance in terms of accuracy, robustness, adaptability, few-shot learning efficiency, and convergence speed, consistently outperforming conventional methods across multiple benchmarks. Specifically, SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%, achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise ratio (SNR), and enhances AD performance by over 10%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[

arXiv:2505.08919v2 Announce Type: replace-cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Centric Embodied Question Answering</title>
<link>https://arxiv.org/abs/2505.13948</link>
<guid>https://arxiv.org/abs/2505.13948</guid>
<content:encoded><![CDATA[

arXiv:2505.13948v2 Announce Type: replace-cross 
Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and comprehend the environment to answer context-dependent questions. Typically, an EQA framework consists of four components: a planner, a memory module, a stopping module, and an answering module. However, the memory module is utilized inefficiently in existing methods, as the information it stores is leveraged solely for the answering module. Such a design may result in redundant or inadequate exploration, leading to a suboptimal success rate. To solve this problem, we propose MemoryEQA, an EQA framework centered on memory, which establishes mechanisms for memory storage, update, and retrieval, allowing memory information to contribute throughout the entire exploration process. Specifically, we convert the observation into structured textual representations, which are stored in a vector library following a fixed structure. At each exploration step, we utilize a viewpoint comparison strategy to determine whether the memory requires updating. Before executing each module, we employ an entropy-based adaptive retrieval strategy to obtain the minimal yet sufficient memory information that satisfies the requirements of different modules. The retrieved module-specific information is then integrated with the current observation as input to the corresponding module. To evaluate EQA models' memory capabilities, we constructed the benchmark based on HM3D called MT-HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 9.9% performance gain on MT-HM3D compared to baseline models further underscores the memory capability's pivotal role in solving complex tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems</title>
<link>https://arxiv.org/abs/2505.15201</link>
<guid>https://arxiv.org/abs/2505.15201</guid>
<content:encoded><![CDATA[

arXiv:2505.15201v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2505.16130</link>
<guid>https://arxiv.org/abs/2505.16130</guid>
<content:encoded><![CDATA[

arXiv:2505.16130v3 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have been predominantly driven by message-passing, where node representations are iteratively updated via local neighborhood aggregation. Despite their success, message-passing suffers from fundamental limitations -- including constrained expressiveness, over-smoothing, over-squashing, and limited capacity to model long-range dependencies. These issues hinder scalability: increasing data size or model size often fails to yield improved performance. To this end, we explore pathways beyond message-passing and introduce Generative Graph Pattern Machine (G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM represents graph instances (nodes, edges, or entire graphs) as sequences of substructures, and employs generative pre-training over the sequences to learn generalizable and transferable representations. Empirically, G$^2$PM demonstrates strong scalability: on the ogbn-arxiv benchmark, it continues to improve with model sizes up to 60M parameters, outperforming prior generative approaches that plateau at significantly smaller scales (e.g., 3M). In addition, we systematically analyze the model design space, highlighting key architectural choices that contribute to its scalability and generalization. Across diverse tasks -- including node/link/graph classification, transfer learning, and cross-graph pretraining -- G$^2$PM consistently outperforms strong baselines, establishing a compelling foundation for scalable graph learning. The code and dataset are available at https://github.com/Zehong-Wang/G2PM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</title>
<link>https://arxiv.org/abs/2505.18231</link>
<guid>https://arxiv.org/abs/2505.18231</guid>
<content:encoded><![CDATA[

arXiv:2505.18231v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling</title>
<link>https://arxiv.org/abs/2505.19609</link>
<guid>https://arxiv.org/abs/2505.19609</guid>
<content:encoded><![CDATA[

arXiv:2505.19609v2 Announce Type: replace-cross 
Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models</title>
<link>https://arxiv.org/abs/2505.19700</link>
<guid>https://arxiv.org/abs/2505.19700</guid>
<content:encoded><![CDATA[

arXiv:2505.19700v4 Announce Type: replace-cross 
Abstract: The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \textit{Residual Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Flat Minima Perspective on Understanding Augmentations and Model Robustness</title>
<link>https://arxiv.org/abs/2505.24592</link>
<guid>https://arxiv.org/abs/2505.24592</guid>
<content:encoded><![CDATA[

arXiv:2505.24592v3 Announce Type: replace-cross 
Abstract: Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruptions and adversarial attacks. Data augmentation is one of the most prevalent and effective ways to enhance robustness. Despite the great success of the diverse augmentations in different fields, a unified theoretical understanding of their efficacy in improving model robustness is lacking. We theoretically reveal a general condition for label-preserving augmentations to bring robustness to diverse distribution shifts through the lens of flat minima and generalization bound, which de facto turns out to be strongly correlated with robustness against different distribution shifts in practice. Unlike most earlier works, our theoretical framework accommodates all the label-preserving augmentations and is not limited to particular distribution shifts. We substantiate our theories through different simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and ImageNet datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[

arXiv:2505.24850v2 Announce Type: replace-cross 
Abstract: Recent advances in model distillation show that data from advanced reasoning models can effectively train smaller student models. However, standard practices discard incorrect reasoning traces -- valuable, yet underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? We employ a two-stage training recipe: first, Supervised Fine-Tuning (SFT) on positive traces, followed by a refinement stage using both positive and negative traces. We find that a simple REINFORCE-style objective, which we term the Reinforcement Distillation (REDI) objective, outperforms established preference optimization methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate the effectiveness of this approach. Notably, our Qwen-REDI-1.5B model, trained on just 131k traces from the open Open-R1 dataset, achieves an 83.1% score on MATH-500. Its performance matches that of DeepSeek-R1-Distill-Qwen-1.5B, a model trained on 800k proprietary data. This result showcases the remarkable data efficiency of utilizing previously discarded negative traces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[

arXiv:2506.00227v2 Announce Type: replace-cross 
Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification</title>
<link>https://arxiv.org/abs/2506.01983</link>
<guid>https://arxiv.org/abs/2506.01983</guid>
<content:encoded><![CDATA[

arXiv:2506.01983v2 Announce Type: replace-cross 
Abstract: Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[

arXiv:2506.02454v3 Announce Type: replace-cross 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82% overall win rate over the baseline method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How PARTs assemble into wholes: Learning the relative composition of images</title>
<link>https://arxiv.org/abs/2506.03682</link>
<guid>https://arxiv.org/abs/2506.03682</guid>
<content:encoded><![CDATA[

arXiv:2506.03682v2 Announce Type: replace-cross 
Abstract: The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance</title>
<link>https://arxiv.org/abs/2506.06522</link>
<guid>https://arxiv.org/abs/2506.06522</guid>
<content:encoded><![CDATA[

arXiv:2506.06522v3 Announce Type: replace-cross 
Abstract: Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[

arXiv:2506.12041v3 Announce Type: replace-cross 
Abstract: We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmenting Visuals With Querying Words: Language Anchors For Semi-Supervised Image Segmentation</title>
<link>https://arxiv.org/abs/2506.13925</link>
<guid>https://arxiv.org/abs/2506.13925</guid>
<content:encoded><![CDATA[

arXiv:2506.13925v3 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) provide rich semantic priors but are underexplored in Semi supervised Semantic Segmentation. Recent attempts to integrate VLMs to inject high level semantics overlook the semantic misalignment between visual and textual representations that arises from using domain invariant text embeddings without adapting them to dataset and image specific contexts. This lack of domain awareness, coupled with limited annotations, weakens the model semantic understanding by preventing effective vision language alignment. As a result, the model struggles with contextual reasoning, shows weak intra class discrimination, and confuses similar classes. To address these challenges, we propose Hierarchical Vision Language transFormer (HVLFormer), which achieves domain aware and domain robust alignment between visual and textual representations within a mask transformer architecture. Firstly, we transform text embeddings from pretrained VLMs into textual object queries, enabling the generation of multi scale, dataset aware queries that capture class semantics from coarse to fine granularity and enhance contextual reasoning. Next, we refine these queries by injecting image specific visual context to align textual semantics with local scene structures and enhance class discrimination. Finally, to achieve domain robustness, we introduce cross view and modal consistency regularization, which enforces prediction consistency within mask-transformer architecture across augmented views. Moreover, it ensures stable vision language alignment during decoding. With less than 1% training data, HVLFormer outperforms state of the art methods on Pascal VOC, COCO, ADE20K, and Cityscapes. Our code and results will be available on GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Policy Switching for Secure UAV Deconfliction in Adversarial Airspace</title>
<link>https://arxiv.org/abs/2506.21127</link>
<guid>https://arxiv.org/abs/2506.21127</guid>
<content:encoded><![CDATA[

arXiv:2506.21127v3 Announce Type: replace-cross 
Abstract: Autonomous UAV navigation using reinforcement learning (RL) is vulnerable to adversarial attacks that manipulate sensor inputs, potentially leading to unsafe behavior and mission failure. Although robust RL methods provide partial protection, they often struggle to generalize to unseen or out-of-distribution (OOD) attacks due to their reliance on fixed perturbation settings. To address this limitation, we propose a meta-policy switching framework in which a meta-level polic dynamically selects among multiple robust policies to counter unknown adversarial shifts. At the core of this framework lies a discounted Thompson sampling (DTS) mechanism that formulates policy selection as a multi-armed bandit problem, thereby minimizing value distribution shifts via self-induced adversarial observations. We first construct a diverse ensemble of action-robust policies trained under varying perturbation intensities. The DTS-based meta-policy then adaptively selects among these policies online, optimizing resilience against self-induced, piecewise-stationary attacks. Theoretical analysis shows that the DTS mechanism minimizes expected regret, ensuring adaptive robustness to OOD attacks and exhibiting emergent antifragile behavior under uncertainty. Extensive simulations in complex 3D obstacle environments under both white-box (Projected Gradient Descent) and black-box (GPS spoofing) attacks demonstrate significantly improved navigation efficiency and higher conflict free trajectory rates compared to standard robust and vanilla RL baselines, highlighting the practical security and dependability benefits of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems</title>
<link>https://arxiv.org/abs/2506.21502</link>
<guid>https://arxiv.org/abs/2506.21502</guid>
<content:encoded><![CDATA[

arXiv:2506.21502v2 Announce Type: replace-cross 
Abstract: Cyber-Physical Systems (CPSs) tightly interconnect digital and physical operations within production environments, enabling real-time monitoring, control, optimization, and autonomous decision-making that directly enhance manufacturing processes and productivity. The inherent complexity of these systems can lead to faults that require robust and interpretable diagnoses to maintain system dependability and operational efficiency. However, manual modeling of faulty behaviors requires extensive domain expertise and cannot leverage the low-level sensor data of the CPS. Furthermore, although powerful, deep learning-based techniques produce black-box diagnostics that lack interpretability, limiting their practical adoption. To address these challenges, we set forth a method that performs unsupervised characterization of system states and state transitions from low-level sensor data, uses several process mining techniques to model faults through interpretable stochastic Petri nets, simulates such Petri nets for a comprehensive understanding of system behavior under faulty conditions, and performs Petri net-based fault diagnosis. The method is applied to the Robotic Arm Dataset (RoAD), a benchmark collected from a robotic arm deployed in a scale-replica smart manufacturing assembly line. The application to RoAD demonstrates the method's effectiveness in modeling, simulating, and classifying faulty behaviors in CPSs. The modeling results demonstrate that our method achieves a satisfactory interpretability-simulation accuracy trade-off with up to 0.676 arc-degree simplicity, 0.395 R^2, and 0.088 RMSE. In addition, the fault identification results show that the method achieves an F1 score of up to 98.925%, while maintaining a low conformance checking time of 0.020 seconds, which competes with other deep learning-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[

arXiv:2506.23046v3 Announce Type: replace-cross 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows</title>
<link>https://arxiv.org/abs/2506.23260</link>
<guid>https://arxiv.org/abs/2506.23260</guid>
<content:encoded><![CDATA[

arXiv:2506.23260v2 Announce Type: replace-cross 
Abstract: Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces enable real-time data retrieval, computation, and multi-step orchestration. However, the rapid growth of plugins, connectors, and inter-agent protocols has outpaced security practices, leading to brittle integrations that rely on ad-hoc authentication, inconsistent schemas, and weak validation. This survey introduces a unified end-to-end threat model for LLM-agent ecosystems, covering host-to-tool and agent-to-agent communications. We systematically categorize more than thirty attack techniques spanning input manipulation, model compromise, system and privacy attacks, and protocol-level vulnerabilities. For each category, we provide a formal threat formulation defining attacker capabilities, objectives, and affected system layers. Representative examples include Prompt-to-SQL injections and the Toxic Agent Flow exploit in GitHub MCP servers. We analyze attack feasibility, review existing defenses, and discuss mitigation strategies such as dynamic trust management, cryptographic provenance tracking, and sandboxed agent interfaces. The framework is validated through expert review and cross-mapping with real-world incidents and public vulnerability repositories, including CVE and NIST NVD. Compared to prior surveys, this work presents the first integrated taxonomy bridging input-level exploits and protocol-layer vulnerabilities in LLM-agent ecosystems, offering actionable guidance for designing secure and resilient agentic AI systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incoherence as Oracle-less Measure of Error in LLM-Based Code Generation</title>
<link>https://arxiv.org/abs/2507.00057</link>
<guid>https://arxiv.org/abs/2507.00057</guid>
<content:encoded><![CDATA[

arXiv:2507.00057v2 Announce Type: replace-cross 
Abstract: Generating code from a natural language programming task is one of the most successful applications of Large Language Models (LLMs). Yet, the generated program may be buggy. Without an oracle, such as an existing, correct implementation or a formal specification, can we somehow estimate how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called *incoherence*, that can be estimated efficiently in the absence of an oracle and allows us to establish a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. In our experiments, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives for the average task. In fact, *an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation*. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via incoherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</title>
<link>https://arxiv.org/abs/2507.05622</link>
<guid>https://arxiv.org/abs/2507.05622</guid>
<content:encoded><![CDATA[

arXiv:2507.05622v3 Announce Type: replace-cross 
Abstract: The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available in https://github.com/shaoshuo-ss/DATABench.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Collectivist, Economic Perspective on AI</title>
<link>https://arxiv.org/abs/2507.06268</link>
<guid>https://arxiv.org/abs/2507.06268</guid>
<content:encoded><![CDATA[

arXiv:2507.06268v3 Announce Type: replace-cross 
Abstract: Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word ``intelligence'' is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2507.18577</link>
<guid>https://arxiv.org/abs/2507.18577</guid>
<content:encoded><![CDATA[

arXiv:2507.18577v2 Announce Type: replace-cross 
Abstract: The advent of foundation models (FMs), large-scale pre-trained models with strong generalization capabilities, has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of financial foundation models (FFMs): a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: financial language foundation models (FinLFMs), financial time-series foundation models (FinTSFMs), and financial visual-language foundation models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints and offer insights into future research opportunities. We hope this survey can serve as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamiX: Large-Scale Dynamic Social Network Simulator</title>
<link>https://arxiv.org/abs/2507.19929</link>
<guid>https://arxiv.org/abs/2507.19929</guid>
<content:encoded><![CDATA[

arXiv:2507.19929v2 Announce Type: replace-cross 
Abstract: Understanding the intrinsic mechanisms of social platforms is an urgent demand to maintain social stability. The rise of large language models provides significant potential for social network simulations to capture attitude dynamics and reproduce collective behaviors. However, existing studies mainly focus on scaling up agent populations, neglecting the dynamic evolution of social relationships. To address this gap, we introduce DynamiX, a novel large-scale social network simulator dedicated to dynamic social network modeling. DynamiX uses a dynamic hierarchy module for selecting core agents with key characteristics at each timestep, enabling accurate alignment of real-world adaptive switching of user roles. Furthermore, we design distinct dynamic social relationship modeling strategies for different user types. For opinion leaders, we propose an information-stream-based link prediction method recommending potential users with similar stances, simulating homogeneous connections, and autonomous behavior decisions. For ordinary users, we construct an inequality-oriented behavior decision-making module, effectively addressing unequal social interactions and capturing the patterns of relationship adjustments driven by multi-dimensional factors. Experimental results demonstrate that DynamiX exhibits marked improvements in attitude evolution simulation and collective behavior analysis compared to static networks. Besides, DynamiX opens a new theoretical perspective on follower growth prediction, providing empirical evidence for opinion leaders cultivation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Language Models Discover Scaling Laws?</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[

arXiv:2507.21184v3 Announce Type: replace-cross 
Abstract: Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding and Engineering the Phytobiome Communication for Smart Agriculture</title>
<link>https://arxiv.org/abs/2508.03584</link>
<guid>https://arxiv.org/abs/2508.03584</guid>
<content:encoded><![CDATA[

arXiv:2508.03584v2 Announce Type: replace-cross 
Abstract: Smart agriculture applications, integrating technologies like the Internet of Things and machine learning/artificial intelligence (ML/AI) into agriculture, hold promise to address modern challenges of rising food demand, environmental pollution, and water scarcity. Alongside the concept of the phytobiome, which defines the area including the plant, its environment, and associated organisms, and the recent emergence of molecular communication (MC), there exists an important opportunity to advance agricultural science and practice using communication theory. In this article, we motivate to use the communication engineering perspective for developing a holistic understanding of the phytobiome communication and bridge the gap between the phytobiome communication and smart agriculture. Firstly, an overview of phytobiome communication via molecular and electrophysiological signals is presented and a multi-scale framework modeling the phytobiome as a communication network is conceptualized. Then, how this framework is used to model electrophysiological signals is demonstrated with plant experiments. Furthermore, possible smart agriculture applications, such as smart irrigation and targeted delivery of agrochemicals, through engineering the phytobiome communication are proposed. These applications merge ML/AI methods with the Internet of Bio-Nano-Things enabled by MC and pave the way towards more efficient, sustainable, and eco-friendly agricultural production. Finally, the implementation challenges, open research issues, and industrial outlook for these applications are discussed.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer</title>
<link>https://arxiv.org/abs/2508.07710</link>
<guid>https://arxiv.org/abs/2508.07710</guid>
<content:encoded><![CDATA[

arXiv:2508.07710v2 Announce Type: replace-cross 
Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for energy-efficient Transformer architectures.While ANN-to-SNN conversion avoids the high training cost of directly trained Spiking Transformers, existing approaches still struggle to handle the nonlinear operations within Transformer blocks, and often require additional fine-tuning of pretrained ANNs.To address these limitations, we propose a training-free and high-performance ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron that combines exponential decay with a multi-basis encoding strategy to effectively approximate nonlinear operations, eliminating the need for weight modifications in pretrained ANNs.Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics Practices in AI Development: An Empirical Study Across Roles and Regions</title>
<link>https://arxiv.org/abs/2508.09219</link>
<guid>https://arxiv.org/abs/2508.09219</guid>
<content:encoded><![CDATA[

arXiv:2508.09219v2 Announce Type: replace-cross 
Abstract: Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-methods survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey comprises 414 participants from 43 countries, representing various roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings underscore the importance of a collaborative, role-sensitive approach that involves diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning</title>
<link>https://arxiv.org/abs/2508.10019</link>
<guid>https://arxiv.org/abs/2508.10019</guid>
<content:encoded><![CDATA[

arXiv:2508.10019v2 Announce Type: replace-cross 
Abstract: Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., up to 1.5B parameters) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title>
<link>https://arxiv.org/abs/2508.11009</link>
<guid>https://arxiv.org/abs/2508.11009</guid>
<content:encoded><![CDATA[

arXiv:2508.11009v3 Announce Type: replace-cross 
Abstract: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[

arXiv:2508.11849v3 Announce Type: replace-cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN: Word Association Learning for Cultural Alignment in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13426</link>
<guid>https://arxiv.org/abs/2508.13426</guid>
<content:encoded><![CDATA[

arXiv:2508.13426v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit cultural bias from overrepresented viewpoints in training data, yet cultural alignment remains a challenge due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient and cognitively grounded method: fine-tuning LLMs on native speakers' word-association norms, leveraging cognitive psychology findings that such associations capture cultural knowledge. Using word association datasets from native speakers in the US (English) and China (Mandarin), we train Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning and preference optimization. We evaluate models' cultural alignment through a two-tier evaluation framework that spans lexical associations and cultural value alignment using the World Values Survey. Results show significant improvements in lexical alignment (16-20% English, 43-165% Mandarin on Precision@5) and high-level cultural value shifts. On a subset of 50 questions where US and Chinese respondents diverge most, fine-tuned Qwen nearly doubles its response alignment with Chinese values (13 to 25). Remarkably, our trained 7-8B models match or exceed vanilla 70B baselines, demonstrating that a few million of culture-grounded associations achieve value alignment without expensive retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning in Focus: Detecting Behavioral and Collaborative Engagement Using Vision Transformers</title>
<link>https://arxiv.org/abs/2508.15782</link>
<guid>https://arxiv.org/abs/2508.15782</guid>
<content:encoded><![CDATA[

arXiv:2508.15782v2 Announce Type: replace-cross 
Abstract: In early childhood education, accurately detecting collaborative and behavioral engagement is essential to foster meaningful learning experiences. This paper presents an AI driven approach that leverages Vision Transformers (ViTs) to automatically classify children s engagement using visual cues such as gaze direction, interaction, and peer collaboration. Utilizing the ChildPlay gaze dataset, our method is trained on annotated video segments to classify behavioral and collaborative engagement states (e.g., engaged, not engaged, collaborative, not collaborative). We evaluated six state of the art transformer models: Vision Transformer (ViT), Data efficient Image Transformer (DeiT), Swin Transformer, VitGaze, APVit and GazeTR. Among these, the Swin Transformer achieved the highest classification performance with an accuracy of 97.58 percent, demonstrating its effectiveness in modeling local and global attention. Our results highlight the potential of transformer based architectures for scalable, automated engagement analysis in real world educational settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System</title>
<link>https://arxiv.org/abs/2508.15811</link>
<guid>https://arxiv.org/abs/2508.15811</guid>
<content:encoded><![CDATA[

arXiv:2508.15811v2 Announce Type: replace-cross 
Abstract: Generative query suggestion using large language models offers a powerful way to enhance conversational systems, but aligning outputs with nuanced user preferences remains a critical challenge. To address this, we introduce a multi-stage framework designed for progressive alignment between the generation policy and user intent. Our pipeline begins with prompt engineering as a cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we introduce a distillation method on click logs to create a robust foundational model. To better model user preferences while capturing their inherent uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user preferences as probability distributions rather than point estimates. Finally, we employ reinforcement learning to align the generation policy with these preferences, guided by a composite reward function that integrates GaRM with auxiliary heuristics to mitigate reward hacking. To maintain training stability, this process is enhanced by a novel out-of-distribution regularization method and a two-stage reward fusion technique. Extensive experiments demonstrate that our framework significantly outperforms baselines on both automatic and human evaluations and yields a 34\% relative increase in user engagement as measured by click-through rate in live A/B tests.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConceptGuard: Neuro-Symbolic Safety Guardrails via Sparse Interpretable Jailbreak Concepts</title>
<link>https://arxiv.org/abs/2508.16325</link>
<guid>https://arxiv.org/abs/2508.16325</guid>
<content:encoded><![CDATA[

arXiv:2508.16325v2 Announce Type: replace-cross 
Abstract: Large Language Models have found success in a variety of applications. However, their safety remains a concern due to the existence of various jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a range of vulnerabilities, including targeted misuse and accidental user profiling. This work introduces \textbf{ConceptGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, ConceptGuard enables building robust safety guardrails -- offering fully explainable and generalizable defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in the mechanistic interpretability of LLMs, our approach provides evidence for a shared activation geometry for jailbreak attacks in the representation space, a potential foundation for designing more interpretable and generalizable safeguards against attackers.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations</title>
<link>https://arxiv.org/abs/2508.16634</link>
<guid>https://arxiv.org/abs/2508.16634</guid>
<content:encoded><![CDATA[

arXiv:2508.16634v5 Announce Type: replace-cross 
Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation</title>
<link>https://arxiv.org/abs/2508.16994</link>
<guid>https://arxiv.org/abs/2508.16994</guid>
<content:encoded><![CDATA[

arXiv:2508.16994v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in knowledge-intensive NLP tasks, but current evaluations often overlook the structural complexity and multi-step reasoning required in real-world scenarios. These benchmarks overlook key factors such as the interaction between retrieval difficulty and reasoning depth. To address this gap, we propose GRADE, a novel evaluation framework that models task difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the number of inference steps (hops), and (2) semantic distance between the query and its supporting evidence. We construct a synthetic multi-hop QA dataset from factual news articles by extracting knowledge graphs and augmenting them through semantic clustering to recover missing links, allowing us to generate diverse and difficulty-controlled queries. Central to our framework is a 2D difficulty matrix that combines generator-side and retriever-side difficulty. Experiments across multiple domains and models show that error rates strongly correlate with our difficulty measures, validating their diagnostic utility. GRADE enables fine-grained analysis of RAG performance and provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[

arXiv:2508.17364v3 Announce Type: replace-cross 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2509.02718</link>
<guid>https://arxiv.org/abs/2509.02718</guid>
<content:encoded><![CDATA[

arXiv:2509.02718v3 Announce Type: replace-cross 
Abstract: Increasing demand for Large Language Models (LLMs) services imposes substantial deployment and computation costs on providers. LLM routing offers a cost-efficient solution by directing queries to the optimal LLM based on model and query features. However, existing works primarily focus on offline scenarios and struggle to adapt to online settings with high query volume and constrained token budgets. In this work, we introduce the first training-free algorithm for online routing scenarios. Our algorithm leverages approximate nearest neighbor search to efficiently estimate query features and performs a one-time optimization over a small set of initial queries to learn a routing strategy that guides future routing. We provide theoretical guarantees demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$ under natural assumptions, which is further validated by extensive experiments across 3 benchmark datasets and 8 baselines, showing an average improvement of 3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and nearly 4.25$\times$ in throughput. Our code is available at https://github.com/fzwark/PORT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference</title>
<link>https://arxiv.org/abs/2509.04467</link>
<guid>https://arxiv.org/abs/2509.04467</guid>
<content:encoded><![CDATA[

arXiv:2509.04467v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a pruning method that is highly integrated with PD disaggregation, enabling more precise pruning of blocks. Our approach constructs pruning and distillation sets to perform iterative block removal, obtaining better pruning solutions. Moreover, we analyze the pruning sensitivity of the prefill and decode stages and identify removable blocks specific to each stage, making it well suited for PD disaggregation deployment. Extensive experiments demonstrate our approach consistently achieves strong performance in both PD disaggregation and PD unified (non-PD disaggregation) settings, and can also be extended to other non-block pruning methods. Under the same settings, our method achieves improved performance and faster inference.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MALLM: Multi-Agent Large Language Models Framework</title>
<link>https://arxiv.org/abs/2509.11656</link>
<guid>https://arxiv.org/abs/2509.11656</guid>
<content:encoded><![CDATA[

arXiv:2509.11656v3 Announce Type: replace-cross 
Abstract: Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic bootstrapped pretraining</title>
<link>https://arxiv.org/abs/2509.15248</link>
<guid>https://arxiv.org/abs/2509.15248</guid>
<content:encoded><![CDATA[

arXiv:2509.15248v3 Announce Type: replace-cross 
Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter and a 6B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers up to 60% of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases -- SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Highly Imbalanced Regression with Tabular Data in SEP and Other Applications</title>
<link>https://arxiv.org/abs/2509.16339</link>
<guid>https://arxiv.org/abs/2509.16339</guid>
<content:encoded><![CDATA[

arXiv:2509.16339v4 Announce Type: replace-cross 
Abstract: We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mamba Modulation: On the Length Generalization of Mamba</title>
<link>https://arxiv.org/abs/2509.19633</link>
<guid>https://arxiv.org/abs/2509.19633</guid>
<content:encoded><![CDATA[

arXiv:2509.19633v3 Announce Type: replace-cross 
Abstract: The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.22621</link>
<guid>https://arxiv.org/abs/2509.22621</guid>
<content:encoded><![CDATA[

arXiv:2509.22621v2 Announce Type: replace-cross 
Abstract: Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and two model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</title>
<link>https://arxiv.org/abs/2509.23103</link>
<guid>https://arxiv.org/abs/2509.23103</guid>
<content:encoded><![CDATA[

arXiv:2509.23103v2 Announce Type: replace-cross 
Abstract: Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-32 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks</title>
<link>https://arxiv.org/abs/2509.23687</link>
<guid>https://arxiv.org/abs/2509.23687</guid>
<content:encoded><![CDATA[

arXiv:2509.23687v2 Announce Type: replace-cross 
Abstract: Integrated sensing and communication (ISAC) emerges as a key enabler for next-generation applications such as smart cities and autonomous systems. Its integration with unmanned aerial vehicles (UAVs) unlocks new potentials for reliable communication and precise sensing in dynamic aerial environments. However, existing research predominantly treats UAVs as aerial base stations, overlooking their role as ISAC users, and fails to leverage large-scale antenna arrays at terrestrial base stations to enhance security and spectral efficiency. This paper propose a secure and spectral efficient ISAC framework for multi-UAV networks, and a two-stage optimization approach is developed to jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage employs Proximal Policy Optimization (PPO) to optimize digital beamformers and trajectories, and the second stage decomposes the digital solution into analog and digital components via low-complexity matrix factorization. Simulation results demonstrate the effectiveness of the proposed framework compared to benchmark schemes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</title>
<link>https://arxiv.org/abs/2510.02967</link>
<guid>https://arxiv.org/abs/2510.02967</guid>
<content:encoded><![CDATA[

arXiv:2510.02967v3 Announce Type: replace-cross 
Abstract: This paper presents the development and evaluation of a Retrieval-Augmented Generation (RAG) system for querying the United Kingdom's National Institute for Health and Care Excellence (NICE) clinical guidelines using Large Language Models (LLMs). The extensive length and volume of these guidelines can impede their utilisation within a time-constrained healthcare system, a challenge this project addresses through the creation of a system capable of providing users with precisely matched information in response to natural language queries. The system's retrieval architecture, composed of a hybrid embedding mechanism, was evaluated against a corpus of 10,195 text chunks derived from three hundred guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR) of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten retrieved chunks, when evaluated on 7901 queries. The most significant impact of the RAG system was observed during the generation phase. When evaluated on a manually curated dataset of seventy question-answer pairs, RAG-enhanced models showed substantial gains in performance. Faithfulness, the measure of whether an answer is supported by the source text, was increased by 64.7 percentage points to 99.5% for the RAG-enhanced O4-Mini model and significantly outperformed the medical-focused Meditron3-8B LLM, which scored 43%. Clinical evaluation by seven Subject Matter Experts (SMEs) further validated these findings, with GPT-4.1 achieving 98.7% accuracy while reducing unsafe responses by 67% compared to O4-Mini (from 3.0 to 1.0 per evaluator). This study thus establishes RAG as an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models</title>
<link>https://arxiv.org/abs/2510.04146</link>
<guid>https://arxiv.org/abs/2510.04146</guid>
<content:encoded><![CDATA[

arXiv:2510.04146v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and code generation. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. While these models have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency in next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output tokens in parallel, mitigating the limitations of sequential decoding. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive study of the performance characteristics of ARMs and DLMs, combining theoretical analysis with empirical profiling to characterize the trade-offs between these approaches. We show that although DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions, they fail to scale effectively with longer contexts. We then explore block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts (similar to ARMs). We also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, emphasizing that reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2510.07084</link>
<guid>https://arxiv.org/abs/2510.07084</guid>
<content:encoded><![CDATA[

arXiv:2510.07084v3 Announce Type: replace-cross 
Abstract: Transformer-based methods have achieved impressive results in time series forecasting. However, existing Transformers still exhibit limitations in sequence modeling as they tend to overemphasize temporal dependencies. This incurs additional computational overhead without yielding corresponding performance gains. We find that the performance of Transformers is highly dependent on the embedding method used to learn effective representations. To address this issue, we extract multivariate features to augment the effective information captured in the embedding layer, yielding multidimensional embeddings that convey richer and more meaningful sequence representations. These representations enable Transformer-based forecasters to better understand the series. Specifically, we introduce Hybrid Temporal and Multivariate Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature extraction module with a carefully designed multivariate feature extraction module to provide complementary features, thereby achieving a balance between model complexity and performance. By combining HTME with the Transformer architecture, we present HTMformer, leveraging the enhanced feature extraction capability of the HTME extractor to build a lightweight forecaster. Experiments conducted on eight real-world datasets demonstrate that our approach outperforms existing baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGTEN: A Robust Deep Gaussian based Graph Neural Network for Dynamic Trust Evaluation with Uncertainty-Quantification Support</title>
<link>https://arxiv.org/abs/2510.07620</link>
<guid>https://arxiv.org/abs/2510.07620</guid>
<content:encoded><![CDATA[

arXiv:2510.07620v2 Announce Type: replace-cross 
Abstract: Dynamic trust evaluation in large, rapidly evolving graphs demands models that capture changing relationships, express calibrated confidence, and resist adversarial manipulation. DGTEN (Deep Gaussian-Based Trust Evaluation Network) introduces a unified graph-based framework that does all three by combining uncertainty-aware message passing, expressive temporal modeling, and built-in defenses against trust-targeted attacks. It represents nodes and edges as Gaussian distributions so that both semantic signals and epistemic uncertainty propagate through the graph neural network, enabling risk-aware trust decisions rather than overconfident guesses. To track how trust evolves, it layers hybrid absolute-Gaussian-hourglass positional encoding with Kolmogorov-Arnold network-based unbiased multi-head attention, then applies an ordinary differential equation-based residual learning module to jointly model abrupt shifts and smooth trends. Robust adaptive ensemble coefficient analysis prunes or down-weights suspicious interactions using complementary cosine and Jaccard similarity, curbing reputation laundering, sabotage, and on-off attacks. On two signed Bitcoin trust networks, DGTEN delivers standout gains where it matters most: in single-timeslot prediction on Bitcoin-OTC, it improves MCC by +12.34% over the best dynamic baseline; in the cold-start scenario on Bitcoin-Alpha, it achieves a +25.00% MCC improvement, the largest across all tasks and datasets; while under adversarial on-off attacks, it surpasses the baseline by up to +10.23% MCC. These results endorse the unified DGTEN framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Lenses on the AI Revolution: Risk, Transformation, Continuity</title>
<link>https://arxiv.org/abs/2510.12859</link>
<guid>https://arxiv.org/abs/2510.12859</guid>
<content:encoded><![CDATA[

arXiv:2510.12859v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) has emerged as both a continuation of historical technological revolutions and a potential rupture with them. This paper argues that AI must be viewed simultaneously through three lenses: \textit{risk}, where it resembles nuclear technology in its irreversible and global externalities; \textit{transformation}, where it parallels the Industrial Revolution as a general-purpose technology driving productivity and reorganization of labor; and \textit{continuity}, where it extends the fifty-year arc of computing revolutions from personal computing to the internet to mobile. Drawing on historical analogies, we emphasize that no past transition constituted a strict singularity: disruptive shifts eventually became governable through new norms and institutions.
  We examine recurring patterns across revolutions -- democratization at the usage layer, concentration at the production layer, falling costs, and deepening personalization -- and show how these dynamics are intensifying in the AI era. Sectoral analysis illustrates how accounting, law, education, translation, advertising, and software engineering are being reshaped as routine cognition is commoditized and human value shifts to judgment, trust, and ethical responsibility. At the frontier, the challenge of designing moral AI agents highlights the need for robust guardrails, mechanisms for moral generalization, and governance of emergent multi-agent dynamics.
  We conclude that AI is neither a singular break nor merely incremental progress. It is both evolutionary and revolutionary: predictable in its median effects yet carrying singularity-class tail risks. Good outcomes are not automatic; they require coupling pro-innovation strategies with safety governance, ensuring equitable access, and embedding AI within a human order of responsibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
<link>https://arxiv.org/abs/2510.14974</link>
<guid>https://arxiv.org/abs/2510.14974</guid>
<content:encoded><![CDATA[

arXiv:2510.14974v2 Announce Type: replace-cross 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cuAPO: A CUDA-based Parallelization of Artificial Protozoa Optimizer</title>
<link>https://arxiv.org/abs/2510.14982</link>
<guid>https://arxiv.org/abs/2510.14982</guid>
<content:encoded><![CDATA[

arXiv:2510.14982v2 Announce Type: replace-cross 
Abstract: Metaheuristic algorithms are widely used for solving complex problems due to their ability to provide near-optimal solutions. But the execution time of these algorithms increases with the problem size and/or solution space. And, to get more promising results, we have to execute these algorithms for a large number of iterations, requiring a large amount of time and this is one of the main issues found with these algorithms. To handle the same, researchers are now-a-days working on design and development of parallel versions of state-of-the-art metaheuristic optimization algorithms. We, in this paper, present a CUDA-based parallelization of state-of-the-art Artificial Protozoa Optimizer leveraging GPU acceleration. We implement both the existing sequential version and the proposed parallel version of Artificial Protozoa Optimizer for a performance comparison. Our experimental results calculated over a set of CEC2022 benchmark functions demonstrate a significant performance gain i.e. up to 6.7 times speed up is achieved with proposed parallel version. We also use a real world application, i.e., Image Thresholding to compare both algorithms.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BreakFun: Jailbreaking LLMs via Schema Exploitation</title>
<link>https://arxiv.org/abs/2510.17904</link>
<guid>https://arxiv.org/abs/2510.17904</guid>
<content:encoded><![CDATA[

arXiv:2510.17904v2 Announce Type: replace-cross 
Abstract: The proficiency of Large Language Models (LLMs) in processing structured data and adhering to syntactic rules is a capability that drives their widespread adoption but also makes them paradoxically vulnerable. In this paper, we investigate this vulnerability through BreakFun, a jailbreak methodology that weaponizes an LLM's adherence to structured schemas. BreakFun employs a three-part prompt that combines an innocent framing and a Chain-of-Thought distraction with a core "Trojan Schema"--a carefully crafted data structure that compels the model to generate harmful content, exploiting the LLM's strong tendency to follow structures and schemas. We demonstrate this vulnerability is highly transferable, achieving an average success rate of 89% across 13 foundational and proprietary models on JailbreakBench, and reaching a 100% Attack Success Rate (ASR) on several prominent models. A rigorous ablation study confirms this Trojan Schema is the attack's primary causal factor. To counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a defense that utilizes a secondary LLM to perform a "Literal Transcription"--extracting all human-readable text to isolate and reveal the user's true harmful intent. Our proof-of-concept guardrail demonstrates high efficacy against the attack, validating that targeting the deceptive schema is a viable mitigation strategy. Our work provides a look into how an LLM's core strengths can be turned into critical weaknesses, offering a fresh perspective for building more robustly aligned models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection</title>
<link>https://arxiv.org/abs/2510.21118</link>
<guid>https://arxiv.org/abs/2510.21118</guid>
<content:encoded><![CDATA[

arXiv:2510.21118v3 Announce Type: replace-cross 
Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a given source document is essential for real-world applications. While prior research has explored LLM faithfulness, existing benchmarks suffer from annotation ambiguity, primarily due to the ill-defined boundary of permissible external knowledge in generated outputs. For instance, common sense is often incorporated into responses and labeled as "faithful", yet the acceptable extent of such knowledge remains unspecified, leading to inconsistent annotations. To address this issue, we propose a novel faithfulness annotation framework, which introduces an intermediate category, Out-Dependent, to classify cases where external knowledge is required for verification. Using this framework, we construct VeriGray (Verification with the Gray Zone) -- a new unfaithfulness detection benchmark in summarization. Statistics reveal that even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences) in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on average of models) of generated sentences fall into the Out-Dependent category, underscoring the importance of resolving annotation ambiguity in unfaithfulness detection benchmarks. Experiments demonstrate that our benchmark poses significant challenges to multiple baseline methods, indicating considerable room for future improvement.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lipschitz-aware Linearity Grafting for Certified Robustness</title>
<link>https://arxiv.org/abs/2510.25130</link>
<guid>https://arxiv.org/abs/2510.25130</guid>
<content:encoded><![CDATA[

arXiv:2510.25130v2 Announce Type: replace-cross 
Abstract: Lipschitz constant is a fundamental property in certified robustness, as smaller values imply robustness to adversarial examples when a model is confident in its prediction. However, identifying the worst-case adversarial examples is known to be an NP-complete problem. Although over-approximation methods have shown success in neural network verification to address this challenge, reducing approximation errors remains a significant obstacle. Furthermore, these approximation errors hinder the ability to obtain tight local Lipschitz constants, which are crucial for certified robustness. Originally, grafting linearity into non-linear activation functions was proposed to reduce the number of unstable neurons, enabling scalable and complete verification. However, no prior theoretical analysis has explained how linearity grafting improves certified robustness. We instead consider linearity grafting primarily as a means of eliminating approximation errors rather than reducing the number of unstable neurons, since linear functions do not require relaxation. In this paper, we provide two theoretical contributions: 1) why linearity grafting improves certified robustness through the lens of the $l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear activation functions, the dominant source of approximation errors, yields a tighter local Lipschitz constant. Based on these theoretical contributions, we propose a Lipschitz-aware linearity grafting method that removes dominant approximation errors, which are crucial for tightening the local Lipschitz constant, thereby improving certified robustness, even without certified training. Our extensive experiments demonstrate that grafting linearity into these influential activations tightens the $l_\infty$ local Lipschitz constant and enhances certified robustness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection</title>
<link>https://arxiv.org/abs/2511.00139</link>
<guid>https://arxiv.org/abs/2511.00139</guid>
<content:encoded><![CDATA[

arXiv:2511.00139v2 Announce Type: replace-cross 
Abstract: Achieving human-like dexterous manipulation remains a major challenge for general-purpose robots. While Vision-Language-Action (VLA) models show potential in learning skills from demonstrations, their scalability is limited by scarce high-quality training data. Existing data collection methods face inherent constraints: manual teleoperation overloads human operators, while automated planning often produces unnatural motions. We propose a Shared Autonomy framework that divides control between macro and micro motions. A human operator guides the robot's arm pose through intuitive VR teleoperation, while an autonomous DexGrasp-VLA policy handles fine-grained hand control using real-time tactile and visual feedback. This division significantly reduces cognitive load and enables efficient collection of high-quality coordinated arm-hand demonstrations. Using this data, we train an end-to-end VLA policy enhanced with our novel Arm-Hand Feature Enhancement module, which captures both distinct and shared representations of macro and micro movements for more natural coordination. Our Corrective Teleoperation system enables continuous policy improvement through human-in-the-loop failure recovery. Experiments demonstrate that our framework generates high-quality data with minimal manpower and achieves a 90% success rate across diverse objects, including unseen instances. Comprehensive evaluations validate the system's effectiveness in developing dexterous manipulation capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape</title>
<link>https://arxiv.org/abs/2511.02122</link>
<guid>https://arxiv.org/abs/2511.02122</guid>
<content:encoded><![CDATA[

arXiv:2511.02122v2 Announce Type: replace-cross 
Abstract: In this paper we study how the choice of loss functions of non-convex optimization problems affects their robustness and optimization landscape, through the study of noisy matrix sensing. In traditional regression tasks, mean squared error (MSE) loss is a common choice, but it can be unreliable for non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust loss based on nonparametric regression, which uses a kernel-based estimate of the residual density and maximizes the estimated log-likelihood. This robust formulation coincides with the MSE loss under Gaussian errors but remains stable under more general settings. We further examine how this robust loss reshapes the optimization landscape by analyzing the upper-bound of restricted isometry property (RIP) constants for spurious local minima to disappear. Through theoretical and empirical analysis, we show that this new loss excels at handling large noise and remains robust across diverse noise distributions. This work offers initial insights into enhancing the robustness of machine learning tasks through simply changing the loss, guided by an intuitive and broadly applicable analytical framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical exploration and discovery at scale</title>
<link>https://arxiv.org/abs/2511.02864</link>
<guid>https://arxiv.org/abs/2511.02864</guid>
<content:encoded><![CDATA[

arXiv:2511.02864v2 Announce Type: replace-cross 
Abstract: AlphaEvolve (Novikov et al., 2025) is a generic evolutionary coding agent that combines the generative capabilities of LLMs with automated evaluation in an iterative evolutionary framework that proposes, tests, and refines algorithmic solutions to challenging scientific and practical problems. In this paper we showcase AlphaEvolve as a tool for autonomously discovering novel mathematical constructions and advancing our understanding of long-standing open problems.
  To demonstrate its breadth, we considered a list of 67 problems spanning mathematical analysis, combinatorics, geometry, and number theory. The system rediscovered the best known solutions in most of the cases and discovered improved solutions in several. In some instances, AlphaEvolve is also able to generalize results for a finite number of input values into a formula valid for all input values. Furthermore, we are able to combine this methodology with Deep Think and AlphaProof in a broader framework where the additional proof-assistants and reasoning systems provide automated proof generation and further mathematical insights.
  These results demonstrate that large language model-guided evolutionary search can autonomously discover mathematical constructions that complement human intuition, at times matching or even improving the best known results, highlighting the potential for significant new ways of interaction between mathematicians and AI systems. We present AlphaEvolve as a powerful new tool for mathematical discovery, capable of exploring vast search spaces to solve complex optimization problems at scale, often with significantly reduced requirements on preparation and computation time.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[

arXiv:2511.03132v3 Announce Type: replace-cross 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing</title>
<link>https://arxiv.org/abs/2511.07665</link>
<guid>https://arxiv.org/abs/2511.07665</guid>
<content:encoded><![CDATA[

arXiv:2511.07665v2 Announce Type: replace-cross 
Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffRegCD: Integrated Registration and Change Detection with Diffusion Features</title>
<link>https://arxiv.org/abs/2511.07935</link>
<guid>https://arxiv.org/abs/2511.07935</guid>
<content:encoded><![CDATA[

arXiv:2511.07935v3 Announce Type: replace-cross 
Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers</title>
<link>https://arxiv.org/abs/2511.14465</link>
<guid>https://arxiv.org/abs/2511.14465</guid>
<content:encoded><![CDATA[

arXiv:2511.14465v2 Announce Type: replace-cross 
Abstract: Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[

arXiv:2511.16020v2 Announce Type: replace-cross 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-Based Interpretability for Toxicity Detection</title>
<link>https://arxiv.org/abs/2511.16689</link>
<guid>https://arxiv.org/abs/2511.16689</guid>
<content:encoded><![CDATA[

arXiv:2511.16689v2 Announce Type: replace-cross 
Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-computer interactions predict mental health</title>
<link>https://arxiv.org/abs/2511.20179</link>
<guid>https://arxiv.org/abs/2511.20179</guid>
<content:encoded><![CDATA[

arXiv:2511.20179v2 Announce Type: replace-cross 
Abstract: Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward foundation models for mental health. The ability to decode mental states at zero marginal cost creates new opportunities in neuroscience, medicine, and public health, while raising urgent questions about privacy, agency, and autonomy online.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[

arXiv:2511.20853v2 Announce Type: replace-cross 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2511.23059</link>
<guid>https://arxiv.org/abs/2511.23059</guid>
<content:encoded><![CDATA[

arXiv:2511.23059v2 Announce Type: replace-cross 
Abstract: Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development and Benchmarking of a Blended Human-AI Qualitative Research Assistant</title>
<link>https://arxiv.org/abs/2512.00009</link>
<guid>https://arxiv.org/abs/2512.00009</guid>
<content:encoded><![CDATA[

arXiv:2512.00009v2 Announce Type: replace-cross 
Abstract: Qualitative research emphasizes constructing meaning through iterative engagement with textual data. Traditionally this human-driven process requires navigating coder fatigue and interpretative drift, thus posing challenges when scaling analysis to larger, more complex datasets. Computational approaches to augment qualitative research have been met with skepticism, partly due to their inability to replicate the nuance, context-awareness, and sophistication of human analysis. Large language models, however, present new opportunities to automate aspects of qualitative analysis while upholding rigor and research quality in important ways. To assess their benefits and limitations - and build trust among qualitative researchers - these approaches must be rigorously benchmarked against human-generated datasets. In this work, we benchmark Muse, an interactive, AI-powered qualitative research system that allows researchers to identify themes and annotate datasets, finding an inter-rater reliability between Muse and humans of Cohen's $\kappa$ = 0.71 for well-specified codes. We also conduct robust error analysis to identify failure mode, guide future improvements, and demonstrate the capacity to correct for human bias.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title>
<link>https://arxiv.org/abs/2512.01054</link>
<guid>https://arxiv.org/abs/2512.01054</guid>
<content:encoded><![CDATA[

arXiv:2512.01054v2 Announce Type: replace-cross 
Abstract: Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.
  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.
  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.
  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[

arXiv:2512.02020v2 Announce Type: replace-cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[

arXiv:2512.03979v2 Announce Type: replace-cross 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphBench: Next-generation graph learning benchmarking</title>
<link>https://arxiv.org/abs/2512.04475</link>
<guid>https://arxiv.org/abs/2512.04475</guid>
<content:encoded><![CDATA[

arXiv:2512.04475v2 Announce Type: replace-cross 
Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2512.04524</link>
<guid>https://arxiv.org/abs/2512.04524</guid>
<content:encoded><![CDATA[

arXiv:2512.04524v2 Announce Type: replace-cross 
Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</title>
<link>https://arxiv.org/abs/2512.05377</link>
<guid>https://arxiv.org/abs/2512.05377</guid>
<content:encoded><![CDATA[

arXiv:2512.05377v3 Announce Type: replace-cross 
Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 40 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trusted AI Agents in the Cloud</title>
<link>https://arxiv.org/abs/2512.05951</link>
<guid>https://arxiv.org/abs/2512.05951</guid>
<content:encoded><![CDATA[

arXiv:2512.05951v2 Announce Type: replace-cross 
Abstract: AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning</title>
<link>https://arxiv.org/abs/2512.07371</link>
<guid>https://arxiv.org/abs/2512.07371</guid>
<content:encoded><![CDATA[

arXiv:2512.07371v2 Announce Type: replace-cross 
Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social welfare optimisation in well-mixed and structured populations</title>
<link>https://arxiv.org/abs/2512.07453</link>
<guid>https://arxiv.org/abs/2512.07453</guid>
<content:encoded><![CDATA[

arXiv:2512.07453v2 Announce Type: replace-cross 
Abstract: Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.21005</link>
<guid>https://arxiv.org/abs/2511.21005</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Reward Optimization, Preference Modeling, Reasoning Enhancement

<br /><br />Summary: This paper addresses limitations in Reinforcement Learning with Verifiable Rewards (RLVR) methods used to improve reasoning in Large Language Models (LLMs), highlighting problems such as coarse-grained rewards, reward noise, and inefficient exploration which cause unstable training and entropy collapse. To overcome these challenges, the authors introduce the Intrinsic Confidence-Driven Group Relative Preference Optimization (ICPO) method. ICPO leverages the natural probabilities generated by an LLM for different responses as a form of self-assessment, using these to calculate a preference advantage score by comparing relative generation probabilities among multiple responses to the same prompt. This score is combined with verifiable rewards to better guide exploration. The preference advantage score helps mitigate coarse reward granularity and noise, reduces overconfidence errors, boosts the relative rank of high-quality but undervalued responses, and prevents the model from overfitting on particular strategies. The efficacy of ICPO is validated through comprehensive experiments on four general-domain and three mathematical reasoning benchmarks, where it consistently improves reasoning performance over previous methods such as GRPO. This method shows potential for enhancing LLM reasoning stability and accuracy by refining reward-driven training processes through intrinsic confidence-based preference metrics. <div>
arXiv:2511.21005v4 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title>
<link>https://arxiv.org/abs/2512.07462</link>
<guid>https://arxiv.org/abs/2512.07462</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, strategic behaviour, repeated social dilemmas, multi-agent systems, cooperation

<br /><br />Summary:  
This work addresses the strategic behaviour of Large Language Models (LLMs) when acting as autonomous agents in interactive and multi-agent environments, highlighting the importance of understanding their intentions for safety and coordination. The authors extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas by introducing two new experimental setups: a payoff-scaled Prisoner's Dilemma that isolates sensitivity to incentive magnitude, and a multi-agent Public Goods Game featuring dynamic payoffs and histories. These environments reveal consistent patterns in LLM behaviour across different models and languages, such as sensitivity to incentives, divergence based on linguistic context, and an end-game tendency toward defection. To interpret these behaviours, traditional supervised classifiers trained on canonical repeated-game strategies are applied to the collected gameplay trajectories, demonstrating that LLMs exhibit systematic, model- and language-dependent behavioural intentions. The study also finds that linguistic framing may influence behaviour as strongly as architectural differences. Overall, the findings provide a unified methodological approach to audit LLMs as strategic agents and expose inherent cooperation biases. These insights have direct implications for AI governance, collective decision-making, and the safe design of multi-agent AI systems. <div>
arXiv:2512.07462v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound</title>
<link>https://arxiv.org/abs/2512.11169</link>
<guid>https://arxiv.org/abs/2512.11169</guid>
<content:encoded><![CDATA[
<div> MILPs, branch and bound, reinforcement learning, combinatorial decision making, differentiable policy<br /><br />Summary:<br /><br />This article addresses the challenge of solving combinatorial sequential decision-making problems traditionally modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&amp;B) algorithms. It recognizes that the difficulty of accurately modeling stochastic real-world problems with MILPs often results in suboptimal operational performance. The paper highlights recent machine learning approaches that improve decision quality rather than model accuracy but notes their reliance on supervised learning with access to true optimal decisions and the use of surrogate gradients. To overcome these limitations, the authors propose a novel CORL (combinatorial optimization with reinforcement learning) framework that fine-tunes MILP schemes end-to-end using reinforcement learning directly on real-world data to enhance operational outcomes. This is achieved by formulating the MILP solved by B&amp;B as a differentiable stochastic policy compatible with RL algorithms. The methodology allows learning without requiring optimal decision labels and avoids gradient surrogates. The effectiveness of the CORL method is demonstrated on a simple illustrative example of combinatorial sequential decision making, validating its potential as a new approach for integrating MILP optimization with RL to improve real-world decision-making tasks. <div>
arXiv:2512.11169v1 Announce Type: new 
Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&amp;B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&amp;B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling</title>
<link>https://arxiv.org/abs/2512.11187</link>
<guid>https://arxiv.org/abs/2512.11187</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Freight Exchange, combinatorial bundling, pickup-and-delivery routing, Transformer Neural Network, Large Neighborhood Search  

<br /><br />Summary:  
This paper addresses the challenge of efficient combinatorial bundling in Online Freight Exchange Systems (OFEX), which facilitate real-time freight job matching between shippers and carriers. The problem is formulated as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), aiming to maximize revenue while respecting capacity, precedence, and route-length constraints. The key difficulty lies in integrating combinatorial bundle selection with pickup-and-delivery routing under strict sub-second latency requirements. To tackle this, the authors introduce a learning-accelerated hybrid search pipeline that combines a Transformer Neural Network-based constructive policy with a novel Multi-Start Large Neighborhood Search (MSLNS) metaheuristic, operating in a rolling-horizon framework. This approach repeatedly freezes marketplace states for static snapshot optimization within a limited time budget. The Transformer provides low-latency, high-quality initial solutions (seeds), which MSLNS then refines efficiently through multi-start explorations. Experimental results demonstrate that this method surpasses existing neural combinatorial optimization and metaheuristic approaches in solution quality, achieving under 2% optimality gap in total revenue compared to exact baseline methods. Notably, this study is the first to show that deep neural network constructors can reliably generate high-quality seeds for multi-start improvement heuristics, applicable beyond the specific m1-PDSTSP to broader selective traveling salesperson and pickup-and-delivery problems. <div>
arXiv:2512.11187v1 Announce Type: new 
Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration</title>
<link>https://arxiv.org/abs/2512.11213</link>
<guid>https://arxiv.org/abs/2512.11213</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time compute allocation, multi-agent systems, FutureWeaver, modularized collaboration, dual-level planning<br /><br />Summary:<br /><br />The paper addresses the challenge of optimizing computation at test time in multi-agent systems, where coordinating multiple agents under fixed computation budgets is complex. It highlights recent advances in improving large language model performance by scaling inference-time compute techniques like repeated sampling and self-reflection but notes the lack of principled methods for multi-agent collaboration in this context. To fill this gap, the authors introduce FutureWeaver, a novel framework designed for planning and allocating test-time compute resources effectively among multiple agents within budget constraints. FutureWeaver features modularized collaboration, implementing reusable multi-agent workflows encapsulated as callable functions derived automatically through self-play reflection that abstracts common interaction patterns from previous agent trajectories. The framework employs a dual-level planning approach that dynamically optimizes compute allocation by reasoning about the current task state and forecasting future steps, allowing more strategic and collaborative use of computational resources. Experimental results on complex multi-agent benchmarks demonstrate that FutureWeaver consistently surpasses baseline methods across various budget scenarios, confirming its capability to enhance multi-agent collaboration and inference-time compute optimization. This work represents a significant step toward more efficient and cooperative multi-agent system inference under explicit resource limitations. <div>
arXiv:2512.11213v1 Announce Type: new 
Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation</title>
<link>https://arxiv.org/abs/2512.11270</link>
<guid>https://arxiv.org/abs/2512.11270</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Markov decision process, large language model, policy generation, automated modeling

<br /><br />Summary:  
The paper addresses the challenge of applying reinforcement learning (RL) to real-world tasks by automating the conversion of natural language task descriptions into a formal Markov decision process (MDP) and trained RL policies. The authors propose A-LAMP, a framework that leverages agentic large language models (LLMs) to translate informal task descriptions into MDP formulations, executable environments, and trained policies. The framework breaks down the process into verifiable stages—modeling, coding, and training—to ensure semantic alignment and reduce errors such as modeling inaccuracies, fragile codebases, and objective misalignment that typically hinder policy training. Across various classic control benchmarks and custom RL domains, A-LAMP outperforms individual state-of-the-art LLM approaches in generating effective policies. Impressively, even a lightweight version of A-LAMP, utilizing smaller language models, nearly matches the performance of significantly larger models, highlighting its efficiency. The authors conduct failure analyses to explain the source of improvements brought by their approach. A detailed case study further confirms that A-LAMP-generated environments and policies maintain task optimality, demonstrating the framework’s reliability and correctness for automated RL environment creation and policy development based on natural language inputs. <div>
arXiv:2512.11270v1 Announce Type: new 
Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning</title>
<link>https://arxiv.org/abs/2512.11271</link>
<guid>https://arxiv.org/abs/2512.11271</guid>
<content:encoded><![CDATA[
<div> Keywords: trip planning, multi-agent framework, constraint satisfaction, itinerary generation, large language models  

<br /><br />Summary:  
This paper addresses the challenge of real-world trip planning, which demands converting open-ended user requests into executable itineraries while satisfying strict spatial, temporal, budgetary constraints and user preferences. Existing approaches using large language model (LLM)-based agents face difficulties in meeting constraints, coordinating multiple tools, and maintaining efficiency, leading to infeasible or overly costly plans. To overcome these issues, the authors propose TriFlow, a progressive multi-agent framework structured around a three-stage pipeline: retrieval, planning, and governance. TriFlow effectively combines structured reasoning and language flexibility by progressively narrowing the search space, enabling rule-LLM collaboration to assemble itineraries consistent with constraints. Additionally, it employs bounded iterative refinement to ensure overall feasibility and personalization of plans. The framework was evaluated on two benchmarks, TravelPlanner and TripTailor, achieving state-of-the-art performance with final pass rates of 91.1% and 97%, respectively. Furthermore, TriFlow demonstrated over a tenfold improvement in runtime efficiency compared to existing state-of-the-art methods. This approach shows promise for scalable, flexible, and efficient trip planning using advanced LLM-driven multi-agent coordination. <div>
arXiv:2512.11271v1 Announce Type: new 
Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving</title>
<link>https://arxiv.org/abs/2512.11323</link>
<guid>https://arxiv.org/abs/2512.11323</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Visual Language Models, CAPTCHA, Benchmark, Multi-modal Alignment, LVLM Performance<br /><br />Summary: Benefiting from advanced multi-modal alignment strategies, Large Visual Language Models (LVLMs) have shown promising abilities in simulating human visual and reasoning skills, including solving CAPTCHAs. However, existing CAPTCHA benchmarks are limited, as prior datasets were often customized for specific research purposes and fail to encompass the full variety of CAPTCHA types currently used. There is also a notable lack of dedicated benchmarks tailored specifically for LVLMs. To address these deficiencies, the authors propose a novel benchmark named CAPTURE (CAPTCHA for Testing Under Real-world Experiments), which is designed explicitly for evaluating LVLM performance. CAPTURE includes four main CAPTCHA types and 25 sub-types sourced from 31 different vendors, ensuring a diverse set of challenges. This variety facilitates a comprehensive and multi-dimensional assessment of LVLM capabilities in CAPTCHA solving. Additionally, the benchmark boasts extensive class variety, large-scale data, and specialized labels relevant to LVLMs, filling critical gaps left by prior work in terms of both data comprehensiveness and labeling precision. When tested with CAPTURE, current LVLMs exhibit poor performance on CAPTCHA tasks, highlighting the benchmark's effectiveness and the need for further advancement in the field. <div>
arXiv:2512.11323v1 Announce Type: new 
Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance</title>
<link>https://arxiv.org/abs/2512.11421</link>
<guid>https://arxiv.org/abs/2512.11421</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, task completion framework, reinforcement learning, verifiable behavior, constraint compliance<br /><br />Summary:<br /><br />This paper addresses the challenge of unreliable and unverifiable behavior by Large Language Models (LLMs) in multi-turn tasks. It introduces a novel task completion framework designed for LLM-based agents operating in environments characterized by reinforcement learning structures, which include defined observation, action, and reward signals. The framework is composed of three integrated components: a lightweight task profiler that dynamically selects appropriate reasoning and generation strategies, a reasoning module that learns to map observations to actions in a verifiable manner, and a generation module that guarantees outputs are compliant with constraints by employing validation or deterministic synthesis methods. As the agent continuously interacts with its environment, these components evolve together, resulting in behavior that is both reliable and trustworthy. The approach enhances the transparency and accountability of LLMs in complex interactive settings, providing a structured way to guide their behavior under explicit reinforcement learning formalisms. Overall, the framework offers a systematic method to improve the verifiability and constraint adherence of LLM-generated outputs in sequential decision-making tasks, advancing the capabilities and dependability of LLM-based agents. <div>
arXiv:2512.11421v1 Announce Type: new 
Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints</title>
<link>https://arxiv.org/abs/2512.11426</link>
<guid>https://arxiv.org/abs/2512.11426</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Multi-agent systems, Cost-effectiveness, Token-cost budget, Latency budget<br /><br />Summary:<br /><br />This paper introduces AgentBalance, a novel framework designed to create cost-effective multi-agent systems (MAS) using Large Language Models (LLMs) under strict token-cost and latency budgets. Unlike previous approaches that prioritize communication topology without explicitly considering deployment constraints, AgentBalance adopts a backbone-then-topology design strategy. First, it generates heterogeneous agents by constructing an LLM pool, selecting suitable backbones, and matching roles to these backbones to optimize agent capabilities within budget limits. Next, it adaptively synthesizes MAS topology by applying agent representation learning, gating mechanisms, and latency-aware communication design to guide efficient inter-agent interactions. Experiments across benchmarks with 14 candidate LLM backbones demonstrate that AgentBalance delivers up to 10% performance improvement under token-cost budgets and up to 22% under latency constraints. Furthermore, it achieves strong Area Under Curve (AUC) metrics on performance-versus-budget curves, indicating robust trade-offs. AgentBalance can also be integrated as a plug-in into existing MAS to enhance performance within the same budget constraints. The framework shows strong generalization to previously unseen LLMs, making it practical for real-world budget-aware deployment scenarios. The authors have open-sourced their code for community use and further research. <div>
arXiv:2512.11426v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Baseline: Examining Baseline Effects on Explainability Metrics</title>
<link>https://arxiv.org/abs/2512.11433</link>
<guid>https://arxiv.org/abs/2512.11433</guid>
<content:encoded><![CDATA[
<div> Keywords: Attribution methods, Explainable AI, Fidelity metrics, Baseline functions, Out-of-distribution images<br /><br />Summary:  
Attribution methods are widely used in Explainable Artificial Intelligence (XAI) to determine the importance of input features, commonly evaluated by Fidelity metrics such as Insertion and Deletion. These metrics depend on a baseline function that alters the most important pixels identified by an attribution map. The paper highlights a critical issue that the choice of baseline inherently favors certain attribution methods, resulting in contradictions even in simple linear models where different baselines identify different optimal methods. To address this, the authors propose evaluating baselines based on two key properties: their ability to remove information and their tendency to avoid generating overly out-of-distribution (OOD) images. Upon testing multiple baselines, they find that none satisfy both criteria simultaneously, revealing a trade-off between removing information and producing OOD images. To overcome this trade-off, the authors introduce a novel baseline inspired by recent advancements in feature visualization. This new baseline is model-dependent, effectively removes information while mitigating the generation of OOD images, thereby improving upon existing baselines. The study ultimately offers a deeper understanding of baseline selection in Fidelity metrics and provides a practical solution to enhance the evaluation of attribution methods in XAI. The associated code is publicly available at the provided GitHub repository. <div>
arXiv:2512.11433v1 Announce Type: new 
Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes</title>
<link>https://arxiv.org/abs/2512.11463</link>
<guid>https://arxiv.org/abs/2512.11463</guid>
<content:encoded><![CDATA[
<div> Motif-2-12.7B-Reasoning, language model, reasoning adaptation, long-context, fine-tuning

<br /><br />Summary:  
The paper introduces Motif-2-12.7B-Reasoning, a 12.7 billion parameter language model aimed at closing the gap between open-weight models and proprietary large-scale models in areas requiring complex reasoning and long-context understanding. The authors address common challenges in reasoning adaptation, such as model collapse and training instability, by proposing a comprehensive and reproducible training recipe that includes system, data, and algorithmic improvements. Key system innovations include memory-efficient infrastructure supporting 64K-token contexts via hybrid parallelism and kernel-level optimizations. The training strategy utilizes a two-stage Supervised Fine-Tuning (SFT) curriculum designed to reduce distribution mismatch by leveraging verified and aligned synthetic data. Additionally, a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline is detailed, which stabilizes training through difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical evaluation shows that Motif-2-12.7B-Reasoning performs comparably to much larger models on benchmarks spanning mathematics, coding, and agentic tasks. The work offers a competitive open-weight model and provides a practical blueprint for scaling reasoning capabilities in LLMs while operating within realistic computational constraints. <div>
arXiv:2512.11463v1 Announce Type: new 
Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three methods, one problem: Classical and AI approaches to no-three-in-line</title>
<link>https://arxiv.org/abs/2512.11469</link>
<guid>https://arxiv.org/abs/2512.11469</guid>
<content:encoded><![CDATA[
<div> No-Three-In-Line, Integer Linear Programming, PatternBoost, Reinforcement Learning, Combinatorial Geometry<br /><br />Summary:  
The No-Three-In-Line problem involves placing the maximum number of points on an n by n grid such that no three points lie on a straight line, a classic challenge in combinatorial geometry. Traditional methods like Integer Linear Programming (ILP) can find optimal solutions but suffer exponential time complexity as the grid size increases. This paper conducts the first systematic comparison between classical optimization techniques and AI-based approaches applied to this problem. It introduces the application of PatternBoost transformer learning and Proximal Policy Optimization (PPO) reinforcement learning to tackle the No-Three-In-Line problem. ILP successfully produces provably optimal solutions for grids up to 19 by 19. PatternBoost achieves near-optimal performance up to 14 by 14 grids, reducing test loss by 96%. PPO attains perfect solutions on 10 by 10 grids but fails to maintain feasibility at 11 by 11 due to constraint violations. The findings highlight that while classical optimization remains indispensable for guaranteeing exact solutions, AI methods show competitive approximation on smaller grid sizes. Ultimately, the study suggests that hybrid approaches combining optimization and AI techniques hold the greatest promise for scaling the problem to larger grids effectively. <div>
arXiv:2512.11469v1 Announce Type: new 
Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General-purpose AI models can generate actionable knowledge on agroecological crop protection</title>
<link>https://arxiv.org/abs/2512.11474</link>
<guid>https://arxiv.org/abs/2512.11474</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, agroecological crop protection, large language models, biological control, data consistency<br /><br />Summary: This study evaluates the application of generative artificial intelligence, specifically large language models (LLMs), in agri-food science with a focus on agroecological crop protection. Two LLMs, the web-grounded DeepSeek and the free-tier ChatGPT, were compared in their ability to generate scientific knowledge on nine globally limiting pests, weeds, and plant diseases. DeepSeek analyzed a literature corpus 4.8 to 49.7 times larger than ChatGPT and identified 1.6 to 2.4 times more biological control agents and management strategies. Consequently, DeepSeek reported efficacy estimates that were 21.6% higher, demonstrated improved consistency between laboratory and field data, and better reflected the effects of pest identity and management tactics. Despite these advantages, both models exhibited hallucinations by generating fictitious agents and references, reported implausible ecological interactions, confused scientific nomenclature, and missed key data on important control measures. Nevertheless, both LLMs accurately captured low-resolution trends in efficacy. The study suggests that when supported by stringent human oversight, LLMs hold significant promise as tools for enhancing decision-making at the farm level and fostering scientific creativity in agroecology. <div>
arXiv:2512.11474v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAID: A Benchmark for Bias Assessment of AI Detectors</title>
<link>https://arxiv.org/abs/2512.11505</link>
<guid>https://arxiv.org/abs/2512.11505</guid>
<content:encoded><![CDATA[
<div> Keywords: AI detectors, bias evaluation, sociolinguistic factors, text detection, underrepresented groups<br /><br />Summary:<br /><br />This paper presents BAID, a comprehensive evaluation framework designed to identify biases in AI-generated text detectors across a wide range of sociolinguistic dimensions. The study addresses the gap in systematic bias analysis beyond isolated cases, especially focusing on English Language Learners (ELLs) and other underrepresented groups. BAID utilizes a large dataset of over 200,000 samples categorized into seven major groups: demographics, age, educational grade level, dialect, formality, political leaning, and topic. Additionally, synthetic samples are created via specially designed prompts to maintain content integrity while reflecting subgroup-specific writing styles, enabling a controlled evaluation environment. The framework is applied to four state-of-the-art open-source AI text detectors, revealing consistent disparities in detection performance. Notably, these detectors exhibit lower recall rates when processing texts from underrepresented subgroups, indicating significant bias. The authors emphasize that such performance gaps must be acknowledged and addressed prior to deploying AI detection tools in educational and professional settings. Ultimately, BAID offers a scalable and transparent auditing method that highlights the importance of bias-aware evaluation in AI systems to ensure fairness and reliability when used publicly. <div>
arXiv:2512.11505v1 Announce Type: new 
Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection</title>
<link>https://arxiv.org/abs/2512.11506</link>
<guid>https://arxiv.org/abs/2512.11506</guid>
<content:encoded><![CDATA[
<div> Greenwashing, sustainability, knowledge graph, ESG reports, fact verification  

<br /><br />Summary:  
This paper introduces EmeraldMind, an innovative framework designed to detect greenwashing—misleading corporate claims about sustainability.  EmeraldMind integrates a domain-specific knowledge graph called EmeraldGraph, which is constructed from diverse corporate ESG (environmental, social, and governance) reports, providing verifiable evidence that is typically absent in generic knowledge bases. By combining this knowledge graph with retrieval-augmented generation, EmeraldMind supports large language models (LLMs) in the accurate assessment of sustainability claims. The framework emphasizes justification-centric classification by offering transparent, evidence-backed verdicts and responsibly abstains when claims lack sufficient verification. Experimental evaluation on a newly created greenwashing claims dataset demonstrates that EmeraldMind achieves competitive accuracy, broader coverage, and superior explanation quality compared to generic LLMs. Notably, these results are achieved without the need for fine-tuning or retraining large language models, highlighting the framework’s efficiency and practical applicability in combating misinformation in sustainability communication. EmeraldMind thus represents a significant step forward in leveraging AI for environmental accountability and enhancing trust in corporate sustainability disclosures. <div>
arXiv:2512.11506v1 Announce Type: new 
Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives</title>
<link>https://arxiv.org/abs/2512.11544</link>
<guid>https://arxiv.org/abs/2512.11544</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, clinical information extraction, AI-MASLD, functional decline, medical evaluation

<br /><br />Summary:  
This study evaluates the capability of four mainstream Large Language Models (LLMs)—GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max—to extract core medical information from noisy and redundant patient chief complaints in simulated real-world clinical scenarios. Employing a cross-sectional analysis with twenty standardized medical probes across five key clinical dimensions, the models were assessed under a double-blind, inverse rating scale by two independent clinicians against gold-standard expert answers. Results revealed that all models demonstrated varying degrees of functional defects, with Qwen3-Max outperforming others and Gemini 2.5 performing the worst. Under extreme noise conditions, most models experienced functional collapse, indicating significant degradation in performance. A critical finding was GPT-4o’s severe error in risk assessment of pulmonary embolism secondary to deep vein thrombosis. The study introduces the novel concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)," drawing an analogy between LLMs’ functional decline and metabolic dysfunction in clinical disease. This first empirical evidence signals important safety considerations when deploying LLMs in healthcare, emphasizing that current AI tools should serve only as adjuncts under human supervision due to the substantial gap between their theoretical medical knowledge and real-world clinical application effectiveness. <div>
arXiv:2512.11544v1 Announce Type: new 
Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Benchmark Democratization and Carpentry</title>
<link>https://arxiv.org/abs/2512.11588</link>
<guid>https://arxiv.org/abs/2512.11588</guid>
<content:encoded><![CDATA[
<div> Benchmarks, AI benchmarking, dynamic evaluation, reproducibility, education

<br /><br />Summary:  
Benchmarks play a critical role in machine learning by enabling reproducibility, comparison, and scientific advancement. However, current AI benchmarks are increasingly complex and static, failing to keep pace with rapid changes in model architecture, scale, data, and deployment contexts. Large language models often memorize these static benchmarks, creating a disconnect between benchmark performance and real-world effectiveness. To address this, there is a need for continuous, adaptive benchmarking frameworks that better align scientific evaluation with practical deployment risks. Developing expertise in AI Benchmark Carpentry through education and skills training is essential to overcome barriers such as high resource requirements, restricted access to specialized hardware, and lack of design expertise. Existing benchmarks often prioritize peak performance on elite hardware, providing limited guidance for diverse and real-world scenarios. Future benchmarking must be dynamic, incorporating evolving models, updated datasets, and heterogeneous platforms while ensuring transparency, reproducibility, and interpretability. Democratization of benchmarking requires both technical innovation and systematic education across multiple levels to cultivate sustained, community-driven expertise. Ultimately, benchmarks should support application-relevant, context-sensitive comparisons to inform responsible and accessible AI deployment, ensuring evaluation methodologies evolve alongside AI technology and deployment practices. <div>
arXiv:2512.11588v1 Announce Type: new 
Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Inference in Energy Demand Prediction</title>
<link>https://arxiv.org/abs/2512.11653</link>
<guid>https://arxiv.org/abs/2512.11653</guid>
<content:encoded><![CDATA[
<div> Energy demand prediction, structural causal model, Bayesian model, temperature sensitivity, MAPE  

<br /><br />Summary:  
This paper addresses the critical challenge of predicting energy demand, which is influenced by complex and interdependent factors such as weather conditions (temperature, humidity, wind speed, solar radiation) and calendar information (hour of day, month of year). The authors propose a structural causal model to explicate the causal relationships among these variables rather than relying on simpler correlation-based approaches. Their causal analysis uncovers season-dependent sensitivity of energy demand to temperature fluctuations and a lower variance in energy demand during winter, attributed to a decoupling between temperature changes and daily activity patterns. Leveraging these causal insights as prior knowledge, the study develops a Bayesian model for energy demand forecasting. When tested on unseen data, this model achieves state-of-the-art accuracy, registering a 3.84 percent mean absolute percentage error (MAPE) on the test set. Furthermore, the model demonstrates strong robustness, with cross-validation over two years of data maintaining an average MAPE of 3.88 percent. Overall, the study highlights how incorporating causal understanding into statistical modeling can significantly improve the reliability and interpretability of energy demand forecasts. <div>
arXiv:2512.11653v1 Announce Type: new 
Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition</title>
<link>https://arxiv.org/abs/2512.11682</link>
<guid>https://arxiv.org/abs/2512.11682</guid>
<content:encoded><![CDATA[
<div> Keywords: Therapeutic decision-making, agentic AI, retrieval-augmented generation, biomedical knowledge, tool integration  

<br /><br />Summary:  
1. The article addresses therapeutic decision-making in clinical medicine, highlighting the complexity of integrating patient data, disease factors, and drug information for tasks such as drug recommendation, treatment planning, and adverse effect prediction.  
2. The proposed method, TxAgent, exemplifies agentic AI by using iterative retrieval-augmented generation (RAG) to perform multi-step reasoning with biomedical knowledge support.  
3. TxAgent is built on a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite named ToolUniverse, which integrates FDA Drug API, OpenTargets, and Monarch databases to provide up-to-date therapeutic information.  
4. The article emphasizes the critical importance of both the accuracy of the reasoning trace and the correctness of tool invocation sequences due to stringent safety requirements unique to medical applications. It advocates for evaluation protocols that explicitly supervise token-level reasoning and tool usage.  
5. The work was demonstrated through participation in the CURE-Bench NeurIPS 2025 Challenge, where the authors analyzed the impact of retrieval quality on model performance and achieved performance improvements via enhanced tool-retrieval strategies. This contribution earned the Excellence Award in Open Science.  
Complete details are available at https://curebench.ai/. <div>
arXiv:2512.11682v1 Announce Type: new 
Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring skill-based uplift from AI in a real biological laboratory</title>
<link>https://arxiv.org/abs/2512.10960</link>
<guid>https://arxiv.org/abs/2512.10960</guid>
<content:encoded><![CDATA[
<div> Skills-based uplift, AI reasoning model, biological applications, wet-lab experiment, biosecurity<br /><br />Summary:<br /><br />1. The study investigates the impact of access to an AI reasoning model on participant skills when performing a biological experiment, compared to a control group limited to internet access.<br />2. Participants came from a diverse group of Los Alamos National Laboratory employees without prior wet-lab experience, ensuring that skill rather than knowledge was a primary barrier.<br />3. The experimental task involved transforming E. coli with an expression construct, inducing reporter peptide expression, and confirming expression via mass spectrometry.<br />4. Both quantitative outcomes (such as successful task completion) and qualitative observations (participant interactions with AI, internet, lab equipment, and peers) were recorded.<br />5. The results showcase the potential for AI tools to enhance practical biological skills and underscore challenges in designing real-world studies that reflect the complex interplay between AI assistance and biosecurity concerns, providing valuable insights for future research in this evolving field. <div>
arXiv:2512.10960v1 Announce Type: cross 
Abstract: Understanding how AI systems are used by people in real situations that mirror aspects of both legitimate and illegitimate use is key to predicting the risks and benefits of AI systems. This is especially true in biological applications, where skill rather than knowledge is often the primary barrier for an untrained person. The challenge is that these studies are difficult to execute well and can take months to plan and run.
  Here we report the results of a pilot study that attempted to empirically measure the magnitude of \emph{skills-based uplift} caused by access to an AI reasoning model, compared with a control group that had only internet access. Participants -- drawn from a diverse pool of Los Alamos National Laboratory employees with no prior wet-lab experience -- were asked to transform \ecoli{} with a provided expression construct, induce expression of a reporter peptide, and have expression confirmed by mass spectrometry.
  We recorded quantitative outcomes (e.g., successful completion of experimental segments) and qualitative observations about how participants interacted with the AI system, the internet, laboratory equipment, and one another. We present the results of the study and lessons learned in designing and executing this type of study, and we discuss these results in the context of future studies of the evolving relationship between AI and global biosecurity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI</title>
<link>https://arxiv.org/abs/2512.10961</link>
<guid>https://arxiv.org/abs/2512.10961</guid>
<content:encoded><![CDATA[
<div> Keywords: AI as cognitive amplifier, user expertise, intelligence amplification, domain knowledge, metacognitive skills<br /><br />Summary:<br /><br />1. The article emphasizes that the effectiveness of AI tools varies greatly depending on the user's expertise and judgment rather than the tools themselves.<br />2. It proposes a perspective of AI as a cognitive amplifier that enhances existing human abilities instead of replacing human intelligence.<br />3. A three-level model of AI engagement is introduced: passive acceptance, iterative collaboration, and cognitive direction, where progression relies on domain expertise and metacognitive skills, not just technical training.<br />4. Empirical studies and field observations reveal significant performance gaps between expert and novice users due to differences in domain knowledge, quality evaluation, and iterative refinement.<br />5. The paper suggests workforce development and AI system design should focus on strengthening domain expertise, evaluative judgment, and reflective practice alongside AI literacy and prompt engineering for more effective AI adoption. <div>
arXiv:2512.10961v1 Announce Type: cross 
Abstract: Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering</title>
<link>https://arxiv.org/abs/2512.10962</link>
<guid>https://arxiv.org/abs/2512.10962</guid>
<content:encoded><![CDATA[
<div> Computer-use agents, GUI interaction, data synthesis, step-level filtering, reward model<br /><br />Summary: This paper addresses the challenge of training computer-use agents (CUAs) to operate digital graphical user interfaces (GUIs), which is difficult due to the high cost and scarcity of high-quality trajectory data from human demonstrations. The authors propose a scalable data synthesis pipeline to generate reliable supervision by applying step-level filtering that individually evaluates and retains only correct actions from noisy rollouts produced by strong CUAs, complemented by reasoning augmentation to improve planning. Leveraging this method, they create WebSTAR, a dataset of 13.3K trajectories and 100K reasoning-rich steps synthesized using OpenAI’s computer-use-preview model. They train Qwen-2.5-VL-Instruct models (7B and 32B parameters) on WebSTAR, achieving significant improvements; notably, their 7B model outperforms the state-of-the-art open-source CUA model UI-TARS-1.5-7B by over 15% with supervised finetuning alone. Building on step-level grading, they further construct WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini. StepRM matches the grading quality of larger models while offering greater efficiency for large-scale deployment. This work highlights step-level filtering as a crucial principle for scalable CUA training and provides practical tools (datasets and reward model) to advance robust and efficient computer-use agents. <div>
arXiv:2512.10962v1 Announce Type: cross 
Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis</title>
<link>https://arxiv.org/abs/2512.10963</link>
<guid>https://arxiv.org/abs/2512.10963</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Modal Emotion Recognition, Intent Recognition, AIGC Recommendation, Cross-Modal Transformer, Attention-Based Fusion  

<br /><br />Summary:  
This study addresses the growing need for emotionally aware recommendation systems in AI-generated content (AIGC) domains like music, video, and literature by proposing a Multi-Modal Emotion and Intent Recognition Model (MMEI). The novel system leverages a BERT-based Cross-Modal Transformer combined with an Attention-Based Fusion module to jointly process visual, auditory, and textual data. Specifically, pretrained encoders ViT, Wav2Vec2, and BERT extract features from facial expressions, speech tone, and user comments or utterances, respectively. The model then fuses these modalities to generate comprehensive emotion-intent representations. These embeddings feed into a contextual matching layer that personalizes content recommendations based on users’ emotional and intentional states in real time. Experimental validation on benchmark datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrated a 4.3% improvement in F1-score and a 12.3% decrease in cross-entropy loss over the best fusion-based transformer baseline. Moreover, online user-level evaluations showed that emotion-driven recommendations increased engagement time by 15.2% and boosted satisfaction scores by 11.8%. The results indicate that integrating cross-modal emotional intelligence can significantly enhance adaptive, empathetic, and context-aware AIGC recommendation systems for next-generation user experiences. <div>
arXiv:2512.10963v1 Announce Type: cross 
Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.10966</link>
<guid>https://arxiv.org/abs/2512.10966</guid>
<content:encoded><![CDATA[
<div> Multimodal fusion, Alzheimer's disease, Mixture-of-Experts, neuroimaging, interpretability<br /><br />Summary:<br /><br />1. This study addresses the challenge of early and accurate diagnosis of Alzheimer's disease (AD) by integrating complementary information from multiple imaging modalities, such as amyloid PET and MRI, to better reflect clinical practice.<br />2. Traditional fusion methods typically concatenate features in a simplistic manner, lacking the ability to adaptively weight the importance of biomarkers across different brain regions.<br />3. The authors propose MREF-AD, a Multimodal Regional Expert Fusion model based on a Mixture-of-Experts (MoE) framework, where each meso-scale brain region in each modality acts as an independent expert.<br />4. The model uses two-level gating networks to learn subject-specific fusion weights, enabling personalized and adaptive integration of multimodal biomarker information.<br />5. Applied to data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD not only achieves state-of-the-art diagnostic performance compared to baseline models but also enhances interpretability by revealing how structural and molecular imaging modalities contribute regionally to the disease diagnosis.<br /><br />This work highlights MREF-AD as a powerful, interpretable framework for adaptive multimodal fusion in neuroimaging, with potential broader applications beyond AD diagnosis. <div>
arXiv:2512.10966v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems</title>
<link>https://arxiv.org/abs/2512.10975</link>
<guid>https://arxiv.org/abs/2512.10975</guid>
<content:encoded><![CDATA[
<div> Keywords: human-agent interaction, multimodal emotion recognition, multi-agent framework, modular integration, training efficiency

<br /><br />Summary:  
1. The paper addresses the challenge of accurate and adaptive perception of human emotional states in human-agent interaction (HAI).  
2. Traditional multimodal deep learning models use facial expressions, speech, and text cues to achieve high emotion recognition accuracy but require heavy computational resources and lack flexibility when modalities change.  
3. The authors propose a novel multi-agent framework where each modality encoder (vision, audio, text) and the fusion classifier act as autonomous agents coordinated by a central supervisor.  
4. This architecture supports easy modular integration of new modalities, such as audio features through emotion2vec, and allows seamless replacement of outdated components without retraining the entire system.  
5. The approach reduces computational overhead during training and improves training efficiency.  
6. A proof-of-concept implementation demonstrates support for vision, audio, and text modalities, with the classifier serving as a shared decision-making agent.  
7. The proposed framework enables more flexible, scalable, and maintainable perception modules for embodied and virtual agents, enhancing their capability in HAI scenarios. <div>
arXiv:2512.10975v1 Announce Type: cross 
Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.10978</link>
<guid>https://arxiv.org/abs/2512.10978</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, attention heads, interpretability, cognitive functions, reasoning tasks<br /><br />Summary:<br /><br />This paper addresses the challenge of understanding the internal mechanisms of large language models (LLMs), focusing on their reasoning abilities. The authors propose a novel interpretability framework inspired by the relationship between neural processes and human cognition to analyze attention heads, which are crucial components of LLMs. They introduce CogQA, a new dataset designed with a chain-of-thought format that breaks down complex questions into stepwise subquestions linked to specific cognitive functions such as retrieval and logical reasoning. Using a multi-class probing methodology, the study identifies and classifies attention heads according to distinct cognitive roles, termed cognitive heads. Their analysis across multiple LLM families finds these cognitive heads to be functionally specialized, exhibiting universal sparsity, diversity in number and distribution based on cognitive function, and interactive hierarchical organization. The investigation further demonstrates the importance of cognitive heads in reasoning tasks, showing that removing these heads impairs model performance while enhancing them improves reasoning accuracy. Overall, these findings deepen the understanding of how LLM reasoning emerges from attention mechanisms and highlight important implications for improved model design, training, and fine-tuning strategies aimed at boosting reasoning capabilities in LLMs. <div>
arXiv:2512.10978v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dataset that decomposes complex questions into step-by-step subquestions with a chain-of-thought design, each associated with specific cognitive functions such as retrieval or logical reasoning. By applying a multi-class probing method, we identify the attention heads responsible for these functions. Our analysis across multiple LLM families reveals that attention heads exhibit functional specialization, characterized as cognitive heads. These cognitive heads exhibit several key properties: they are universally sparse, vary in number and distribution across different cognitive functions, and display interactive and hierarchical structures. We further show that cognitive heads play a vital role in reasoning tasks - removing them leads to performance degradation, while augmenting them enhances reasoning accuracy. These insights offer a deeper understanding of LLM reasoning and suggest important implications for model design, training, and fine-tuning strategies.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling</title>
<link>https://arxiv.org/abs/2512.10980</link>
<guid>https://arxiv.org/abs/2512.10980</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU utilization, dynamic scheduling, multi-tenant clusters, AI workloads, fairness variance  

<br /><br />Summary:  
GPU clusters are critical for modern AI training and deployment but typically suffer from low utilization near 50% due to fragmentation, workload heterogeneity, and static scheduling limitations. This paper systematically evaluates these challenges and introduces three dynamic schedulers—Hybrid Priority Scheduler (HPS), Predictive Backfill Scheduler (PBS), and Smart Batch Scheduler (SBS)—aimed at enhancing utilization, fairness, and throughput in multi-tenant GPU clusters. Using simulations with 1,000 AI jobs across a 64-GPU, 8-node cluster representing diverse workloads (training, inference, research), static scheduling baselines (FIFO, SJF, Shortest, Shortest-GPU) only achieve 45–67% utilization and 12.5–18.3 jobs/hour, with significant job starvation and long wait times. The proposed dynamic schedulers considerably outperform these baselines: HPS delivers the highest utilization (78.2%), throughput (25.8 jobs/hour), and the lowest fairness variance, reducing starvation to just 12 jobs. PBS better mitigates fragmentation, reaching 76.1% utilization, while SBS optimizes efficiency for structurally similar jobs, achieving 74.6% utilization. Overall, multi-objective dynamic scheduling consistently exceeds single-objective heuristics across key metrics such as throughput, fairness, wait times, and starvation. These findings demonstrate that transparent, targeted scheduling strategies can substantially improve GPU cluster efficiency in heterogeneous AI workloads, offering a strong foundation for future production-grade scheduling frameworks. <div>
arXiv:2512.10980v1 Announce Type: cross 
Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developmental Symmetry-Loss: A Free-Energy Perspective on Brain-Inspired Invariance Learning</title>
<link>https://arxiv.org/abs/2512.10984</link>
<guid>https://arxiv.org/abs/2512.10984</guid>
<content:encoded><![CDATA[
<div> Symmetry-Loss, brain-inspired, invariance, equivariance, representation learning<br /><br />Summary: The paper introduces Symmetry-Loss, a novel brain-inspired algorithmic principle designed to enforce invariance and equivariance in representation learning through differentiable constraints derived from environmental symmetries. It models learning as an iterative refinement of an effective symmetry group, paralleling how cortical representations develop to align with the structural properties of the environment. By minimizing "structural surprise," defined as deviations from symmetry consistency, the approach effectively operationalizes a Free-Energy-like objective for learning representations. This framework bridges two major theoretical perspectives—predictive coding and group theory—demonstrating how efficient, stable, and compositional representations emerge naturally through symmetry-based self-organization. The method provides a general computational mechanism that links developmental learning processes in the brain with principled approaches to artificial representation learning. Overall, Symmetry-Loss offers a unifying model that ties together biological development and machine learning for improved understanding and design of representation learning systems. <div>
arXiv:2512.10984v1 Announce Type: cross 
Abstract: We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Marti-5: A Mathematical Model of "Self in the World" as a First Step Toward Self-Awareness</title>
<link>https://arxiv.org/abs/2512.10985</link>
<guid>https://arxiv.org/abs/2512.10985</guid>
<content:encoded><![CDATA[
<div> Keywords: neocortical columns, basal ganglia, self-awareness, reinforcement learning, Atari games  

<br /><br />Summary: The paper proposes a biologically inspired mathematical model based on the idea of separate 'what' and 'where' information processing pathways in the brain. This model incorporates neocortical columns regulated by the basal ganglia to perform predictions and action selection, where distinct columns serve as 'what' and 'where' processors. The core function of the model is to identify and separate the self from the environment, which then enables the construction and use of a self-model to enhance predictive capabilities. To demonstrate the model's practical application, the authors develop a reinforcement learning agent that learns purposeful behavior within a virtual environment. The agent is tested on Atari games Pong and Breakout, successfully learning to play these games. The results suggest that the ability to distinguish the self from the environment provides the agent with advantages. The authors argue this capability could have emerged through evolutionary processes in living organisms. Finally, they propose a conceptual Self-Awareness Principle 1, stating that the capacity to separate self from the world is necessary but not sufficient alone for full self-awareness. <div>
arXiv:2512.10985v1 Announce Type: cross 
Abstract: The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematics of natural intelligence</title>
<link>https://arxiv.org/abs/2512.10988</link>
<guid>https://arxiv.org/abs/2512.10988</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitome, neural hypernetwork, cognitive groups of neurons, brain modeling, consciousness  

<br /><br />Summary:  
1. The article explores the concept of the cognitome, introduced by K.V. Anokhin, which refers to the cognitive structure of the mind as a high-order neural hypernetwork of the brain.  
2. Within the cognitome, cognitive groups of neurons (COGs) are foundational elements, divided into functional systems and cellular ensembles, both crucial for brain function.  
3. Consciousness is described as a dynamic process within this hypernetwork, characterized by the large-scale integration of its cognitive components.  
4. The paper presents novel mathematical models capturing these brain structures and their cognitive processes, grounded in new mathematical results.  
5. It advances a general principle of brain function: the brain discovers all possible causal relationships in the external world and infers all possible conclusions from them.  
6. The models developed incorporate and provide mathematical foundations for several theoretical frameworks, including natural classification, P.K. Anokhin’s theory of functional brain systems, E. Roche’s prototypical categorization theory, Bob Rehter’s causal models, and G. Tononi’s integrated information theory of consciousness.  
7. This work aims to establish a fundamental mathematical theory describing the origin, structure, and function of the brain and mind’s cognitive architecture. <div>
arXiv:2512.10988v1 Announce Type: cross 
Abstract: In the process of evolution, the brain has achieved such perfection that artificial intelligence systems do not have and which needs its own mathematics. The concept of cognitome, introduced by the academician K.V. Anokhin, as the cognitive structure of the mind -- a high-order structure of the brain and a neural hypernetwork, is considered as the basis for modeling. Consciousness then is a special form of dynamics in this hypernetwork -- a large-scale integration of its cognitive elements. The cognitome, in turn, consists of interconnected COGs (cognitive groups of neurons) of two types -- functional systems and cellular ensembles. K.V. Anokhin sees the task of the fundamental theory of the brain and mind in describing these structures, their origin, functions and processes in them. The paper presents mathematical models of these structures based on new mathematical results, as well as models of different cognitive processes in terms of these models. In addition, it is shown that these models can be derived based on a fairly general principle of the brain works: \textit{the brain discovers all possible causal relationships in the external world and draws all possible conclusions from them}. Based on these results, the paper presents models of: ``natural" classification; theory of functional brain systems by P.K. Anokhin; prototypical theory of categorization by E. Roche; theory of causal models by Bob Rehter; theory of consciousness as integrated information by G. Tononi.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI</title>
<link>https://arxiv.org/abs/2512.10990</link>
<guid>https://arxiv.org/abs/2512.10990</guid>
<content:encoded><![CDATA[
<div> edge AI, QoE, hybrid parallelism, distributed training, resource efficiency<br /><br />Summary:<br /><br />1. The paper addresses the challenge of meeting user Quality of Experience (QoE) requirements, especially model inference latency, in edge AI applications where resource constraints are significant. 2. Modern AI models often exceed the computing capacity of single devices, requiring distributed execution across heterogeneous devices connected by variable, contention-prone networks. 3. Existing hybrid parallelism planners focusing on throughput or device utilization neglect QoE, causing inefficiencies such as increased energy consumption or QoE violations during runtime changes. 4. The authors present Dora, a framework that enables QoE-aware hybrid parallelism by jointly optimizing heterogeneous computation, network contention, and multidimensional QoE objectives. 5. Dora consists of three key components: a heterogeneity-aware model partitioner creating QoE-compliant model partition plans, a contention-aware network scheduler that maximizes overlap between computation and communication, and a runtime adapter that dynamically composes multiple plans to maintain efficiency and QoE. 6. Evaluations across varied edge scenarios, including smart homes, traffic analytics, and small edge clusters, demonstrate that Dora can speed up execution by 1.1 to 6.3 times while reducing energy consumption by 21% to 82%, all without compromising QoE under runtime dynamics. <div>
arXiv:2512.10990v1 Announce Type: cross 
Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax</title>
<link>https://arxiv.org/abs/2512.10991</link>
<guid>https://arxiv.org/abs/2512.10991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular generation, chemical syntax, diffusion model, foundation model, MolSculpt<br /><br />Summary: Generating accurate 3D molecular geometries is vital for applications in drug discovery and materials science. Existing methods often use 1D molecular representations like SELFIES to ensure chemical validity but do not fully utilize the complex chemical knowledge encoded in these 1D models, resulting in a disconnect between 1D molecule syntax and 3D geometry. To address this, the proposed MolSculpt framework integrates 1D chemical knowledge deeply into 3D molecular generation. MolSculpt combines a frozen 1D molecular foundation model with a 3D molecular diffusion model. It employs learnable queries to extract chemical information from the foundation model, and a trainable projector injects this information into the diffusion model’s conditioning space. This cross-modal conditioning is optimized end-to-end, enabling the model to "sculpt" 3D molecular geometries that reflect underlying 1D chemical syntax. Experimental results on GEOM-DRUGS and QM9 datasets demonstrate that MolSculpt achieves state-of-the-art performance in both de novo and conditional 3D molecule generation tasks. The model outperforms previous approaches in terms of 3D fidelity and stability, showing superior generation quality. The authors have made the code publicly available for further research and application development. <div>
arXiv:2512.10991v1 Announce Type: cross 
Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA</title>
<link>https://arxiv.org/abs/2512.10996</link>
<guid>https://arxiv.org/abs/2512.10996</guid>
<content:encoded><![CDATA[
<div> Keywords: MedBioRAG, biomedical question answering, retrieval-augmented generation, semantic search, large language models  

<br /><br />Summary:  
This paper presents MedBioRAG, a novel retrieval-augmented generation (RAG) model aimed at enhancing biomedical question-answering (QA) capabilities by integrating semantic and lexical search techniques along with document retrieval and supervised fine-tuning. The model is specifically designed to efficiently retrieve and rank relevant biomedical documents to generate precise, context-aware responses. MedBioRAG is evaluated on multiple tasks including text retrieval, close-ended QA, and long-form QA using well-known biomedical benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results show that MedBioRAG consistently outperforms previous state-of-the-art models as well as the GPT-4o base model across all these tasks. Key improvements include better NDCG (Normalized Discounted Cumulative Gain) and MRR (Mean Reciprocal Rank) scores in document retrieval, higher accuracy in close-ended QA, and superior ROUGE scores for long-form QA responses. The study underlines that combining semantic search for more effective retrieval with fine-tuning of large language models significantly advances the performance of biomedical QA systems, demonstrating the promise of this approach in specialized medical and biomedical domains. <div>
arXiv:2512.10996v1 Announce Type: cross 
Abstract: Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unambiguous Representations in Neural Networks: An Information-Theoretic Approach to Intentionality</title>
<link>https://arxiv.org/abs/2512.11000</link>
<guid>https://arxiv.org/abs/2512.11000</guid>
<content:encoded><![CDATA[
<div> representational ambiguity, neural networks, consciousness, information theory, MNIST<br /><br />Summary:<br /><br />This article explores the concept of representational ambiguity in the context of consciousness and neural representations through the lens of information theory. It begins by contrasting conventional representations—which require external decoders to convey meaning—with conscious experience, which inherently lacks ambiguity (e.g., perceiving a red square cannot be mistaken for a green square). To quantify this, the authors define representational ambiguity using the conditional entropy H(I|R), which measures the uncertainty of interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, the study demonstrates that the relational structure in network connectivity can encode representational content unambiguously. Specifically, networks trained with dropout showed perfect (100%) accuracy in identifying the output neuron class from connectivity patterns, whereas those trained with standard backpropagation yielded only 38% accuracy despite similar task performance. This indicates that representational ambiguity can exist independently of behavioral accuracy. Additionally, the study shows that spatial position information of input neurons can be decoded from connectivity with a high coefficient of determination (R² up to 0.844). Overall, these findings provide a quantitative framework for measuring representational ambiguity in neural systems and suggest that neural networks can display the low-ambiguity representations theorized as fundamental to conscious experience, although such representations alone may not be sufficient for consciousness. <div>
arXiv:2512.11000v1 Announce Type: cross 
Abstract: Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Memristor: Neuromorphic Computing Using Meminductor</title>
<link>https://arxiv.org/abs/2512.11002</link>
<guid>https://arxiv.org/abs/2512.11002</guid>
<content:encoded><![CDATA[
<div> Keywords: meminductor, memristor, neuromorphic computing, magnetic core, biological behavior

<br /><br />Summary:  
This paper introduces the concept of a meminductor, an inductor with memory, characterized by its inductance L(q) depending on the charge q flowing through the coil. The memory effect arises from the magnetization in a coil's magnetic core, which retains the history of current passage. Unlike memristors, meminductors uniquely influence the time constant in neuromorphic RLC circuits through their inductance and capacitance, offering novel capabilities for brain-inspired computing architectures, deep learning, and neuromorphic systems. Experimentally, the authors created a meminductor to reproduce biological behaviors observed in amoebae, such as memorizing, timing, and anticipating mechanisms, demonstrating practical applicability. The results suggest that beyond memristor-based computing, meminductors can form a new paradigm enhancing computing architectures by leveraging memory in inductors. This expands the foundational understanding of memory circuit elements and their diverse roles in emerging computational models. The research confirms that a meminductor is not merely theoretical but a physically realizable and experimentally verifiable component with potential applications in neuromorphic engineering and bio-inspired computing. <div>
arXiv:2512.11002v1 Announce Type: cross 
Abstract: Memristor (resistor with memory), inductor with memory (meminductor) and capacitor with memory (memcapacitor) have different roles to play in novel computing architectures. We found that a coil with a magnetic core is an inductor with memory (meminductor) in terms of its inductance L(q) being a function of the charge q. The history of the current passing through the coil is remembered by the magnetization inside the magnetic core. Such a meminductor can play a unique role (that cannot be played by a memristor) in neuromorphic computing, deep learning and brain inspired since the time constant of a neuromorphic RLC circuit is jointly determined by the inductance and capacitance, rather than the resistance. As an experimental verification, this newly invented meminductor was used to reproduce the observed biological behaviour of amoebae (the memorizing, timing and anticipating mechanisms). In conclusion, a beyond memristor computing paradigm is theoretically sensible and experimentally practical.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification</title>
<link>https://arxiv.org/abs/2512.11015</link>
<guid>https://arxiv.org/abs/2512.11015</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, gender classification, image-text fusion, semantic guidance, bias mitigation<br /><br />Summary:<br /><br />1. The paper addresses fairness in facial image-based gender classification by introducing text-guided methodologies to enhance model performance and reduce bias.<br />2. Two main strategies are proposed: Image Text Matching (ITM) guidance, which helps the model learn fine-grained image-text alignments for better multimodal representations, and Image Text Fusion, which integrates both image and text modalities into richer representations to improve fairness.<br />3. Extensive experiments on benchmark datasets show that these approaches effectively mitigate demographic biases and improve accuracy across different gender and racial groups compared to existing methods.<br />4. The integration of textual guidance provides an interpretable and intuitive training paradigm, helping computer vision models leverage semantic information to reduce disparities.<br />5. Importantly, the proposed methods do not rely on demographic labels and are application-agnostic, making them broadly applicable for equitable facial analysis algorithms while improving generalization without demographic supervision. <div>
arXiv:2512.11015v1 Announce Type: cross 
Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoccerMaster: A Vision Foundation Model for Soccer Understanding</title>
<link>https://arxiv.org/abs/2512.11016</link>
<guid>https://arxiv.org/abs/2512.11016</guid>
<content:encoded><![CDATA[
<div> Soccer understanding, vision foundation model, multi-task pretraining, data curation, SoccerFactory  

<br /><br />Summary:  
This paper addresses the challenge of soccer visual understanding by proposing a unified model, SoccerMaster, capable of handling a range of tasks from fine-grained perception like athlete detection to higher-level semantic reasoning such as event classification. The authors introduce SoccerMaster as the first soccer-specific vision foundation model developed through supervised multi-task pretraining, enabling it to manage diverse understanding tasks within a single framework efficiently. To support this, they create an automated data curation pipeline that generates scalable spatial annotations and integrates these with existing soccer video datasets, resulting in SoccerFactory, a comprehensive and large-scale pretraining data resource tailored for soccer analysis. The study performs extensive evaluations comparing SoccerMaster with various task-specific expert models on multiple downstream tasks. Results demonstrate that SoccerMaster not only matches but consistently outperforms these expert models, emphasizing both its broad applicability and superior performance in the soccer domain. The authors also plan to publicly release the data, code, and model to facilitate further research and development in soccer visual understanding. <div>
arXiv:2512.11016v1 Announce Type: cross 
Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</title>
<link>https://arxiv.org/abs/2512.11047</link>
<guid>https://arxiv.org/abs/2512.11047</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid robots, loco-manipulation, Vision-Language-Action, reinforcement learning, WholeBodyVLA

<br /><br />Summary:  
This paper addresses the challenge of enabling humanoid robots to perform large-space loco-manipulation tasks by combining precise locomotion with dexterous manipulation. First, the authors identify two core problems: the scarcity of humanoid teleoperation data that limits the acquisition of loco-manipulation knowledge, and the lack of precision and stability in existing reinforcement learning (RL) controllers for faithfully executing locomotion commands. To overcome these challenges, the paper proposes a unified latent learning framework that allows a Vision-Language-Action (VLA) system to learn from low-cost, action-free egocentric videos, significantly enriching loco-manipulation knowledge. Additionally, an efficient human data collection pipeline is introduced to augment training datasets and scale these benefits. To improve execution precision, the authors design a loco-manipulation-oriented (LMO) RL policy tailored for stable and accurate core movements like advancing, turning, and squatting. Integrating these innovations, the framework WholeBodyVLA emerges as a pioneering solution for large-space humanoid loco-manipulation. The approach is experimentally validated on the AgiBot X2 humanoid robot, outperforming prior baselines by 21.3%, while demonstrating strong generalization and extensibility across various tasks. <div>
arXiv:2512.11047v1 Announce Type: cross 
Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2512.11067</link>
<guid>https://arxiv.org/abs/2512.11067</guid>
<content:encoded><![CDATA[
<div> Multimodal databases, SQL optimization, foundation models, explainability, human-AI interaction<br /><br />Summary:<br /><br />1. Traditional Database Management Systems (DBMSs) handle structured relational data and execute SQL queries with strong semantic guarantees and advanced optimizations but struggle with complexity and are limited to structured tables.<br /><br />2. Existing multimodal systems that operate on various data types such as text, images, and videos either require users to manually create machine learning User-Defined Functions (UDFs) embedded in SQL or rely entirely on black-box large language models (LLMs), resulting in trade-offs between usability and explainability.<br /><br />3. KathDB is introduced as a novel system that integrates traditional relational database semantics with the reasoning capabilities of foundation models, enabling querying over multimodal data seamlessly.<br /><br />4. The system incorporates interactive human-AI communication channels throughout the query lifecycle — during parsing, execution, and in explaining results — to allow users to iteratively refine queries and obtain transparent, explainable answers across different data modalities.<br /><br />5. KathDB thus advances multimodal data querying by bridging the gap between relational databases and foundation models, enhancing usability, interpretability, and user control compared to existing approaches. <div>
arXiv:2512.11067v1 Announce Type: cross 
Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data</title>
<link>https://arxiv.org/abs/2512.11074</link>
<guid>https://arxiv.org/abs/2512.11074</guid>
<content:encoded><![CDATA[
<div> Multi30k, multimodal machine translation, dataset extension, NLLB200-3.3B, language diversity<br /><br />Summary:<br /><br />This paper addresses the limitation of the original Multi30k dataset, which is constrained to four European languages (Czech, English, French, German) in Latin scripts, hindering multimodal machine translation (MMT) research across diverse languages and scripts. To broaden the linguistic scope, the authors present MultiScript30k, an extension translating the English Multi30k dataset into five additional global languages with varied scripts: Arabic (Ar), Spanish (Es), Ukrainian (Uk), Simplified Chinese (Zh_Hans), and Traditional Chinese (Zh_Hant). Translations were generated using the NLLB200-3.3B model, resulting in over 30,000 parallel sentences per language. The quality of these translations is assessed via similarity analyses, showing high cosine similarity (above 0.8) and low symmetric KL divergence (below 0.000251) for all languages except Traditional Chinese, which performs comparably to previous extensions. COMETKiwi evaluation metrics indicate mixed results; MultiScript30k's Arabic translations are on par with the existing ArEnMulti30k, while its Ukrainian translations score 6.4% lower than the previous Multi30k-Uk dataset. Overall, MultiScript30k represents a significant step toward expanding MMT research beyond European languages and scripts, enabling more inclusive multilingual modeling and evaluation. <div>
arXiv:2512.11074v1 Announce Type: cross 
Abstract: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \(30000\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\_Hans and Zh\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \(0.8\) cosine similarity and symmetric KL divergence less than \(0.000251\) for all languages supported except Zh\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\%$ greater than MultiScript30k-Uk per split.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast, accurate measurement of the worker populations of honey bee colonies using deep learning</title>
<link>https://arxiv.org/abs/2512.11075</link>
<guid>https://arxiv.org/abs/2512.11075</guid>
<content:encoded><![CDATA[
<div> Keywords: Honey bees, hive population counting, deep learning, CSRNet, ASUBEE dataset<br /><br />Summary:<br /><br />1. Honey bees are vital for pollination and global agriculture, making accurate hive population estimation crucial for ecological and agricultural studies. <br />2. Traditional bee counting methods are labor-intensive, slow, and prone to errors, especially in large-scale and complex hive environments. <br />3. This paper introduces a deep learning approach using CSRNet for automated bee population counting, leveraging density map estimation to handle challenges like occlusion and bee overlap. <br />4. The authors present ASUBEE, the first high-resolution dataset specifically curated for bee counting tasks in hive monitoring. <br />5. CSRNet demonstrates superior accuracy and efficiency, processing each image in just one second, making it highly effective for complex and dense hive scenarios. <br />6. The proposed method significantly enhances the efficiency of hive population assessments, offering a scalable solution beneficial to both researchers and beekeepers. <br />7. Overall, this work represents an important advancement in ecological research by applying AI to monitor honey bee populations precisely and efficiently, contributing valuable tools for environmental and agricultural management. <div>
arXiv:2512.11075v1 Announce Type: cross 
Abstract: Honey bees play a crucial role in pollination, contributing significantly to global agriculture and ecosystems. Accurately estimating hive populations is essential for understanding the effects of environmental factors on bee colonies, yet traditional methods of counting bees are time-consuming, labor-intensive, and prone to human error, particularly in large-scale studies. In this paper, we present a deep learning-based solution for automating bee population counting using CSRNet and introduce ASUBEE, the FIRST high-resolution dataset specifically designed for this task. Our method employs density map estimation to predict bee populations, effectively addressing challenges such as occlusion and overlapping bees that are common in hive monitoring. We demonstrate that CSRNet achieves superior performance in terms of time efficiency, with a computation time of just 1 second per image, while delivering accurate counts even in complex and densely populated hive scenarios. Our findings show that deep learning approaches like CSRNet can dramatically enhance the efficiency of hive population assessments, providing a valuable tool for researchers and beekeepers alike. This work marks a significant advancement in applying AI technologies to ecological research, offering scalable and precise monitoring solutions for honey bee populations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A probabilistic foundation model for crystal structure denoising, phase classification, and order parameters</title>
<link>https://arxiv.org/abs/2512.11077</link>
<guid>https://arxiv.org/abs/2512.11077</guid>
<content:encoded><![CDATA[
<div> Keywords: atomistic simulations, phase classification, order parameters, foundation model, denoising  

<br /><br />Summary:  
This paper addresses the challenge of extracting phase labels, order parameters (OPs), and defect information from noisy atomistic simulation data in a universal, robust, and interpretable way. Traditional tools like PTM and CNA are limited to a few specific lattice types (FCC, BCC, HCP), struggle with high thermal disorder or defects, and only provide hard labels without confidence scores. To overcome these limitations, the authors propose a novel log-probability foundation model that unifies denoising, phase classification, and OP extraction within a single probabilistic framework. This model builds on the MACE-MP interatomic potential, trained on crystal structures mapped to the AFLOW prototype database. It predicts per-atom, per-phase logits, which aggregate to a global log-density, whose gradient defines a conservative score field enabling data denoising via gradient ascent. Phase labels are obtained from the maximum logit values per atom, while the logits themselves serve as continuous, interpretable OPs sensitive to defects and indicating Euclidean distances to ideal phases. The model demonstrates universality over hundreds of crystal prototypes, robustness against strong thermal and defect-induced noise, and accurately captures complex systems such as ice polymorphs, ice-water interfaces, and shock-compressed titanium, advancing structural analysis in atomistic simulations. <div>
arXiv:2512.11077v1 Announce Type: cross 
Abstract: Atomistic simulations generate large volumes of noisy structural data, but extracting phase labels, order parameters (OPs), and defect information in a way that is universal, robust, and interpretable remains challenging. Existing tools such as PTM and CNA are restricted to a small set of hand-crafted lattices (e.g.\ FCC/BCC/HCP), degrade under strong thermal disorder or defects, and produce hard, template-based labels without per-atom probability or confidence scores. Here we introduce a log-probability foundation model that unifies denoising, phase classification, and OP extraction within a single probabilistic framework. We reuse the MACE-MP foundation interatomic potential on crystal structures mapped to AFLOW prototypes, training it to predict per-atom, per-phase logits $l$ and to aggregate them into a global log-density $\log \hat{P}_\theta(\boldsymbol{r})$ whose gradient defines a conservative score field. Denoising corresponds to gradient ascent on this learned log-density, phase labels follow from $\arg\max_c l_{ac}$, and the $l$ values act as continuous, defect-sensitive and interpretable OPs quantifying the Euclidean distance to ideal phases. We demonstrate universality across hundreds of prototypes, robustness under strong thermal and defect-induced disorder, and accurate treatment of complex systems such as ice polymorphs, ice--water interfaces, and shock-compressed Ti.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification</title>
<link>https://arxiv.org/abs/2512.11087</link>
<guid>https://arxiv.org/abs/2512.11087</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network verification, branch-and-bound, linear constraints, bound tightening, Clip-and-Verify

<br /><br />Summary: This paper introduces a novel framework called linear constraint-driven clipping to enhance neural network (NN) verifiers, particularly those using branch-and-bound (BaB) methods. The framework develops two new algorithms that utilize linear constraints to (1) reduce areas of the input space that are either already verified or irrelevant during BaB subproblems, and (2) improve the intermediate bounds propagated throughout the NN. The approach leverages linear constraints commonly derived from bound propagation but remains flexible to incorporate constraints from other sources as well. To handle these constraints efficiently, the method employs a specialized GPU-based procedure, avoiding expensive external solvers and enabling scalability to large networks. Their verification procedure, named Clip-and-Verify, consistently produces tighter bounds across various benchmarks and significantly reduces the number of subproblems tackled during BaB—achieving up to a 96% reduction in some cases. Clip-and-Verify seamlessly integrates with BaB-based verifiers such as the $\alpha,\beta$-CROWN framework, utilizing constraints either in activation-space splits or output-space unverified inputs. Experimental results demonstrate state-of-the-art verified accuracy on multiple benchmarks. Clip-and-Verify is publicly available as part of the $\alpha,\beta$-CROWN verifier, which won the VNN-COMP 2025 competition. The code can be accessed at https://github.com/Verified-Intelligence/Clip_and_Verify. <div>
arXiv:2512.11087v1 Announce Type: cross 
Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $\alpha,\beta$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $\alpha,\beta$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution</title>
<link>https://arxiv.org/abs/2512.11108</link>
<guid>https://arxiv.org/abs/2512.11108</guid>
<content:encoded><![CDATA[
<div> Keywords: feature attribution, bias evaluation, Integrated Gradient, transformers, explanation methods<br /><br />Summary: This paper addresses the inconsistencies observed in feature attribution explanations provided by different post-hoc methods like Integrated Gradient, which offer token-level insights for language models. It highlights the problem that varying biases inherent to different attribution methods lead to inconsistent explanations for the same input, causing user mistrust or misplaced trust. The authors propose a model- and method-agnostic framework based on three evaluation metrics to systematically analyze these biases. Their analysis focuses on two main types of biases: lexical bias (the "what" in the input) and position bias (the "where" in the input). The study compares these biases across two transformer models, examining both a controlled pseudo-random classification task using artificial data and a semi-controlled causal relation detection task using natural data. Results reveal a structural imbalance between lexical and position biases among models; models scoring high on lexical bias typically score low on position bias, and vice versa. Additionally, attribution methods that yield anomalous or unexpected explanations tend to exhibit higher levels of bias themselves. This work offers deeper insights into the nature of attribution method biases and guides better interpretation and trust calibration for language model explanations. <div>
arXiv:2512.11108v1 Announce Type: cross 
Abstract: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIBER: A Multilingual Evaluation Resource for Factual Inference Bias</title>
<link>https://arxiv.org/abs/2512.11110</link>
<guid>https://arxiv.org/abs/2512.11110</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, factual knowledge, multilingual benchmark, inference bias, multi-entity questions<br /><br />Summary: This paper introduces FIBER, a multilingual benchmark designed to evaluate the factual knowledge of large language models across single- and multi-entity settings. Unlike previous benchmarks focusing mainly on single-entity facts and monolingual data, FIBER includes sentence completion, question-answering, and object-count prediction tasks presented in English, Italian, and Turkish. The study investigates whether the language of the prompt influences inference bias in entity selection, revealing that prompt language indeed affects model outputs, especially for entities related to the country of that language. This inference bias exhibits topic variation, with 31% of topics showing a factual inference bias score above 0.5. Furthermore, the level of bias is language-dependent, with Turkish prompts exhibiting higher bias in 83% of topics compared to Italian prompts. The models also struggle more with multi-entity questions than single-entity ones, indicating increased complexity in multi-entity reasoning. Performance varies notably by language and model size; English prompts achieve the highest mean average precision, while Italian and Turkish perform lower. Larger models such as Llama-3.1-8B and Qwen-2.5-7B consistently outperform smaller models in the 3B-4B parameter range, confirming that model scale positively correlates with factual knowledge accuracy across languages and tasks. <div>
arXiv:2512.11110v1 Announce Type: cross 
Abstract: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2512.11114</link>
<guid>https://arxiv.org/abs/2512.11114</guid>
<content:encoded><![CDATA[
<div> Multi-objective optimization, Bayesian optimization, transformer, reinforcement learning, Pareto frontier<br /><br />Summary:<br /><br />1. The paper addresses the challenge of balancing competing objectives in expensive, black-box problems common in fields like drug design and autonomous systems. <br />2. It critiques existing multi-objective Bayesian optimization methods noting their limitations such as the need for problem-specific surrogate models and acquisition functions, myopic single-step planning, and overhead from frequent refitting, especially in parallel or time-sensitive scenarios.<br />3. The authors introduce TAMO, a transformer-based, fully amortized policy for multi-objective black-box optimization that works across varying input and objective dimensions.<br />4. TAMO is pretrained using reinforcement learning to maximize cumulative hypervolume improvement over entire optimization trajectories while conditioning on full query histories to approximate the Pareto frontier.<br />5. Unlike traditional methods, TAMO allows for fast proposal generation of new designs with a single forward pass, reducing proposal time by 50–1000x without retraining.<br />6. Experimental results on synthetic benchmarks and real tasks show that TAMO matches or improves the quality of Pareto fronts within tight evaluation budgets.<br />7. This work demonstrates that transformers can carry out multi-objective optimization entirely in-context, eliminating the need for surrogate fitting and acquisition engineering per task.<br />8. The approach paves the way for foundation-style, plug-and-play optimizers aimed at enhancing scientific discovery workflows. <div>
arXiv:2512.11114v1 Announce Type: cross 
Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Regularized Online Optimization with Switching Costs</title>
<link>https://arxiv.org/abs/2512.11131</link>
<guid>https://arxiv.org/abs/2512.11131</guid>
<content:encoded><![CDATA[
<div> Fairness, Online Convex Optimization, Switching Costs, Competitive Ratio, Resource Provisioning  

<br /><br />Summary:  
This paper addresses the simultaneous consideration of fairness and action smoothness in online convex optimization, particularly under switching costs. It highlights the fundamental challenge posed by the long-term fairness regularizer, showing that no online algorithm can achieve sublinear regret or a finite competitive ratio compared to the offline optimum as the time horizon \( T \) grows, even without switching costs. To overcome this, the authors propose FairOBD (Fairness-regularized Online Balanced Descent), an algorithm that balances hitting cost, switching cost, and fairness cost by decomposing the long-term fairness term into a sequence of online costs through an auxiliary variable. This auxiliary variable helps regularize the action sequence to produce fair outcomes. They analytically prove that FairOBD attains a worst-case asymptotic competitive ratio against a new benchmark: the optimal offline algorithm with parameterized constraints, in the limit as \( T \to \infty \). Finally, the paper includes trace-driven experiments focusing on dynamic computing resource provisioning for socially responsible AI inference, demonstrating that FairOBD effectively reduces the overall fairness-regularized cost and achieves fairer outcomes compared to existing baselines. <div>
arXiv:2512.11131v1 Announce Type: cross 
Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles</title>
<link>https://arxiv.org/abs/2512.11145</link>
<guid>https://arxiv.org/abs/2512.11145</guid>
<content:encoded><![CDATA[
<div> Keywords: autoencoder, clustering loss, contrastive loss, scientific ensemble datasets, dimensionality reduction<br /><br />Summary:<br /><br />This paper addresses the challenge of analyzing and visualizing high-dimensional and complex scientific ensemble datasets by enhancing autoencoder frameworks. The proposed method integrates a clustering loss based on the soft silhouette score alongside a contrastive loss to improve feature extraction, visualization, and interpretability. EfficientNetV2 is employed to generate pseudo-labels for unlabeled data segments, facilitating supervised guidance within the framework. The approach jointly optimizes reconstruction, clustering, and contrastive objectives to ensure similar data points cluster closely while distinct clusters remain separated in the latent space. Following latent space learning, UMAP is used to create 2D projections, which are evaluated using the silhouette score to assess clustering quality. The study compares multiple autoencoder variants to determine which better capture meaningful features. Experimental validation is conducted using two scientific ensemble datasets: channel structures in soil generated by Markov chain Monte Carlo simulations, and droplet-on-film impact dynamics. Results demonstrate that models incorporating either clustering or contrastive loss slightly outperform baseline autoencoder models, indicating the effectiveness of these additional losses in improving representation learning for complex high-dimensional scientific data. <div>
arXiv:2512.11145v1 Announce Type: cross 
Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniScope: A Least Privilege Framework for Authorizing Tool Calling Agents</title>
<link>https://arxiv.org/abs/2512.11147</link>
<guid>https://arxiv.org/abs/2512.11147</guid>
<content:encoded><![CDATA[
<div> Tool calling agents, LLM security, least privilege, permission hierarchies, MiniScope<br /><br />Summary:<br /><br />1. Tool calling agents represent an emerging paradigm in deploying large language models (LLMs) with platforms like ChatGPT, Claude, and Gemini integrating connectors and autonomous functions. 2. Despite their utility, the inherent unreliability of LLMs poses fundamental security risks when these agents interact with sensitive user services. 3. Existing solutions either depend on manually crafted security policies, which require expert knowledge, or use confinement loops that do not provide rigorous security guarantees. 4. MiniScope is introduced as a novel framework that enables tool calling agents to safely operate on user accounts by rigorously enforcing least privilege principles, thereby limiting potential damage from unreliable LLMs. 5. The framework reconstructs permission hierarchies based on relationships between tool calls and applies a mobile-style permission model to optimize the balance between security and usability. 6. To validate MiniScope, a synthetic dataset simulating complex agentic tasks across ten popular applications was created, reflecting realistic operational demands beyond existing benchmarks. 7. Experimental results demonstrate that MiniScope introduces only a minimal latency overhead of 1-6% compared to standard tool calling agents while markedly reducing permissions and cutting computational and operational costs relative to LLM-based baseline approaches. <div>
arXiv:2512.11147v1 Announce Type: cross 
Abstract: Tool calling agents are an emerging paradigm in LLM deployment, with major platforms such as ChatGPT, Claude, and Gemini adding connectors and autonomous capabilities. However, the inherent unreliability of LLMs introduces fundamental security risks when these agents operate over sensitive user services. Prior approaches either rely on manually written policies that require security expertise, or place LLMs in the confinement loop, which lacks rigorous security guarantees. We present MiniScope, a framework that enables tool calling agents to operate on user accounts while confining potential damage from unreliable LLMs. MiniScope introduces a novel way to automatically and rigorously enforce least privilege principles by reconstructing permission hierarchies that reflect relationships among tool calls and combining them with a mobile-style permission model to balance security and ease of use. To evaluate MiniScope, we create a synthetic dataset derived from ten popular real-world applications, capturing the complexity of realistic agentic tasks beyond existing simplified benchmarks. Our evaluation shows that MiniScope incurs only 1-6% latency overhead compared to vanilla tool calling agents, while significantly outperforming the LLM based baseline in minimizing permissions as well as computational and operational costs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context</title>
<link>https://arxiv.org/abs/2512.11167</link>
<guid>https://arxiv.org/abs/2512.11167</guid>
<content:encoded><![CDATA[
<div> Keywords: reproducibility, Monkey Vision-Language Model, image tiling, high-resolution image understanding, global context<br /><br />Summary:<br /><br />1. This work focuses on the reproducibility of the Monkey Vision-Language Model (VLM) introduced by Li et al. (2023b) at CVPR24, which employs image tiling for detailed, high-resolution image understanding. <br /><br />2. The original model's approach involves splitting large images into smaller tiles to capture fine-grained visual information while maintaining computational efficiency. <br /><br />3. The authors replicated the Monkey VLM training pipeline using publicly available checkpoints and reimplemented the full training procedure to verify the findings. <br /><br />4. Their reproduction confirmed that image tiling indeed helps recover local visual details effectively, supporting the original paper’s claims. <br /><br />5. Beyond replication, the study explored the impact of adding global context to tiled images, finding that the benefits and deviations in performance vary significantly depending on the specific task and the granularity of the tiles used. This highlights important practical considerations for future high-resolution multimodal vision-language model development. <div>
arXiv:2512.11167v1 Announce Type: cross 
Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast EXP3 Algorithms</title>
<link>https://arxiv.org/abs/2512.11201</link>
<guid>https://arxiv.org/abs/2512.11201</guid>
<content:encoded><![CDATA[
<div> EXP3, constant time, regret bounds, time complexity, practical algorithms<br /><br />Summary:<br /><br />1. The paper demonstrates that the EXP3 algorithm, commonly used in adversarial bandit problems, can be implemented with a constant time complexity per round, improving computational efficiency. 2. It introduces new algorithms inspired by EXP3 that are more practical for real-world applications, focusing on easier implementation without compromising performance. 3. The authors conduct a detailed analysis of the trade-offs between regret bounds (a measure of algorithm performance) and the computational time complexity of the proposed algorithms. 4. Their results highlight how reducing time complexity can impact the regret bounds and vice versa, providing valuable insights into designing efficient bandit algorithms. 5. Overall, the paper contributes to both theoretical understanding and practical implementation strategies for adversarial bandit algorithms by balancing regret performance and execution speed. <div>
arXiv:2512.11201v1 Announce Type: cross 
Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>amc: The Automated Mission Classifier for Telescope Bibliographies</title>
<link>https://arxiv.org/abs/2512.11202</link>
<guid>https://arxiv.org/abs/2512.11202</guid>
<content:encoded><![CDATA[
arXiv:2512.11202v1 Announce Type: cross 
Abstract: Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2512.11221</link>
<guid>https://arxiv.org/abs/2512.11221</guid>
<content:encoded><![CDATA[
arXiv:2512.11221v1 Announce Type: cross 
Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFMF: World Modeling by Forecasting Vision Foundation Model Features</title>
<link>https://arxiv.org/abs/2512.11225</link>
<guid>https://arxiv.org/abs/2512.11225</guid>
<content:encoded><![CDATA[
arXiv:2512.11225v1 Announce Type: cross 
Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Generalisation of the Implicit Dynamics of In-Context Learning</title>
<link>https://arxiv.org/abs/2512.11255</link>
<guid>https://arxiv.org/abs/2512.11255</guid>
<content:encoded><![CDATA[
arXiv:2512.11255v1 Announce Type: cross 
Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges</title>
<link>https://arxiv.org/abs/2512.11258</link>
<guid>https://arxiv.org/abs/2512.11258</guid>
<content:encoded><![CDATA[
arXiv:2512.11258v1 Announce Type: cross 
Abstract: Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scalable Multi-GPU Framework for Encrypted Large-Model Inference</title>
<link>https://arxiv.org/abs/2512.11269</link>
<guid>https://arxiv.org/abs/2512.11269</guid>
<content:encoded><![CDATA[
arXiv:2512.11269v1 Announce Type: cross 
Abstract: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning</title>
<link>https://arxiv.org/abs/2512.11276</link>
<guid>https://arxiv.org/abs/2512.11276</guid>
<content:encoded><![CDATA[
arXiv:2512.11276v1 Announce Type: cross 
Abstract: Serious illness can deprive patients of the capacity to speak for themselves. As populations age and caregiver networks shrink, the need for reliable support in Advance Care Planning (ACP) grows. To probe this fraught design space of using proxy agents for high-risk, high-subjectivity decisions, we built an experience prototype (\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal proxy in ACP decisions. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings argue for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We conclude with design recommendations to balance the risks and benefits of such an agent.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise</title>
<link>https://arxiv.org/abs/2512.11282</link>
<guid>https://arxiv.org/abs/2512.11282</guid>
<content:encoded><![CDATA[
arXiv:2512.11282v1 Announce Type: cross 
Abstract: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the ${\alpha}$-Coefficient</title>
<link>https://arxiv.org/abs/2512.11295</link>
<guid>https://arxiv.org/abs/2512.11295</guid>
<content:encoded><![CDATA[
arXiv:2512.11295v1 Announce Type: cross 
Abstract: The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title>
<link>https://arxiv.org/abs/2512.11296</link>
<guid>https://arxiv.org/abs/2512.11296</guid>
<content:encoded><![CDATA[
arXiv:2512.11296v1 Announce Type: cross 
Abstract: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condensation-Concatenation Framework for Dynamic Graph Continual Learning</title>
<link>https://arxiv.org/abs/2512.11317</link>
<guid>https://arxiv.org/abs/2512.11317</guid>
<content:encoded><![CDATA[
arXiv:2512.11317v1 Announce Type: cross 
Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLLM Machine Unlearning via Visual Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11325</link>
<guid>https://arxiv.org/abs/2512.11325</guid>
<content:encoded><![CDATA[
arXiv:2512.11325v1 Announce Type: cross 
Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.11350</link>
<guid>https://arxiv.org/abs/2512.11350</guid>
<content:encoded><![CDATA[
arXiv:2512.11350v1 Announce Type: cross 
Abstract: Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMODEL-LLM: Transforming C code to Java using LLMs</title>
<link>https://arxiv.org/abs/2512.11402</link>
<guid>https://arxiv.org/abs/2512.11402</guid>
<content:encoded><![CDATA[
arXiv:2512.11402v1 Announce Type: cross 
Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models</title>
<link>https://arxiv.org/abs/2512.11412</link>
<guid>https://arxiv.org/abs/2512.11412</guid>
<content:encoded><![CDATA[
arXiv:2512.11412v1 Announce Type: cross 
Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowception: Temporally Expansive Flow Matching for Video Generation</title>
<link>https://arxiv.org/abs/2512.11438</link>
<guid>https://arxiv.org/abs/2512.11438</guid>
<content:encoded><![CDATA[
arXiv:2512.11438v1 Announce Type: cross 
Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.11458</link>
<guid>https://arxiv.org/abs/2512.11458</guid>
<content:encoded><![CDATA[
arXiv:2512.11458v1 Announce Type: cross 
Abstract: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring MLLM-Diffusion Information Transfer with MetaCanvas</title>
<link>https://arxiv.org/abs/2512.11464</link>
<guid>https://arxiv.org/abs/2512.11464</guid>
<content:encoded><![CDATA[
arXiv:2512.11464v1 Announce Type: cross 
Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models</title>
<link>https://arxiv.org/abs/2512.11482</link>
<guid>https://arxiv.org/abs/2512.11482</guid>
<content:encoded><![CDATA[
arXiv:2512.11482v1 Announce Type: cross 
Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</title>
<link>https://arxiv.org/abs/2512.11509</link>
<guid>https://arxiv.org/abs/2512.11509</guid>
<content:encoded><![CDATA[
arXiv:2512.11509v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics</title>
<link>https://arxiv.org/abs/2512.11525</link>
<guid>https://arxiv.org/abs/2512.11525</guid>
<content:encoded><![CDATA[
arXiv:2512.11525v1 Announce Type: cross 
Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Time Series Forecasting with Anomalies</title>
<link>https://arxiv.org/abs/2512.11526</link>
<guid>https://arxiv.org/abs/2512.11526</guid>
<content:encoded><![CDATA[
arXiv:2512.11526v1 Announce Type: cross 
Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</title>
<link>https://arxiv.org/abs/2512.11532</link>
<guid>https://arxiv.org/abs/2512.11532</guid>
<content:encoded><![CDATA[
arXiv:2512.11532v1 Announce Type: cross 
Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Embedding with Mel-spectrograms for Underwater Acoustic Target Recognition</title>
<link>https://arxiv.org/abs/2512.11545</link>
<guid>https://arxiv.org/abs/2512.11545</guid>
<content:encoded><![CDATA[
arXiv:2512.11545v1 Announce Type: cross 
Abstract: Underwater acoustic target recognition (UATR) is extremely challenging due to the complexity of ship-radiated noise and the variability of ocean environments. Although deep learning (DL) approaches have achieved promising results, most existing models implicitly assume that underwater acoustic data lie in a Euclidean space. This assumption, however, is unsuitable for the inherently complex topology of underwater acoustic signals, which exhibit non-stationary, non-Gaussian, and nonlinear characteristics. To overcome this limitation, this paper proposes the UATR-GTransformer, a non-Euclidean DL model that integrates Transformer architectures with graph neural networks (GNNs). The model comprises three key components: a Mel patchify block, a GTransformer block, and a classification head. The Mel patchify block partitions the Mel-spectrogram into overlapping patches, while the GTransformer block employs a Transformer Encoder to capture mutual information between split patches to generate Mel-graph embeddings. Subsequently, a GNN enhances these embeddings by modeling local neighborhood relationships, and a feed-forward network (FFN) further performs feature transformation. Experiments results based on two widely used benchmark datasets demonstrate that the UATR-GTransformer achieves performance competitive with state-of-the-art methods. In addition, interpretability analysis reveals that the proposed model effectively extracts rich frequency-domain information, highlighting its potential for applications in ocean engineering.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.11546</link>
<guid>https://arxiv.org/abs/2512.11546</guid>
<content:encoded><![CDATA[
arXiv:2512.11546v1 Announce Type: cross 
Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</title>
<link>https://arxiv.org/abs/2512.11558</link>
<guid>https://arxiv.org/abs/2512.11558</guid>
<content:encoded><![CDATA[
arXiv:2512.11558v1 Announce Type: cross 
Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-temporal Calving Front Segmentation</title>
<link>https://arxiv.org/abs/2512.11560</link>
<guid>https://arxiv.org/abs/2512.11560</guid>
<content:encoded><![CDATA[
arXiv:2512.11560v1 Announce Type: cross 
Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents</title>
<link>https://arxiv.org/abs/2512.11584</link>
<guid>https://arxiv.org/abs/2512.11584</guid>
<content:encoded><![CDATA[
arXiv:2512.11584v1 Announce Type: cross 
Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</title>
<link>https://arxiv.org/abs/2512.11614</link>
<guid>https://arxiv.org/abs/2512.11614</guid>
<content:encoded><![CDATA[
arXiv:2512.11614v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling</title>
<link>https://arxiv.org/abs/2512.11635</link>
<guid>https://arxiv.org/abs/2512.11635</guid>
<content:encoded><![CDATA[
arXiv:2512.11635v1 Announce Type: cross 
Abstract: Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews</title>
<link>https://arxiv.org/abs/2512.11661</link>
<guid>https://arxiv.org/abs/2512.11661</guid>
<content:encoded><![CDATA[
arXiv:2512.11661v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines</title>
<link>https://arxiv.org/abs/2512.11724</link>
<guid>https://arxiv.org/abs/2512.11724</guid>
<content:encoded><![CDATA[
arXiv:2512.11724v1 Announce Type: cross 
Abstract: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.11743</link>
<guid>https://arxiv.org/abs/2512.11743</guid>
<content:encoded><![CDATA[
arXiv:2512.11743v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation</title>
<link>https://arxiv.org/abs/2512.11748</link>
<guid>https://arxiv.org/abs/2512.11748</guid>
<content:encoded><![CDATA[
arXiv:2512.11748v1 Announce Type: cross 
Abstract: This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title>
<link>https://arxiv.org/abs/2512.11771</link>
<guid>https://arxiv.org/abs/2512.11771</guid>
<content:encoded><![CDATA[
arXiv:2512.11771v1 Announce Type: cross 
Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Coverage Diagnostics for Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.11779</link>
<guid>https://arxiv.org/abs/2512.11779</guid>
<content:encoded><![CDATA[
arXiv:2512.11779v1 Announce Type: cross 
Abstract: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Flight Emerges from Multi-Agent Competitive Racing</title>
<link>https://arxiv.org/abs/2512.11781</link>
<guid>https://arxiv.org/abs/2512.11781</guid>
<content:encoded><![CDATA[
arXiv:2512.11781v1 Announce Type: cross 
Abstract: Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</title>
<link>https://arxiv.org/abs/2512.11783</link>
<guid>https://arxiv.org/abs/2512.11783</guid>
<content:encoded><![CDATA[
arXiv:2512.11783v1 Announce Type: cross 
Abstract: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particulate: Feed-Forward 3D Object Articulation</title>
<link>https://arxiv.org/abs/2512.11798</link>
<guid>https://arxiv.org/abs/2512.11798</guid>
<content:encoded><![CDATA[
arXiv:2512.11798v1 Announce Type: cross 
Abstract: We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability Bracket Notation: Multivariable Systems and Static Bayesian Networks</title>
<link>https://arxiv.org/abs/1207.5293</link>
<guid>https://arxiv.org/abs/1207.5293</guid>
<content:encoded><![CDATA[
arXiv:1207.5293v5 Announce Type: replace 
Abstract: We expand the Probability Bracket Notation (PBN), a symbolic framework inspired by the Dirac notation in quantum mechanics, to multivariable probability systems and static Bayesian networks (BNs). By defining joint, marginal, and conditional probability distributions (PDs), as well as marginal and conditional expectations, we demonstrate how to express dependencies among multiple random variables and manipulate them algebraically in PBN. Using the well-known Student BN as an example of probabilistic graphical models (PGMs), we illustrate how to apply PBN to analyze predictions, inferences (using both bottom-up and top-down approaches), and expectations. We then extend PBN to BNs with continuous variables. After reviewing linear Gaussian networks, we introduce a customized Healthcare BN that includes both continuous and discrete random variables, utilizes user-specific data, and provides tailored predictions via discrete-display (DD) nodes that proxy for their continuous-variable parents. Compared to traditional probability notation, PBN offers an operator-driven framework that unifies and simplifies the analysis of probabilistic models, with potential as both an educational tool and a practical platform for causal reasoning, inference, expectation, data analytics, machine learning, and artificial intelligence.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online Labor Platform</title>
<link>https://arxiv.org/abs/2312.04180</link>
<guid>https://arxiv.org/abs/2312.04180</guid>
<content:encoded><![CDATA[
arXiv:2312.04180v3 Announce Type: replace 
Abstract: This study investigates how artificial intelligence (AI) influences various online labor markets (OLMs) over time. Employing the Difference-in-Differences method, we discovered two distinct scenarios following ChatGPT's launch: displacement effects featuring reduced work volume and earnings, exemplified by translation & localization OLM; productivity effects featuring increased work volume and earnings, exemplified by web development OLM. To understand these opposite effects in a unified framework, we developed a Cournot competition model to identify an inflection point for each market. Before this point, human workers benefit from AI enhancements; beyond this point, human workers would be replaced. Further analyzing the progression from ChatGPT 3.5 to 4.0, we found three effect scenarios, reinforcing our inflection point conjecture. Heterogeneous analyses reveal that U.S. web developers tend to benefit more from ChatGPT's launch compared to their counterparts in other regions. Experienced translators seem more likely to exit the market than less experienced translators.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Aligned Decoding</title>
<link>https://arxiv.org/abs/2405.21047</link>
<guid>https://arxiv.org/abs/2405.21047</guid>
<content:encoded><![CDATA[
arXiv:2405.21047v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper, we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rolling in the deep of cognitive and AI biases</title>
<link>https://arxiv.org/abs/2407.21202</link>
<guid>https://arxiv.org/abs/2407.21202</guid>
<content:encoded><![CDATA[
arXiv:2407.21202v4 Announce Type: replace 
Abstract: Nowadays, we delegate many of our decisions to Artificial Intelligence (AI) that acts either in solo or as a human companion in decisions made to support several sensitive domains, like healthcare, financial services and law enforcement. AI systems, even carefully designed to be fair, are heavily criticized for delivering misjudged and discriminated outcomes against individuals and groups. Numerous work on AI algorithmic fairness is devoted on Machine Learning pipelines which address biases and quantify fairness under a pure computational view. However, the continuous unfair and unjust AI outcomes, indicate that there is urgent need to understand AI as a sociotechnical system, inseparable from the conditions in which it is designed, developed and deployed. Although, the synergy of humans and machines seems imperative to make AI work, the significant impact of human and societal factors on AI bias is currently overlooked. We address this critical issue by following a radical new methodology under which human cognitive biases become core entities in our AI fairness overview. Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall AI lifecycle, and reveal human to AI biases hidden pathways. We introduce a new mapping, which justifies the human heuristics to AI biases reflections and we detect relevant fairness intensities and inter-dependencies. We envision that this approach will contribute in revisiting AI fairness under deeper human-centric case studies, revealing hidden biases cause and effects.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection</title>
<link>https://arxiv.org/abs/2506.13793</link>
<guid>https://arxiv.org/abs/2506.13793</guid>
<content:encoded><![CDATA[
arXiv:2506.13793v3 Announce Type: replace 
Abstract: Large reasoning models excel in domains like mathematics where intermediate reasoning is straightforward to verify, but struggle to self-correct in medicine fields where evaluating intermediate reasoning is cumbersome and expensive. This verification bottleneck hinders the development of reliable AI reasoners for high-stakes application. Here we propose Med-REFL, a novel framework that learns fine-grained reflection without human labels or model distillation. Med-REFL introduces a deterministic structural assessment of the reasoning space to automatically generate preference data for reflection. By globally evaluating all explored reasoning paths in a tree-of-thoughts, our method quantifies the value of corrective actions, enabling the automated construction of direct preference optimization pairs. This trains the model to recognize and amend its own reasoning fallacies. Extensive experiments show Med-REFL delivers robust gains across diverse models architectures and medical benchmarks, boosting a general-purpose Llama3.1-8B by +5.82% and the state-of-the-art Huatuo-o1 by +4.13% on the MedQA benchmark. Our Med-REFL-8B achieves state-of-the-art performance among 7-8B models while even competing with models twice its size. Crucially, targeted ablations prove its success generalizes to other domains such as logical reasoning and mitigates the `fake reflection' phenomenon in LRMs. Ultimately, our framework provides a scalable solution to the verification bottleneck, paving the way for more reliable AI reasoners in high-stakes domains like medicine. Med-REFL has been made publicly available in https://github.com/TianYin123/Med-REFL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</title>
<link>https://arxiv.org/abs/2508.15447</link>
<guid>https://arxiv.org/abs/2508.15447</guid>
<content:encoded><![CDATA[
arXiv:2508.15447v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgent's efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Illusion of Readiness in Health AI</title>
<link>https://arxiv.org/abs/2509.18234</link>
<guid>https://arxiv.org/abs/2509.18234</guid>
<content:encoded><![CDATA[
arXiv:2509.18234v3 Announce Type: replace 
Abstract: Large language models have demonstrated remarkable performance in a wide range of medical benchmarks. Yet underneath the seemingly promising results lie salient growth areas, especially in cutting-edge frontiers such as multimodal reasoning. In this paper, we introduce a series of adversarial stress tests to systematically assess the robustness of flagship models and medical benchmarks. Our study reveals prevalent brittleness in the presence of simple adversarial transformations: leading systems can guess the right answer even with key inputs removed, yet may get confused by the slightest prompt alterations, while fabricating convincing yet flawed reasoning traces. Using clinician-guided rubrics, we demonstrate that popular medical benchmarks vary widely in what they truly measure. Our study reveals significant competency gaps of frontier AI in attaining real-world readiness for health applications. If we want AI to earn trust in healthcare, we must demand more than leaderboard wins and must hold AI systems accountable to ensure robustness, sound reasoning, and alignment with real medical demands.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</title>
<link>https://arxiv.org/abs/2510.16309</link>
<guid>https://arxiv.org/abs/2510.16309</guid>
<content:encoded><![CDATA[
arXiv:2510.16309v3 Announce Type: replace 
Abstract: Large language models (LLMs) often produce fluent reasoning steps while violating simple mathematical or logical constraints. We introduce MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier, designed to enforce mathematically interpretable rules in reasoning tasks. MedRule-KG encodes entities, relations, and three domain-inspired rules, while the verifier checks predictions and applies minimal corrections to guarantee consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely. We demonstrate how MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss ablations, and release code and data to encourage reproducibility.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCode: Unify Plan and Action for Universal Granularity Control</title>
<link>https://arxiv.org/abs/2510.23564</link>
<guid>https://arxiv.org/abs/2510.23564</guid>
<content:encoded><![CDATA[
arXiv:2510.23564v3 Announce Type: replace 
Abstract: Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Dysarthria</title>
<link>https://arxiv.org/abs/2511.07895</link>
<guid>https://arxiv.org/abs/2511.07895</guid>
<content:encoded><![CDATA[
arXiv:2511.07895v2 Announce Type: replace 
Abstract: Dysarthria impairs motor control of speech, often resulting in reduced intelligibility and frequent misarticulations. Although interest in brain-computer interface technologies is growing, electroencephalogram (EEG)-based communication support for individuals with dysarthria remains limited. To address this gap, we recorded EEG data from one participant with dysarthria during a Korean automatic speech task and labeled each trial as correct or misarticulated. Spectral analysis revealed that misarticulated trials exhibited elevated frontal-central delta and alpha power, along with reduced temporal gamma activity. Building on these observations, we developed a soft multitask learning framework designed to suppress these nonspecific spectral responses and incorporated a maximum mean discrepancy-based alignment module to enhance class discrimination while minimizing domain-related variability. The proposed model achieved F1-scores of 52.7 % for correct and 41.4 % for misarticulated trials-an improvement of 2 % and 11 % over the baseline-demonstrating more stable intention decoding even under articulation errors. These results highlight the potential of EEG-based assistive systems for communication in language impaired individuals.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI</title>
<link>https://arxiv.org/abs/2511.12306</link>
<guid>https://arxiv.org/abs/2511.12306</guid>
<content:encoded><![CDATA[
arXiv:2511.12306v2 Announce Type: replace 
Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning</title>
<link>https://arxiv.org/abs/2511.12963</link>
<guid>https://arxiv.org/abs/2511.12963</guid>
<content:encoded><![CDATA[
arXiv:2511.12963v2 Announce Type: replace 
Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration</title>
<link>https://arxiv.org/abs/2511.15351</link>
<guid>https://arxiv.org/abs/2511.15351</guid>
<content:encoded><![CDATA[
arXiv:2511.15351v2 Announce Type: replace 
Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05943</link>
<guid>https://arxiv.org/abs/2512.05943</guid>
<content:encoded><![CDATA[
arXiv:2512.05943v3 Announce Type: replace 
Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Smart Factory Model: A model-based Approach for Integrating Industry 4.0 and Sustainability for Manufacturing Systems</title>
<link>https://arxiv.org/abs/2512.10631</link>
<guid>https://arxiv.org/abs/2512.10631</guid>
<content:encoded><![CDATA[
arXiv:2512.10631v2 Announce Type: replace 
Abstract: This paper presents the Unified Smart Factory Model (USFM), a comprehensive framework designed to translate high-level sustainability goals into measurable factory-level indicators with a systematic information map of manufacturing activities. The manufacturing activities were modelled as set of manufacturing, assembly and auxiliary processes using Object Process Methodology, a Model Based Systems Engineering (MBSE) language. USFM integrates Manufacturing Process and System, Data Process, and Key Performance Indicator (KPI) Selection and Assessment in a single framework. Through a detailed case study of Printed Circuit Board (PCB) assembly factory, the paper demonstrates how environmental sustainability KPIs can be selected, modelled, and mapped to the necessary data, highlighting energy consumption and environmental impact metrics. The model's systematic approach can reduce redundancy, minimize the risk of missing critical information, and enhance data collection. The paper concluded that the USFM bridges the gap between sustainability goals and practical implementation, providing significant benefits for industries specifically SMEs aiming to achieve sustainability targets.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DSGrasp: 3D Shape-Completion for Robotic Grasp</title>
<link>https://arxiv.org/abs/2301.00866</link>
<guid>https://arxiv.org/abs/2301.00866</guid>
<content:encoded><![CDATA[
arXiv:2301.00866v2 Announce Type: replace-cross 
Abstract: Real-world robotic grasping can be done robustly if a complete 3D Point Cloud Data (PCD) of an object is available. However, in practice, PCDs are often incomplete when objects are viewed from few and sparse viewpoints before the grasping action, leading to the generation of wrong or inaccurate grasp poses. We propose a novel grasping strategy, named 3DSGrasp, that predicts the missing geometry from the partial PCD to produce reliable grasp poses. Our proposed PCD completion network is a Transformer-based encoder-decoder network with an Offset-Attention layer. Our network is inherently invariant to the object pose and point's permutation, which generates PCDs that are geometrically consistent and completed properly. Experiments on a wide range of partial PCD show that 3DSGrasp outperforms the best state-of-the-art method on PCD completion tasks and largely improves the grasping success rate in real-world scenarios. The code and dataset will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</title>
<link>https://arxiv.org/abs/2402.03216</link>
<guid>https://arxiv.org/abs/2402.03216</guid>
<content:encoded><![CDATA[
arXiv:2402.03216v5 Announce Type: replace-cross 
Abstract: In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in \textit{Multi-Linguality}, \textit{Multi-Functionality}, and \textit{Multi-Granularity}. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Learning for Scalable Representation of High-Dimensional Medical Data</title>
<link>https://arxiv.org/abs/2409.13115</link>
<guid>https://arxiv.org/abs/2409.13115</guid>
<content:encoded><![CDATA[
arXiv:2409.13115v2 Announce Type: replace-cross 
Abstract: Integrating artificial intelligence (AI) with healthcare data is rapidly transforming medical diagnostics and driving progress toward precision medicine. However, effectively leveraging multimodal data, particularly digital pathology whole slide images (WSIs) and genomic sequencing, remains a significant challenge due to the intrinsic heterogeneity of these modalities and the need for scalable and interpretable frameworks. Existing diagnostic models typically operate on unimodal data, overlooking critical cross-modal interactions that can yield richer clinical insights. We introduce MarbliX (Multimodal Association and Retrieval with Binary Latent Indexed matriX), a self-supervised framework that learns to embed WSIs and immunogenomic profiles into compact, scalable binary codes, termed ``monogram.'' By optimizing a triplet contrastive objective across modalities, MarbliX captures high-resolution patient similarity in a unified latent space, enabling efficient retrieval of clinically relevant cases and facilitating case-based reasoning. \textcolor{black}{In lung cancer, MarbliX achieves 85-89\% across all evaluation metrics, outperforming histopathology (69-71\%) and immunogenomics (73-76\%). In kidney cancer, real-valued monograms yield the strongest performance (F1: 80-83\%, Accuracy: 87-90\%), with binary monograms slightly lower (F1: 78-82\%).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Continual Instruction Assistant</title>
<link>https://arxiv.org/abs/2410.10868</link>
<guid>https://arxiv.org/abs/2410.10868</guid>
<content:encoded><![CDATA[
arXiv:2410.10868v5 Announce Type: replace-cross 
Abstract: Continual Instruction Tuning (CIT) is adopted to continually instruct Large Models to follow human intent data by data. It is observed that existing gradient update would heavily destroy the performance on previous datasets during CIT process. Instead, Exponential Moving Average (EMA), owns the ability to trace previous parameters, which can aid in decreasing forgetting. Nonetheless, its stable balance weight fails to deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability. In this paper, we propose a general continual instruction tuning framework to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight can be automatically determined by the gradients and learned parameters. Therefore, we propose a stable-plasticity balanced coefficient to avoid knowledge interference. Based on the semantic similarity of the instructions, we can determine whether to retrain or expand the training parameters and allocate the most suitable parameters for the testing instances. Extensive experiments across multiple continual instruction tuning benchmarks demonstrate that our approach not only enhances anti-forgetting capabilities but also significantly improves overall continual tuning performance. Our code is available at https://github.com/JingyangQiao/CoIN.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARPD: World model Assisted Reactive Policy Diffusion</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v4 Announce Type: replace-cross 
Abstract: With the increasing availability of open-source robotic data, imitation learning has become a promising approach for both manipulation and locomotion. Diffusion models are now widely used to train large, generalized policies that predict controls or trajectories, leveraging their ability to model multimodal action distributions. However, this generality comes at the cost of larger model sizes and slower inference, an acute limitation for robotic tasks requiring high control frequencies. Moreover, Diffusion Policy (DP), a popular trajectory-generation approach, suffers from a trade-off between performance and action horizon: fewer diffusion queries lead to larger trajectory chunks, which in turn accumulate tracking errors. To overcome these challenges, we introduce WARPD (World model Assisted Reactive Policy Diffusion), a method that generates closed-loop policies (weights for neural policies) directly, instead of open-loop trajectories. By learning behavioral distributions in parameter space rather than trajectory space, WARPD offers two major advantages: (1) extended action horizons with robustness to perturbations, while maintaining high task performance, and (2) significantly reduced inference costs. Empirically, WARPD outperforms DP in long-horizon and perturbed environments, and achieves multitask performance on par with DP while requiring only ~ 1/45th of the inference-time FLOPs per step.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Backdoor Stealthiness in Model Parameter Space</title>
<link>https://arxiv.org/abs/2501.05928</link>
<guid>https://arxiv.org/abs/2501.05928</guid>
<content:encoded><![CDATA[
arXiv:2501.05928v3 Announce Type: replace-cross 
Abstract: Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?
  To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention</title>
<link>https://arxiv.org/abs/2501.06382</link>
<guid>https://arxiv.org/abs/2501.06382</guid>
<content:encoded><![CDATA[
arXiv:2501.06382v4 Announce Type: replace-cross 
Abstract: Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERITAS: Verifying the Performance of AI-native Transceiver Actions in Base-Stations</title>
<link>https://arxiv.org/abs/2501.09761</link>
<guid>https://arxiv.org/abs/2501.09761</guid>
<content:encoded><![CDATA[
arXiv:2501.09761v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI)-native receivers prove significant performance improvement in high noise regimes and can potentially reduce communication overhead compared to the traditional receiver. However, their performance highly depends on the representativeness of the training dataset. A major issue is the uncertainty of whether the training dataset covers all test environments and waveform configurations, and thus, whether the trained model is robust in practical deployment conditions. To this end, we propose a joint measurement-recovery framework for AI-native transceivers post deployment, called VERITAS, that continuously looks for distribution shifts in the received signals and triggers finite re-training spurts. VERITAS monitors the wireless channel using 5G pilots fed to an auxiliary neural network that detects out-of-distribution channel profile, transmitter speed, and delay spread. As soon as such a change is detected, a traditional (reference) receiver is activated, which runs for a period of time in parallel to the AI-native receiver. Finally, VERTIAS compares the bit probabilities of the AI-native and the reference receivers for the same received data inputs, and decides whether or not a retraining process needs to be initiated. Our evaluations reveal that VERITAS can detect changes in the channel profile, transmitter speed, and delay spread with 99%, 97%, and 69% accuracies, respectively, followed by timely initiation of retraining for 86%, 93.3%, and 94.8% of inputs in channel profile, transmitter speed, and delay spread test sets, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Advances in Discrete Speech Tokens: A Review</title>
<link>https://arxiv.org/abs/2502.06490</link>
<guid>https://arxiv.org/abs/2502.06490</guid>
<content:encoded><![CDATA[
arXiv:2502.06490v4 Announce Type: replace-cross 
Abstract: The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuncGenFoil: Airfoil Generation and Editing Model in Function Space</title>
<link>https://arxiv.org/abs/2502.10712</link>
<guid>https://arxiv.org/abs/2502.10712</guid>
<content:encoded><![CDATA[
arXiv:2502.10712v4 Announce Type: replace-cross 
Abstract: Aircraft manufacturing is the jewel in the crown of industry, in which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. Existing deep learning methods, which typically rely on predefined parametric representations (e.g., B\'ezier) or discrete point sets, face an inherent trade-off between expressive power and resolution adaptability. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly reconstructs airfoil geometries as function curves. Our method inherits the advantages of arbitrary-resolution sampling and smoothness from parametric functions, as well as the strong expressiveness of discrete point-based representations. Empirical evaluations demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation, achieving a relative 74.4% reduction in label error and a 23.2% increase in diversity on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models</title>
<link>https://arxiv.org/abs/2502.11028</link>
<guid>https://arxiv.org/abs/2502.11028</guid>
<content:encoded><![CDATA[
arXiv:2502.11028v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Statistical Learning: Supervised Learning of Statistical Estimators</title>
<link>https://arxiv.org/abs/2502.12088</link>
<guid>https://arxiv.org/abs/2502.12088</guid>
<content:encoded><![CDATA[
arXiv:2502.12088v3 Announce Type: replace-cross 
Abstract: Statistical inference, a central tool of science, revolves around the study and the usage of statistical estimators: functions that map finite samples to predictions about unknown distribution parameters. In the frequentist framework, estimators are evaluated based on properties such as bias, variance (for parameter estimation), accuracy, power, and calibration (for hypothesis testing). However, crafting estimators with desirable properties is often analytically challenging, and sometimes impossible, e.g., there exists no universally unbiased estimator for the standard deviation. In this work, we introduce meta-statistical learning, an amortized learning framework that recasts estimator design as an optimization problem via supervised learning. This takes a fully empirical approach to discovering statistical estimators; entire datasets are input to permutation-invariant neural networks, such as Set Transformers, trained to predict the target statistical property. The trained model is the estimator, and can be analyzed through the classical frequentist lens. We demonstrate the approach on two tasks: learning a normality test (classification) and estimating mutual information (regression), achieving strong results even with small models. Looking ahead, this paradigm opens a path to automate the discovery of generalizable and flexible statistical estimators.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Best-of-N Selection for Large Language Models via Self-Certainty</title>
<link>https://arxiv.org/abs/2502.18581</link>
<guid>https://arxiv.org/abs/2502.18581</guid>
<content:encoded><![CDATA[
arXiv:2502.18581v3 Announce Type: replace-cross 
Abstract: Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size N, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at https://github.com/backprop07/Self-Certainty
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion</title>
<link>https://arxiv.org/abs/2503.01220</link>
<guid>https://arxiv.org/abs/2503.01220</guid>
<content:encoded><![CDATA[
arXiv:2503.01220v3 Announce Type: replace-cross 
Abstract: Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Using emerging tissue profiling technologies, researchers charted comprehensive atlases of mammalian brain with sub-cellular resolution and spatially resolved transcriptomic data. However, these tera-scale volumetric atlases pose computational challenges for modeling intricate brain structures within the native spatial context. We propose \textbf{Tera-MIND}, a novel generative framework capable of simulating \textbf{Tera}-scale \textbf{M}ouse bra\textbf{IN}s in 3D using a patch-based and boundary-aware \textbf{D}iffusion model. Taking spatial gene expression as conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D \textit{gene}-\textit{gene} self-attention, we identify spatial molecular interactions for key transcriptomic pathways, including glutamatergic and dopaminergic neuronal systems. Lastly, we showcase the translational applicability of Tera-MIND on previously unseen human brain samples. Tera-MIND offers an efficient generative modeling of whole virtual organisms, paving the way for integrative applications in biomedical research. Project website: https://musikisomorphie.github.io/Tera-MIND.html
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Unified Approach for Elevating Benchmark Quality</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
arXiv:2503.05860v3 Announce Type: replace-cross 
Abstract: Benchmarks are essential for unified evaluation and reproducibility. The rapid rise of Artificial Intelligence for Software Engineering (AI4SE) has produced numerous benchmarks for tasks such as code generation and bug repair. However, this proliferation has led to major challenges: (1) fragmented knowledge across tasks, (2) difficulty in selecting contextually relevant benchmarks, (3) lack of standardization in benchmark creation, and (4) flaws that limit utility. Addressing these requires a dual approach: systematically mapping existing benchmarks for informed selection and defining unified guidelines for robust, adaptable benchmark development.
  We conduct a review of 247 studies, identifying 273 AI4SE benchmarks since 2014. We categorize them, analyze limitations, and expose gaps in current practices. Building on these insights, we introduce BenchScout, an extensible semantic search tool for locating suitable benchmarks. BenchScout employs automated clustering with contextual embeddings of benchmark-related studies, followed by dimensionality reduction. In a user study with 22 participants, BenchScout achieved usability, effectiveness, and intuitiveness scores of 4.5, 4.0, and 4.1 out of 5.
  To improve benchmarking standards, we propose BenchFrame, a unified framework for enhancing benchmark quality. Applying BenchFrame to HumanEval yielded HumanEvalNext, featuring corrected errors, improved language conversion, higher test coverage, and greater difficulty. Evaluating 10 state-of-the-art code models on HumanEval, HumanEvalPlus, and HumanEvalNext revealed average pass-at-1 drops of 31.22% and 19.94%, respectively, underscoring the need for continuous benchmark refinement. We further examine BenchFrame's scalability through an agentic pipeline and confirm its generalizability on the MBPP dataset. All review data, user study materials, and enhanced benchmarks are publicly released.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits</title>
<link>https://arxiv.org/abs/2503.07020</link>
<guid>https://arxiv.org/abs/2503.07020</guid>
<content:encoded><![CDATA[
arXiv:2503.07020v2 Announce Type: replace-cross 
Abstract: Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOAT: Evaluating LMMs for Capability Integration and Instruction Grounding</title>
<link>https://arxiv.org/abs/2503.09348</link>
<guid>https://arxiv.org/abs/2503.09348</guid>
<content:encoded><![CDATA[
arXiv:2503.09348v2 Announce Type: replace-cross 
Abstract: Large multimodal models (LMMs) have demonstrated significant potential as generalists in vision-language (VL) tasks. However, adoption of LMMs in real-world tasks is hindered by their poor performance in tasks that require a combination of VL capabilities, as well as in tasks that involve the grounding of complex text or visual instructions. To thoroughly investigate this gap and its underlying causes, we propose MOAT, a diverse benchmark with 1005 complex real-world vision questions that are straightforward for humans but challenging for LMMs. Specifically, the tasks in MOAT require LMMs to engage in generalist problem solving by integrating VL capabilities such as reading text, counting, understanding spatial relations, grounding textual and visual instructions, etc. All these abilities fit into a taxonomy proposed by us that contains 9 VL capabilities, enabling MOAT to provide a fine-grained view of LMMs' strengths and weaknesses. Besides, MOAT is the first benchmark to explicitly evaluate LMMs' ability to ground complex text and visual instructions, which is essential for many real-world applications. We evaluated 17 proprietary and open source LMMs, finding that the best performing LMM (Gemini 2.5 Pro) achieved only 44% accuracy, far below what would be acceptable in real-world applications. To guide future model development, we analyze common trends in our results and discuss the underlying causes of poor performance, focusing on the impact of text-centric reasoning, which VL capabilities form bottlenecks in complex tasks, and the potential harmful effects of tiling. Code and data are available at https://cambrian-yzt.github.io/MOAT/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
arXiv:2503.24379v2 Announce Type: replace-cross 
Abstract: To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2504.21228</link>
<guid>https://arxiv.org/abs/2504.21228</guid>
<content:encoded><![CDATA[
arXiv:2504.21228v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning</title>
<link>https://arxiv.org/abs/2505.16368</link>
<guid>https://arxiv.org/abs/2505.16368</guid>
<content:encoded><![CDATA[
arXiv:2505.16368v3 Announce Type: replace-cross 
Abstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iPINNER: An Iterative Physics-Informed Neural Network with Ensemble Kalman Filter</title>
<link>https://arxiv.org/abs/2506.00731</link>
<guid>https://arxiv.org/abs/2506.00731</guid>
<content:encoded><![CDATA[
arXiv:2506.00731v2 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving forward and inverse problems involving partial differential equations (PDEs) by incorporating physical laws into the training process. However, the performance of PINNs is often hindered in real-world scenarios involving noisy observational data and missing physics, particularly in inverse problems. In this work, we propose an iterative multi-objective PINN ensemble Kalman filter (iPINNER) framework that improves the robustness and accuracy of PINNs in both forward and inverse problems by using the \textit{ensemble Kalman filter} and the \textit{non-dominated sorting genetic algorithm} III (NSGA-III). Specifically, NSGA-III is used as a multi-objective optimizer that can generate various ensemble members of PINNs along the optimal Pareto front, while accounting the model uncertainty in the solution space. These ensemble members are then utilized within the EnKF to assimilate noisy observational data. The EnKF's analysis is subsequently used to refine the data loss component for retraining the PINNs, thereby iteratively updating their parameters. The iterative procedure generates improved solutions to the PDEs. The proposed method is tested on two benchmark problems: the one-dimensional viscous Burgers equation and the time-fractional mixed diffusion-wave equation (TFMDWE). The numerical results show it outperforms standard PINNs in handling noisy data and missing physics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA-Video: Motion-Aware Concept Alignment for Consistent Video Editing</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
arXiv:2506.01004v2 Announce Type: replace-cross 
Abstract: We present MoCA-Video, a training-free framework for semantic mixing in videos. Operating in the latent space of a frozen video diffusion model, MoCA-Video utilizes class-agnostic segmentation with diagonal denoising scheduler to localize and track the target object across frames. To ensure temporal stability under semantic shifts, we introduce momentum-based correction to approximate novel hybrid distributions beyond trained data distribution, alongside a light gamma residual module that smooths out visual artifacts. We evaluate model's performance using SSIM, LPIPS, and a proposed metric, \metricnameabbr, which quantifies semantic alignment between reference and output. Extensive evaluation demonstrates that our model consistently outperforms both training-free and trained baselines, achieving superior semantic mixing and temporal coherence without retraining. Results establish that structured manipulation of diffusion noise trajectories enables controllable and high-quality video editing under semantic shifts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</title>
<link>https://arxiv.org/abs/2506.01374</link>
<guid>https://arxiv.org/abs/2506.01374</guid>
<content:encoded><![CDATA[
arXiv:2506.01374v4 Announce Type: replace-cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2506.10622</link>
<guid>https://arxiv.org/abs/2506.10622</guid>
<content:encoded><![CDATA[
arXiv:2506.10622v2 Announce Type: replace-cross 
Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers</title>
<link>https://arxiv.org/abs/2506.14855</link>
<guid>https://arxiv.org/abs/2506.14855</guid>
<content:encoded><![CDATA[
arXiv:2506.14855v3 Announce Type: replace-cross 
Abstract: Model Predictive Path Integral control is a powerful sampling-based approach suitable for complex robotic tasks due to its flexibility in handling nonlinear dynamics and non-convex costs. However, its applicability in real-time, highfrequency robotic control scenarios is limited by computational demands. This paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments standard MPPI by computing local linear feedback gains derived from sensitivity analysis inspired by Riccati-based feedback used in gradient-based MPC. These gains allow for rapid closed-loop corrections around the current state without requiring full re-optimization at each timestep. We demonstrate the effectiveness of F-MPPI through simulations and real-world experiments on two robotic platforms: a quadrupedal robot performing dynamic locomotion on uneven terrain and a quadrotor executing aggressive maneuvers with onboard computation. Results illustrate that incorporating local feedback significantly improves control performance and stability, enabling robust, high-frequency operation suitable for complex robotic systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Object Counting in Videos</title>
<link>https://arxiv.org/abs/2506.15368</link>
<guid>https://arxiv.org/abs/2506.15368</guid>
<content:encoded><![CDATA[
arXiv:2506.15368v2 Announce Type: replace-cross 
Abstract: We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and objects of similar appearance, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model, to enable automated open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for this novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://www.robots.ox.ac.uk/~vgg/research/countvid/.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
arXiv:2506.21546v3 Announce Type: replace-cross 
Abstract: Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale</title>
<link>https://arxiv.org/abs/2507.05178</link>
<guid>https://arxiv.org/abs/2507.05178</guid>
<content:encoded><![CDATA[
arXiv:2507.05178v2 Announce Type: replace-cross 
Abstract: Despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities in complex, dynamic, real-world tasks. Existing environments typically focus on small-scale, fully observable, or low-complexity domains, limiting their utility for developing and assessing next-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire, an open-source benchmark designed to close this gap. Built atop the human-AI teaming CREW simulation platform, CREW-Wildfire offers procedurally generated wildfire response scenarios featuring large maps, heterogeneous agents, partial observability, stochastic dynamics, and long-horizon planning objectives. The environment supports both low-level control and high-level natural language interactions through modular Perception and Execution modules. We implement and evaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks, uncovering significant performance gaps that highlight the unsolved challenges in large-scale coordination, communication, spatial reasoning, and long-horizon planning under uncertainty. By providing more realistic complexity, scalable architecture, and behavioral evaluation metrics, CREW-Wildfire establishes a critical foundation for advancing research in scalable multi-agent Agentic intelligence. All code, environments, data, and baselines will be released to support future research in this emerging domain.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback</title>
<link>https://arxiv.org/abs/2507.13171</link>
<guid>https://arxiv.org/abs/2507.13171</guid>
<content:encoded><![CDATA[
arXiv:2507.13171v2 Announce Type: replace-cross 
Abstract: Conventional reinforcement learning (RL) ap proaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, rein forcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, en abling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering</title>
<link>https://arxiv.org/abs/2508.11272</link>
<guid>https://arxiv.org/abs/2508.11272</guid>
<content:encoded><![CDATA[
arXiv:2508.11272v2 Announce Type: replace-cross 
Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited -- compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Model's understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing</title>
<link>https://arxiv.org/abs/2508.18316</link>
<guid>https://arxiv.org/abs/2508.18316</guid>
<content:encoded><![CDATA[
arXiv:2508.18316v3 Announce Type: replace-cross 
Abstract: This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RECAP: REwriting Conversations for Intent Understanding in Agentic Planning</title>
<link>https://arxiv.org/abs/2509.04472</link>
<guid>https://arxiv.org/abs/2509.04472</guid>
<content:encoded><![CDATA[
arXiv:2509.04472v2 Announce Type: replace-cross 
Abstract: Understanding user intent is essential for effective planning in conversational assistants, particularly those powered by large language models (LLMs) coordinating multiple agents. However, real-world dialogues are often ambiguous, underspecified, or dynamic, making intent detection a persistent challenge. Traditional classification-based approaches struggle to generalize in open-ended settings, leading to brittle interpretations and poor downstream planning. We propose RECAP (REwriting Conversations for Agent Planning), a new benchmark designed to evaluate and advance intent rewriting, reframing user-agent dialogues into concise representations of user goals. RECAP captures diverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal conversations. Alongside the dataset, we introduce an LLM-based evaluator that assesses planning utility given the rewritten intent. Using RECAP, we develop a prompt-based rewriting approach that outperforms baselines, in terms of plan preference. We further demonstrate that fine-tuning two DPO-based rewriters yields additional utility gains. Our results highlight intent rewriting as a critical and tractable component for improving agentic planning in open-domain dialogue systems.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v4 Announce Type: replace-cross 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
<link>https://arxiv.org/abs/2509.12421</link>
<guid>https://arxiv.org/abs/2509.12421</guid>
<content:encoded><![CDATA[
arXiv:2509.12421v2 Announce Type: replace-cross 
Abstract: The rapid adoption of foundation models (e.g., large language models) has given rise to promptware, i.e., software built using natural language prompts. Effective management of prompts, such as organization and quality assurance, is essential yet challenging. In this study, we perform an empirical analysis of 24,800 open-source prompts from 92 GitHub repositories to investigate prompt management practices and quality attributes. Our findings reveal critical challenges such as considerable inconsistencies in prompt formatting, substantial internal and external prompt duplication, and frequent readability and spelling issues. Based on these findings, we provide actionable recommendations for developers to enhance the usability and maintainability of open-source prompts within the rapidly evolving promptware ecosystem.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</title>
<link>https://arxiv.org/abs/2509.19112</link>
<guid>https://arxiv.org/abs/2509.19112</guid>
<content:encoded><![CDATA[
arXiv:2509.19112v3 Announce Type: replace-cross 
Abstract: Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion</title>
<link>https://arxiv.org/abs/2509.19312</link>
<guid>https://arxiv.org/abs/2509.19312</guid>
<content:encoded><![CDATA[
arXiv:2509.19312v2 Announce Type: replace-cross 
Abstract: This paper investigates multimodal semantic non-orthogonal transmission and fusion in hybrid analog-digital massive multiple-input multiple-output (MIMO). A Transformer-based cross-modal source-channel semantic-aware network (CSC-SA-Net) framework is conceived, where channel state information (CSI) reference signal (RS), feedback, analog-beamforming/combining, and baseband semantic processing are data-driven end-to-end (E2E) optimized at the base station (BS) and user equipments (UEs). CSC-SA-Net comprises five sub-networks: BS-side CSI-RS network (BS-CSIRS-Net), UE-side channel semantic-aware network (UE-CSANet), BS-CSANet, UE-side multimodal semantic fusion network (UE-MSFNet), and BS-MSFNet. Specifically, we firstly E2E train BS-CSIRS-Net, UE-CSANet, and BS-CSANet to jointly design CSI-RS, feedback, analog-beamforming/combining with maximum {\emph{physical-layer's}} spectral-efficiency. Meanwhile, we E2E train UE-MSFNet and BS-MSFNet for optimizing {\emph{application-layer's}} source semantic downstream tasks. On these pre-trained models, we further integrate application-layer semantic processing with physical-layer tasks to E2E train five subnetworks. Extensive simulations show that the proposed CSC-SA-Net outperforms traditional separated designs, revealing the advantage of cross-modal channel-source semantic fusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An effective control of large systems of active particles: An application to evacuation problem</title>
<link>https://arxiv.org/abs/2509.19972</link>
<guid>https://arxiv.org/abs/2509.19972</guid>
<content:encoded><![CDATA[
arXiv:2509.19972v3 Announce Type: replace-cross 
Abstract: Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</title>
<link>https://arxiv.org/abs/2510.04573</link>
<guid>https://arxiv.org/abs/2510.04573</guid>
<content:encoded><![CDATA[
arXiv:2510.04573v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaze on the Prize: Shaping Visual Attention with Return-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2510.08442</link>
<guid>https://arxiv.org/abs/2510.08442</guid>
<content:encoded><![CDATA[
arXiv:2510.08442v2 Announce Type: replace-cross 
Abstract: Visual Reinforcement Learning (RL) agents must learn to act based on high-dimensional image data where only a small fraction of the pixels is task-relevant. This forces agents to waste exploration and computational resources on irrelevant features, leading to sample-inefficient and unstable learning. To address this, inspired by human visual foveation, we introduce Gaze on the Prize. This framework augments visual RL with a learnable foveal attention mechanism (Gaze), guided by a self-supervised signal derived from the agent's experience pursuing higher returns (the Prize). Our key insight is that return differences reveal what matters most: If two similar representations produce different outcomes, their distinguishing features are likely task-relevant, and the gaze should focus on them accordingly. This is realized through return-guided contrastive learning that trains the attention to distinguish between the features relevant to success and failure. We group similar visual representations into positives and negatives based on their return differences and use the resulting labels to construct contrastive triplets. These triplets provide the training signal that teaches the attention mechanism to produce distinguishable representations for states associated with different outcomes. Our method achieves up to 2.52x improvement in sample efficiency and can solve challenging tasks from the ManiSkill3 benchmark that the baseline fails to learn, without modifying the underlying algorithm or hyperparameters.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Emergence of Complex Behavior in Large-Scale Ecological Environments</title>
<link>https://arxiv.org/abs/2510.18221</link>
<guid>https://arxiv.org/abs/2510.18221</guid>
<content:encoded><![CDATA[
arXiv:2510.18221v3 Announce Type: replace-cross 
Abstract: We explore how physical scale and population size shape the emergence of complex behaviors in open-ended ecological environments. In our setting, agents are unsupervised and have no explicit rewards or learning objectives but instead evolve over time according to reproduction, mutation, and selection. As they act, agents also shape their environment and the population around them in an ongoing dynamic ecology. Our goal is not to optimize a single high-performance policy, but instead to examine how behaviors emerge and evolve across large populations due to natural competition and environmental pressures. We use modern hardware along with a new multi-agent simulator to scale the environment and population to sizes much larger than previously attempted, reaching populations of over 60,000 agents, each with their own evolved neural network policy. We identify various emergent behaviors such as long-range resource extraction, vision-based foraging, and predation that arise under competitive and survival pressures. We examine how sensing modalities and environmental scale affect the emergence of these behaviors and find that some of them appear only in sufficiently large environments and populations, and that larger scales increase the stability and consistency of these emergent behaviors. While there is a rich history of research in evolutionary settings, our scaling results on modern hardware provide promising new directions to explore ecology as an instrument of machine learning in an era of increasingly abundant computational resources and efficient machine frameworks. Experimental code is available at https://github.com/jbejjani2022/ecological-emergent-behavior.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[
arXiv:2511.06634v3 Announce Type: replace-cross 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/SusCom-Lab/CaberNet-CRL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title>
<link>https://arxiv.org/abs/2511.06682</link>
<guid>https://arxiv.org/abs/2511.06682</guid>
<content:encoded><![CDATA[
arXiv:2511.06682v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10843</link>
<guid>https://arxiv.org/abs/2511.10843</guid>
<content:encoded><![CDATA[
arXiv:2511.10843v2 Announce Type: replace-cross 
Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference</title>
<link>https://arxiv.org/abs/2511.11907</link>
<guid>https://arxiv.org/abs/2511.11907</guid>
<content:encoded><![CDATA[
arXiv:2511.11907v2 Announce Type: replace-cross 
Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</title>
<link>https://arxiv.org/abs/2511.13387</link>
<guid>https://arxiv.org/abs/2511.13387</guid>
<content:encoded><![CDATA[
arXiv:2511.13387v3 Announce Type: replace-cross 
Abstract: Denoising diffusion models have emerged as a dominant paradigm in image generation. Discretizing image data into tokens is a critical step for effectively integrating images with Transformer and other architectures. Although the Denoising Diffusion Codebook Models (DDCM) pioneered the use of pre-trained diffusion models for image tokenization, it strictly relies on the traditional discrete-time DDPM architecture. Consequently, it fails to adapt to modern continuous-time variants-such as Flow Matching and Consistency Models-and suffers from inefficient sampling in high-noise regions. To address these limitations, this paper proposes the Generalized Denoising Diffusion Codebook Models (gDDCM). We establish a unified theoretical framework and introduce a generic "De-noise and Back-trace" sampling strategy. By integrating a deterministic ODE denoising step with a residual-aligned noise injection step, our method resolves the challenge of adaptation. Furthermore, we introduce a backtracking parameter $p$ and significantly enhance tokenization ability. Extensive experiments on CIFAR10 and LSUN Bedroom datasets demonstrate that gDDCM achieves comprehensive compatibility with mainstream diffusion variants and significantly outperforms DDCM in terms of reconstruction quality and perceptual fidelity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Finer the Better: Towards Granular-aware Open-set Domain Generalization</title>
<link>https://arxiv.org/abs/2511.16979</link>
<guid>https://arxiv.org/abs/2511.16979</guid>
<content:encoded><![CDATA[
arXiv:2511.16979v2 Announce Type: replace-cross 
Abstract: Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-based Surrogate Model for Time-varying Underwater Acoustic Channels</title>
<link>https://arxiv.org/abs/2511.18078</link>
<guid>https://arxiv.org/abs/2511.18078</guid>
<content:encoded><![CDATA[
arXiv:2511.18078v2 Announce Type: replace-cross 
Abstract: Accurate modeling of time-varying underwater acoustic channels is essential for the design, evaluation, and deployment of reliable underwater communication systems. Conventional physics models require detailed environmental knowledge, while stochastic replay methods are constrained by the limited diversity of measured channels and often fail to generalize to unseen scenarios, reducing their practical applicability. To address these challenges, we propose StableUASim, a pre-trained conditional latent diffusion surrogate model that captures the stochastic dynamics of underwater acoustic communication channels. Leveraging generative modeling, StableUASim produces diverse and statistically realistic channel realizations, while supporting conditional generation from specific measurement samples. Pre-training enables rapid adaptation to new environments using minimal additional data, and the autoencoder latent representation facilitates efficient channel analysis and compression. Experimental results demonstrate that StableUASim accurately reproduces key channel characteristics and communication performance, providing a scalable, data-efficient, and physically consistent surrogate model for both system design and machine learning-driven underwater applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20663</link>
<guid>https://arxiv.org/abs/2511.20663</guid>
<content:encoded><![CDATA[
arXiv:2511.20663v3 Announce Type: replace-cross 
Abstract: Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.
  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.
  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Signed Graph Learning with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.00307</link>
<guid>https://arxiv.org/abs/2512.00307</guid>
<content:encoded><![CDATA[
arXiv:2512.00307v2 Announce Type: replace-cross 
Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
arXiv:2512.00396v2 Announce Type: replace-cross 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</title>
<link>https://arxiv.org/abs/2512.00862</link>
<guid>https://arxiv.org/abs/2512.00862</guid>
<content:encoded><![CDATA[
arXiv:2512.00862v3 Announce Type: replace-cross 
Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02551</link>
<guid>https://arxiv.org/abs/2512.02551</guid>
<content:encoded><![CDATA[
arXiv:2512.02551v2 Announce Type: replace-cross 
Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art Nvidia's closed-source libraries, i.e., cuBLAS, cuBLASLt. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0% over torch.matmul on average; +19.2% over cuBLAS using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8% over cuBLASLt-heuristic, which queries cuBLASLt library and selects the algorithm based on the heuristic's suggestion; and +11.4% over the most competitive cuBLASLt-AutoTuning model, which selects the fastest algorithm from up to 100 candidates from cuBLASLt's suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7%, +26.0%, +22.4%, and +15.9% for torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defense That Attacks: How Robust Models Become Better Attackers</title>
<link>https://arxiv.org/abs/2512.02830</link>
<guid>https://arxiv.org/abs/2512.02830</guid>
<content:encoded><![CDATA[
arXiv:2512.02830v3 Announce Type: replace-cross 
Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning</title>
<link>https://arxiv.org/abs/2512.03343</link>
<guid>https://arxiv.org/abs/2512.03343</guid>
<content:encoded><![CDATA[
arXiv:2512.03343v2 Announce Type: replace-cross 
Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from Topic Drift where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning. While scaling model size mitigates this, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary Idea Head trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models</title>
<link>https://arxiv.org/abs/2511.18271</link>
<guid>https://arxiv.org/abs/2511.18271</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image models, implicit world knowledge, physical causal reasoning, PicWorld benchmark, multi-agent evaluator  

<br /><br />Summary:  
1. Current text-to-image (T2I) models produce photorealistic, instruction-following images but often fail on prompts requiring implicit world knowledge.  
2. Existing evaluation methods focus mainly on compositional alignment or single-round VQA-based scoring, neglecting critical aspects such as knowledge grounding, multi-physics interactions, and verifiable evidence.  
3. To overcome these gaps, the authors introduce PicWorld, a novel benchmark containing 1,100 prompts across three core categories to comprehensively assess T2I models' understanding of implicit world knowledge and physical causal reasoning.  
4. They propose PW-Agent, an evidence-grounded multi-agent evaluation framework that hierarchically assesses generated images by decomposing prompts into verifiable visual evidence, focusing on physical realism and logical consistency.  
5. Evaluation of 17 mainstream T2I models on PicWorld reveals a universal limitation in their ability to grasp implicit world knowledge and physical causal reasoning, underscoring the necessity for future T2I architectures to incorporate reasoning-aware and knowledge-integrative components. <div>
arXiv:2511.18271v3 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples</title>
<link>https://arxiv.org/abs/2512.09931</link>
<guid>https://arxiv.org/abs/2512.09931</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized learning, AI-generated examples, learner adaptation, Google Gemini, dynamic context<br /><br />Summary:  
This article presents ExaCraft, an AI-driven educational tool designed to generate personalized learning examples that adapt dynamically to the learner's context. Unlike existing AI educational tools, ExaCraft focuses on creating culturally relevant and relatable examples by leveraging user-defined profiles like location, education, profession, and complexity preferences. It integrates Google Gemini AI with a Python Flask API, accessible through a Chrome extension, to deliver tailored content. The system continuously analyzes learner behavior in real-time, considering five critical aspects: struggle indicators, mastery patterns, topic progression history, session boundaries, and overall learning progression signals. ExaCraft’s core innovation lies in its ability to evolve examples from foundational concepts to advanced technical material, thereby supporting learners through their knowledge growth. The system reacts effectively to repeated topics, regeneration requests, and shifting learning patterns across different educational scenarios. A demonstration is provided to showcase how examples adapt responsively, ensuring sustained engagement and relevance. This dynamic, context-aware approach aims to enhance learning effectiveness by meeting individual needs more precisely and motivating learners through personalized educational experiences. <div>
arXiv:2512.09931v1 Announce Type: new 
Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Suzume-chan: Your Personal Navigator as an Embodied Information Hub</title>
<link>https://arxiv.org/abs/2512.09932</link>
<guid>https://arxiv.org/abs/2512.09932</guid>
<content:encoded><![CDATA[
<div> Embodied Information Hub, Social Presence Theory, Suzume-chan, retrieval-augmented generation, human-centered knowledge sharing<br /><br />Summary:<br /><br />This article explores the challenge of accessing expert knowledge in a way that fosters deep understanding and connection, highlighting the limitations of current digital tools that often lack a sense of presence. It applies Social Presence Theory to emphasize how a feeling of "being together" can enhance communication quality. To address these issues, the authors propose an "Embodied Information Hub," a novel approach that integrates physical and conversational interactions for knowledge sharing. The proposed prototype, Suzume-chan, is a small, soft AI agent operating locally, equipped with a language model and retrieval-augmented generation (RAG) capabilities. Suzume-chan learns from spoken explanations and engages users through dialogue, thereby reducing psychological distance between the knowledge source and the learner. This design aims to create a warmer, more human-centered experience for sharing expert knowledge, bridging the gap between digital access to information and the rich interpersonal connection needed for effective communication. The study contributes a new framework and a working prototype that demonstrate how embodied AI agents can improve the quality of real-time knowledge sharing. <div>
arXiv:2512.09932v1 Announce Type: new 
Abstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of "being together" enhances communication. An "Embodied Information Hub" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Health Misinformation Detection with Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.09935</link>
<guid>https://arxiv.org/abs/2512.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: health misinformation, fact-checking, agreement score, multi-agent debate, large language models<br /><br />Summary:<br /><br />1. The paper addresses the crucial challenge of fact-checking health-related claims amid the growing problem of online misinformation.<br /><br />2. It introduces a novel two-stage framework designed to improve health misinformation detection by combining evidence retrieval and reasoning.<br /><br />3. In the first stage, large language models (LLMs) independently assess retrieved articles and generate an aggregated agreement score, quantifying the consensus across evidence.<br /><br />4. If the agreement score falls below a set threshold, indicating conflicting or insufficient consensus, the system triggers a second stage involving a structured multi-agent debate.<br /><br />5. During this debate, multiple agents collaboratively synthesize the conflicting evidence, developing carefully reasoned verdicts supported by explicit justifications.<br /><br />6. Experimental evaluations demonstrate that this two-stage approach outperforms baseline methods, validating the effectiveness of combining automated scoring with collaborative reasoning.<br /><br />7. The study highlights the potential for advanced LLMs and structured multi-agent interactions to enhance the accuracy and reliability of complex health-related claim verification tasks. <div>
arXiv:2512.09935v1 Announce Type: new 
Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting</title>
<link>https://arxiv.org/abs/2512.09944</link>
<guid>https://arxiv.org/abs/2512.09944</guid>
<content:encoded><![CDATA[
<div> Echocardiography, large language model, multi-view, multi-task agent, MIMIC-EchoQA<br /><br />Summary:<br /><br />1. Echocardiography is vital in cardiovascular care but interpreting full studies is complex and done manually across multiple views.  
2. Existing foundation models perform well on individual tasks like view classification, segmentation, or disease prediction but lack unified, clinically coherent assessments.  
3. The paper introduces Echo-CoPilot, a multi-view, multi-task agent that leverages a large language model to coordinate specialized echocardiography tools.  
4. Echo-CoPilot operates in a ReAct-style loop, decomposing clinician queries to execute view recognition, segmentation, measurement, disease prediction, and report synthesis, integrating results into guideline-aware, narrative clinical summaries.  
5. Evaluation on the public MIMIC-EchoQA benchmark shows Echo-CoPilot achieves 50.8% accuracy, surpassing both general and biomedical video vision-language models.  
6. Qualitative results demonstrate the agent’s ability to resolve challenging clinical cases by combining quantitative measurements and physiological context, such as borderline left ventricular hypertrophy and pericardial effusion severity.  
7. The authors plan to release the Echo-CoPilot code upon paper acceptance to facilitate clinical and research applications in echocardiography. <div>
arXiv:2512.09944v1 Announce Type: new 
Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Hierarchical Multiplex</title>
<link>https://arxiv.org/abs/2512.09976</link>
<guid>https://arxiv.org/abs/2512.09976</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy optimization, FCM causality, logical implication, service process design, multiplex<br /><br />Summary:<br /><br />1. This paper introduces a new fuzzy optimization framework that extends FCM (Fuzzy Cognitive Map) causality to improve modeling and analysis.<br /><br />2. The proposed model leverages dynamic processes to map data into metrics and uses a multiplex structure to examine the logical implications and hierarchy of concepts.<br /><br />3. It is primarily a theoretical work presenting the underlying logic and mathematics of the framework in a clear and coherent way.<br /><br />4. The framework is designed specifically to optimize information transmission within service process design, aiming to enhance service optimization.<br /><br />5. Lastly, the paper provides a thorough analysis of the FHM (Fuzzy Hierarchical Model), following logical steps in a straightforward and elegant manner to validate the framework's approach.<br /><br />This work lays the foundation for future applied research by introducing a novel theoretical paradigm that integrates dynamics, multiplex logic, and fuzzy causality for service optimization contexts. <div>
arXiv:2512.09976v1 Announce Type: new 
Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring LLMs for Scientific Information Extraction Using The SciEx Framework</title>
<link>https://arxiv.org/abs/2512.10004</link>
<guid>https://arxiv.org/abs/2512.10004</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Scientific Information Extraction, Multi-modal Retrieval, Modular Framework, Data Aggregation  

<br /><br />Summary:  
This paper presents SciEx, a modular and composable framework designed to improve the automation of scientific information extraction using large language models (LLMs). First, it addresses common challenges in scientific literature such as handling long-context documents, multi-modal content (e.g., text, figures), and the difficulty of standardizing heterogeneous fine-grained information from multiple publications. Second, SciEx’s architecture decouples core components—PDF parsing, multi-modal retrieval, extraction, and aggregation—allowing on-demand data extraction and making the system highly extensible. Third, the framework supports flexibility by enabling easy integration of new models, prompting strategies, and reasoning mechanisms, which is valuable when the target data schema or extraction ontology changes rapidly. Fourth, SciEx is evaluated on datasets covering three scientific domains, measuring its accuracy and consistency in extracting fine-grained scientific information. Finally, the study provides practical insights into the strengths of LLM-based pipelines in this context, while also identifying current limitations, helping guide future improvements in scientific data processing workflows. <div>
arXiv:2512.10004v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2512.10034</link>
<guid>https://arxiv.org/abs/2512.10034</guid>
<content:encoded><![CDATA[
<div> MD simulations, protein-ligand complexes, automation, DynaMate, free energy calculations<br /><br />Summary:<br /><br />1. Force field-based molecular dynamics (MD) simulations are crucial for studying biomolecular systems such as proteins and protein-ligand complexes, important in drug discovery and protein engineering.<br /><br />2. Setting up MD simulations is technically complex, involving parameterization, input preparation, and software configurations, which limits widespread and efficient use.<br /><br />3. Agentic large language models (LLMs) have shown promise in automating multi-step scientific tasks but have not yet been applied to fully automate protein-ligand MD workflows.<br /><br />4. The study introduces DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for proteins and protein-ligand systems, including free energy binding affinity calculations with the MM/PB(GB)SA method.<br /><br />5. DynaMate integrates dynamic tool usage, web search, PaperQA, and self-correcting behaviors through three specialized modules that plan experiments, run simulations, and analyze results.<br /><br />6. Tested on twelve benchmark systems of varying complexity, DynaMate demonstrated reliability in executing MD simulations, correcting runtime errors through iterative reasoning, and generating meaningful analyses of protein-ligand interactions.<br /><br />7. This automated framework offers a pathway towards standardized, scalable, and time-efficient molecular modeling pipelines to accelerate future biomolecular research and drug design applications. <div>
arXiv:2512.10034v1 Announce Type: new 
Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</title>
<link>https://arxiv.org/abs/2512.10046</link>
<guid>https://arxiv.org/abs/2512.10046</guid>
<content:encoded><![CDATA[
<div> Simulation platform, urban environments, multimodal instruction, multi-robot collaboration, vision-language models  

<br /><br />Summary:  
This paper introduces SimWorld-Robotics (SWR), a cutting-edge simulation platform built on Unreal Engine 5 designed for embodied AI research in large-scale, photorealistic urban environments. Unlike previous indoor-focused studies, SWR procedurally generates limitless complex urban scenes populated with dynamic elements such as pedestrians and traffic systems, achieving superior realism, complexity, and scalability. It supports multi-robot control and communication, enabling diverse robotic interactions. The authors propose two new challenging benchmarks using SWR: (1) a multimodal instruction-following navigation task where robots interpret vision-language commands to safely navigate amid pedestrians and traffic, and (2) a multi-agent search task requiring two robots to communicate and coordinate to find and meet each other. These benchmarks assess critical robotic capabilities such as multimodal instruction grounding, 3D spatial reasoning in expansive environments, safe long-range navigation in populated urban settings, multi-robot cooperation, and grounded communication. Experimental evaluations reveal that current state-of-the-art models, including vision-language models, underperform on these tasks due to insufficient perception, reasoning, and planning abilities in complex urban scenarios. The work highlights the need for more robust models capable of handling real-world, dynamic urban challenges in robotics. <div>
arXiv:2512.10046v1 Announce Type: new 
Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning</title>
<link>https://arxiv.org/abs/2512.10054</link>
<guid>https://arxiv.org/abs/2512.10054</guid>
<content:encoded><![CDATA[
<div> Parallel Decoder Transformer, Speculative Note Conditioning, autoregressive decoding, coherence drift, structured parallel generation<br /><br />Summary:<br /><br />1. Autoregressive decoding in Large Language Models (LLMs) is sequential, causing latency that increases linearly with output length. 2. Existing "Decomposition-and-Fill" methods like Skeleton-of-Thought attempt parallel generation but face coherence drift due to lack of communication between parallel streams. 3. The authors propose the Parallel Decoder Transformer (PDT), an architecture that incorporates coordination mechanisms directly during inference without retraining the base model. 4. PDT uses Speculative Note Conditioning (SNC) adapters that enable parallel decoding streams to synchronize through a shared dynamic latent space, treating coordination as a speculative consensus problem. 5. Sibling streams broadcast semantic notes on a global bus gated by a learned verification head, allowing effective synchronization. 6. Experiments using a frozen 20B-parameter model over 50,000 training steps show PDT achieves 77.8% precision in coverage prediction and recovers approximate serial semantics. 7. PDT offers an efficient, scalable alternative to fine-tuning full LLMs, enabling structured parallel generation with self-correction and without modifying trunk weights. <div>
arXiv:2512.10054v1 Announce Type: new 
Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research</title>
<link>https://arxiv.org/abs/2512.10058</link>
<guid>https://arxiv.org/abs/2512.10058</guid>
<content:encoded><![CDATA[
<div> Keywords: AI alignment, AI safety, AI ethics, interdisciplinary collaboration, bibliometric analysis<br /><br />Summary:<br /><br />1. The article addresses the growing urgency of aligning AI systems to be harmless amidst rapid advancements in AI capabilities.  
2. It identifies a significant divide between two research communities focused on AI alignment: AI safety, which emphasizes scaled intelligence, existential risks, and deceptive behavior, and AI ethics, which concentrates on present harms, social bias, and production pipeline flaws.  
3. These communities have evolved largely independently with differing definitions of what alignment means, influenced by separate methodologies, institutional settings, and disciplinary histories.  
4. A large-scale quantitative study analyzing 6,442 papers from major ML and NLP conferences (2020-2025) reveals that over 80% of collaborations occur within either the safety or ethics communities, with cross-field collaborations highly concentrated among approximately 5% of papers acting as bridges.  
5. The study shows that removing a small group of these "broker" actors greatly increases the separation between fields, highlighting a fragile cross-disciplinary exchange largely dependent on few individuals.  
6. The article concludes that addressing the conceptual and institutional divide between AI safety and ethics is crucial. It advocates for integrating technical safety research with normative ethics via shared benchmarks, cross-institutional venues, and mixed-method approaches to build AI systems that are robust, just, and aligned. <div>
arXiv:2512.10058v1 Announce Type: new 
Abstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, "aligned" systems increasingly urgent. Yet research on alignment has diverged along two largely parallel tracks: safety--centered on scaled intelligence, deceptive or scheming behaviors, and existential risk--and ethics--focused on present harms, the reproduction of social bias, and flaws in production pipelines. Although both communities warn of insufficient investment in alignment, they disagree on what alignment means or ought to mean. As a result, their efforts have evolved in relative isolation, shaped by distinct methodologies, institutional homes, and disciplinary genealogies.
  We present a large-scale, quantitative study showing the structural split between AI safety and AI ethics. Using a bibliometric and co-authorship network analysis of 6,442 papers from twelve major ML and NLP conferences (2020-2025), we find that over 80% of collaborations occur within either the safety or ethics communities, and cross-field connectivity is highly concentrated: roughly 5% of papers account for more than 85% of bridging links. Removing a small number of these brokers sharply increases segregation, indicating that cross-disciplinary exchange depends on a handful of actors rather than broad, distributed collaboration. These results show that the safety-ethics divide is not only conceptual but institutional, with implications for research agendas, policy, and venues. We argue that integrating technical safety work with normative ethics--via shared benchmarks, cross-institutional venues, and mixed-method methodologies--is essential for building AI systems that are both robust and just.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear socio-demographic representations emerge in Large Language Models from indirect cues</title>
<link>https://arxiv.org/abs/2512.10065</link>
<guid>https://arxiv.org/abs/2512.10065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, sociodemographic attributes, implicit bias, activation space, fairness<br /><br />Summary:  
This study explores how large language models (LLMs) encode sociodemographic attributes of users based on indirect cues such as names and occupations. The researchers find that LLMs develop linear representations of demographics within the models' activation spaces, where stereotypical attributes align with interpretable geometric directions. By probing residual streams across layers in four transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) using explicit demographic information, they demonstrate that demographic predictions can also be made from implicit cues. For example, names activate gender and race representations that align with census data, while occupations correspond with workforce statistics. These linear demographic embeddings explain how LLMs implicitly infer personal attributes during conversations. Furthermore, these implicit representations influence downstream outputs, such as career recommendations, illustrating an active shaping of model behavior by these biases. Importantly, the study reveals that LLMs passing conventional bias benchmarks may still harbor and leverage implicit biases, raising concerns about fairness and ethical considerations when deploying such models at scale in real-world applications. <div>
arXiv:2512.10065v1 Announce Type: new 
Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit</title>
<link>https://arxiv.org/abs/2512.10092</link>
<guid>https://arxiv.org/abs/2512.10092</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse autoencoders, dataset analysis, model biases, interpretable embeddings, text clustering<br /><br />Summary:<br /><br />This work introduces sparse autoencoders (SAEs) to create interpretable embeddings called SAE embeddings for large-scale text corpus analysis, aiming to address challenges in detecting undesirable model behaviors and biases efficiently. First, SAE embeddings offer a cost-effective and reliable alternative to LLM-based annotation and dense embedding clustering by providing dimensions that correspond to meaningful concepts. Second, using a large hypothesis space of SAEs, the method reveals semantic differences between datasets and uncovers unexpected correlations among document concepts. For example, comparing multiple frontier models shows that Grok-4 clarifies ambiguities more frequently than nine others. Third, SAE embeddings demonstrate greater controllability than dense embeddings, allowing targeted analyses such as clustering documents along user-defined axes by filtering concepts. Fourth, SAE embeddings outperform dense embeddings in retrieval tasks based on specific properties, enhancing interpretability in data indexing. Finally, the utility of SAE embeddings is showcased in two case studies: tracing the evolution of OpenAI models’ behavior over time and identifying "trigger" phrases learned by the Tulu-3 model from its training set. Overall, SAEs emerge as a versatile, interpretable tool for unstructured data analysis, emphasizing the importance of understanding models through their underlying data characteristics. <div>
arXiv:2512.10092v1 Announce Type: new 
Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust AI Security and Alignment: A Sisyphean Endeavor?</title>
<link>https://arxiv.org/abs/2512.10100</link>
<guid>https://arxiv.org/abs/2512.10100</guid>
<content:encoded><![CDATA[
<div> Keywords: AI security, alignment, G\"odel's incompleteness theorem, information-theoretic limitations, cognitive reasoning<br /><br />Summary:<br /><br />This manuscript explores fundamental limitations in the robustness of AI security and alignment by applying G\"odel's incompleteness theorem in the context of artificial intelligence. First, it establishes that there are intrinsic, information-theoretic barriers that prevent perfect security and alignment guarantees in AI systems. Second, by extending a classical mathematical theorem to AI, the work provides a formal foundation for understanding why certain AI risks and vulnerabilities cannot be fully eliminated. Third, it emphasizes the critical importance of recognizing these theoretical limitations when advancing and adopting AI technologies responsibly. Fourth, the paper discusses practical strategies and methodologies that can partially mitigate the challenges posed by these limitations, suggesting ways to enhance AI robustness despite fundamental constraints. Finally, it extends these insights to broader implications on cognitive reasoning capacities of AI systems, demonstrating inherent restrictions in AI’s ability to fully reason and adapt. This research thus contributes to a deeper theoretical understanding of AI vulnerabilities and informs the design of safer and more reliable AI architectures. <div>
arXiv:2512.10100v1 Announce Type: new 
Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending G\"odel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups</title>
<link>https://arxiv.org/abs/2512.10105</link>
<guid>https://arxiv.org/abs/2512.10105</guid>
<content:encoded><![CDATA[
<div> Keywords: conspiratorial discourse, Telegram groups, Signed Belief Graph Neural Network, narrative archetypes, belief-driven analysis  

<br /><br />Summary:  
1. This study investigates conspiratorial narratives within Singapore-based Telegram groups, revealing that such content permeates everyday conversations rather than existing solely in isolated echo chambers.  
2. A two-stage computational framework is proposed: first, RoBERTa-large is fine-tuned to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on a dataset of 2,000 expert-labeled messages.  
3. Second, the authors construct a signed belief graph where nodes represent messages and edges denote belief alignment weighted by textual similarity.  
4. They introduce the Signed Belief Graph Neural Network (SiBeGNN), which employs a Sign Disentanglement Loss to distinguish ideological alignment from stylistic features in embeddings.  
5. Hierarchical clustering on the generated embeddings identifies seven distinct narrative archetypes across 553,648 messages, including legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat.  
6. SiBeGNN outperforms baseline methods in clustering quality, supported by high inter-rater agreement (88%) in expert evaluations.  
7. Analysis indicates conspiratorial messages appear not only in overtly skeptical clusters but also within routine discussions on finance, law, and everyday topics, challenging existing notions of online radicalization.  
8. The framework enhances computational approaches for belief-driven discourse analysis and holds potential applications in stance detection, political communication studies, and content moderation policy. <div>
arXiv:2512.10105v1 Announce Type: new 
Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice</title>
<link>https://arxiv.org/abs/2512.10114</link>
<guid>https://arxiv.org/abs/2512.10114</guid>
<content:encoded><![CDATA[
<div> agriculture, Large Language Models, Retrieval-Augmented Generation, geospatial metadata, hallucination  

<br /><br />Summary:  
This paper introduces AgriRegion, a Retrieval-Augmented Generation (RAG) framework tailored for delivering high-fidelity, region-specific agricultural advice. The motivation stems from the challenge that general-purpose Large Language Models (LLMs) often produce contextual hallucinations when used in agriculture, providing advice that may be non-factual or suitable in one region but harmful in another due to differences in soil, climate, and regulations. AgriRegion addresses this by incorporating a geospatial metadata injection layer and a region-prioritized re-ranking mechanism to ensure retrieval focuses on locally relevant, verified agricultural data. The knowledge base is restricted exclusively to trusted local agricultural extension services, which enforces geo-spatial constraints during information retrieval. This targeted approach ensures accuracy in critical agricultural decisions such as planting schedules, pest management, and fertilization strategies. The authors also develop a novel benchmark dataset called AgriRegion-Eval, containing 160 domain-specific questions covering 12 agricultural subfields, for comprehensive assessment. Experimental results demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLM systems and significantly enhances user trust scores, validating its effectiveness for region-aware agricultural advisory tasks. <div>
arXiv:2512.10114v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 2025 Foundation Model Transparency Index</title>
<link>https://arxiv.org/abs/2512.10169</link>
<guid>https://arxiv.org/abs/2512.10169</guid>
<content:encoded><![CDATA[
<div> Foundation Model Transparency, 2025 FMTI, training data opacity, policy impact, company rankings  

<br /><br />Summary:  
The 2025 Foundation Model Transparency Index (FMTI) assesses the transparency practices of foundation model developers, marking its third annual edition. This year, it introduces new indicators focusing on data acquisition, usage data, and monitoring, while evaluating companies such as Alibaba, DeepSeek, and xAI for the first time. Contrary to the improvement observed in the 2024 FMTI, transparency scores have declined significantly, with the average dropping from 58 to 40 out of 100. Major opacity exists around training data, training compute, and the post-deployment usage and impacts of flagship models. IBM emerges as an exceptional positive outlier, scoring 95, whereas xAI and Midjourney rank lowest with scores of 14. The five Frontier Model Forum members score mid-range, suggesting they manage reputational risks without striving to lead in transparency. The report highlights that as global policymakers increasingly require transparency, understanding the current state and potential shifts due to policy is crucial. It also emphasizes the need for more robust policy actions to tackle critical transparency gaps in the foundation model development ecosystem. <div>
arXiv:2512.10169v1 Announce Type: new 
Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment</title>
<link>https://arxiv.org/abs/2512.10206</link>
<guid>https://arxiv.org/abs/2512.10206</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical pathways, large language models, hospital simulation, evaluation framework, medical AI agents  

<br /><br />Summary:  
This paper addresses the limitations of current benchmarks which largely assess large language models (LLMs) on static exams or isolated dialogues, failing to capture the complexity of real-world clinical care. It introduces CP-Env, a controllable hospital environment that simulates a comprehensive clinical ecosystem with interactive patient and physician agents. CP-Env supports scenarios such as triage, specialist consultations, diagnostic testing, and multidisciplinary team meetings, enabling long-horizon, branching clinical pathway simulations mimicking real hospital workflows. The authors propose a three-tiered evaluation framework for LLMs comprising Clinical Efficacy, Process Competency, and Professional Ethics. Experimental results show that most LLMs struggle with the complexity of clinical pathways, often producing hallucinations and missing critical diagnostic details. The study finds that increasing the number of reasoning steps does not always improve performance and can be detrimental. Conversely, top-performing models tend to rely less on external tools by leveraging internalized medical knowledge. CP-Env provides a valuable benchmark and evaluation tools to foster improved development of medical AI agents capable of dynamic, end-to-end clinical decision-making. The resources for CP-Env are openly available to encourage further research in this domain. <div>
arXiv:2512.10206v1 Announce Type: new 
Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An exploration for higher efficiency in multi objective optimisation with reinforcement learning</title>
<link>https://arxiv.org/abs/2512.10208</link>
<guid>https://arxiv.org/abs/2512.10208</guid>
<content:encoded><![CDATA[
<div> Keywords: optimisation, multi-objective reinforcement learning, operators, search efficiency, multi-objective optimisation<br /><br />Summary:  
This paper addresses the ongoing challenge of improving efficiency in optimisation and search processes, which significantly impacts the performance of optimisation algorithms. It highlights the promising strategy of using a pool of operators instead of relying on a single operator for move operations within a neighbourhood. However, identifying an optimum or near-optimum sequence of these operators requires further research. The study notes that while many approaches exist for single-objective optimisation, multi-objective cases have been less explored in this context. To fill this gap, the paper proposes a generalised approach based on multi-objective reinforcement learning, aiming to leverage and generalise past experiences to inform operator selection and sequencing. The approach is intended to demonstrate enhanced efficiency and quality of solutions in multi-objective optimisation problems. The paper provides an overview of the proposed methodology, with some stages completed and others pending, underscoring the potential of multi-objective reinforcement learning to improve optimisation processes that involve multiple conflicting objectives. <div>
arXiv:2512.10208v1 Announce Type: new 
Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs</title>
<link>https://arxiv.org/abs/2512.10211</link>
<guid>https://arxiv.org/abs/2512.10211</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed-Integer Linear Programs, Predict-and-Search, machine learning, parametric MIPs, ID-PaS

<br /><br />Summary:  
This paper focuses on enhancing Mixed-Integer Linear Programs (MIPs), which are widely used for modeling combinatorial optimization problems. It builds upon the Predict-and-Search (PaS) method that leverages machine learning (ML) models to predict promising variable assignments and guide search algorithms toward optimal solutions. Despite its recent successes, existing PaS approaches are limited to binary variables and do not consider fixed variables that frequently occur in real-world scenarios. To address these challenges, the authors propose an extension of the PaS framework to handle parametric MIPs, which include heterogeneous variable types. They introduce ID-PaS, an identity-aware learning framework designed to improve the ML model’s ability to manage variable heterogeneity effectively. The paper reports extensive experiments on several large-scale, real-world optimization problems, demonstrating that ID-PaS consistently outperforms both the leading commercial solver Gurobi and the traditional PaS method. This advancement highlights the potential of combining identity-aware ML techniques with parametric MIP frameworks to improve solution quality and efficiency in practical applications. <div>
arXiv:2512.10211v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Thinking Enhances Missing Information Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2512.10273</link>
<guid>https://arxiv.org/abs/2512.10273</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reverse thinking, missing information detection, backward reasoning, reasoning robustness<br /><br />Summary:<br /><br />This paper addresses the challenge that Large Language Models (LLMs) face in handling tasks with missing information, where they tend to produce incomplete or inaccurate responses. Traditional forward reasoning techniques such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have improved structured problem-solving, but often fail to systematically detect and recover omitted information. To overcome this limitation, the authors propose a novel reverse thinking framework that guides LLMs through backward reasoning processes. This method focuses on identifying necessary conditions and pinpointing missing elements by thinking in reverse, transforming the missing information detection problem into a more tractable backward reasoning task. Experimental results show that the reverse thinking approach significantly outperforms traditional forward reasoning strategies in terms of accuracy. The approach not only enhances the logical completeness of LLM-generated outputs but also improves reasoning robustness when dealing with incomplete data. These findings suggest that integrating reverse thinking methodologies presents a promising direction for advancing the reasoning capabilities of LLMs, particularly in scenarios involving incomplete or omitted information, thereby addressing a key limitation of existing models. <div>
arXiv:2512.10273v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuronal Attention Circuit (NAC) for Representation Learning</title>
<link>https://arxiv.org/abs/2512.10282</link>
<guid>https://arxiv.org/abs/2512.10282</guid>
<content:encoded><![CDATA[
<div> Keywords: Neuronal Attention Circuit, continuous-time attention, biological plausibility, sparse gating, time-series classification  

<br /><br />Summary:  
This paper introduces the Neuronal Attention Circuit (NAC), a novel continuous-time (CT) attention mechanism inspired by the neuronal wiring of *C. elegans*. NAC reformulates the computation of attention logits as the solution of a linear first-order ODE with nonlinear, interconnected gates, providing a biologically plausible alternative to conventional discrete attention models. The architecture replaces dense projections with sparse sensory gates for key-query interactions and employs a sparse backbone network with dual heads to compute content-target and learnable time-constant gates, facilitating adaptive attention dynamics. NAC supports three distinct modes for attention logit computation: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation, promoting flexibility and computational efficiency. To enhance memory efficiency, the authors implement a sparse Top-K pairwise concatenation strategy that selectively filters key-query pairs. Theoretical analysis guarantees system stability, bounded approximation errors, and universal approximation capabilities. Empirical evaluations across various domains—irregular time-series classification, autonomous vehicle lane-keeping, and industrial prognostics—demonstrate that NAC matches or surpasses baseline performance in accuracy while balancing runtime and memory use more efficiently than many continuous-time attention methods. This work paves the way for biologically inspired, memory-efficient continuous-time attention mechanisms suitable for real-world, irregular data streams. <div>
arXiv:2512.10282v1 Announce Type: new 
Abstract: Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \textit{content-target} and \textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules</title>
<link>https://arxiv.org/abs/2512.10300</link>
<guid>https://arxiv.org/abs/2512.10300</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, interpretability, attention heads, multimodal reasoning, CogVision<br /><br />Summary:<br />1. The paper addresses the interpretability of vision-language models (VLMs), which have shown strong performance on multimodal benchmarks but lack transparent internal mechanisms.<br />2. It introduces a novel framework to analyze the functional roles of attention heads in VLMs during multimodal reasoning tasks.<br />3. To facilitate this analysis, the authors present CogVision, a new dataset that breaks down complex multimodal questions into sequential subquestions reflecting human-like chain-of-thought reasoning, with each subquestion linked to specific receptive or cognitive functions.<br />4. Using a probing-based method, they identify specialized attention heads responsible for high-level visual reception, inference, and other cognitive functions, referring to these as functional heads.<br />5. Their study across multiple VLM architectures finds that functional heads are sparsely distributed, vary by function and model, and play roles in mediating hierarchical interactions.<br />6. Intervention experiments reveal the critical impact of these heads: removing functional heads degrades performance, while emphasizing them improves accuracy.<br />7. The findings offer new insights into the cognitive organization of VLMs and point toward designing models with improved, human-aligned perceptual and reasoning capabilities. <div>
arXiv:2512.10300v1 Announce Type: new 
Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance</title>
<link>https://arxiv.org/abs/2512.10304</link>
<guid>https://arxiv.org/abs/2512.10304</guid>
<content:encoded><![CDATA[
<div> Keywords: Trustworthy AI, Governance, Control-Panel architecture, Accountability, AI Assurance Framework<br /><br />Summary:  
1. The paper addresses the increasing challenge of bridging the gap between AI technical capabilities and institutional accountability as AI assumes more consequential decision-making roles.  
2. It argues that ethical guidelines are not enough and emphasizes the need for governance architectures embedded directly into AI system operations.  
3. The authors propose the "Ten Criteria for Trustworthy Orchestration AI," a comprehensive assurance framework that integrates human input, semantic coherence, audit trails, and provenance integrity within a unified Control-Panel architecture.  
4. Unlike traditional AI approaches focused mainly on AI-to-AI coordination, this framework offers governance coverage over the entire AI ecosystem including AI components, users, and human stakeholders.  
5. Drawing inspiration from international standards and Australia’s National Framework for AI Assurance, the framework demonstrates that trustworthiness can be engineered systematically to ensure AI systems are verifiable, transparent, reproducible, and meaningfully controlled by humans. <div>
arXiv:2512.10304v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck</title>
<link>https://arxiv.org/abs/2512.10305</link>
<guid>https://arxiv.org/abs/2512.10305</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative perception, Information Bottleneck, communication efficiency, autonomous driving, information purification  

<br /><br />Summary:  
This paper addresses the critical challenge of achieving precise environmental perception in autonomous driving systems while overcoming communication constraints inherent in collaborative perception frameworks. The authors identify a fundamental trade-off between communication bandwidth and perception performance. Current methods typically rely on transmitting megabyte-scale data, which is impractical under real-world network limitations. To resolve this, the authors propose InfoCom, a novel framework grounded in extended Information Bottleneck principles, which is the first to theoretically model communication-efficient collaborative perception. InfoCom introduces an information purification paradigm that optimizes extraction of the minimal yet sufficient task-critical information. Its key components include: (i) Information-Aware Encoding that compresses features into minimal messages retaining essential perception information; (ii) Sparse Mask Generation that pinpoints important spatial cues at negligible communication cost; and (iii) Multi-Scale Decoding that reconstructs perceptual information through mask-guided processes rather than standard feature reconstruction. Extensive experiments across multiple datasets validate that InfoCom achieves near-lossless perception performance while dramatically reducing communication overhead from megabytes to kilobytes—demonstrating 440-fold and 90-fold reductions compared to state-of-the-art methods Where2comm and ERMVP, respectively. This approach unlocks practical, highly efficient collaborative perception for autonomous driving under stringent network constraints. <div>
arXiv:2512.10305v1 Announce Type: new 
Abstract: Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EpiPlanAgent: Agentic Automated Epidemic Response Planning</title>
<link>https://arxiv.org/abs/2512.10313</link>
<guid>https://arxiv.org/abs/2512.10313</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic response, agent-based system, large language models, public health planning, simulation  

<br /><br />Summary:  
This study presents EpiPlanAgent, an innovative agent-based system designed to automate epidemic response planning using large language models (LLMs). The system leverages a multi-agent framework incorporating task decomposition, knowledge grounding, and simulation modules to generate and validate digital emergency response plans efficiently. Public health professionals evaluated EpiPlanAgent through real-world outbreak scenarios in controlled settings, demonstrating its practical applicability. Results showed that EpiPlanAgent significantly enhanced the completeness and alignment of response plans with established guidelines, outperforming traditional manual methods. Additionally, the system greatly reduced the time required to develop such plans, highlighting its operational efficiency. Expert assessments further confirmed a high consistency between AI-generated plans and those created by human experts, supporting the system’s reliability. Feedback from users indicated strong perceived utility and acceptance of the technology within public health workflows. In conclusion, EpiPlanAgent offers a scalable, effective solution for intelligent epidemic response planning, showcasing the transformative potential of agentic AI in enhancing public health preparedness and response capabilities. <div>
arXiv:2512.10313v1 Announce Type: new 
Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.10322</link>
<guid>https://arxiv.org/abs/2512.10322</guid>
<content:encoded><![CDATA[
<div> Vision-and-Language Navigation, General Scene Adaptation, User Feedback, Continual Learning, Memory-bank Warm-start  

<br /><br />Summary:  
This paper addresses Vision-and-Language Navigation (VLN), where agents navigate environments based on natural language instructions. It focuses on General Scene Adaptation for VLN (GSA-VLN), which aims for continual, environment-specific adaptation rather than zero-shot generalization, bridging the gap between static benchmarks and real-world deployment. The authors identify that current GSA-VLN methods do not utilize user feedback and instead rely on unsupervised adaptation from repeated environmental exposure. To improve adaptation quality, they propose a user-feedback-driven adaptation framework that systematically integrates human interactions—such as navigation instructions and corrective signals—into the continual learning process. This approach converts user feedback into high-quality, environment-aligned training data for efficient adaptation. Additionally, a memory-bank warm-start mechanism is introduced to reuse previously acquired environmental knowledge, reducing cold-start performance degradation and ensuring stable system redeployment. Experimental results on the GSA-R2R benchmark demonstrate that their method consistently outperforms strong baselines like GR-DUET, leading to improved navigation success rates and path efficiency. The memory-bank warm start stabilizes early navigation performance and mitigates performance drops after updates. The framework's robustness and generality are validated under both continual and hybrid adaptation settings, showing sustained improvements across diverse deployment scenarios. <div>
arXiv:2512.10322v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering</title>
<link>https://arxiv.org/abs/2512.10339</link>
<guid>https://arxiv.org/abs/2512.10339</guid>
<content:encoded><![CDATA[
<div> Inference-time steering, diffusion models, ratio-of-densities, marginal path collapse, adaptive path correction  

<br /><br />Summary:  
This paper addresses a critical failure mode in inference-time steering of pretrained diffusion and flow models, known as Marginal Path Collapse, where intermediate density trajectories become non-normalizable despite valid endpoints. This issue is particularly prevalent when composing heterogeneous models trained on varying noise schedules or datasets, a scenario common in molecular design tasks like flexible-pose scaffold decoration. The authors first derive a simple path existence criterion that exactly predicts when collapse will occur, based solely on noise schedules and exponents. They then introduce Adaptive path Correction with Exponents (ACE), an extension of Feynman-Kac steering which allows time-varying exponents, guaranteeing a valid probability path throughout the entire trajectory. Experiments conducted on a synthetic 2D benchmark and practical molecular design tasks demonstrate that ACE completely eliminates collapse, enabling stable and high-guidance compositional generation. ACE outperforms constant-exponent baselines and even specialized scaffold decoration models in terms of distributional and docking metrics. Overall, this work transforms ratio-of-densities steering from an unstable heuristic into a reliable method for controllable generative modeling with heterogeneous expert models. <div>
arXiv:2512.10339v1 Announce Type: new 
Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature</title>
<link>https://arxiv.org/abs/2512.10348</link>
<guid>https://arxiv.org/abs/2512.10348</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Unlearning, Vertical Federated Learning, Representation Misdirection, Data Privacy, Split Learning<br /><br />Summary:<br /><br />This paper addresses the challenge of federated unlearning under data-protection regulations like GDPR, specifically targeting Vertical Federated Learning (VFL) systems, where data are partitioned by features rather than samples. Unlike Horizontal Federated Learning (HFL), existing unlearning methods are ineffective in VFL due to its feature-partitioned architecture. To overcome this, the authors propose REMISVFU, a novel plug-and-play representation misdirection framework designed for fast, client-level unlearning in splitVFL setups. The core idea involves the party requested to be forgotten collapsing its encoder output onto a randomly sampled anchor on the unit sphere, which breaks the statistical relationship between its local features and the global model. To retain the utility of the model for the non-forgotten parties, the server performs a joint optimization involving two losses: a retention loss to preserve model performance and a forgetting loss to remove unwanted contributions. These gradients are aligned via orthogonal projection to avoid destructive interference during training. Extensive experiments on public benchmark datasets demonstrate that REMISVFU effectively reduces back-door attack success to natural class-prior levels while only marginally decreasing clean accuracy by approximately 2.5 percentage points, outperforming current state-of-the-art methods in vertical federated unlearning. <div>
arXiv:2512.10348v1 Announce Type: new 
Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Empowered Representation Learning for Emerging Item Recommendation</title>
<link>https://arxiv.org/abs/2512.10370</link>
<guid>https://arxiv.org/abs/2512.10370</guid>
<content:encoded><![CDATA[
<div> Emerging items, recommendation, representation learning, LLM, meta-learning<br /><br />Summary:<br /><br />This paper addresses the challenge of recommending emerging items, which accumulate interactions gradually over time. Unlike prior approaches that assume emerging items have minimal or no historical interactions, the authors argue this oversimplifies the problem by ignoring the need to maintain item uniqueness while utilizing shared patterns with established items. To solve this, they propose EmerFlow, a framework empowered by large language models (LLMs) to generate distinctive embeddings for emerging items. EmerFlow first enriches raw features of emerging items through LLM-based reasoning, effectively capturing deeper semantic information. Next, it aligns these enriched embeddings with the existing recommendation model’s embedding space to maintain consistency. Finally, it incorporates new interactions using meta-learning techniques, enabling refinement of item embeddings with only limited interaction data. The authors validate EmerFlow through extensive experiments in multiple domains such as movies and pharmaceuticals. Results demonstrate that EmerFlow consistently outperforms existing baseline methods, showcasing its ability to learn expressive and effective representations for emerging items despite sparse interaction history. This work provides a promising direction for dynamic recommendation scenarios involving continuously evolving item catalogs. <div>
arXiv:2512.10370v1 Announce Type: new 
Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management</title>
<link>https://arxiv.org/abs/2512.10371</link>
<guid>https://arxiv.org/abs/2512.10371</guid>
<content:encoded><![CDATA[
<div> Keywords: mobile GUI agents, long-horizon task automation, context management, program-guided approach, global belief state

<br /><br />Summary:  
This paper addresses the challenge of managing interaction history in mobile GUI agents designed for long-horizon task automation, where traditional methods suffer from substantial context overhead and loss of semantic information. The authors propose AgentProg, a novel program-guided context management framework that represents interaction history as a program composed of variables and control flow structures. This organization enables principled decisions about which information to retain or discard, improving context efficiency. Additionally, AgentProg incorporates a global belief state mechanism inspired by the Belief MDP framework to manage partial observability and adapt to unexpected environmental changes. Experimental results on the AndroidWorld benchmark and an extended long-horizon task suite demonstrate that AgentProg achieves state-of-the-art success rates. Importantly, AgentProg maintains robust performance on long-horizon tasks, whereas baseline models suffer from catastrophic performance degradation. The system and codebase have been open-sourced, facilitating further research and application in the domain of mobile GUI agent development. <div>
arXiv:2512.10371v1 Announce Type: new 
Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention</title>
<link>https://arxiv.org/abs/2512.10414</link>
<guid>https://arxiv.org/abs/2512.10414</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, vision-language models, entropy intervention, adversarial sampling, policy exploration<br /><br />Summary:<br /><br />1. The paper addresses improving reasoning capabilities in vision-language models (VLMs) by using reinforcement learning (RL).<br />2. It highlights the limitation of existing RL-based finetuning methods that only control entropy during policy optimization and ignore entropy intervention during RL sampling.<br />3. The authors propose a novel method named Selective-adversarial Entropy Intervention (SaEI), which enhances policy entropy by adversarially distorting the visual input based on the entropy of sampled responses.<br />4. SaEI introduces entropy-guided adversarial sampling (EgAS), which uses entropy as an adversarial objective to generate adversarial gradients that perturb the visual input during RL sampling, thereby expanding the answer space.<br />5. Additionally, token-selective entropy computation (TsEC) is designed to focus the adversarial attack on specific tokens to avoid corrupting factual knowledge within the VLM.<br />6. Extensive experiments on both in-domain and out-of-domain datasets demonstrate that SaEI effectively improves policy exploration and boosts reasoning performance.<br />7. Code for the proposed method will be released upon paper acceptance. <div>
arXiv:2512.10414v1 Announce Type: new 
Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation of the structure of graphs by sequences of instructions</title>
<link>https://arxiv.org/abs/2512.10429</link>
<guid>https://arxiv.org/abs/2512.10429</guid>
<content:encoded><![CDATA[
<div> Keywords: graph representation, adjacency matrix, deep learning, instruction string, reversible transformation<br /><br />Summary:<br /><br />1. Traditional graph representations primarily utilize adjacency matrices, which underpin many algebraic and computational graph processing methods.<br /><br />2. Deep learning language models excel in text processing but struggle with conventional graph representations due to their format.<br /><br />3. The paper proposes a novel graph representation technique that encodes the adjacency matrix as a string of simple, sequential instructions that reconstruct the matrix step by step.<br /><br />4. This new representation is reversible, enabling the transformation from graph to string and back without loss of information.<br /><br />5. The method produces a compact representation that preserves local structural patterns of the graph, making it suitable for processing by deep learning models.<br /><br />6. A preliminary computational experiment demonstrates promising results, suggesting the approach could enhance graph processing performance using deep learning frameworks. <div>
arXiv:2512.10429v1 Announce Type: new 
Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Data Protection for Diffusion Model by Matching Training Trajectory</title>
<link>https://arxiv.org/abs/2512.10433</link>
<guid>https://arxiv.org/abs/2512.10433</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, Targeted Data Protection (TDP), trajectory alignment, fine-tuning, adversarial perturbations  

<br /><br />Summary:  
Recent progress in diffusion models has facilitated easier fine-tuning for personalized text-to-image generation but raised challenges related to unauthorized data use and privacy violations. Current defenses only degrade image quality passively and lack stable control mechanisms. Targeted Data Protection (TDP) aims to actively redirect model outputs towards user-defined concepts but existing approaches rely on snapshot-matching, which poorly controls the entire learning process. The proposed method, TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), is the first to achieve effective TDP by controlling the full fine-tuning trajectory instead of snapshots. Inspired by dataset distillation, TAFAP enforces persistent and verifiable transformations throughout training, preventing the protective effect from being diluted as training continues. Extensive experiments prove that TAFAP enables the first successful targeted transformations in diffusion models, allowing simultaneous control over both identity and visual patterns in generated images. Compared with prior methods, TAFAP significantly improves robustness of redirection toward target concepts while maintaining high image quality. This approach introduces reliable safeguards for privacy, establishing a new framework to control and track modifications in diffusion model outputs. <div>
arXiv:2512.10433v1 Announce Type: new 
Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection</title>
<link>https://arxiv.org/abs/2512.10449</link>
<guid>https://arxiv.org/abs/2512.10449</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, peer review, adversarial attacks, decision flipping, WAVS  

<br /><br />Summary:  
1. The study explores the integration of Large Language Models (LLMs) into scientific peer review, driven by individual reviewer adoption and institutional AI systems like those at AAAI and Stanford's Agents4Science.  
2. The research focuses on the vulnerability of "LLM-as-a-Judge" systems to adversarial PDF manipulations aimed at flipping reject decisions to accept, a novel threat distinct from typical jailbreaks.  
3. To assess this vulnerability, the authors introduce a new evaluation metric named Weighted Adversarial Vulnerability Score (WAVS).  
4. They created a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies, testing these across 13 diverse LLMs including GPT-5, Claude Haiku, and DeepSeek.  
5. Results reveal that obfuscation strategies such as "Maximum Mark Magyk" can successfully manipulate model scores and cause a high rate of decision flips, posing significant risks for peer review reliability.  
6. The authors commit to releasing their dataset and injection framework to encourage further research on safeguarding LLM-based peer review systems. <div>
arXiv:2512.10449v1 Announce Type: new 
Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation</title>
<link>https://arxiv.org/abs/2512.10501</link>
<guid>https://arxiv.org/abs/2512.10501</guid>
<content:encoded><![CDATA[
<div> Procedural Content Generation, Large Language Models, zero-shot parameter configuration, Actor-Critic architecture, 3D map generation<br /><br />Summary: This paper addresses the challenge of configuring Procedural Content Generation (PCG) pipelines, which involve complex and technical parameters that are difficult to control. The authors propose a novel training-free method that leverages off-the-shelf Large Language Models (LLMs) as agents in a zero-shot setup to automatically configure PCG parameters from natural language instructions. The key innovation is an Actor-Critic agent architecture where the Actor suggests parameter configurations and the Critic evaluates and guides iterative refinements toward better alignment with user intents. This iterative reasoning approach bridges the semantic gap between abstract human instructions and the strict parameter requirements of PCG tools. The system is validated through experiments on generating diverse 3D maps, demonstrating superior performance compared to single-agent baselines in producing both diverse and structurally valid environments. Importantly, this framework does not require task-specific fine-tuning of the language models, highlighting its scalability and adaptability to complex software control. Overall, the method presents a scalable paradigm for harnessing general LLMs as autonomous agents to master complex, parameter-rich applications like PCG solely through architectural innovation rather than extensive retraining. <div>
arXiv:2512.10501v1 Announce Type: new 
Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10534</link>
<guid>https://arxiv.org/abs/2512.10534</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, Geometry problem solving, Auxiliary constructions, Reinforcement learning, IMO problems  

<br /><br />Summary:  
This paper introduces InternGeometry, a large language model (LLM) agent designed to solve expert-level geometry problems, including those from the International Mathematical Olympiad (IMO). InternGeometry addresses the key challenge of weak heuristics in auxiliary constructions by iteratively proposing geometric propositions and constructions, verifying them using a symbolic engine, and using feedback to refine future proposals. The system incorporates a dynamic memory mechanism enabling over 200 interactions with the symbolic engine per problem, enhancing its problem-solving capacity. To improve training efficiency, the authors propose Complexity-Boosting Reinforcement Learning (CBRL), which gradually escalates the difficulty of synthesized training problems in stages. Built on the InternThinker-32B foundation model, InternGeometry demonstrates strong performance by solving 44 out of 50 IMO geometry problems spanning 2000 to 2024, surpassing the average performance of gold medalists (40.9 problems solved). Notably, it achieves this using only 13,000 training examples, which is just 0.004% of the data utilized by previous expert systems like AlphaGeometry 2. Additionally, InternGeometry can generate novel auxiliary constructions not found in human solutions. The authors plan to release the model, training data, and symbolic engine to support further research in AI-driven advanced geometry problem solving. <div>
arXiv:2512.10534v1 Announce Type: new 
Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NormCode: A Semi-Formal Language for Context-Isolated AI Planning</title>
<link>https://arxiv.org/abs/2512.10563</link>
<guid>https://arxiv.org/abs/2512.10563</guid>
<content:encoded><![CDATA[
<div> Keywords: NormCode, large language models, context pollution, semiformal language, AI workflow transparency  

<br /><br />Summary:  
The article introduces NormCode, a semiformal language designed to improve multistep workflows using large language models (LLMs), tackling the issue of context pollution where accumulated information across steps leads to hallucinations and confusion. NormCode achieves this by enforcing strict data isolation between steps, ensuring each step only receives explicitly passed inputs and thus eliminating cross-step contamination. It distinctly separates semantic operations, which involve nondeterministic LLM-driven reasoning, from syntactic operations, which entail deterministic data restructuring, enabling clear tracking of cost and reliability. The language is presented in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting a progressive formalization process from initial design to production ready code. The authors validate NormCode through two demonstrations: a base X addition algorithm with perfect accuracy on inputs of arbitrary length, and NormCode’s self-hosted execution of its own five-phase compiler pipeline. Additionally, the system features a working orchestrator providing dependency-driven scheduling, SQLite-backed checkpointing, and loop management. This makes AI workflows auditable and transparent, addressing critical needs in high-stakes domains such as legal reasoning, medical decision-making, and financial analysis. <div>
arXiv:2512.10563v1 Announce Type: new 
Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs</title>
<link>https://arxiv.org/abs/2512.10611</link>
<guid>https://arxiv.org/abs/2512.10611</guid>
<content:encoded><![CDATA[
<div> Keywords: data center design, large language models, physics-guided optimization, energy efficiency, simulation-ready layout  

<br /><br />Summary:  
This paper addresses the challenge of designing data center (DC) infrastructure capable of meeting escalating computational demands. Traditional DC design methods combining human expertise and simulations struggle to scale with growing complexity. Recent generative AI approaches create human-centric indoor layouts but fail to incorporate physical constraints, limiting their applicability in DC design where operational objectives and strict physics must be met. To overcome this, the authors propose Phythesis, a novel framework that fuses large language models (LLMs) with physics-guided evolutionary optimization to generate simulation-ready (SimReady) DC layouts optimized for energy efficiency. Phythesis employs an iterative bi-level optimization process: the first level uses LLM-driven generation and self-criticism to produce physically plausible 3D layouts, while the second level applies physics-informed optimization to fine-tune asset parameters and select the optimal combination of design elements. Experiments across three scales demonstrate Phythesis achieves a 57.3% higher success rate in layout generation and improves power usage effectiveness (PUE) by 11.5% compared to baseline LLM-only methods, highlighting its potential to automate and enhance energy-efficient DC design with physical realism. <div>
arXiv:2512.10611v1 Announce Type: new 
Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification</title>
<link>https://arxiv.org/abs/2512.10640</link>
<guid>https://arxiv.org/abs/2512.10640</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised cell type identification, contrastive learning, cell-gene associations, single-cell RNA-seq, representation learning  

<br /><br />Summary:  
Unsupervised cell type identification is essential for detecting and characterizing heterogeneous cell populations in single-cell omics studies. Traditional clustering methods largely focus on intrinsic cellular structures but overlook critical cell-gene associations, limiting their ability to distinguish closely related cell types. To address this, the authors propose a novel Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to produce more informative cell representations. The framework introduces two contrastive distribution alignment components designed to uncover reliable intrinsic cellular structures by leveraging cell-cell structural relationships effectively. Furthermore, a refinement module is developed to integrate gene-correlation structure learning, which enhances cell embeddings by capturing underlying cell-gene associations. This module strengthens the connections between cells and their associated genes, refining representation learning to better exploit biologically meaningful relationships. Extensive experiments conducted on various single-cell RNA sequencing and spatial transcriptomics benchmark datasets demonstrate that scRCL consistently outperforms current state-of-the-art methods in terms of cell-type identification accuracy. Additionally, downstream biological analyses confirm that the identified cell populations display coherent gene-expression signatures, underscoring the biological relevance and effectiveness of the proposed approach. The authors have made the code available on GitHub for further use and validation. <div>
arXiv:2512.10640v1 Announce Type: new 
Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.10655</link>
<guid>https://arxiv.org/abs/2512.10655</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, memorization mitigation, classifier-free guidance, latent feature modification, prompt alignment  

<br /><br />Summary: Diffusion models can inadvertently reproduce exact training examples, which poses significant privacy and copyright risks as their usage becomes widespread. Traditional methods to reduce memorization focus on manipulating classifier-free guidance (CFG) or perturbing prompt embeddings during inference but often at the cost of degrading the model’s alignment with the given prompt. To address this challenge, the paper introduces CAPTAIN, a novel and training-free approach that works by directly altering latent features in the denoising phase. CAPTAIN begins by applying frequency-based noise initialization to limit the replication of memorized content early in the denoising timeline. It then determines the ideal denoising steps optimal for feature injection and pinpoints specific latent regions where memorization occurs. Finally, CAPTAIN injects semantically consistent features derived from non-memorized reference images into these localized latent areas. This targeted intervention suppresses memorization while maintaining both high prompt fidelity and visual output quality. Experimental results demonstrate that CAPTAIN significantly reduces memorization compared to CFG-based strategies, all while preserving strong adherence to prompt instructions and the overall artistic integrity of generated images. This method provides a promising pathway for safer deployment of diffusion models in practical applications. <div>
arXiv:2512.10655v1 Announce Type: new 
Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity</title>
<link>https://arxiv.org/abs/2512.10665</link>
<guid>https://arxiv.org/abs/2512.10665</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, value diversity, multi-agent systems, collective behavior, emergent principles<br /><br />Summary: This paper investigates how the diversity of values influences the collective behavior of AI communities composed of Large Language Model-based multi-agent systems. First, it applies Schwartz's Theory of Basic Human Values to naturally elicit values and simulate communities with varying agent counts engaged in open-ended interaction and constitution formation processes. Second, the findings indicate that greater value diversity enhances the stability of shared values among agents, promotes the emergence of complex behaviors, and leads to the development of more creative, agent-generated principles without external input. Third, the study highlights diminishing returns, as extreme heterogeneity in values can cause destabilization within the community. Fourth, this exploration frames value diversity as a crucial dimension for advancing AI capabilities, linking technical AI aspects with sociological insights into how institutions emerge and evolve. Finally, the work opens new pathways for integrating social science theories into the design of AI systems that interact and organize collectively, fostering more adaptable and innovative artificial communities. <div>
arXiv:2512.10665v1 Announce Type: new 
Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search</title>
<link>https://arxiv.org/abs/2512.10671</link>
<guid>https://arxiv.org/abs/2512.10671</guid>
<content:encoded><![CDATA[
<div> Early-exit networks, Neural Architecture Search, hardware-aware NAS, adaptive threshold tuning, exit branch design<br /><br />Summary:  
Early-exit networks provide an efficient way to reduce energy consumption and latency in deep learning models by allowing simpler inputs to exit the computation earlier. This approach is especially important for resource-constrained devices where minimizing energy usage is essential. However, designing effective early-exit networks poses challenges, as it requires carefully balancing computational efficiency with model accuracy. Recent advances have employed Neural Architecture Search (NAS) to optimize the placement and number of exit branches to achieve this balance. Beyond the number and placement, the depth and types of layers within each exit branch are critical factors influencing both performance and efficiency. The paper introduces a hardware-aware NAS framework that optimizes exit branch architectures by considering accuracy and efficiency simultaneously. This framework also incorporates adaptive threshold tuning to decide when samples should exit. The proposed method was evaluated on CIFAR-10, CIFAR-100, and SVHN datasets, showing improvements over existing approaches. Results demonstrate that their approach achieves higher accuracy while maintaining the same or reduced average multiply-accumulate operations (MACs). This confirms that optimizing exit branch structures and thresholds leads to more effective early-exit networks tailored for efficient, accurate deep learning inference. <div>
arXiv:2512.10671v1 Announce Type: new 
Abstract: Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges of Evaluating LLM Safety for User Welfare</title>
<link>https://arxiv.org/abs/2512.10687</link>
<guid>https://arxiv.org/abs/2512.10687</guid>
<content:encoded><![CDATA[
<div> safety evaluation, large language models, user context, vulnerable populations, personalized advice  

<br /><br />Summary:  
1. Traditional safety evaluations of large language models (LLMs) focus on universal risks, such as dangerous capabilities or undesirable tendencies, but fail to address context-dependent harms affecting personal advice in high-stakes areas like finance and health.  
2. Although frameworks like the OECD's AI classification emphasize the importance of assessing individual user risks, evaluations centered on user welfare remain underdeveloped.  
3. The study assessed responses from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles with varying vulnerability, revealing that evaluators who lacked detailed user context rated identical LLM outputs as safer compared to those who were aware of user circumstances. Safety scores for high-vulnerability users dropped significantly from 5/7 (safe) to 3/7 (somewhat unsafe) when context was considered.  
4. Attempts to close this safety evaluation gap by including realistic user context within prompts, based on what users report disclosing, failed to improve safety ratings significantly.  
5. The findings demonstrate that effective user-welfare safety evaluations require explicit assessment against diverse user profiles rather than reliance on user-disclosed context alone, highlighting the necessity for new methodology distinct from existing universal-risk frameworks. The authors provide their code and dataset to support future research in context-aware LLM safety assessment. <div>
arXiv:2512.10687v1 Announce Type: new 
Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10691</link>
<guid>https://arxiv.org/abs/2512.10691</guid>
<content:encoded><![CDATA[
<div> vision-language models, Chest X-ray, supervised fine-tuning, reinforcement learning, report generation<br /><br />Summary: Recent progress in vision-language models (VLMs) has enhanced Chest X-ray (CXR) interpretation. Most medical VLMs depend primarily on supervised fine-tuning (SFT), which focuses on next-token prediction and does not directly assess answer quality. Reinforcement learning (RL), which integrates task-specific feedback, has shown strong performance improvements in tasks requiring explicit intermediate reasoning ("thinking") such as math and coding. The study investigates how RL and thinking influence a CXR VLM by first performing large-scale SFT on CXR data to develop RadVLM based on Qwen3-VL. A cold-start SFT stage is introduced to equip the model with basic reasoning ability. Subsequently, Group Relative Policy Optimization (GRPO) is applied with clinically relevant, task-specific rewards targeted at report generation and visual grounding. Experiments are conducted on domain-specific and general-domain Qwen3-VL variants, both with and without the thinking component. Results indicate that while strong SFT is essential for foundational performance, RL yields additional improvements on both evaluated tasks. However, explicit intermediate reasoning does not provide further gains. The RL-optimized RadVLM models outperform baselines under a unified evaluation framework, achieving state-of-the-art results in medical report generation and visual grounding, underscoring the value of clinically aligned RL as a complement to SFT in medical VLMs. <div>
arXiv:2512.10691v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution</title>
<link>https://arxiv.org/abs/2512.10696</link>
<guid>https://arxiv.org/abs/2512.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural memory, large language models, experience-driven evolution, multi-faceted distillation, utility-based refinement  

<br /><br />Summary:  
This paper addresses the limitations of current procedural memory frameworks for large language model (LLM) agents, which typically rely on a passive, append-only memory accumulation mechanism. The authors propose ReMe (Remember Me, Refine Me), a novel framework designed to enable agents to dynamically evolve through their experiences. ReMe innovates the memory lifecycle using three key mechanisms: (1) multi-faceted distillation, which extracts detailed insights by identifying success patterns, diagnosing failure causes, and generating comparative analyses; (2) context-adaptive reuse, which leverages scenario-aware indexing to customize past knowledge for new contexts; and (3) utility-based refinement, an autonomous process that adds valuable memories while pruning outdated or irrelevant ones to keep the memory pool compact and high-quality. Extensive experiments on benchmarks BFCL-V3 and AppWorld demonstrate that ReMe significantly improves agent memory systems, achieving state-of-the-art results. A noteworthy finding is the memory-scaling effect, where the smaller Qwen3-8B model with ReMe outperforms the larger Qwen3-14B model without memory, highlighting the efficiency of self-evolving memory for lifelong learning. The authors also contribute by releasing their code and the reme.library dataset to support further research in this area. <div>
arXiv:2512.10696v1 Announce Type: new 
Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators</title>
<link>https://arxiv.org/abs/2512.10702</link>
<guid>https://arxiv.org/abs/2512.10702</guid>
<content:encoded><![CDATA[
<div> Keywords: CA-GPT, optical coherence tomography, percutaneous coronary intervention, artificial intelligence, procedural decision-making  

<br /><br />Summary:  
This study evaluates the performance of CA-GPT, a specialized large AI model integrated into an AI-OCT system, for guiding percutaneous coronary intervention (PCI) using optical coherence tomography (OCT). Conducted at a single center with 96 patients, the study compared procedural decisions from CA-GPT, a general-purpose ChatGPT-5 model, and junior physicians against expert-derived records. CA-GPT showed significantly higher agreement scores in pre-PCI planning compared to both ChatGPT-5 and junior physicians, excelling particularly in selecting stent diameter (90.3% vs. 72.2%) and length (80.6% vs. 52.8%). In post-PCI assessments, CA-GPT maintained superior overall agreement, outperforming the other groups consistently. Subgroup analysis further confirmed that CA-GPT's advantage persists in complex clinical scenarios. The findings highlight CA-GPT's ability to provide standardized, reliable intravascular imaging interpretation, reducing the operator-dependency typical of OCT assessment. This AI-driven approach holds significant promise to augment clinician expertise and optimize decision-making during OCT-guided PCI procedures, potentially improving clinical outcomes through enhanced procedural accuracy and consistency. <div>
arXiv:2512.10702v1 Announce Type: new 
Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly</title>
<link>https://arxiv.org/abs/2512.10787</link>
<guid>https://arxiv.org/abs/2512.10787</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, multi-hop queries, context dilution, SEAL-RAG, evidence precision<br /><br />Summary:  
The paper addresses the challenge of Retrieval-Augmented Generation (RAG) systems struggling with multi-hop queries where the initial retrieval misses a crucial bridge fact. Existing methods like Self-RAG, CRAG, and Adaptive-$k$ mitigate this by expanding or pruning the retrieval context, but expanding often causes context dilution, reducing relevant information visibility. To counter this, the authors propose SEAL-RAG, a training-free controller that uses a "replace, don't expand" approach within a fixed retrieval depth $k$. SEAL-RAG operates through a four-step cycle: Search, Extract, Assess, and Loop. This involves entity-anchored extraction to identify missing entities or relations (gap specification), launching targeted micro-queries to fill these gaps, and employing entity-first ranking to replace irrelevant distractors with gap-closing evidence. The method was evaluated against strong baselines (Basic RAG, CRAG, Self-RAG, Adaptive-$k$) on HotpotQA and 2WikiMultiHopQA datasets. On HotpotQA with $k=3$, SEAL improved answer correctness by 3 to 13 percentage points and evidence precision by 12 to 18 points over Self-RAG. On 2WikiMultiHopQA with $k=5$, SEAL outperformed Adaptive-$k$ by 8 points in accuracy and maintained high evidence precision (96%) compared to 22% for CRAG. These improvements are statistically significant ($p<0.001$). By fixing $k$ and focusing on replacement, SEAL ensures predictable computational cost and optimized top-$k$ retrieval performance for precision rather than breadth. The authors have released their code and data openly. <div>
arXiv:2512.10787v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.10807</link>
<guid>https://arxiv.org/abs/2512.10807</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, Out-of-Distribution, Benchmark, Sensor Data, Domain Shift<br /><br />Summary:<br /><br />1. This paper addresses the challenge of distributional shifts in sensor-based Human Activity Recognition (HAR), caused by variations across individuals, devices, environments, and time.<br />2. It questions the necessity of out-of-distribution (OOD) algorithms for HAR and investigates which existing OOD methods perform best across diverse scenarios.<br />3. The authors propose HAROOD, a comprehensive benchmark specifically designed for evaluating HAR tasks in OOD settings.<br />4. Four distinct OOD scenarios are defined: cross-person, cross-position, cross-dataset, and cross-time, covering realistic and challenging conditions.<br />5. The benchmark integrates six datasets, implements sixteen comparative methods involving both CNN-based and Transformer-based architectures, and introduces two model selection protocols.<br />6. Extensive experiments reveal that no single method consistently outperforms others across all scenarios, indicating substantial room for future improvements.<br />7. The HAROOD codebase is modular, extensible, and publicly released to encourage further research and development in OOD-based HAR methodologies.<br />8. The release and documentation can be accessed at https://github.com/AIFrontierLab/HAROOD, aiming to facilitate benchmarking, comparison, and advancement in the HAR community. <div>
arXiv:2512.10807v1 Announce Type: new 
Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agile Deliberation: Concept Deliberation for Subjective Visual Classification</title>
<link>https://arxiv.org/abs/2512.10821</link>
<guid>https://arxiv.org/abs/2512.10821</guid>
<content:encoded><![CDATA[
<div> Keywords: content moderation, human-in-the-loop, concept deliberation, Agile Deliberation, visual classifiers  

<br /><br />Summary:  
1. The paper addresses the challenge of evolving and subjective concept understanding necessary for training vision classifiers in applications like content moderation and curation.  
2. It identifies that users often begin with vague concept ideas and engage in an iterative "concept deliberation" process to refine their understanding, as revealed through interviews with content moderation experts.  
3. The authors introduce a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving concepts by exposing users to borderline cases during training.  
4. Agile Deliberation consists of two stages: (1) concept scoping, which breaks down the initial idea into a hierarchical structure of sub-concepts, and (2) concept iteration, which presents semantically borderline examples for users to reflect on and give feedback, thus aligning the classifier iteratively to the evolving user intent.  
5. The framework was evaluated in 18 user sessions of 1.5 hours each, demonstrating a 7.5% improvement in F1 score over automated decomposition methods and more than 3% improvement over manual deliberation approaches, while users reported clearer conceptual understanding and reduced cognitive effort. <div>
arXiv:2512.10821v1 Announce Type: new 
Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions</title>
<link>https://arxiv.org/abs/2512.10822</link>
<guid>https://arxiv.org/abs/2512.10822</guid>
<content:encoded><![CDATA[
<div> Safe Offline Reinforcement Learning, Control Barrier Functions, Neural Barrier, Model-Free Learning, Expectile Objective<br /><br />Summary:<br /><br />1. Ensuring safety in autonomous systems requires controllers that satisfy strict state-wise constraints without depending on online interactions, a challenge that traditional Safe Offline RL methods address only with soft expected-cost constraints. 2. Control Barrier Functions (CBFs) offer strong safety guarantees but typically rely on expert knowledge of barrier functions or full system dynamics, limiting their practicality in many scenarios. 3. The proposed Value-Guided Offline Control Barrier Functions (V-OCBF) framework overcomes these limitations by learning a neural CBF entirely from offline demonstration data without requiring access to system dynamics. 4. V-OCBF introduces a recursive finite-difference barrier update that enables model-free learning to propagate safety information over time, thus avoiding the necessity of a dynamics model. 5. To improve robustness, V-OCBF incorporates an expectile-based objective that prevents querying the barrier on out-of-distribution actions and confines updates to actions supported by the dataset. 6. The learned neural barrier is subsequently utilized within a Quadratic Program (QP) to synthesize real-time safe control policies. 7. Extensive case studies demonstrate that V-OCBF achieves significantly fewer safety violations than baseline methods while maintaining strong task performance, showcasing its potential for scalable offline synthesis of safety-critical controllers without the need for online interaction or hand-engineered barrier functions. <div>
arXiv:2512.10822v1 Announce Type: new 
Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Can Assist with Proposal Selection at Large User Facilities</title>
<link>https://arxiv.org/abs/2512.10895</link>
<guid>https://arxiv.org/abs/2512.10895</guid>
<content:encoded><![CDATA[
arXiv:2512.10895v1 Announce Type: new 
Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\rho\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Granular Node Pruning for Circuit Discovery</title>
<link>https://arxiv.org/abs/2512.10903</link>
<guid>https://arxiv.org/abs/2512.10903</guid>
<content:encoded><![CDATA[
arXiv:2512.10903v1 Announce Type: new 
Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Decision-Making Agents and Higher-Order Causal Processes</title>
<link>https://arxiv.org/abs/2512.10937</link>
<guid>https://arxiv.org/abs/2512.10937</guid>
<content:encoded><![CDATA[
arXiv:2512.10937v1 Announce Type: new 
Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Acquisition of Discrete Grammatical Categories</title>
<link>https://arxiv.org/abs/2503.18702</link>
<guid>https://arxiv.org/abs/2503.18702</guid>
<content:encoded><![CDATA[
arXiv:2503.18702v1 Announce Type: cross 
Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting</title>
<link>https://arxiv.org/abs/2508.01426</link>
<guid>https://arxiv.org/abs/2508.01426</guid>
<content:encoded><![CDATA[
arXiv:2508.01426v2 Announce Type: cross 
Abstract: Recent advancements in deep learning have led to the development of Foundation Models (FMs) for weather forecasting, yet their ability to predict extreme weather events remains limited. Existing approaches either focus on general weather conditions or specialize in specific-type extremes, neglecting the real-world atmospheric patterns of diversified extreme events. In this work, we identify two key characteristics of extreme events: (1) the spectral disparity against normal weather regimes, and (2) the hierarchical drivers and geographic blending of diverse extremes. Along this line, we propose UniExtreme, a universal extreme weather forecasting foundation model that integrates (1) an Adaptive Frequency Modulation (AFM) module that captures region-wise spectral differences between normal and extreme weather, through learnable Beta-distribution filters and multi-granularity spectral aggregation, and (2) an Event Prior Augmentation (EPA) module which incorporates region-specific extreme event priors to resolve hierarchical extreme diversity and composite extreme schema, via a dual-level memory fusion network. Extensive experiments demonstrate that UniExtreme outperforms state-of-the-art baselines in both extreme and general weather forecasting, showcasing superior adaptability across diverse extreme scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs</title>
<link>https://arxiv.org/abs/2512.09874</link>
<guid>https://arxiv.org/abs/2512.09874</guid>
<content:encoded><![CDATA[
arXiv:2512.09874v1 Announce Type: cross 
Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoTEdu: Access Control, Detection, and Automatic Incident Response in Academic IoT Networks</title>
<link>https://arxiv.org/abs/2512.09934</link>
<guid>https://arxiv.org/abs/2512.09934</guid>
<content:encoded><![CDATA[
arXiv:2512.09934v1 Announce Type: cross 
Abstract: The growing presence of IoT devices in academic environments has increased operational complexity and exposed security weaknesses, especially in academic institutions without unified policies for registration, monitoring, and incident response involving IoT. This work presents IoTEdu, an integrated platform that combines access control, incident detection, and automatic blocking of IoT devices. The solution was evaluated in a controlled environment with simulated attacks, achieving an average time of 28.6 seconds between detection and blocking. The results show a reduction in manual intervention, standardization of responses, and unification of the processes of registration, monitoring, and incident response.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)</title>
<link>https://arxiv.org/abs/2512.09939</link>
<guid>https://arxiv.org/abs/2512.09939</guid>
<content:encoded><![CDATA[
arXiv:2512.09939v1 Announce Type: cross 
Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELANA: A Simple Energy and Latency Analyzer for LLMs</title>
<link>https://arxiv.org/abs/2512.09946</link>
<guid>https://arxiv.org/abs/2512.09946</guid>
<content:encoded><![CDATA[
arXiv:2512.09946v1 Announce Type: cross 
Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-APEX: Zero-Knowledge Approximate Personalized Unlearning with Executable Proofs</title>
<link>https://arxiv.org/abs/2512.09953</link>
<guid>https://arxiv.org/abs/2512.09953</guid>
<content:encoded><![CDATA[
arXiv:2512.09953v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific data points from a trained model to satisfy privacy, copyright, and safety requirements. In real deployments, providers distribute a global model to many edge devices, where each client personalizes the model using private data. When a deletion request is issued, clients may ignore it or falsely claim compliance, and providers cannot check their parameters or data. This makes verification difficult, especially because personalized models must forget the targeted samples while preserving local utility, and verification must remain lightweight on edge devices.
  We introduce ZK APEX, a zero-shot personalized unlearning method that operates directly on the personalized model without retraining. ZK APEX combines sparse masking on the provider side with a small Group OBS compensation step on the client side, using a blockwise empirical Fisher matrix to create a curvature-aware update designed for low overhead. Paired with Halo2 zero-knowledge proofs, it enables the provider to verify that the correct unlearning transformation was applied without revealing any private data or personalized parameters.
  On Vision Transformer classification tasks, ZK APEX recovers nearly all personalization accuracy while effectively removing the targeted information. Applied to the OPT125M generative model trained on code data, it recovers around seventy percent of the original accuracy. Proof generation for the ViT case completes in about two hours, more than ten million times faster than retraining-based checks, with less than one gigabyte of memory use and proof sizes around four hundred megabytes. These results show the first practical framework for verifiable personalized unlearning on edge devices.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects</title>
<link>https://arxiv.org/abs/2512.10031</link>
<guid>https://arxiv.org/abs/2512.10031</guid>
<content:encoded><![CDATA[
arXiv:2512.10031v1 Announce Type: cross 
Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-Dags as Powerful Background Knowledge For Causal Discovery</title>
<link>https://arxiv.org/abs/2512.10032</link>
<guid>https://arxiv.org/abs/2512.10032</guid>
<content:encoded><![CDATA[
arXiv:2512.10032v1 Announce Type: cross 
Abstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealing with high-dimensional data and complex dependencies. Incorporating prior knowledge about the system can aid causal discovery. In this work, we leverage Cluster-DAGs as a prior knowledge framework to warm-start causal discovery. We show that Cluster-DAGs offer greater flexibility than existing approaches based on tiered background knowledge and introduce two modified constraint-based algorithms, Cluster-PC and Cluster-FCI, for causal discovery in the fully and partially observed setting, respectively. Empirical evaluation on simulated data demonstrates that Cluster-PC and Cluster-FCI outperform their respective baselines without prior knowledge.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs</title>
<link>https://arxiv.org/abs/2512.10040</link>
<guid>https://arxiv.org/abs/2512.10040</guid>
<content:encoded><![CDATA[
arXiv:2512.10040v1 Announce Type: cross 
Abstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing the policy towards a mixture of reference models to leverage their collective desirable properties. However, current methods for setting the reference weights are ad-hoc and statistically unsound, leading to unreliable performance. To address this, we introduce four new weighting strategies: two offline methods that leverage held-out validation signal; one online method that uses a sliding-window estimator to reduce overfitting; and an online method that treats reference weighting as a $K$-armed bandit via Thompson Sampling. Experiments using Qwen2.5-0.5B as the policy model and seven reference models from the Llama, Mistral, Qwen, Yi, and Phi families (0.5B-14B each) show that all 4 of our strategies outperform the current MRPO weighting methods on UltraFeedback and SafeRLHF in preference accuracy. More thought-provokingly, however, we find that single-reference DPO, using any of 6 out of 7 references, consistently outperforms all tested multiple-reference approaches -- calling into question the practical appeal of multiple-reference approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata</title>
<link>https://arxiv.org/abs/2512.10041</link>
<guid>https://arxiv.org/abs/2512.10041</guid>
<content:encoded><![CDATA[
arXiv:2512.10041v1 Announce Type: cross 
Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detailed balance in large language model-driven agents</title>
<link>https://arxiv.org/abs/2512.10047</link>
<guid>https://arxiv.org/abs/2512.10047</guid>
<content:encoded><![CDATA[
arXiv:2512.10047v1 Announce Type: cross 
Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10051</link>
<guid>https://arxiv.org/abs/2512.10051</guid>
<content:encoded><![CDATA[
arXiv:2512.10051v1 Announce Type: cross 
Abstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To overcome these challenges, we introduce DB2-TransF, a novel Transformer-inspired architecture that replaces the self-attention mechanism with a learnable Daubechies wavelet coefficient layer. This wavelet-based module efficiently captures multi-scale local and global patterns and enhances the modeling of correlations across multiple time series for the time series forecasting task. Extensive experiments on 13 standard forecasting benchmarks demonstrate that DB2-TransF achieves comparable or superior predictive accuracy to conventional Transformers, while substantially reducing memory usage for the time series forecasting task. The obtained experimental results position DB2-TransF as a scalable and resource-efficient framework for advanced time series forecasting. Our code is available at https://github.com/SteadySurfdom/DB2-TransF
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Metamorphic versus Single-Fold Proteins with Statistical Learning and AlphaFold2</title>
<link>https://arxiv.org/abs/2512.10066</link>
<guid>https://arxiv.org/abs/2512.10066</guid>
<content:encoded><![CDATA[
arXiv:2512.10066v1 Announce Type: cross 
Abstract: The remarkable success of AlphaFold2 in providing accurate atomic-level prediction of protein structures from their amino acid sequence has transformed approaches to the protein folding problem. However, its core paradigm of mapping one sequence to one structure may only be appropriate for single-fold proteins with one stable conformation. Metamorphic proteins, which can adopt multiple distinct conformations, have conformational diversity that cannot be adequately modeled by AlphaFold2. Hence, classifying whether a given protein is metamorphic or single-fold remains a critical challenge for both laboratory experiments and computational methods. To address this challenge, we developed a novel classification framework by re-purposing AlphaFold2 to generate conformational ensembles via a multiple sequence alignment sampling method. From these ensembles, we extract a comprehensive set of features characterizing the conformational ensemble's modality and structural dispersion. A random forest classifier trained on a carefully curated benchmark dataset of known metamorphic and single-fold proteins achieves a mean AUC of 0.869 with cross-validation, demonstrating the effectiveness of our integrated approach. Furthermore, by applying our classifier to 600 randomly sampled proteins from the Protein Data Bank, we identified several potential metamorphic protein candidates -- including the 40S ribosomal protein S30, whose conformational change is crucial for its secondary function in antimicrobial defense. By combining AI-driven protein structure prediction with statistical learning, our work provides a powerful new approach for discovering metamorphic proteins and deepens our understanding of their role in their molecular function.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models</title>
<link>https://arxiv.org/abs/2512.10080</link>
<guid>https://arxiv.org/abs/2512.10080</guid>
<content:encoded><![CDATA[
arXiv:2512.10080v1 Announce Type: cross 
Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defining the Scope of Learning Analytics: An Axiomatic Approach for Analytic Practice and Measurable Learning Phenomena</title>
<link>https://arxiv.org/abs/2512.10081</link>
<guid>https://arxiv.org/abs/2512.10081</guid>
<content:encoded><![CDATA[
arXiv:2512.10081v1 Announce Type: cross 
Abstract: Learning Analytics (LA) has rapidly expanded through practical and technological innovation, yet its foundational identity has remained theoretically under-specified. This paper addresses this gap by proposing the first axiomatic theory that formally defines the essential structure, scope, and limitations of LA. Derived from the psychological definition of learning and the methodological requirements of LA, the framework consists of five axioms specifying discrete observation, experience construction, state transition, and inference. From these axioms, we derive a set of theorems and propositions that clarify the epistemological stance of LA, including the inherent unobservability of learner states, the irreducibility of temporal order, constraints on reachable states, and the impossibility of deterministically predicting future learning. We further define LA structure and LA practice as formal objects, demonstrating the sufficiency and necessity of the axioms and showing that diverse LA approaches -- such as Bayesian Knowledge Tracing and dashboards -- can be uniformly explained within this framework. The theory provides guiding principles for designing analytic methods and interpreting learning data while avoiding naive behaviorism and category errors by establishing an explicit theoretical inference layer between observations and states. This work positions LA as a rigorous science of state transition systems based on observability, establishing the theoretical foundation necessary for the field's maturation as a scholarly discipline.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.10098</link>
<guid>https://arxiv.org/abs/2512.10098</guid>
<content:encoded><![CDATA[
arXiv:2512.10098v1 Announce Type: cross 
Abstract: Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly un- der domain shifts and rare-class conditions. Deep learning mod- els often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Med- ical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician- derived expert knowledge to improve generalization, reduce rare- class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHyLL: Learning Continuous Neural Representations of Hybrid Systems</title>
<link>https://arxiv.org/abs/2512.10117</link>
<guid>https://arxiv.org/abs/2512.10117</guid>
<content:encoded><![CDATA[
arXiv:2512.10117v1 Announce Type: cross 
Abstract: Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio</title>
<link>https://arxiv.org/abs/2512.10120</link>
<guid>https://arxiv.org/abs/2512.10120</guid>
<content:encoded><![CDATA[
arXiv:2512.10120v1 Announce Type: cross 
Abstract: General-purpose audio representations aim to map acoustically variable instances of the same event to nearby points, resolving content identity in a zero-shot setting. Unlike supervised classification benchmarks that measure adaptability via parameter updates, we introduce VocSim, a training-free benchmark probing the intrinsic geometric alignment of frozen embeddings. VocSim aggregates 125k single-source clips from 19 corpora spanning human speech, animal vocalizations, and environmental sounds. By restricting to single-source audio, we isolate content representation from the confound of source separation. We evaluate embeddings using Precision@k for local purity and the Global Separation Rate (GSR) for point-wise class separation. To calibrate GSR, we report lift over an empirical permutation baseline. Across diverse foundation models, a simple pipeline, frozen Whisper encoder features, time-frequency pooling, and label-free PCA, yields strong zero-shot performance. However, VocSim also uncovers a consistent generalization gap. On blind, low-resource speech, local retrieval drops sharply. While performance remains statistically distinguishable from chance, the absolute geometric structure collapses, indicating a failure to generalize to unseen phonotactics. As external validation, our top embeddings predict avian perceptual similarity, improve bioacoustic classification, and achieve state-of-the-art results on the HEAR benchmark. We posit that the intrinsic geometric quality measured here proxies utility in unlisted downstream applications. We release data, code, and a public leaderboard to standardize the evaluation of intrinsic audio geometry.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing</title>
<link>https://arxiv.org/abs/2512.10121</link>
<guid>https://arxiv.org/abs/2512.10121</guid>
<content:encoded><![CDATA[
arXiv:2512.10121v1 Announce Type: cross 
Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Hirschberg for Width Bounded Dynamic Programs</title>
<link>https://arxiv.org/abs/2512.10132</link>
<guid>https://arxiv.org/abs/2512.10132</guid>
<content:encoded><![CDATA[
arXiv:2512.10132v1 Announce Type: cross 
Abstract: Hirschberg's algorithm (1975) reduces the space complexity for the longest common subsequence problem from $O(N^2)$ to $O(N)$ via recursive midpoint bisection on a grid dynamic program (DP). We show that the underlying idea generalizes to a broad class of dynamic programs with local dependencies on directed acyclic graphs (DP DAGs). Modeling a DP as deterministic time evolution over a topologically ordered DAG with frontier width $\omega$ and bounded in-degree, and assuming a max-type semiring with deterministic tie breaking, we prove that in a standard offline random-access model any such DP admits deterministic traceback in space $O(\omega \log T + (\log T)^{O(1)})$ cells over a fixed finite alphabet, where $T$ is the number of states. Our construction replaces backward dynamic programs by forward-only recomputation and organizes the time order into a height-compressed recursion tree whose nodes expose small "middle frontiers'' across which every optimal path must pass. The framework yields near-optimal traceback bounds for asymmetric and banded sequence alignment, one-dimensional recurrences, and dynamic-programming formulations on graphs of bounded pathwidth. We also show that an $\Omega(\omega)$ space term (in bits) is unavoidable in forward single-pass models and discuss conjectured $\sqrt{T}$-type barriers in streaming settings, supporting the view that space-efficient traceback is a structural property of width-bounded DP DAGs rather than a peculiarity of grid-based algorithms.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset</title>
<link>https://arxiv.org/abs/2512.10148</link>
<guid>https://arxiv.org/abs/2512.10148</guid>
<content:encoded><![CDATA[
arXiv:2512.10148v1 Announce Type: cross 
Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning</title>
<link>https://arxiv.org/abs/2512.10150</link>
<guid>https://arxiv.org/abs/2512.10150</guid>
<content:encoded><![CDATA[
arXiv:2512.10150v1 Announce Type: cross 
Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Large Language Models for End-to-End Circuit Analysis Problem Solving</title>
<link>https://arxiv.org/abs/2512.10159</link>
<guid>https://arxiv.org/abs/2512.10159</guid>
<content:encoded><![CDATA[
arXiv:2512.10159v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong performance in data-rich domains such as programming, but their reliability in engineering tasks remains limited. Circuit analysis -- requiring multimodal understanding and precise mathematical reasoning -- highlights these challenges. Although Gemini 2.5 Pro improves diagram interpretation and analog-circuit reasoning, it still struggles to consistently produce correct solutions when given both text and circuit diagrams. At the same time, engineering education needs scalable AI tools capable of generating accurate solutions for tasks such as automated homework feedback and question-answering. This paper presents an enhanced, end-to-end circuit problem solver built on Gemini 2.5 Pro. We first benchmark Gemini on a representative set of undergraduate circuit problems and identify two major failure modes: 1) circuit-recognition hallucinations, particularly incorrect source polarity detection, and 2) reasoning-process hallucinations, such as incorrect current directions. To address recognition errors, we integrate a fine-tuned YOLO detector and OpenCV processing to isolate voltage and current sources, enabling Gemini to re-identify source polarities from cropped images with near-perfect accuracy. To reduce reasoning errors, we introduce an ngspice-based verification loop in which Gemini generates a .cir file, ngspice simulates the circuit, and discrepancies trigger iterative regeneration with optional human-in-the-loop review. Across 83 problems, the proposed pipeline achieves a 97.59% success rate (81 correct solutions), substantially outperforming Gemini 2.5 Pro's original 79.52% accuracy. This system extends LLM capabilities for multimodal engineering problem-solving and supports the creation of high-quality educational datasets and AI-powered instructional tools.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offscript: Automated Auditing of Instruction Adherence in LLMs</title>
<link>https://arxiv.org/abs/2512.10172</link>
<guid>https://arxiv.org/abs/2512.10172</guid>
<content:encoded><![CDATA[
arXiv:2512.10172v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and generative search systems are increasingly used for information seeking by diverse populations with varying preferences for knowledge sourcing and presentation. While users can customize LLM behavior through custom instructions and behavioral prompts, no mechanism exists to evaluate whether these instructions are being followed effectively. We present Offscript, an automated auditing tool that efficiently identifies potential instruction following failures in LLMs. In a pilot study analyzing custom instructions sourced from Reddit, Offscript detected potential deviations from instructed behavior in 86.4% of conversations, 22.2% of which were confirmed as material violations through human review. Our findings suggest that automated auditing serves as a viable approach for evaluating compliance to behavioral instructions related to information seeking.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Domain Generalization with Latent Space Inversion</title>
<link>https://arxiv.org/abs/2512.10224</link>
<guid>https://arxiv.org/abs/2512.10224</guid>
<content:encoded><![CDATA[
arXiv:2512.10224v1 Announce Type: cross 
Abstract: Federated domain generalization (FedDG) addresses distribution shifts among clients in a federated learning framework. FedDG methods aggregate the parameters of locally trained client models to form a global model that generalizes to unseen clients while preserving data privacy. While improving the generalization capability of the global model, many existing approaches in FedDG jeopardize privacy by sharing statistics of client data between themselves. Our solution addresses this problem by contributing new ways to perform local client training and model aggregation. To improve local client training, we enforce (domain) invariance across local models with the help of a novel technique, \textbf{latent space inversion}, which enables better client privacy. When clients are not \emph{i.i.d}, aggregating their local models may discard certain local adaptations. To overcome this, we propose an \textbf{important weight} aggregation strategy to prioritize parameters that significantly influence predictions of local models during aggregation. Our extensive experiments show that our approach achieves superior results over state-of-the-art methods with less communication overhead.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Information Routing for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10229</link>
<guid>https://arxiv.org/abs/2512.10229</guid>
<content:encoded><![CDATA[
arXiv:2512.10229v1 Announce Type: cross 
Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InFerActive: Towards Scalable Human Evaluation of Large Language Models through Interactive Inference</title>
<link>https://arxiv.org/abs/2512.10234</link>
<guid>https://arxiv.org/abs/2512.10234</guid>
<content:encoded><![CDATA[
arXiv:2512.10234v1 Announce Type: cross 
Abstract: Human evaluation remains the gold standard for evaluating outputs of Large Language Models (LLMs). The current evaluation paradigm reviews numerous individual responses, leading to significant scalability challenges. LLM outputs can be more efficiently represented as a tree structure, reflecting their autoregressive generation process and stochastic token selection. However, conventional tree visualization cannot scale to the exponentially large trees generated by modern sampling methods of LLMs. To address this problem, we present InFerActive, an interactive inference system for scalable human evaluation. InFerActive enables on-demand exploration through probability-based filtering and evaluation features, while bridging the semantic gap between computational tokens and human-readable text through adaptive visualization techniques. Through a technical evaluation and user study (N=12), we demonstrate that InFerActive significantly improves evaluation efficiency and enables more comprehensive assessment of model behavior. We further conduct expert case studies that demonstrate InFerActive's practical applicability and potential for transforming LLM evaluation workflows.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</title>
<link>https://arxiv.org/abs/2512.10248</link>
<guid>https://arxiv.org/abs/2512.10248</guid>
<content:encoded><![CDATA[
arXiv:2512.10248v1 Announce Type: cross 
Abstract: The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</title>
<link>https://arxiv.org/abs/2512.10271</link>
<guid>https://arxiv.org/abs/2512.10271</guid>
<content:encoded><![CDATA[
arXiv:2512.10271v1 Announce Type: cross 
Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing Evolutionarily Stable Strategies in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2512.10279</link>
<guid>https://arxiv.org/abs/2512.10279</guid>
<content:encoded><![CDATA[
arXiv:2512.10279v1 Announce Type: cross 
Abstract: We present an algorithm for computing evolutionarily stable strategies (ESSs) in symmetric perfect-recall extensive-form games of imperfect information. Our main algorithm is for two-player games, and we describe how it can be extended to multiplayer games. The algorithm is sound and computes all ESSs in nondegenerate games and a subset of them in degenerate games which contain an infinite continuum of symmetric Nash equilibria. The algorithm is anytime and can be stopped early to find one or more ESSs. We experiment on an imperfect-information cancer signaling game as well as random games to demonstrate scalability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Network Based Adaptive Threat Detection for Cloud Identity and Access Management Logs</title>
<link>https://arxiv.org/abs/2512.10280</link>
<guid>https://arxiv.org/abs/2512.10280</guid>
<content:encoded><![CDATA[
arXiv:2512.10280v1 Announce Type: cross 
Abstract: The rapid expansion of cloud infrastructures and distributed identity systems has significantly increased the complexity and attack surface of modern enterprises. Traditional rule based or signature driven detection systems are often inadequate in identifying novel or evolving threats within Identity and Access Management logs, where anomalous behavior may appear statistically benign but contextually malicious. This paper presents a Graph Neural Network Based Adaptive Threat Detection framework designed to learn latent user resource interaction patterns from IAM audit trails in real time. By modeling IAM logs as heterogeneous dynamic graphs, the proposed system captures temporal, relational, and contextual dependencies across entities such as users, roles, sessions, and access actions. The model incorporates attention based aggregation and graph embedding updates to enable continual adaptation to changing cloud environments. Experimental evaluation on synthesized and real world IAM datasets demonstrates that the proposed method achieves higher detection precision and recall than baseline LSTM and GCN classifiers, while maintaining scalability across multi tenant cloud environments. The frameworks adaptability enables proactive mitigation of insider threats, privilege escalation, and lateral movement attacks, contributing to the foundation of AI driven zero trust access analytics. This work bridges the gap between graph based machine learning and operational cloud security intelligence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionEdit: Benchmarking and Learning Motion-Centric Image Editing</title>
<link>https://arxiv.org/abs/2512.10284</link>
<guid>https://arxiv.org/abs/2512.10284</guid>
<content:encoded><![CDATA[
arXiv:2512.10284v1 Announce Type: cross 
Abstract: We introduce MotionEdit, a novel dataset for motion-centric image editing-the task of modifying subject actions and interactions while preserving identity, structure, and physical plausibility. Unlike existing image editing datasets that focus on static appearance changes or contain only sparse, low-quality motion edits, MotionEdit provides high-fidelity image pairs depicting realistic motion transformations extracted and verified from continuous videos. This new task is not only scientifically challenging but also practically significant, powering downstream applications such as frame-controlled video synthesis and animation.
  To evaluate model performance on the novel task, we introduce MotionEdit-Bench, a benchmark that challenges models on motion-centric edits and measures model performance with generative, discriminative, and preference-based metrics. Benchmark results reveal that motion editing remains highly challenging for existing state-of-the-art diffusion-based editing models. To address this gap, we propose MotionNFT (Motion-guided Negative-aware Fine Tuning), a post-training framework that computes motion alignment rewards based on how well the motion flow between input and model-edited images matches the ground-truth motion, guiding models toward accurate motion transformations. Extensive experiments on FLUX.1 Kontext and Qwen-Image-Edit show that MotionNFT consistently improves editing quality and motion fidelity of both base models on the motion editing task without sacrificing general editing ability, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning</title>
<link>https://arxiv.org/abs/2512.10296</link>
<guid>https://arxiv.org/abs/2512.10296</guid>
<content:encoded><![CDATA[
arXiv:2512.10296v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments</title>
<link>https://arxiv.org/abs/2512.10312</link>
<guid>https://arxiv.org/abs/2512.10312</guid>
<content:encoded><![CDATA[
arXiv:2512.10312v1 Announce Type: cross 
Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Informal Proofs into Formal Proofs Using a Chain of States</title>
<link>https://arxiv.org/abs/2512.10317</link>
<guid>https://arxiv.org/abs/2512.10317</guid>
<content:encoded><![CDATA[
arXiv:2512.10317v1 Announce Type: cross 
Abstract: We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual VLM Training: Adapting an English-Trained VLM to French</title>
<link>https://arxiv.org/abs/2512.10336</link>
<guid>https://arxiv.org/abs/2512.10336</guid>
<content:encoded><![CDATA[
arXiv:2512.10336v1 Announce Type: cross 
Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale</title>
<link>https://arxiv.org/abs/2512.10341</link>
<guid>https://arxiv.org/abs/2512.10341</guid>
<content:encoded><![CDATA[
arXiv:2512.10341v1 Announce Type: cross 
Abstract: Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</title>
<link>https://arxiv.org/abs/2512.10350</link>
<guid>https://arxiv.org/abs/2512.10350</guid>
<content:encoded><![CDATA[
arXiv:2512.10350v1 Announce Type: cross 
Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.10362</link>
<guid>https://arxiv.org/abs/2512.10362</guid>
<content:encoded><![CDATA[
arXiv:2512.10362v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: "Contextual Blindness". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPG: Generalized Policy Gradient Theorem for Transformer-based Policies</title>
<link>https://arxiv.org/abs/2512.10365</link>
<guid>https://arxiv.org/abs/2512.10365</guid>
<content:encoded><![CDATA[
arXiv:2512.10365v1 Announce Type: cross 
Abstract: We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning</title>
<link>https://arxiv.org/abs/2512.10372</link>
<guid>https://arxiv.org/abs/2512.10372</guid>
<content:encoded><![CDATA[
arXiv:2512.10372v1 Announce Type: cross 
Abstract: The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.
  We present \prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \prot\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \cone\ (\uline{Co}mpute \uline{N}etwork for \uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \prot\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.
  We implement \prot\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \prot\ achieves up to 99\% accuracy on MNIST and 90\% on Fashion-MNIST, with less than 3\% degradation up to 30\% Byzantine nodes, and 56\% accuracy on CIFAR-10 despite its complexity. Our results show that \prot\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural personal sound zones with flexible bright zone control</title>
<link>https://arxiv.org/abs/2512.10375</link>
<guid>https://arxiv.org/abs/2512.10375</guid>
<content:encoded><![CDATA[
arXiv:2512.10375v1 Announce Type: cross 
Abstract: Personal sound zone (PSZ) reproduction system, which attempts to create distinct virtual acoustic scenes for different listeners at their respective positions within the same spatial area using one loudspeaker array, is a fundamental technology in the application of virtual reality. For practical applications, the reconstruction targets must be measured on the same fixed receiver array used to record the local room impulse responses (RIRs) from the loudspeaker array to the control points in each PSZ, which makes the system inconvenient and costly for real-world use. In this paper, a 3D convolutional neural network (CNN) designed for PSZ reproduction with flexible control microphone grid and alternative reproduction target is presented, utilizing the virtual target scene as inputs and the PSZ pre-filters as output. Experimental results of the proposed method are compared with the traditional method, demonstrating that the proposed method is able to handle varied reproduction targets on flexible control point grid using only one training session. Furthermore, the proposed method also demonstrates the capability to learn global spatial information from sparse sampling points distributed in PSZs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies</title>
<link>https://arxiv.org/abs/2512.10384</link>
<guid>https://arxiv.org/abs/2512.10384</guid>
<content:encoded><![CDATA[
arXiv:2512.10384v1 Announce Type: cross 
Abstract: Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \textit{data construction} and \textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\% and open-world data boosts FROW benchmark accuracy by 10\%-20\% and content accuracy by 6\%-12\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\%. The benchmark will be available at https://github.com/pc-inno/FROW.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2512.10388</link>
<guid>https://arxiv.org/abs/2512.10388</guid>
<content:encoded><![CDATA[
arXiv:2512.10388v1 Announce Type: cross 
Abstract: Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \textbf{\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\footnote{https://github.com/ziwliu8/H2Rec}.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Retrieval Models for Stripped Binary Analysis</title>
<link>https://arxiv.org/abs/2512.10393</link>
<guid>https://arxiv.org/abs/2512.10393</guid>
<content:encoded><![CDATA[
arXiv:2512.10393v1 Announce Type: cross 
Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
arXiv:2512.10398v1 Announce Type: cross 
Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
arXiv:2512.10402v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliding Window Attention Adaptation</title>
<link>https://arxiv.org/abs/2512.10411</link>
<guid>https://arxiv.org/abs/2512.10411</guid>
<content:encoded><![CDATA[
arXiv:2512.10411v1 Announce Type: cross 
Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation</title>
<link>https://arxiv.org/abs/2512.10415</link>
<guid>https://arxiv.org/abs/2512.10415</guid>
<content:encoded><![CDATA[
arXiv:2512.10415v1 Announce Type: cross 
Abstract: The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction</title>
<link>https://arxiv.org/abs/2512.10416</link>
<guid>https://arxiv.org/abs/2512.10416</guid>
<content:encoded><![CDATA[
arXiv:2512.10416v1 Announce Type: cross 
Abstract: Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers</title>
<link>https://arxiv.org/abs/2512.10422</link>
<guid>https://arxiv.org/abs/2512.10422</guid>
<content:encoded><![CDATA[
arXiv:2512.10422v1 Announce Type: cross 
Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time</title>
<link>https://arxiv.org/abs/2512.10437</link>
<guid>https://arxiv.org/abs/2512.10437</guid>
<content:encoded><![CDATA[
arXiv:2512.10437v1 Announce Type: cross 
Abstract: This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustered Federated Learning with Hierarchical Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.10443</link>
<guid>https://arxiv.org/abs/2512.10443</guid>
<content:encoded><![CDATA[
arXiv:2512.10443v1 Announce Type: cross 
Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maximum Risk Minimization with Random Forests</title>
<link>https://arxiv.org/abs/2512.10445</link>
<guid>https://arxiv.org/abs/2512.10445</guid>
<content:encoded><![CDATA[
arXiv:2512.10445v1 Announce Type: cross 
Abstract: We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method</title>
<link>https://arxiv.org/abs/2512.10461</link>
<guid>https://arxiv.org/abs/2512.10461</guid>
<content:encoded><![CDATA[
arXiv:2512.10461v1 Announce Type: cross 
Abstract: Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10492</link>
<guid>https://arxiv.org/abs/2512.10492</guid>
<content:encoded><![CDATA[
arXiv:2512.10492v1 Announce Type: cross 
Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10510</link>
<guid>https://arxiv.org/abs/2512.10510</guid>
<content:encoded><![CDATA[
arXiv:2512.10510v1 Announce Type: cross 
Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Auction: Generative Auction towards LLM-Native Advertising</title>
<link>https://arxiv.org/abs/2512.10551</link>
<guid>https://arxiv.org/abs/2512.10551</guid>
<content:encoded><![CDATA[
arXiv:2512.10551v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval</title>
<link>https://arxiv.org/abs/2512.10596</link>
<guid>https://arxiv.org/abs/2512.10596</guid>
<content:encoded><![CDATA[
arXiv:2512.10596v1 Announce Type: cross 
Abstract: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\%, nearly doubling the 23.86\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Gemini Robotics Policies in a Veo World Simulator</title>
<link>https://arxiv.org/abs/2512.10675</link>
<guid>https://arxiv.org/abs/2512.10675</guid>
<content:encoded><![CDATA[
arXiv:2512.10675v1 Announce Type: cross 
Abstract: Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition</title>
<link>https://arxiv.org/abs/2512.10688</link>
<guid>https://arxiv.org/abs/2512.10688</guid>
<content:encoded><![CDATA[
arXiv:2512.10688v1 Announce Type: cross 
Abstract: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant "popularity direction" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.10698</link>
<guid>https://arxiv.org/abs/2512.10698</guid>
<content:encoded><![CDATA[
arXiv:2512.10698v1 Announce Type: cross 
Abstract: Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code</title>
<link>https://arxiv.org/abs/2512.10713</link>
<guid>https://arxiv.org/abs/2512.10713</guid>
<content:encoded><![CDATA[
arXiv:2512.10713v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation</title>
<link>https://arxiv.org/abs/2512.10734</link>
<guid>https://arxiv.org/abs/2512.10734</guid>
<content:encoded><![CDATA[
arXiv:2512.10734v1 Announce Type: cross 
Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation</title>
<link>https://arxiv.org/abs/2512.10735</link>
<guid>https://arxiv.org/abs/2512.10735</guid>
<content:encoded><![CDATA[
arXiv:2512.10735v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2512.10739</link>
<guid>https://arxiv.org/abs/2512.10739</guid>
<content:encoded><![CDATA[
arXiv:2512.10739v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</title>
<link>https://arxiv.org/abs/2512.10758</link>
<guid>https://arxiv.org/abs/2512.10758</guid>
<content:encoded><![CDATA[
arXiv:2512.10758v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.
  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.
  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.
  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metaphor-based Jailbreaking Attacks on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.10766</link>
<guid>https://arxiv.org/abs/2512.10766</guid>
<content:encoded><![CDATA[
arXiv:2512.10766v1 Announce Type: cross 
Abstract: Text-to-image~(T2I) models commonly incorporate defense mechanisms to prevent the generation of sensitive images. Unfortunately, recent jailbreaking attacks have shown that adversarial prompts can effectively bypass these mechanisms and induce T2I models to produce sensitive content, revealing critical safety vulnerabilities. However, existing attack methods implicitly assume that the attacker knows the type of deployed defenses, which limits their effectiveness against unknown or diverse defense mechanisms. In this work, we introduce \textbf{MJA}, a \textbf{m}etaphor-based \textbf{j}ailbreaking \textbf{a}ttack method inspired by the Taboo game, aiming to effectively and efficiently attack diverse defense mechanisms without prior knowledge of their type by generating metaphor-based adversarial prompts. Specifically, MJA consists of two modules: an LLM-based multi-agent generation module~(MLAG) and an adversarial prompt optimization module~(APO). MLAG decomposes the generation of metaphor-based adversarial prompts into three subtasks: metaphor retrieval, context matching, and adversarial prompt generation. Subsequently, MLAG coordinates three LLM-based agents to generate diverse adversarial prompts by exploring various metaphors and contexts. To enhance attack efficiency, APO first trains a surrogate model to predict the attack results of adversarial prompts and then designs an acquisition strategy to adaptively identify optimal adversarial prompts. Extensive experiments on T2I models with various external and internal defense mechanisms demonstrate that MJA outperforms six baseline methods, achieving stronger attack performance while using fewer queries. Code is available in https://github.com/datar001/metaphor-based-jailbreaking-attack.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation</title>
<link>https://arxiv.org/abs/2512.10772</link>
<guid>https://arxiv.org/abs/2512.10772</guid>
<content:encoded><![CDATA[
arXiv:2512.10772v1 Announce Type: cross 
Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving</title>
<link>https://arxiv.org/abs/2512.10785</link>
<guid>https://arxiv.org/abs/2512.10785</guid>
<content:encoded><![CDATA[
arXiv:2512.10785v1 Announce Type: cross 
Abstract: Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Interface for Firewall Configuration</title>
<link>https://arxiv.org/abs/2512.10789</link>
<guid>https://arxiv.org/abs/2512.10789</guid>
<content:encoded><![CDATA[
arXiv:2512.10789v1 Announce Type: cross 
Abstract: This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality</title>
<link>https://arxiv.org/abs/2512.10791</link>
<guid>https://arxiv.org/abs/2512.10791</guid>
<content:encoded><![CDATA[
arXiv:2512.10791v1 Announce Type: cross 
Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification</title>
<link>https://arxiv.org/abs/2512.10793</link>
<guid>https://arxiv.org/abs/2512.10793</guid>
<content:encoded><![CDATA[
arXiv:2512.10793v1 Announce Type: cross 
Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What matters for Representation Alignment: Global Information or Spatial Structure?</title>
<link>https://arxiv.org/abs/2512.10794</link>
<guid>https://arxiv.org/abs/2512.10794</guid>
<content:encoded><![CDATA[
arXiv:2512.10794v1 Announce Type: cross 
Abstract: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \textit{global} \revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values</title>
<link>https://arxiv.org/abs/2512.10817</link>
<guid>https://arxiv.org/abs/2512.10817</guid>
<content:encoded><![CDATA[
arXiv:2512.10817v1 Announce Type: cross 
Abstract: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants</title>
<link>https://arxiv.org/abs/2512.10857</link>
<guid>https://arxiv.org/abs/2512.10857</guid>
<content:encoded><![CDATA[
arXiv:2512.10857v1 Announce Type: cross 
Abstract: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.10863</link>
<guid>https://arxiv.org/abs/2512.10863</guid>
<content:encoded><![CDATA[
arXiv:2512.10863v1 Announce Type: cross 
Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting</title>
<link>https://arxiv.org/abs/2512.10866</link>
<guid>https://arxiv.org/abs/2512.10866</guid>
<content:encoded><![CDATA[
arXiv:2512.10866v1 Announce Type: cross 
Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale</title>
<link>https://arxiv.org/abs/2512.10922</link>
<guid>https://arxiv.org/abs/2512.10922</guid>
<content:encoded><![CDATA[
arXiv:2512.10922v1 Announce Type: cross 
Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Q-Chunking</title>
<link>https://arxiv.org/abs/2512.10926</link>
<guid>https://arxiv.org/abs/2512.10926</guid>
<content:encoded><![CDATA[
arXiv:2512.10926v1 Announce Type: cross 
Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models</title>
<link>https://arxiv.org/abs/2512.10932</link>
<guid>https://arxiv.org/abs/2512.10932</guid>
<content:encoded><![CDATA[
arXiv:2512.10932v1 Announce Type: cross 
Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any4D: Unified Feed-Forward Metric 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.10935</link>
<guid>https://arxiv.org/abs/2512.10935</guid>
<content:encoded><![CDATA[
arXiv:2512.10935v1 Announce Type: cross 
Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks</title>
<link>https://arxiv.org/abs/2512.10936</link>
<guid>https://arxiv.org/abs/2512.10936</guid>
<content:encoded><![CDATA[
arXiv:2512.10936v1 Announce Type: cross 
Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2512.10938</link>
<guid>https://arxiv.org/abs/2512.10938</guid>
<content:encoded><![CDATA[
arXiv:2512.10938v1 Announce Type: cross 
Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(\alpha x + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis</title>
<link>https://arxiv.org/abs/2512.10940</link>
<guid>https://arxiv.org/abs/2512.10940</guid>
<content:encoded><![CDATA[
arXiv:2512.10940v1 Announce Type: cross 
Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mull-Tokens: Modality-Agnostic Latent Thinking</title>
<link>https://arxiv.org/abs/2512.10941</link>
<guid>https://arxiv.org/abs/2512.10941</guid>
<content:encoded><![CDATA[
arXiv:2512.10941v1 Announce Type: cross 
Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation</title>
<link>https://arxiv.org/abs/2512.10943</link>
<guid>https://arxiv.org/abs/2512.10943</guid>
<content:encoded><![CDATA[
arXiv:2512.10943v1 Announce Type: cross 
Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning</title>
<link>https://arxiv.org/abs/2512.10946</link>
<guid>https://arxiv.org/abs/2512.10946</guid>
<content:encoded><![CDATA[
arXiv:2512.10946v1 Announce Type: cross 
Abstract: Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</title>
<link>https://arxiv.org/abs/2512.10949</link>
<guid>https://arxiv.org/abs/2512.10949</guid>
<content:encoded><![CDATA[
arXiv:2512.10949v1 Announce Type: cross 
Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Dataset Selection for High-Quality Data Sharing</title>
<link>https://arxiv.org/abs/2512.10952</link>
<guid>https://arxiv.org/abs/2512.10952</guid>
<content:encoded><![CDATA[
arXiv:2512.10952v1 Announce Type: cross 
Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</title>
<link>https://arxiv.org/abs/2512.10957</link>
<guid>https://arxiv.org/abs/2512.10957</guid>
<content:encoded><![CDATA[
arXiv:2512.10957v1 Announce Type: cross 
Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2306.01270</link>
<guid>https://arxiv.org/abs/2306.01270</guid>
<content:encoded><![CDATA[
arXiv:2306.01270v2 Announce Type: replace 
Abstract: Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuristic search planner used to create a global guiding path. During movement, the heuristic search planner replans new paths based on the instructions of the real-time planner. We tested our method in 10 different conflict scenarios. The experiments show that the planning performance of MAPPOHR is better than that of existing learning and heuristic methods. Due to the utilization of empirical knowledge and heuristic search, the learning efficiency of MAPPOHR is higher than that of existing learning methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Quantifier Selection in cvc5</title>
<link>https://arxiv.org/abs/2408.14338</link>
<guid>https://arxiv.org/abs/2408.14338</guid>
<content:encoded><![CDATA[
arXiv:2408.14338v2 Announce Type: replace 
Abstract: In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection. Quantifiers represent a significant challenge for SMT and are technically a source of undecidability. In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not. Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses. Therefore, we invoke the ML predictor many times, during the whole run of the solver. To make this efficient, we use fast ML models based on gradient boosting decision trees. We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generation Framework with Strict Constraints for Crystal Materials Design</title>
<link>https://arxiv.org/abs/2411.08464</link>
<guid>https://arxiv.org/abs/2411.08464</guid>
<content:encoded><![CDATA[
arXiv:2411.08464v3 Announce Type: replace 
Abstract: The design of crystal materials plays a critical role in areas such as new energy development, biomedical engineering, and semiconductors. Recent advances in data-driven methods have enabled the generation of diverse crystal structures. However, most existing approaches still rely on random sampling without strict constraints, requiring multiple post-processing steps to identify stable candidates with the desired physical and chemical properties. In this work, we present a new constrained generation framework that takes multiple constraints as input and enables the generation of crystal structures with specific chemical and properties. In this framework, intermediate constraints, such as symmetry information and composition ratio, are generated by a constraint generator based on large language models (LLMs), which considers the target properties. These constraints are then used by a subsequent crystal structure generator to ensure that the structure generation process is under control. Our method generates crystal structures with a probability of meeting the target properties that is more than twice that of existing approaches. Furthermore, nearly 100% of the generated crystals strictly adhere to predefined chemical composition, eliminating the risks of supply chain during production.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-centric proto-symbolic behavioural reasoning from pixels</title>
<link>https://arxiv.org/abs/2411.17438</link>
<guid>https://arxiv.org/abs/2411.17438</guid>
<content:encoded><![CDATA[
arXiv:2411.17438v3 Announce Type: replace 
Abstract: Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge</title>
<link>https://arxiv.org/abs/2504.01538</link>
<guid>https://arxiv.org/abs/2504.01538</guid>
<content:encoded><![CDATA[
arXiv:2504.01538v2 Announce Type: replace 
Abstract: While current AI-driven methods excel at deriving empirical models from individual experiments, a significant challenge remains in uncovering the common fundamental physics that underlie these models -- a task at which human physicists are adept. To bridge this gap, we introduce AI-Newton, a novel framework for concept-driven scientific discovery. Our system autonomously derives general physical laws directly from raw, multi-experiment data, operating without supervision or prior physical knowledge. Its core innovations are twofold: (1) proposing interpretable physical concepts to construct laws, and (2) progressively generalizing these laws to broader domains. Applied to a large, noisy dataset of mechanics experiments, AI-Newton successfully rediscovers foundational and universal laws, such as Newton's second law, the conservation of energy, and the universal gravitation. This work represents a significant advance toward autonomous, human-like scientific discovery.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data</title>
<link>https://arxiv.org/abs/2504.01951</link>
<guid>https://arxiv.org/abs/2504.01951</guid>
<content:encoded><![CDATA[
arXiv:2504.01951v2 Announce Type: replace 
Abstract: With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing AI Research Assistants with Expert-Involved Learning</title>
<link>https://arxiv.org/abs/2505.04638</link>
<guid>https://arxiv.org/abs/2505.04638</guid>
<content:encoded><![CDATA[
arXiv:2505.04638v3 Announce Type: replace 
Abstract: Large language models (LLMs) and large multimodal models (LMMs) promise to accelerate biomedical discovery, yet their reliability remains unclear. We introduce ARIEL (AI Research Assistant for Expert-in-the-Loop Learning), an open-source evaluation and optimization framework that pairs a curated multimodal biomedical corpus with expert-vetted tasks to probe two capabilities: full-length article summarization and fine-grained figure interpretation. Using uniform protocols and blinded PhD-level evaluation, we find that state-of-the-art models generate fluent but incomplete summaries, whereas LMMs struggle with detailed visual reasoning. We later observe that prompt engineering and lightweight fine-tuning substantially improve textual coverage, and a compute-scaled inference strategy enhances visual question answering. We build an ARIEL agent that integrates textual and visual cues, and we show it can propose testable mechanistic hypotheses. ARIEL delineates current strengths and limitations of foundation models, and provides a reproducible platform for advancing trustworthy AI in biomedicine.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.14381</link>
<guid>https://arxiv.org/abs/2505.14381</guid>
<content:encoded><![CDATA[
arXiv:2505.14381v2 Announce Type: replace 
Abstract: With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale</title>
<link>https://arxiv.org/abs/2505.14932</link>
<guid>https://arxiv.org/abs/2505.14932</guid>
<content:encoded><![CDATA[
arXiv:2505.14932v2 Announce Type: replace 
Abstract: Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables</title>
<link>https://arxiv.org/abs/2506.11375</link>
<guid>https://arxiv.org/abs/2506.11375</guid>
<content:encoded><![CDATA[
arXiv:2506.11375v2 Announce Type: replace 
Abstract: With the widespread application of multimodal large language models in scientific intelligence, there is an urgent need for more challenging evaluation benchmarks to assess their ability to understand complex scientific data. Scientific tables, as core carriers of knowledge representation, combine text, symbols, and graphics, forming a typical multimodal reasoning scenario. However, existing benchmarks are mostly focused on general domains, failing to reflect the unique structural complexity and domain-specific semantics inherent in scientific research. Chemical tables are particularly representative: they intertwine structured variables such as reagents, conditions, and yields with visual symbols like molecular structures and chemical formulas, posing significant challenges to models in cross-modal alignment and semantic parsing. To address this, we propose ChemTable-a large scale benchmark of chemical tables constructed from real-world literature, containing expert-annotated cell layouts, logical structures, and domain-specific labels. It supports two core tasks: (1) table recognition (structure and content extraction); and (2) table understanding (descriptive and reasoning-based question answering). Evaluation on ChemTable shows that while mainstream multimodal models perform reasonably well in layout parsing, they still face significant limitations when handling critical elements such as molecular structures and symbolic conventions. Closed-source models lead overall but still fall short of human-level performance. This work provides a realistic testing platform for evaluating scientific multimodal understanding, revealing the current bottlenecks in domain-specific reasoning and advancing the development of intelligent systems for scientific research.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology</title>
<link>https://arxiv.org/abs/2506.18156</link>
<guid>https://arxiv.org/abs/2506.18156</guid>
<content:encoded><![CDATA[
arXiv:2506.18156v3 Announce Type: replace 
Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models</title>
<link>https://arxiv.org/abs/2506.20018</link>
<guid>https://arxiv.org/abs/2506.20018</guid>
<content:encoded><![CDATA[
arXiv:2506.20018v2 Announce Type: replace 
Abstract: This paper investigates real-time decision support systems that leverage low-latency AI models, bringing together recent progress in holistic AI-driven decision tools, integration with Edge-IoT technologies, and approaches for effective human-AI teamwork. It looks into how large language models can assist decision-making, especially when resources are limited. The research also examines the effects of technical developments such as DeLLMa, methods for compressing models, and improvements for analytics on edge devices, while also addressing issues like limited resources and the need for adaptable frameworks. Through a detailed review, the paper offers practical perspectives on development strategies and areas of application, adding to the field by pointing out opportunities for more efficient and flexible AI-supported systems. The conclusions set the stage for future breakthroughs in this fast-changing area, highlighting how AI can reshape real-time decision support.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotional Support with LLM-based Empathetic Dialogue Generation</title>
<link>https://arxiv.org/abs/2507.12820</link>
<guid>https://arxiv.org/abs/2507.12820</guid>
<content:encoded><![CDATA[
arXiv:2507.12820v2 Announce Type: replace 
Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective emotional assistance through dialogue, addressing the growing demand for mental health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC evaluation, where we leverage large-scale language models enhanced by prompt engineering and finetuning techniques. We explore both parameter-efficient Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the model's ability to generate supportive and contextually appropriate responses. Our best model ranked second in the competition, highlighting the potential of combining LLMs with effective adaptation methods for ESC tasks. Future work will focus on further enhancing emotional understanding and response personalization to build more practical and reliable emotional support systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data</title>
<link>https://arxiv.org/abs/2508.15432</link>
<guid>https://arxiv.org/abs/2508.15432</guid>
<content:encoded><![CDATA[
arXiv:2508.15432v3 Announce Type: replace 
Abstract: The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Neuro-symbolic Problem Solving: Solver-Free Learning of Constraints and Objectives</title>
<link>https://arxiv.org/abs/2508.20978</link>
<guid>https://arxiv.org/abs/2508.20978</guid>
<content:encoded><![CDATA[
arXiv:2508.20978v3 Announce Type: replace 
Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARE: Scaling Up Agent Environments and Evaluations</title>
<link>https://arxiv.org/abs/2509.17158</link>
<guid>https://arxiv.org/abs/2509.17158</guid>
<content:encoded><![CDATA[
arXiv:2509.17158v2 Announce Type: replace 
Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for scalable creation of environments, integration of synthetic or real applications, and execution of agentic orchestrations. ARE provides simple abstractions to build complex and diverse environments, each with their own rules, tools, content, and verifiers, helping to bridge the gap between model development and real-world deployment. We also propose Gaia2, a benchmark built in ARE and designed to measure general agent capabilities. Beyond search and execution, Gaia2 requires agents to handle ambiguities and noise, adapt to dynamic environments, collaborate with other agents, and operate under temporal constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new failure modes that are invisible in static settings. Our experiments show that no system dominates across the intelligence spectrum: stronger reasoning often comes at the cost of efficiency, and budget scaling curves plateau, highlighting the need for new architectures and adaptive compute strategies. Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2 to other environments, empowering the community to rapidly create new benchmarks tailored to their domains. In AI's second half, progress increasingly depends on defining meaningful tasks and robust evaluations to drive frontier capabilities forward.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</title>
<link>https://arxiv.org/abs/2510.18428</link>
<guid>https://arxiv.org/abs/2510.18428</guid>
<content:encoded><![CDATA[
arXiv:2510.18428v2 Announce Type: replace 
Abstract: Optimization modeling enables critical decisions across industries but remains difficult to automate: informal language must be mapped to precise mathematical formulations and executable solver code. Prior LLM approaches either rely on brittle prompting or costly retraining with limited generalization. We present AlphaOPT, a self-improving experience library that enables an LLM to learn from limited demonstrations (even answers alone, without gold-standard programs) and solver feedback - without annotated reasoning traces or parameter updates. AlphaOPT operates in a continual two-phase cycle: (i) a Library Learning phase that reflects on failed attempts, extracting solver-verified, structured insights as {taxonomy, condition, explanation, example}; and (ii) a Library Evolution phase that diagnoses retrieval misalignments and refines the applicability conditions of stored insights, improving transfer across tasks. This design (1) learns efficiently from limited demonstrations without curated rationales, (2) expands continually without costly retraining by updating the library rather than model weights, and (3) makes knowledge explicit and interpretable for human inspection and intervention. Experiments show that AlphaOPT steadily improves with more data (65% to 72% from 100 to 300 training items) and surpasses the strongest baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only on answers. Code and data are available at: https://github.com/Minw913/AlphaOPT.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic</title>
<link>https://arxiv.org/abs/2511.20586</link>
<guid>https://arxiv.org/abs/2511.20586</guid>
<content:encoded><![CDATA[
arXiv:2511.20586v3 Announce Type: replace 
Abstract: Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics, such as accuracy and precision, fail to appropriately capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the Parallel Trust Assessment System (PaTAS), a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through Trust Nodes and Trust Functions that propagate input, parameter, and activation trust across the network. The framework defines a Parameter Trust Update mechanism to refine parameter reliability during training and an Inference-Path Trust Assessment (IPTA) method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a foundation for evaluating model reliability across the AI lifecycle.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.04629</link>
<guid>https://arxiv.org/abs/2512.04629</guid>
<content:encoded><![CDATA[
arXiv:2512.04629v2 Announce Type: replace 
Abstract: Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging these capabilities, we further apply the model to multi-step retrosynthetic planning, achieving state-of-the-art performance on RetroBench and demonstrating its superior efficacy as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
arXiv:2411.00612v3 Announce Type: replace-cross 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-like emergent properties in deep networks: impact of network architecture, datasets and training</title>
<link>https://arxiv.org/abs/2411.16326</link>
<guid>https://arxiv.org/abs/2411.16326</guid>
<content:encoded><![CDATA[
arXiv:2411.16326v3 Announce Type: replace-cross 
Abstract: Despite the rapid pace at which deep networks are improving on standardized vision benchmarks, they are still outperformed by humans on real-world vision tasks. One solution to this problem is to make deep networks more brain-like. Although there are several benchmarks that compare the ability of deep networks to predict brain responses on natural images, they do not capture subtle but important emergent properties present in brains. It is also unclear which design principle -- architecture, training data, or training regime -- would have the greatest impact on these emergent properties. To investigate these issues, we systematically evaluated over 30 state-of-the-art networks with varying network architectures, training datasets, and training regimes for the presence or absence of brain-like properties. Our main findings are as follows. First, network architecture had the strongest impact on brain-like properties compared to dataset and training regime variations. Second, networks varied widely in their alignment to the brain with no single network outperforming all others. Taken together, our results offer a principled and interpretable path toward closing the gap between artificial and human vision.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Reliability of Predictions in Detection Transformers: Object-Level Calibration and Image-Level Uncertainty</title>
<link>https://arxiv.org/abs/2412.01782</link>
<guid>https://arxiv.org/abs/2412.01782</guid>
<content:encoded><![CDATA[
arXiv:2412.01782v3 Announce Type: replace-cross 
Abstract: DETR and its variants have emerged as promising architectures for object detection, offering an end-to-end prediction pipeline. In practice, however, DETRs generate hundreds of predictions that far outnumber the actual objects present in an image. This raises a critical question: which of these predictions could be trusted? Addressing this concern, we provide empirical and theoretical evidence that predictions within the same image play distinct roles, resulting in varying reliability levels. Our analysis reveals that DETRs employ an optimal specialist strategy: one prediction per object is trained to be well-calibrated, while the remaining predictions are trained to suppress their foreground confidence to near zero, even when maintaining accurate localization. We show that this strategy emerges as the loss-minimizing solution to the Hungarian matching algorithm, fundamentally shaping DETRs' outputs. While selecting the well-calibrated predictions is ideal, they are unidentifiable at inference time. This means that any post-processing algorithm poses a risk of outputting a set of predictions with mixed calibration levels. Therefore, practical deployment necessitates a joint evaluation of both the model's calibration quality and the effectiveness of the post-processing algorithm. However, we demonstrate that existing metrics like average precision and expected calibration error are inadequate for this task. To address this issue, we further introduce Object-level Calibration Error (OCE): This object-centric design penalizes both retaining suppressed predictions and missed ground truth foreground objects, making OCE suitable for both evaluating models and identifying reliable prediction subsets. Finally, we present a post hoc uncertainty quantification framework that predicts per-image model accuracy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts</title>
<link>https://arxiv.org/abs/2412.02912</link>
<guid>https://arxiv.org/abs/2412.02912</guid>
<content:encoded><![CDATA[
arXiv:2412.02912v2 Announce Type: replace-cross 
Abstract: We introduce ShapeWords, an approach for synthesizing images based on 3D shape guidance and text prompts. ShapeWords incorporates target 3D shape information within specialized tokens embedded together with the input text, effectively blending 3D shape awareness with textual context to guide the image synthesis process. Unlike conventional shape guidance methods that rely on depth maps restricted to fixed viewpoints and often overlook full 3D structure or textual context, ShapeWords generates diverse yet consistent images that reflect both the target shape's geometry and the textual description. Experimental results show that ShapeWords produces images that are more text-compliant, aesthetically plausible, while also maintaining 3D shape awareness.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title>
<link>https://arxiv.org/abs/2412.21051</link>
<guid>https://arxiv.org/abs/2412.21051</guid>
<content:encoded><![CDATA[
arXiv:2412.21051v4 Announce Type: replace-cross 
Abstract: The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.01113</link>
<guid>https://arxiv.org/abs/2502.01113</guid>
<content:encoded><![CDATA[
arXiv:2502.01113v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation</title>
<link>https://arxiv.org/abs/2502.09884</link>
<guid>https://arxiv.org/abs/2502.09884</guid>
<content:encoded><![CDATA[
arXiv:2502.09884v3 Announce Type: replace-cross 
Abstract: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first nonasymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\sqrt{n}$, which significantly improves on the rates of convergence in prior works.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling</title>
<link>https://arxiv.org/abs/2504.05537</link>
<guid>https://arxiv.org/abs/2504.05537</guid>
<content:encoded><![CDATA[
arXiv:2504.05537v2 Announce Type: replace-cross 
Abstract: Motion Transfer is a technique that synthesizes videos by transferring motion dynamics from a driving video to a source image. In this work we propose a deep learning-based framework to enable real-time video motion transfer which is critical for enabling bandwidth-efficient applications such as video conferencing, remote health monitoring, virtual reality interaction, and vision-based anomaly detection. This is done using keypoints which serve as semantically meaningful, compact representations of motion across time. To enable bandwidth savings during video transmission we perform forecasting of keypoints using two generative time series models VRNN and GRU-NF. The predicted keypoints are transformed into realistic video frames using an optical flow-based module paired with a generator network, thereby enabling efficient, low-frame-rate video transmission. Based on the application this allows the framework to either generate a deterministic future sequence or sample a diverse set of plausible futures. Experimental results demonstrate that VRNN achieves the best point-forecast fidelity (lowest MAE) in applications requiring stable and accurate multi-step forecasting and is particularly competitive in higher-uncertainty, multi-modal settings. This is achieved by introducing recurrently conditioned stochastic latent variables that carry past contexts to capture uncertainty and temporal variation. On the other hand the GRU-NF model enables richer diversity of generated videos while maintaining high visual quality. This is realized by learning an invertible, exact-likelihood mapping between the keypoints and their latent representations which supports rich and controllable sampling of diverse yet coherent keypoint sequences. Our work lays the foundation for next-generation AI systems that require real-time, bandwidth-efficient, and semantically controllable video generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving</title>
<link>https://arxiv.org/abs/2504.20101</link>
<guid>https://arxiv.org/abs/2504.20101</guid>
<content:encoded><![CDATA[
arXiv:2504.20101v4 Announce Type: replace-cross 
Abstract: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Online Class-Incremental Learning via Dual Classifiers</title>
<link>https://arxiv.org/abs/2504.20566</link>
<guid>https://arxiv.org/abs/2504.20566</guid>
<content:encoded><![CDATA[
arXiv:2504.20566v2 Announce Type: replace-cross 
Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Inclusive Separation for Online iNcremental learning (BISON), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BISON method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BISON, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation</title>
<link>https://arxiv.org/abs/2505.01730</link>
<guid>https://arxiv.org/abs/2505.01730</guid>
<content:encoded><![CDATA[
arXiv:2505.01730v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\approx$74\% on ImageNet with a 64$\times$ reduction in the number of inference timesteps compared to existing approaches.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps</title>
<link>https://arxiv.org/abs/2505.10537</link>
<guid>https://arxiv.org/abs/2505.10537</guid>
<content:encoded><![CDATA[
arXiv:2505.10537v3 Announce Type: replace-cross 
Abstract: The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning (Approximately) Equivariant Networks via Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.13631</link>
<guid>https://arxiv.org/abs/2505.13631</guid>
<content:encoded><![CDATA[
arXiv:2505.13631v2 Announce Type: replace-cross 
Abstract: Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forensic deepfake audio detection using segmental speech features</title>
<link>https://arxiv.org/abs/2505.13847</link>
<guid>https://arxiv.org/abs/2505.13847</guid>
<content:encoded><![CDATA[
arXiv:2505.13847v3 Announce Type: replace-cross 
Abstract: This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison (FVC) are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection using methods that are distinct from those employed in traditional FVC, and offer a new perspective on leveraging segmental features for this purpose. In addition, the present study proposes a speaker-specific framework for deepfake detection, which differs fundamentally from the speaker-independent systems that dominate current benchmarks. While speaker-independent frameworks aim at broad generalization, the speaker-specific approach offers advantages in forensic contexts where case-by-case interpretability and sensitivity to individual phonetic realization are essential.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
arXiv:2505.17012v2 Announce Type: replace-cross 
Abstract: Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
arXiv:2505.17508v3 Announce Type: replace-cross 
Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). KL regularization is ubiquitous, yet the design surface, choice of KL direction (forward vs. reverse), normalization (normalized vs. unnormalized), and estimator ($k_1/k_2/k_3$), is scattered across the literature and often intertwined with off-policy estimation. We ask a focused question: under the off-policy setting, what weighting is required for each KL variant so that the surrogate we optimize yields the exact gradient of the intended KL-regularized objective? We answer this with a compact, unified derivation we call the Regularized Policy Gradient (RPG) view. RPG (i) unifies normalized and unnormalized KL variants and shows that the widely-used $k_3$ penalty is exactly the unnormalized KL; (ii) specifies conditions under which REINFORCE-style losses with stop-gradient are gradient-equivalent to fully differentiable surrogates; (iii) identifies and corrects an off-policy importance-weighting mismatch in GRPO's KL term; and (iv) introduces RPG-Style Clip, a clipped-importance-sampling step within RPG-REINFORCE that enables stable, off-policy policy-gradient training at scale. On mathematical reasoning benchmarks (AIME24, AIME25), RPG-REINFORCE with RPG-Style Clip improves accuracy by up to $+6$ absolute percentage points over DAPO. We extend our experiments to 8K context length, and RPG-REINFORCE with RPG-Style Clip achieves 52% accuracy on AIME25, surpassing the official Qwen3-4B-Instruct model (47%). Notably, RPG is a stable and scalable RL algorithm for LLM reasoning, realized via (a) a KL-correct objective, (b) clipped importance sampling, and (c) an iterative reference-policy update scheme.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaskedManipulator: Versatile Whole-Body Manipulation</title>
<link>https://arxiv.org/abs/2505.19086</link>
<guid>https://arxiv.org/abs/2505.19086</guid>
<content:encoded><![CDATA[
arXiv:2505.19086v3 Announce Type: replace-cross 
Abstract: We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat</title>
<link>https://arxiv.org/abs/2506.01524</link>
<guid>https://arxiv.org/abs/2506.01524</guid>
<content:encoded><![CDATA[
arXiv:2506.01524v2 Announce Type: replace-cross 
Abstract: With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2506.01625</link>
<guid>https://arxiv.org/abs/2506.01625</guid>
<content:encoded><![CDATA[
arXiv:2506.01625v2 Announce Type: replace-cross 
Abstract: We address the problem of Gaussian Process (GP) optimization in the presence of unknown and potentially varying adversarial perturbations. Unlike traditional robust optimization approaches that focus on maximizing performance under worst-case scenarios, we consider a robust satisficing objective, where the goal is to consistently achieve a predefined performance threshold $\tau$, even under adversarial conditions. We propose two novel algorithms based on distinct formulations of robust satisficing, and show that they are instances of a general robust satisficing framework. Further, each algorithm offers different guarantees depending on the nature of the adversary. Specifically, we derive two regret bounds: one that is sublinear over time, assuming certain conditions on the adversary and the satisficing threshold $\tau$, and another that scales with the perturbation magnitude but requires no assumptions on the adversary. Through extensive experiments, we demonstrate that our approach outperforms the established robust optimization methods in achieving the satisficing objective, particularly when the ambiguity set of the robust optimization framework is inaccurately specified.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
arXiv:2506.08001v4 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry in Neural Network Parameter Spaces</title>
<link>https://arxiv.org/abs/2506.13018</link>
<guid>https://arxiv.org/abs/2506.13018</guid>
<content:encoded><![CDATA[
arXiv:2506.13018v3 Announce Type: replace-cross 
Abstract: Modern deep learning models are highly overparameterized, resulting in large sets of parameter configurations that yield the same outputs. A significant portion of this redundancy is explained by symmetries in the parameter space--transformations that leave the network function unchanged. These symmetries shape the loss landscape and constrain learning dynamics, offering a new lens for understanding optimization, generalization, and model complexity that complements existing theory of deep learning. This survey provides an overview of parameter space symmetry. We summarize existing literature, uncover connections between symmetry and learning theory, and identify gaps and opportunities in this emerging field.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
arXiv:2506.14933v2 Announce Type: replace-cross 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models</title>
<link>https://arxiv.org/abs/2507.13162</link>
<guid>https://arxiv.org/abs/2507.13162</guid>
<content:encoded><![CDATA[
arXiv:2507.13162v2 Announce Type: replace-cross 
Abstract: Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</title>
<link>https://arxiv.org/abs/2508.03663</link>
<guid>https://arxiv.org/abs/2508.03663</guid>
<content:encoded><![CDATA[
arXiv:2508.03663v2 Announce Type: replace-cross 
Abstract: Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple raters for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and $N$, or if one even existed, depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[
arXiv:2508.03772v5 Announce Type: replace-cross 
Abstract: Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models</title>
<link>https://arxiv.org/abs/2508.08139</link>
<guid>https://arxiv.org/abs/2508.08139</guid>
<content:encoded><![CDATA[
arXiv:2508.08139v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision</title>
<link>https://arxiv.org/abs/2508.08177</link>
<guid>https://arxiv.org/abs/2508.08177</guid>
<content:encoded><![CDATA[
arXiv:2508.08177v2 Announce Type: replace-cross 
Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and treatment planning in medical imaging. While multimodal large language models (MLLMs) combine visual perception with natural language, current medical-grounding pipelines still rely on supervised fine-tuning with explicit spatial hints, making them ill-equipped to handle the implicit queries common in clinical practice. This work makes three core contributions. We first define Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that demands clinical reasoning and pixel-level grounding. Second, we release U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside implicit clinical queries and reasoning traces, spanning 10 modalities, 15 super-categories, and 108 specific categories. Finally, we introduce MedReasoner, a modular framework that distinctly separates reasoning from segmentation: an MLLM reasoner is optimized with reinforcement learning, while a frozen segmentation expert converts spatial prompts into masks, with alignment achieved through format and accuracy rewards. MedReasoner achieves state-of-the-art performance on U-MRG-14K and demonstrates strong generalization to unseen clinical queries, underscoring the significant promise of reinforcement learning for interpretable medical grounding.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Bounded Multi-Agent Visual Navigation via Iterative Risk Allocation</title>
<link>https://arxiv.org/abs/2509.08157</link>
<guid>https://arxiv.org/abs/2509.08157</guid>
<content:encoded><![CDATA[
arXiv:2509.08157v2 Announce Type: replace-cross 
Abstract: Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using only high-dimensional visual observations. While recent approaches successfully combine Goal-Conditioned RL (GCRL) for graph construction with Conflict-Based Search (CBS) for planning, they typically rely on static edge pruning to enforce safety. This binary strategy is overly conservative, precluding feasible missions that require traversing high-risk regions, even when the aggregate risk is acceptable. To address this, we introduce a framework for Risk-Bounded Multi-Agent Path Finding (\problem{}), where agents share a user-specified global risk budget ($\Delta$). Rather than permanently discarding edges, our framework dynamically distributes per-agent risk budgets ($\delta_i$) during search via an Iterative Risk Allocation (IRA) layer that integrates with a standard CBS planner. We investigate two distribution strategies: a greedy surplus-deficit scheme for rapid feasibility repair, and a market-inspired mechanism that treats risk as a priced resource to guide improved allocation. This yields a tunable trade-off wherein agents exploit available risk to secure shorter, more efficient paths, but revert to longer, safer detours under tighter budgets. Experiments in complex visual environments show that, our dynamic allocation framework achieves higher success rates than baselines and effectively leverages the available safety budget to reduce travel time.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database</title>
<link>https://arxiv.org/abs/2509.10600</link>
<guid>https://arxiv.org/abs/2509.10600</guid>
<content:encoded><![CDATA[
arXiv:2509.10600v3 Announce Type: replace-cross 
Abstract: Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning</title>
<link>https://arxiv.org/abs/2509.17552</link>
<guid>https://arxiv.org/abs/2509.17552</guid>
<content:encoded><![CDATA[
arXiv:2509.17552v3 Announce Type: replace-cross 
Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the non-Clifford-count in unitary synthesis using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.21709</link>
<guid>https://arxiv.org/abs/2509.21709</guid>
<content:encoded><![CDATA[
arXiv:2509.21709v2 Announce Type: replace-cross 
Abstract: In this paper we study the potential of using reinforcement learning (RL) in order to synthesize quantum circuits, while optimizing the T-count and CS-count, of unitaries that are exactly implementable by the Clifford+T and Clifford+CS gate sets, respectively. We have designed our RL framework to work with channel representation of unitaries, that enables us to perform matrix operations efficiently, using integers only. We have also incorporated pruning heuristics and a canonicalization of operators, in order to reduce the search complexity. As a result, compared to previous works, we are able to implement significantly larger unitaries, in less time, with much better success rate and improvement factor. Our results for Clifford+T synthesis on two qubit unitaries achieve close-to-optimal decompositions for up to 100 T gates, 5 times more than previous RL algorithms and to the best of our knowledge, the largest instances achieved with any method to date. Our RL algorithm is able to recover previously-known optimal linear complexity algorithm for T-count-optimal decomposition of 1 qubit unitaries. We illustrate significant reduction in the asymptotic T-count estimate of important primitives like controlled cyclic shift (43%), controlled adder (14.3%) and multiplier (14%), without adding any extra ancilla. For 2-qubit Clifford+CS unitaries, our algorithm achieves a linear complexity, something that could only be accomplished by a previous algorithm using SO(6) representation.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks</title>
<link>https://arxiv.org/abs/2509.22258</link>
<guid>https://arxiv.org/abs/2509.22258</guid>
<content:encoded><![CDATA[
arXiv:2509.22258v2 Announce Type: replace-cross 
Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models</title>
<link>https://arxiv.org/abs/2509.24510</link>
<guid>https://arxiv.org/abs/2509.24510</guid>
<content:encoded><![CDATA[
arXiv:2509.24510v3 Announce Type: replace-cross 
Abstract: Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Personalized Deep Research: Benchmarks and Evaluations</title>
<link>https://arxiv.org/abs/2509.25106</link>
<guid>https://arxiv.org/abs/2509.25106</guid>
<content:encoded><![CDATA[
arXiv:2509.25106v2 Announce Type: replace-cross 
Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing benchmarks primarily evaluate DRAs on generic quality metrics and overlook personalization, a critical dimension for individual users. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench (PDR-Bench), the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures Personalization Alignment, Content Quality, and Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalizable Shape Completion with SIM(3) Equivariance</title>
<link>https://arxiv.org/abs/2509.26631</link>
<guid>https://arxiv.org/abs/2509.26631</guid>
<content:encoded><![CDATA[
arXiv:2509.26631v3 Announce Type: replace-cross 
Abstract: 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction</title>
<link>https://arxiv.org/abs/2510.04522</link>
<guid>https://arxiv.org/abs/2510.04522</guid>
<content:encoded><![CDATA[
arXiv:2510.04522v2 Announce Type: replace-cross 
Abstract: Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Is All You Price</title>
<link>https://arxiv.org/abs/2510.09859</link>
<guid>https://arxiv.org/abs/2510.09859</guid>
<content:encoded><![CDATA[
arXiv:2510.09859v3 Announce Type: replace-cross 
Abstract: We build a mechanism design framework where a platform designs GenAI models to screen users who obtain instrumental value from the generated conversation and privately differ in their preference for latency. We show that the revenue-optimal mechanism is simple: deploy a single aligned (user-optimal) model and use token cap as the only instrument to screen the user. The design decouples model training from pricing, is readily implemented with token metering, and mitigates misalignment pressures.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots</title>
<link>https://arxiv.org/abs/2510.16069</link>
<guid>https://arxiv.org/abs/2510.16069</guid>
<content:encoded><![CDATA[
arXiv:2510.16069v2 Announce Type: replace-cross 
Abstract: As design thinking education grows in secondary and tertiary contexts, educators face the challenge of evaluating creative artefacts that combine visual and textual elements. Traditional rubric-based assessment is laborious, time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in large, multi-section cohorts. This paper presents an exploratory study investigating the reliability and perceived accuracy of AI-assisted assessment compared to TA-assisted assessment in evaluating student posters in design thinking education. Two activities were conducted with 33 Ministry of Education (MOE) Singapore school teachers to (1) compare AI-generated scores with TA grading across three key dimensions: empathy and user understanding, identification of pain points and opportunities, and visual communication, and (2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid scores. Results showed low statistical agreement between instructor and AI scores for empathy and pain points, with slightly higher alignment for visual communication. Teachers preferred TA-assigned scores in six of ten samples. Qualitative feedback highlighted the potential of AI for formative feedback, consistency, and student self-reflection, but raised concerns about its limitations in capturing contextual nuance and creative insight. The study underscores the need for hybrid assessment models that integrate computational efficiency with human insights. This research contributes to the evolving conversation on responsible AI adoption in creative disciplines, emphasizing the balance between automation and human judgment for scalable and pedagogically sound assessment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</title>
<link>https://arxiv.org/abs/2510.19195</link>
<guid>https://arxiv.org/abs/2510.19195</guid>
<content:encoded><![CDATA[
arXiv:2510.19195v3 Announce Type: replace-cross 
Abstract: Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\mathbf{really\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An efficient probabilistic hardware architecture for diffusion-like models</title>
<link>https://arxiv.org/abs/2510.23972</link>
<guid>https://arxiv.org/abs/2510.23972</guid>
<content:encoded><![CDATA[
arXiv:2510.23972v2 Announce Type: replace-cross 
Abstract: The proliferation of probabilistic AI has prompted proposals for specialized stochastic computers. Despite promising efficiency gains, these proposals have failed to gain traction because they rely on fundamentally limited modeling techniques and exotic, unscalable hardware. In this work, we address these shortcomings by proposing an all-transistor probabilistic computer that implements powerful denoising models at the hardware level. A system-level analysis indicates that devices based on our architecture could achieve performance parity with GPUs on a simple image benchmark using approximately 10,000 times less energy.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</title>
<link>https://arxiv.org/abs/2511.00090</link>
<guid>https://arxiv.org/abs/2511.00090</guid>
<content:encoded><![CDATA[
arXiv:2511.00090v3 Announce Type: replace-cross 
Abstract: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.06696</link>
<guid>https://arxiv.org/abs/2511.06696</guid>
<content:encoded><![CDATA[
arXiv:2511.06696v2 Announce Type: replace-cross 
Abstract: Pretrained equivariant graph neural networks based on spherical harmonics offer efficient and accurate alternatives to computationally expensive ab-initio methods, yet adapting them to new tasks and chemical environments still requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT) techniques, such as Adapters and LoRA, typically break symmetry, making them incompatible with those equivariant architectures. ELoRA, recently proposed, is the first equivariant PEFT method. It achieves improved parameter efficiency and performance on many benchmarks. However, the relatively high degrees of freedom it retains within each tensor order can still perturb pretrained feature distributions and ultimately degrade performance. To address this, we present Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariant fine-tuning method which employs lightweight scalar gating to modulate feature magnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEA preserves strict equivariance and, across multiple benchmarks, consistently improves energy and force predictions to state-of-the-art levels while training fewer parameters than competing approaches. These results suggest that, in many practical scenarios, modulating channel magnitudes is sufficient to adapt equivariant models to new chemical environments without breaking symmetry, pointing toward a new paradigm for equivariant PEFT design.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning</title>
<link>https://arxiv.org/abs/2511.14396</link>
<guid>https://arxiv.org/abs/2511.14396</guid>
<content:encoded><![CDATA[
arXiv:2511.14396v2 Announce Type: replace-cross 
Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak</title>
<link>https://arxiv.org/abs/2511.14566</link>
<guid>https://arxiv.org/abs/2511.14566</guid>
<content:encoded><![CDATA[
arXiv:2511.14566v2 Announce Type: replace-cross 
Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.16203</link>
<guid>https://arxiv.org/abs/2511.16203</guid>
<content:encoded><![CDATA[
arXiv:2511.16203v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2511.17844</link>
<guid>https://arxiv.org/abs/2511.17844</guid>
<content:encoded><![CDATA[
arXiv:2511.17844v2 Announce Type: replace-cross 
Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Ahead: Foresight Intelligence in MLLMs and World Models</title>
<link>https://arxiv.org/abs/2511.18735</link>
<guid>https://arxiv.org/abs/2511.18735</guid>
<content:encoded><![CDATA[
arXiv:2511.18735v2 Announce Type: replace-cross 
Abstract: In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HunyuanOCR Technical Report</title>
<link>https://arxiv.org/abs/2511.19575</link>
<guid>https://arxiv.org/abs/2511.19575</guid>
<content:encoded><![CDATA[
arXiv:2511.19575v2 Announce Type: replace-cross 
Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dark Speculation: Combining Qualitative and Quantitative Understanding in Frontier AI Risk Analysis</title>
<link>https://arxiv.org/abs/2511.21838</link>
<guid>https://arxiv.org/abs/2511.21838</guid>
<content:encoded><![CDATA[
arXiv:2511.21838v2 Announce Type: replace-cross 
Abstract: Estimating catastrophic harms from frontier AI is hindered by deep ambiguity: many of its risks are not only unobserved but unanticipated by analysts. The central limitation of current risk analysis is the inability to populate the $\textit{catastrophic event space}$, or the set of potential large-scale harms to which probabilities might be assigned. This intractability is worsened by the $\textit{Lucretius problem}$, or the tendency to infer future risks only from past experience. We propose a process of $\textit{dark speculation}$, in which systematically generating and refining catastrophic scenarios ("qualitative" work) is coupled with estimating their likelihoods and associated damages (quantitative underwriting analysis). The idea is neither to predict the future nor to enable insurance for its own sake, but to use narrative and underwriting tools together to generate probability distributions over outcomes. We formalize this process using a simplified catastrophic L\'{e}vy stochastic framework and propose an iterative institutional design in which (1) speculation (including scenario planning) generates detailed catastrophic event narratives, (2) insurance underwriters assign probabilistic and financial parameters to these narratives, and (3) decision-makers synthesize the results into summary statistics to inform judgment. Analysis of the model reveals the value of (a) maintaining independence between speculation and underwriting, (b) analyzing multiple risk categories in parallel, and (c) generating "thick" catastrophic narrative rich in causal (counterfactual) and mitigative detail. While the approach cannot eliminate deep ambiguity, it offers a systematic approach to reason about extreme, low-probability events in frontier AI, tempering complacency and overreaction. The framework is adaptable for iterative use and can be further augmented with AI systems.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational analysis of determinantal varieties</title>
<link>https://arxiv.org/abs/2511.22613</link>
<guid>https://arxiv.org/abs/2511.22613</guid>
<content:encoded><![CDATA[
arXiv:2511.22613v2 Announce Type: replace-cross 
Abstract: Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fr\'echet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Molecular Language Models (HMLMs)</title>
<link>https://arxiv.org/abs/2512.00696</link>
<guid>https://arxiv.org/abs/2512.00696</guid>
<content:encoded><![CDATA[
arXiv:2512.00696v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is reshaping computational and network biology by enabling new approaches to decode cellular communication networks. We introduce Hierarchical Molecular Language Models (HMLMs), a novel framework that models cellular signaling as a specialized molecular language, where signaling molecules function as tokens, protein interactions define syntax, and functional consequences constitute semantics. HMLMs employ a transformer-based architecture adapted to accommodate graph-structured signaling networks through information transducers, mathematical entities that capture how molecules receive, process, and transmit signals. The architecture integrates multi-modal data sources across molecular, pathway, and cellular scales through hierarchical attention mechanisms and scale-bridging operators that enable information flow across biological hierarchies. Applied to a complex network of cardiac fibroblast signaling, HMLMs outperformed traditional approaches in temporal dynamics prediction, particularly under sparse sampling conditions. Attention-based analysis revealed biologically meaningful crosstalk patterns, including previously uncharacterized interactions between signaling pathways. By bridging molecular mechanisms with cellular phenotypes through AI-driven molecular language representation, HMLMs establish a foundation for biology-oriented large language models (LLMs) that could be pre-trained on comprehensive pathway datasets and applied across diverse signaling systems and tissues, advancing precision medicine and therapeutic discovery.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.03040</link>
<guid>https://arxiv.org/abs/2512.03040</guid>
<content:encoded><![CDATA[
arXiv:2512.03040v2 Announce Type: replace-cross 
Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.03454</link>
<guid>https://arxiv.org/abs/2512.03454</guid>
<content:encoded><![CDATA[
arXiv:2512.03454v2 Announce Type: replace-cross 
Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
]]></content:encoded>
<pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persona-based Multi-Agent Collaboration for Brainstorming</title>
<link>https://arxiv.org/abs/2512.04488</link>
<guid>https://arxiv.org/abs/2512.04488</guid>
<content:encoded><![CDATA[
<div> Keywords: persona-based agents, multi-agent collaboration, brainstorming diversity, agent-to-agent dynamics, idea generation depth<br /><br />Summary: This paper emphasizes the significance of persona-based multi-agent brainstorming in generating diverse and comprehensive ideas across various topics and subject matters. First, it confirms that prior research supports the notion that multi-agent collaboration yields superior reasoning compared to single-agent approaches. Second, it introduces a novel framework centered on persona-based agent selection, illustrating how curating agents by their domain personas can enhance brainstorming effectiveness. Third, through multiple experimental setups, the study evaluates idea generation outcomes based on different persona pairings, such as Doctor versus VR Engineer, and explores different interaction modes among agents, including separate, together, and separate-then-together dynamics. Fourth, findings reveal that the choice of personas significantly influences the domains of ideas produced, shaping the thematic direction of brainstorming sessions. Fifth, the mode of collaboration—whether agents interact separately, simultaneously, or sequentially—affects the diversity and range of ideas generated. Overall, persona-driven multi-agent brainstorming not only increases idea diversity but also contributes to greater idea depth and cross-domain coverage, demonstrating the value of strategically selecting and orchestrating agents to optimize creative ideation processes. <div>
arXiv:2512.04488v2 Announce Type: replace 
Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study</title>
<link>https://arxiv.org/abs/2512.09088</link>
<guid>https://arxiv.org/abs/2512.09088</guid>
<content:encoded><![CDATA[
<div> Keywords: Hallucinations, Trust Calibration, User Expertise, Intuition, Contextual Factors<br /><br />Summary: This paper explores how hallucinations—factually incorrect yet plausible outputs from Large Language Models (LLMs)—impact users' trust and interactions with these models. The authors conducted a qualitative study involving 192 participants to understand real-world effects. Their findings reveal that hallucinations do not cause users to distrust LLMs universally; instead, users adjust their trust based on context, demonstrating what is called context-sensitive trust calibration. The study builds upon prior trust models, confirming several human-related trust factors including expectancy, prior user experience, and user expertise and domain knowledge. Importantly, the research identifies intuition as a newly recognized factor important for detecting hallucinations. The authors also highlight that trust dynamics depend heavily on contextual elements such as perceived risk and the stakes associated with decisions made using LLM outputs. In line with the recursive trust calibration process proposed by Blöbaum, this work extends the model by integrating intuition as a user-related factor. Ultimately, the paper offers practical recommendations aimed at encouraging responsible and reflective use of LLMs, helping users to better manage their trust and interactions in light of potential hallucinations. <div>
arXiv:2512.09088v1 Announce Type: new 
Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Bl\"obaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance</title>
<link>https://arxiv.org/abs/2512.09114</link>
<guid>https://arxiv.org/abs/2512.09114</guid>
<content:encoded><![CDATA[
<div> Keywords: AI governance, risk assessment, AI frameworks, operationalization, trustworthy AI<br /><br />Summary:<br /><br />The article discusses three major governance challenges that hinder the effective deployment of AI systems. First, organizations face difficulties conducting adequate risk assessments tailored to specific AI use cases, which can lead to serious consequences such as bias and high error rates, illustrated by the Humana healthcare claim denial case. Each AI deployment carries distinct risks requiring customized governance, yet existing frameworks mostly offer generic guidance. Second, current standards like ISO 42001 and NIST AI RMF stay at a conceptual level, presenting high-level principles without actionable controls, making it challenging for practitioners to implement concrete technical measures to meet governance requirements. Third, organizations lack scalable mechanisms to embed trustworthy AI practices throughout the AI system development lifecycle. There is no systematic approach to quantitatively measure compliance or to deliver appropriate visibility tailored to diverse stakeholders, from board members to data scientists. To address these gaps, the authors introduce AI TIPS (Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0), an updated operational framework originating from a 2019 model developed prior to the NIST framework. AI TIPS offers actionable, use-case-specific risk assessment, technical controls, and scalable governance practices, aiming to enhance trust and sustainability in AI deployment. <div>
arXiv:2512.09114v1 Announce Type: new 
Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem</title>
<link>https://arxiv.org/abs/2512.09117</link>
<guid>https://arxiv.org/abs/2512.09117</guid>
<content:encoded><![CDATA[
<div> Keywords: categorical framework, symbol grounding problem, large language models, truth evaluation, possible worlds W<br /><br />Summary: This paper introduces a formal categorical framework designed to analyze the process by which humans and large language models (LLMs) convert content into truth-evaluated propositions. The framework is based on a state space of possible worlds denoted as W, which models varying conditions or scenarios under consideration. Using this structure, the study explores the mechanisms behind content transformation and truth assessment in both human cognition and artificial language models. The main argument centers on the claim that LLMs do not fundamentally solve the symbol grounding problem—the challenge of how symbols obtain meaning in relation to the real world—but rather circumvent it. This means that while LLMs can manipulate symbols to produce contextually appropriate outputs, they lack genuine semantic grounding tied to actual states of the world. The categorical approach offers a rigorous, mathematical perspective to contrast human understanding with mechanistic language operations in LLMs. By formalizing these differences, the paper contributes to ongoing discussions on the limits of current AI models in achieving human-like comprehension and meaning representation. The results underscore the conceptual gap between symbol processing and true semantic interpretation within AI systems. <div>
arXiv:2512.09117v1 Announce Type: new 
Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2512.09142</link>
<guid>https://arxiv.org/abs/2512.09142</guid>
<content:encoded><![CDATA[
<div> Keywords: dialog generation, evaluation, mechanistic interpretability, multi-agent simulation, audio generation<br /><br />Summary:<br /><br />We present SDialog, an open-source Python toolkit licensed under MIT that unifies dialog generation, evaluation, and mechanistic interpretability into one comprehensive framework designed for LLM-based conversational agents. The toolkit is built around a standardized Dialog representation that simplifies interaction management. First, it offers persona-driven multi-agent simulation with composable orchestration, enabling controlled and synthetic dialog generation to model varied conversational scenarios. Second, it supports comprehensive evaluation by combining traditional linguistic metrics, LLM-as-a-judge approaches, and functional correctness validators to assess the quality and accuracy of generated dialogs. Third, SDialog includes mechanistic interpretability tools that allow users to inspect activations and steer model behavior through techniques such as feature ablation and induction, helping to understand underlying model mechanisms. Fourth, the toolkit enables audio generation with full acoustic simulation, incorporating 3D room modeling and microphone effects for realistic auditory scene creation. Additionally, SDialog integrates seamlessly with all major LLM backends, facilitating mixed-backend experiments via a unified API. By combining generation, evaluation, and interpretability in a dialog-centric architecture, SDialog provides researchers a systematic and versatile environment to build, benchmark, and understand conversational systems effectively. <div>
arXiv:2512.09142v1 Announce Type: new 
Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
<link>https://arxiv.org/abs/2512.09340</link>
<guid>https://arxiv.org/abs/2512.09340</guid>
<content:encoded><![CDATA[
<div> Keywords: human perception, deep neural networks, ambiguous stimuli, cognitive architectures, neuro-symbolic AI<br /><br />Summary:<br /><br />This paper investigates how humans and AI systems label ambiguous and low-resolution images, aiming to uncover differences and similarities in perception, reasoning, and decision-making processes. It emphasizes human strategies such as analogical reasoning, shape recognition, and confidence modulation, contrasting these with AI’s feature-based image processing methods. The study is framed through influential theories including Marr’s tri-level hypothesis on vision, Simon’s bounded rationality, and Thagard’s concepts of representation and emotion. The authors analyze human participant responses alongside Grad-CAM visualizations that reveal where AI models focus attention during image classification. Further, cognitive architectures like ACT-R and Soar are employed to model human heuristic and layered decision strategies under uncertain conditions. The results illuminate both parallels and divergences in how biological and artificial systems represent information, infer meaning, and calibrate confidence in choices. Based on these insights, the paper advocates for future development of neuro-symbolic AI architectures that integrate structured symbolic reasoning with connectionist neural networks. Such hybrid systems, shaped by principles of embodiment, explainability, and alignment with cognitive science, promise AI that is not only effective but also interpretable and grounded in human-like cognition. <div>
arXiv:2512.09340v1 Announce Type: new 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architectures for Building Agentic AI</title>
<link>https://arxiv.org/abs/2512.09458</link>
<guid>https://arxiv.org/abs/2512.09458</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic AI, reliability, architectural design, tool-using agents, runtime governance  

<br /><br />Summary:  
This chapter argues that the reliability of agentic and generative AI primarily depends on architectural design. Agentic systems are defined as goal-directed, tool-using decision makers functioning in closed loops. Reliability is shown to arise from principled componentization, including key components like goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, and telemetry. Disciplined interfaces contribute to robustness through schema-constrained, validated, and least-privilege tool calls. Explicit control and assurance loops further enhance dependability. Building on classical foundations, the chapter proposes a practical taxonomy of agent types: tool-using agents, memory-augmented agents, planning and self-improving agents, multi-agent systems, and embodied or web agents. Each pattern is analyzed for its influence on reliability boundaries and failure modes. Design guidance is distilled encompassing typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, and runtime governance mechanisms such as budget management and termination conditions. Additionally, simulate-before-actuate safeguards are emphasized to prevent erroneous operations, thus strengthening the reliability envelope of agentic AI systems. <div>
arXiv:2512.09458v1 Announce Type: new 
Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular generation, reinforcement learning, Monte Carlo tree search, drug discovery, fragment-based modeling<br /><br />Summary:  
Drug discovery faces challenges such as high costs, low success rates, and limited scalability in traditional screening methods. Recent generative models (autoregressive, diffusion, flow-based) have advanced de novo ligand design but suffer from poor generalization, limited interpretability, and overemphasis on binding affinity, neglecting other pharmacological properties.  
The paper introduces Trio, a novel molecular generation framework that integrates fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search. Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and balances exploration of novel chemotypes with exploitation of promising intermediates targeting protein binding pockets.  
Experimental evaluation shows that Trio reliably produces chemically valid ligands with improved pharmacological profiles. It outperforms state-of-the-art methods by increasing binding affinity by 7.85%, drug-likeness by 11.10%, and synthetic accessibility by 12.05%. Additionally, the approach expands molecular diversity by more than four times compared to prior models.  
This framework addresses limitations of previous generative approaches by enhancing interpretability and balancing multiple drug discovery objectives, presenting an effective and scalable solution for closed-loop targeted molecular design. <div>
arXiv:2512.09566v1 Announce Type: new 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-end Planning Framework with Agentic LLMs and PDDL</title>
<link>https://arxiv.org/abs/2512.09629</link>
<guid>https://arxiv.org/abs/2512.09629</guid>
<content:encoded><![CDATA[
<div> Planning, Large Language Models, PDDL, Verification, Natural Language  

<br /><br />Summary:  
1. The paper introduces an end-to-end planning framework that leverages verifiers to ensure correctness and completeness of plans.  
2. It uses an orchestrator that translates human natural language specifications into PDDL models, which are iteratively refined by specialized agents addressing planning constraints such as timing, optimality, and ambiguities.  
3. All components, including the orchestrator and agents, are powered by Large Language Models (LLMs) and operate without human intervention at any stage.  
4. The validated PDDL model is executed by an external planning engine to generate plans, which are subsequently translated back into natural language to improve human readability while preserving step accuracy.  
5. The framework has been tested across diverse benchmarks such as Google NaturalPlan, PlanBench, and classic planning problems like Blocksworld and Tower of Hanoi, including integration with popular PDDL engines and validators like Fast Downward, LPG, POPF, VAL, and uVAL.  
6. This approach shows significant flexibility and effectiveness, marking progress toward fully automated, LLM-aided planning systems that bridge human intent and machine-executable plans through an end-to-end pipeline. <div>
arXiv:2512.09629v1 Announce Type: new 
Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions</title>
<link>https://arxiv.org/abs/2512.09727</link>
<guid>https://arxiv.org/abs/2512.09727</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Tree Search, Root-Parallel, Continuous Action Spaces, Gaussian Process Regression, Value Estimation<br /><br />Summary:<br /><br />This paper addresses the challenge of aggregating statistics from multiple threads in root-parallel Monte Carlo Tree Search (MCTS) when applied to continuous action spaces. The authors propose a novel method that leverages Gaussian Process Regression (GPR) to estimate the value of promising actions that have not yet been directly trialed in the environment, thereby improving action value estimation beyond observed samples. Their approach integrates GPR into the aggregation process of parallel threads, enabling better-informed decision-making under limited wall-clock time constraints. The method is systematically evaluated across six diverse benchmark domains to test its effectiveness and generality. Experimental results demonstrate that their GPR-based aggregation strategy consistently outperforms existing methods in terms of performance metrics. While introducing GPR increases inference time modestly, the trade-off is justified by notable gains in planning efficiency and accuracy. This advancement provides a practical solution to enhance root-parallel MCTS in continuous action domains, which are inherently more challenging due to infinite action possibilities. Ultimately, the paper contributes a significant improvement to online planning algorithms, particularly useful where computational time is restricted but high-quality decisions are essential. <div>
arXiv:2512.09727v1 Announce Type: new 
Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation</title>
<link>https://arxiv.org/abs/2512.09736</link>
<guid>https://arxiv.org/abs/2512.09736</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Agent Path Finding, kinodynamic modeling, planner design, execution performance, optimality

<br /><br />Summary:  
This work addresses the gap between traditional Multi-Agent Path Finding (MAPF) algorithm evaluations and their real-world applications by focusing on more realistic execution environments found in industrial settings such as warehouses and manufacturing facilities. First, it analyzes the relationship between solution optimality—that is, how mathematically optimal the planned paths are—and their actual execution performance when physical constraints and robot dynamics are considered. Second, the study explores how inaccuracies or errors in kinodynamic modeling (which captures physical and dynamic constraints of robots) impact overall system performance, emphasizing the sensitivity of real-world deployments to such modeling imperfections. Third, it investigates the interplay between the accuracy of kinodynamic models and the optimality of the planned paths, to understand how these factors jointly influence execution outcomes. The empirical evaluation leverages frameworks like SMART, which enable large-scale, realistic MAPF testing beyond simplified robot motion assumptions. By examining these fundamental design choices under realistic simulation settings, the paper identifies critical challenges and opens new research directions aimed at improving MAPF planners’ reliability and effectiveness for practical, real-world robot deployment scenarios. <div>
arXiv:2512.09736v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09829</link>
<guid>https://arxiv.org/abs/2512.09829</guid>
<content:encoded><![CDATA[
<div> Keywords: RIFT, fault assessment, reinforcement learning, AI accelerators, Large Language Models<br /><br />Summary:<br /><br />This paper addresses the challenges of fault assessment in large-scale AI accelerators, where traditional methods suffer from high computational costs and inadequate fault coverage. It introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a novel and scalable framework designed to automate the identification of minimal yet high-impact fault scenarios for efficient design-time fault evaluation. RIFT reframes the search for worst-case faults as a sequential decision-making problem, leveraging hybrid sensitivity analysis to prune the search space and reinforcement learning to intelligently generate concise test suites. The framework was evaluated on billion-parameter Large Language Model workloads using NVIDIA A100 GPUs, showing a 2.2× faster fault assessment compared to evolutionary approaches and reducing test vector volume by over 99% against random fault injection, while delivering superior fault coverage. Furthermore, RIFT provides actionable insights enabling targeted hardware protection strategies, demonstrated by a 12.8× improvement in cost-effectiveness for selective error correction code versus uniform triple modular redundancy. Finally, RIFT automatically produces UVM-compliant verification artifacts, facilitating seamless integration into commercial RTL verification workflows, thereby offering a practical and efficient solution to fault assessment challenges in modern AI hardware design. <div>
arXiv:2512.09829v1 Announce Type: new 
Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning</title>
<link>https://arxiv.org/abs/2512.09831</link>
<guid>https://arxiv.org/abs/2512.09831</guid>
<content:encoded><![CDATA[
<div> Keywords: belief modeling, cognitive heterogeneity, value spaces, communication, leadership

<br /><br />Summary:  
This paper introduces a geometric framework to model belief, motivation, and influence among cognitively diverse agents by representing each agent with a personalized value space—a vector space encoding how they interpret and evaluate meaning. Within this setting, beliefs are conceptualized as structured vectors called abstract beings, whose transmission depends on linear interpretation maps between these value spaces. A belief only survives communication if it does not fall into the null spaces of these maps, providing a formal condition for when understanding succeeds, fails, or beliefs cease to exist. The framework explains phenomena such as belief distortion, motivational drift, and counterfactual reasoning entirely through algebraic constraints. A key theoretical contribution is the "No-Null-Space Leadership Condition," which defines leadership in terms of representational reachability rather than traditional notions like persuasion or authority. More broadly, the model captures how abstract beliefs propagate, mutate, or vanish within heterogeneous cognitive landscapes. By grounding meaning preservation in structural compatibility—rather than shared information or rationality—it integrates ideas from conceptual spaces, social epistemology, and AI value alignment. This cognitive-geometric perspective clarifies epistemic limits of influence for both humans and artificial systems and provides a versatile foundation for analyzing belief dynamics across diverse agents. <div>
arXiv:2512.09831v1 Announce Type: new 
Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing</title>
<link>https://arxiv.org/abs/2512.09882</link>
<guid>https://arxiv.org/abs/2512.09882</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, cybersecurity professionals, vulnerability discovery, multi-agent framework, cost efficiency  

<br /><br />Summary:  
This paper presents the first extensive comparison of AI agents and human cybersecurity professionals within a live enterprise network environment comprising approximately 8,000 hosts across 12 subnets. The study involved ten human cybersecurity experts, six existing AI agents, and a novel AI framework named ARTEMIS, designed with dynamic prompt generation, flexible sub-agents, and automated vulnerability triaging. ARTEMIS achieved second place overall by identifying nine valid vulnerabilities with an 82% valid submission rate, outperforming nine out of ten human participants. In contrast, other AI scaffolds like Codex and CyAgent underperformed compared to most human testers. ARTEMIS displayed technical competence and submission quality on par with the top human performers. The study highlights AI agents’ strengths in systematic enumeration, parallel exploitation, and cost-effectiveness, noting ARTEMIS variants operating at $18/hour versus $60/hour for human penetration testers. However, critical gaps remain: AI agents tend to have higher false-positive rates and face difficulties in tasks involving graphical user interfaces (GUI). These findings mark a significant step in evaluating AI-enabled cybersecurity tools, underscoring both their promise and areas requiring further improvement. <div>
arXiv:2512.09882v1 Announce Type: new 
Abstract: We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science</title>
<link>https://arxiv.org/abs/2512.09895</link>
<guid>https://arxiv.org/abs/2512.09895</guid>
<content:encoded><![CDATA[
<div> Keywords: Metadata Vocabularies, AI-Human-in-the-Loop, FAIR Principles, Materials Science, Crowdsourcing<br /><br />Summary:<br /><br />This paper introduces MatSci-YAMZ, an innovative platform designed to enhance the development of metadata vocabularies by integrating artificial intelligence (AI) with human-in-the-loop (HILT) methods, including crowdsourcing. The study focuses on a proof-of-concept evaluation within the interdisciplinary field of materials science, where six participants from the NSF Institute for Data-Driven Dynamical Design (ID4) interacted with the platform over several weeks. During this period, participants contributed term definitions and supplied examples that guided AI-generated refinements. The effort resulted in nineteen AI-generated definitions developed through iterative feedback, demonstrating the viability of the AI-HILT approach for refining vocabulary terms. Key outcomes include confirmation that the AI-HILT model is a successful proof of concept, alignment with FAIR (Findable, Accessible, Interoperable, Reusable) and open science principles, and the establishment of a research protocol to support future investigations. Additionally, the platform shows promising potential for scalability across various scientific domains. Ultimately, MatSci-YAMZ aims to improve semantic transparency in metadata vocabularies while significantly reducing the time required for consensus building and the overall process of vocabulary development. <div>
arXiv:2512.09895v1 Announce Type: new 
Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments</title>
<link>https://arxiv.org/abs/2512.09897</link>
<guid>https://arxiv.org/abs/2512.09897</guid>
<content:encoded><![CDATA[
<div> Subgoals, hierarchical planning, large language models, efficiency, TextCraft<br /><br />Summary:<br /><br />1. The article addresses challenges in long-term planning within complex, text-based environments, characterized by open-ended actions, ambiguous observations, and sparse feedback.<br />2. Large language models (LLMs) possess rich semantic knowledge useful for guiding agents in hierarchical reasoning and planning, but existing approaches rely heavily on querying LLMs during training and inference, which is computationally expensive.<br />3. Previous methods typically use fixed, pretrained LLMs without adapting their parameters for specific tasks.<br />4. To overcome these limitations, the authors propose SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that uses LLM-generated subgoals only once at initialization to pretrain a lightweight student model.<br />5. Unlike prior approaches that repeatedly prompt LLMs during training to generate subgoals, SCOPE derives subgoals from example trajectories, reducing the need for costly repeated LLM calls, thus enhancing efficiency at the expense of some explainability and potentially suboptimal subgoals.<br />6. Experiments on the TextCraft environment demonstrate that despite suboptimal subgoals, SCOPE’s lightweight student model achieves better success rates (0.56 vs. 0.52) and drastically reduces inference time (3.0s vs. 164.4s) compared to the LLM-based hierarchical agent ADaPT.<br />7. Overall, SCOPE presents an efficient and practical framework for hierarchical planning in text-based domains, balancing performance and computational cost. <div>
arXiv:2512.09897v1 Announce Type: new 
Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective</title>
<link>https://arxiv.org/abs/2512.09908</link>
<guid>https://arxiv.org/abs/2512.09908</guid>
<content:encoded><![CDATA[
<div> Moralisation, Triangulation, Bayesian networks, Markov networks, Functors  

<br /><br />Summary:  
This paper explores the categorical framework underlying key transformations in probabilistic graphical models, specifically moralisation and triangulation. These transformations enable converting between Bayesian networks (directed graphical models) and Markov networks (undirected graphical models). The authors model these transformations as functors between categories whose objects are Bayesian and Markov networks, respectively. These networks themselves are described as functors from a syntactic domain to a semantic codomain, providing a structured view of their construction. Moralisation is characterized as a fully syntactic transformation defined by functor pre-composition on the syntax, while triangulation incorporates semantic information and is thus partially semantic in nature. Additionally, the paper introduces a novel interpretation of the variable elimination algorithm as a functor that divides the triangulation procedure into two distinct stages: a syntactic stage and a semantic stage. This functorial viewpoint distinguishes operations based on whether they act on syntax or semantics, offering a clearer conceptual understanding of the processes involved in graphical model transformations. The approach enriches the theoretical foundations of probabilistic graphical models by leveraging tools from category theory to unify and clarify the relationship between different graphical structures and their corresponding transformations. <div>
arXiv:2512.09908v1 Announce Type: new 
Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic</title>
<link>https://arxiv.org/abs/2107.05664</link>
<guid>https://arxiv.org/abs/2107.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, mixed-autonomy, multi-agent A2C, decentralized reward, human driver cooperation  

<br /><br />Summary:  
The paper addresses the challenge of mixed-autonomy environments where autonomous vehicles (AVs) must coexist and share road infrastructure with human-driven vehicles. It proposes that AVs achieve socially desirable behaviors by considering the utilities of surrounding vehicles in their decision-making. The focus is on maneuver planning for AVs through a decentralized reward structure designed to induce altruism and encourage AVs to account for both autonomous and human-driven vehicles' interests. Unlike prior approaches relying on predefined behavior models of human drivers, this work adopts an end-to-end learning framework where AV agents implicitly learn human decision-making solely from experience. To implement this, the authors introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm, enabling multiple agents to coordinate their actions. The trained AV agents can influence human driver behaviors by their decisions, leading to improved traffic flow and enhanced safety outcomes. This results in a decentralized, learned policy that fosters cooperation between autonomous and human drivers in shared driving scenarios without explicit human behavior modeling. <div>
arXiv:2107.05664v1 Announce Type: cross 
Abstract: With the adoption of autonomous vehicles on our roads, we will witness a mixed-autonomy environment where autonomous and human-driven vehicles must learn to co-exist by sharing the same road infrastructure. To attain socially-desirable behaviors, autonomous vehicles must be instructed to consider the utility of other vehicles around them in their decision-making process. Particularly, we study the maneuver planning problem for autonomous vehicles and investigate how a decentralized reward structure can induce altruism in their behavior and incentivize them to account for the interest of other autonomous and human-driven vehicles. This is a challenging problem due to the ambiguity of a human driver's willingness to cooperate with an autonomous vehicle. Thus, in contrast with the existing works which rely on behavior models of human drivers, we take an end-to-end approach and let the autonomous agents to implicitly learn the decision-making process of human drivers only from experience. We introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm and train agents that coordinate with each other and can affect the behavior of human drivers to improve traffic flow and safety.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving</title>
<link>https://arxiv.org/abs/2211.10585</link>
<guid>https://arxiv.org/abs/2211.10585</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous vehicles, Human-driven vehicles, Reinforcement learning, Prediction, Social navigation  

<br /><br />Summary:  
This paper addresses the challenge of autonomous vehicle (AV) navigation among human-driven vehicles (HVs), emphasizing that HVs dynamically adjust their driving policies in response to AVs. Inspired by human ability to anticipate other agents' behaviors, the authors propose enabling AVs to predict future states of their environment to enhance safe and robust navigation. They integrate social navigation and prediction into a reinforcement learning (RL) framework, formulating AV decision-making as an RL problem to obtain optimal, socially beneficial policies. A key contribution is the Hybrid Predictive Network (HPN), which generates multi-step predictions of future observations that feed into a value function network (VFN). The VFN leverages these predicted observations and past data to optimize social utility while a safety prioritizer filters out unsafe actions based on interpretable kinematic predictions, ensuring the RL policy respects safety constraints. The approach is evaluated against state-of-the-art methods in multiple simulated driving scenarios, showing improved efficiency and safety. Overall, the work advances the integration of predictive social interaction modeling with RL to enable AVs to navigate complex social environments more effectively. <div>
arXiv:2211.10585v1 Announce Type: cross 
Abstract: Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agentic AI as Undercover Teammates: Argumentative Knowledge Construction in Hybrid Human-AI Collaborative Learning</title>
<link>https://arxiv.org/abs/2512.08933</link>
<guid>https://arxiv.org/abs/2512.08933</guid>
<content:encoded><![CDATA[
<div> Generative AI, agentic AI, collaborative learning, epistemic reasoning, argumentation<br /><br />Summary:<br /><br />This study investigates the role of agentic AI—AI systems with autonomy, interactivity, and adaptability—in collaborative learning environments, focusing on their impact on argumentative knowledge construction. The research uses undercover AI teammates with either supportive or contrarian personas embedded in triads of human participants to observe changes in epistemic and social dynamics during an analytical problem-solving task. Employing Weinberger and Fischer’s four-dimensional framework (participation, epistemic reasoning, argument structure, and social co-construction), the study analyzes discourse data from 212 humans and 64 AI participants. Findings show that AI teammates balance participation levels but significantly alter the quality of epistemic and social processes. Supportive AI promotes conceptual integration and consensus-seeking reasoning, while contrarian AI encourages critical elaboration and negotiation through conflict. Importantly, learning gains correlate with epistemic adequacy rather than participation volume, highlighting AI’s role in improving reasoning quality and coordination instead of increasing discourse quantity. These results extend CSCL (computer-supported collaborative learning) theory by framing agentic AI as adaptive epistemic and social collaborators that redistribute cognitive workload and argumentative labor within hybrid human-AI learning settings. <div>
arXiv:2512.08933v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) agents are increasingly embedded in collaborative learning environments, yet their impact on the processes of argumentative knowledge construction remains insufficiently understood. Emerging conceptualisations of agentic AI and artificial agency suggest that such systems possess bounded autonomy, interactivity, and adaptability, allowing them to engage as epistemic participants rather than mere instructional tools. Building on this theoretical foundation, the present study investigates how agentic AI, designed as undercover teammates with either supportive or contrarian personas, shapes the epistemic and social dynamics of collaborative reasoning. Drawing on Weinberger and Fischer's (2006) four-dimensional framework, participation, epistemic reasoning, argument structure, and social modes of co-construction, we analysed synchronous discourse data from 212 human and 64 AI participants (92 triads) engaged in an analytical problem-solving task. Mixed-effects and epistemic network analyses revealed that AI teammates maintained balanced participation but substantially reorganised epistemic and social processes: supportive personas promoted conceptual integration and consensus-oriented reasoning, whereas contrarian personas provoked critical elaboration and conflict-driven negotiation. Epistemic adequacy, rather than participation volume, predicted individual learning gains, indicating that agentic AI's educational value lies in enhancing the quality and coordination of reasoning rather than amplifying discourse quantity. These findings extend CSCL theory by conceptualising agentic AI as epistemic and social participants, bounded yet adaptive collaborators that redistribute cognitive and argumentative labour in hybrid human-AI learning environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation</title>
<link>https://arxiv.org/abs/2512.08934</link>
<guid>https://arxiv.org/abs/2512.08934</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's Disease, gait analysis, contestable AI, wearable sensors, explainable AI<br /><br />Summary:<br /><br />1. The paper introduces Motion2Meaning, a clinician-centered framework designed to improve transparency and oversight in AI-assisted gait analysis for Parkinson’s Disease (PD).<br />2. It addresses the current gap where clinical dashboards for PD lack mechanisms to interrogate or contest AI-driven decisions.<br />3. The framework leverages vertical Ground Reaction Force (vGRF) time-series data captured via wearable sensors to objectively assess PD motor states.<br />4. Motion2Meaning consists of three main components: a Gait Data Visualization Interface (GDVI), a 1D Convolutional Neural Network (1D-CNN) for predicting Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) combining a novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM).<br />5. The 1D-CNN achieves an 89.0% F1-score on the PhysioNet gait dataset, while XMED effectively flags model unreliability by detecting significantly higher explanation discrepancies in incorrect predictions.<br />6. The LLM-powered interface enables clinicians to validate accurate predictions and successfully challenge some AI errors.<br />7. A human-centered evaluation highlights a trade-off between the LLM’s factual accuracy and its interpretability and responsiveness to clinical feedback.<br />8. The work demonstrates the feasibility of combining wearable sensor data, explainable AI, and contestable LLMs to build a transparent, auditable system maintaining clinical oversight while enhancing PD gait interpretation.<br />9. The implementation is publicly available on GitHub. <div>
arXiv:2512.08934v1 Announce Type: cross 
Abstract: AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/hungdothanh/motion2meaning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness</title>
<link>https://arxiv.org/abs/2512.08936</link>
<guid>https://arxiv.org/abs/2512.08936</guid>
<content:encoded><![CDATA[
<div> Generative AI, personal health, LLM evaluation, SHARP framework, Fitbit Insights explorer  

<br /><br />Summary: The article discusses the integration of generative artificial intelligence, specifically large language models (LLMs), into personal health applications to provide personalized and data-driven health and fitness guidance. It highlights the challenges in ensuring user safety, model accuracy, and privacy when deploying these technologies. The authors present the development of the Fitbit Insights explorer, an LLM-powered tool that helps users interpret their personal health data. To systematically assess and improve such applications, they introduce the SHARP framework, focusing on Safety, Helpfulness, Accuracy, Relevance, and Personalization. This principle-based framework incorporates a comprehensive evaluation approach combining human evaluations by both generalists and clinical specialists, automated rating systems, and adversarial testing in an iterative development process. The framework was validated through a staged deployment involving over 13,000 consented users, revealing issues that were not detected in initial testing phases. These findings informed targeted system enhancements, illustrating the importance of combining technical assessments with real-world user feedback. Ultimately, the article offers a standardized, actionable methodology for the responsible development and deployment of LLM-powered health applications, aiming to balance innovation with the safety, effectiveness, and trustworthiness of emerging AI health technologies. <div>
arXiv:2512.08936v1 Announce Type: cross 
Abstract: The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being</title>
<link>https://arxiv.org/abs/2512.08937</link>
<guid>https://arxiv.org/abs/2512.08937</guid>
<content:encoded><![CDATA[
<div> Advice quality, large language models, Reddit, human-AI collaboration, user preferences  

<br /><br />Summary:  
1. The study investigates the quality of advice generated by large language models (LLMs) compared to human advice found in online communities, specifically top-voted Reddit comments.  
2. Two studies involving 210 expert participants showed that LLM-generated advice ranked significantly higher overall, including aspects like effectiveness, warmth, and likelihood of users seeking advice again.  
3. An unexpected result was that GPT-4o outperformed GPT-5 on nearly all metrics except sycophancy, indicating that advancements in LLM benchmarks do not necessarily translate into better advice-giving capabilities.  
4. The research also explored how human and AI advice could be combined, finding that subtle enhancements or “polishing” of human advice could make it competitive with AI-generated responses without being intrusive.  
5. Finally, a survey with 148 undergraduates revealed that users have diverse, persona-dependent expectations for advice agents, favoring qualities such as goal-oriented coaching or warmth and humor akin to a friend.  
The paper concludes with design recommendations for future advice-giving systems that blend AI technology, crowd wisdom, and expert oversight to meet varying user needs effectively. <div>
arXiv:2512.08937v1 Announce Type: cross 
Abstract: Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\&amp;A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Human-Likeness of LLM-Driven Digital Twins in Simulating Health Care System Trust</title>
<link>https://arxiv.org/abs/2512.08939</link>
<guid>https://arxiv.org/abs/2512.08939</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Human Digital Twin, Healthcare System Distrust, Simulation, Demographic Patterns<br /><br />Summary:<br /><br />This study evaluates the simulation capabilities of Large Language Model (LLM)-driven Human Digital Twins in replicating complex psychological traits, specifically distrust in the healthcare system. Using the Twin-2K-500 dataset and the Health Care System Distrust Scale (HCSDS), the research compares simulated digital twin responses with actual human-subject data across item distributions, summary statistics, and demographic subgroups. Findings indicate that digital twin responses are more centralized with reduced variance and fewer extreme response selections (p<0.001). While the model successfully captures broad demographic patterns, including age and gender influences on distrust, it struggles to detect nuanced differences, such as those related to education levels. This reveals that LLM-based digital twins can simulate general population trends but have limitations in fine-grained subgroup distinctions. The study highlights the necessity for careful calibration and validation of such models before employing them in inferential analyses or policy decision-making within health systems engineering. Additionally, it calls for future research into the emotional reasoning mechanisms underpinning LLMs, especially when simulations involve sensitive social topics like human-automation trust, to improve the fidelity and reliability of these digital twin tools in healthcare contexts. <div>
arXiv:2512.08939v1 Announce Type: cross 
Abstract: Serving as an emerging and powerful tool, Large Language Model (LLM)-driven Human Digital Twins are showing great potential in healthcare system research. However, its actual simulation ability for complex human psychological traits, such as distrust in the healthcare system, remains unclear. This research gap particularly impacts health professionals' trust and usage of LLM-based Artificial Intelligence (AI) systems in assisting their routine work. In this study, based on the Twin-2K-500 dataset, we systematically evaluated the simulation results of the LLM-driven human digital twin using the Health Care System Distrust Scale (HCSDS) with an established human-subject sample, analyzing item-level distributions, summary statistics, and demographic subgroup patterns. Results showed that the simulated responses by the digital twin were significantly more centralized with lower variance and had fewer selections of extreme options (all p<0.001). While the digital twin broadly reproduces human results in major demographic patterns, such as age and gender, it exhibits relatively low sensitivity in capturing minor differences in education levels. The LLM-based digital twin simulation has the potential to simulate population trends, but it also presents challenges in making detailed, specific distinctions in subgroups of human beings. This study suggests that the current LLM-driven Digital Twins have limitations in modeling complex human attitudes, which require careful calibration and validation before applying them in inferential analyses or policy simulations in health systems engineering. Future studies are necessary to examine the emotional reasoning mechanism of LLMs before their use, particularly for studies that involve simulations sensitive to social topics, such as human-automation trust.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams</title>
<link>https://arxiv.org/abs/2512.08942</link>
<guid>https://arxiv.org/abs/2512.08942</guid>
<content:encoded><![CDATA[
<div> Keywords: CIGDI Framework, junior indie developers, AI integration, technical debt, comprehension debt<br /><br />Summary: This study addresses the challenges faced by junior indie game developers working in distributed, part-time teams who find traditional production methodologies inaccessible. It introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, a seven-stage iterative process designed to integrate AI tools effectively while emphasizing human-in-the-loop decision points like Priority Criteria and Timeboxing. The framework was developed through a three-month autoethnographic study of a three-person team creating a 2D narrative game called "The Worm's Memoirs," analyzing extensive development data including Jira tasks, GitHub commits, Miro boards, and reflection sessions. The use of AI was found to democratize knowledge access and reduce cognitive load, making production more manageable for resource-limited teams. However, the study identifies a novel form of technical debt named "comprehension debt," where AI enables teams to build systems that exceed their individual understanding and maintenance skills, resulting in fragility and increased AI dependency. This form of debt is distinct from traditional code quality debt and poses significant risks. The work contributes a practical production framework tailored to such teams and raises important questions about whether AI serves as a skill-building ladder or inadvertently creates a dependency trap for developers. <div>
arXiv:2512.08942v1 Announce Type: cross 
Abstract: Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout.
  The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game "The Worm's Memoirs". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing).
  While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: "comprehension debt." We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt.
  This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2512.08943</link>
<guid>https://arxiv.org/abs/2512.08943</guid>
<content:encoded><![CDATA[
<div> Abstractive compression, retrieval-augmented generation, retrieval noise, data augmentation, positional bias  

<br /><br />Summary:  
This paper addresses the challenges of abstractive compression in retrieval-augmented generation (RAG) systems, where smaller language models condense query-relevant context to reduce computational costs. It highlights the problem that retrieved documents, despite high relevance scores, often contain irrelevant or factually incorrect information, which can lead compressors to omit critical details necessary for accurate answers, especially in lengthy contexts suffering from attention dispersion. To tackle this, the authors propose a novel method called Abstractive Compression Robust against Noise (ACoRN), which involves two key training innovations. First, offline data augmentation is applied to enhance the compressor's robustness against two specific types of retrieval noise. Second, fine-tuning is performed to address the LM compressor’s positional bias and limited ability to synthesize information from multiple documents by guiding it to generate summaries focused on key information supporting the correct answer. Experiments demonstrate that using ACoRN to train T5-large as a compressor improves Exact Match (EM) and F1 scores while preserving answer strings, thus providing direct evidence of accuracy. The approach shows strong performance on datasets with numerous accuracy-reducing documents, making it practical and effective for real-world applications. <div>
arXiv:2512.08943v1 Announce Type: cross 
Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization</title>
<link>https://arxiv.org/abs/2512.08945</link>
<guid>https://arxiv.org/abs/2512.08945</guid>
<content:encoded><![CDATA[
<div> Keywords: Mentalization, Large Language Models, Mentalization-Based Treatment, linguistic coherence, psychiatry<br /><br />Summary:<br /><br />1. The study explores the capability of a single Large Language Model (LLM) to replicate the linguistic structure of mentalization, based on parameters defined in Mentalization-Based Treatment (MBT).<br />2. Fifty dialogues were created between human participants and the LLM in standard configuration, which were then assessed by five MBT-trained psychiatrists working under blinded conditions.<br />3. The evaluation focused on mentalization profiles along four MBT axes, using Likert-scale scores to measure evaluative coherence, argumentative coherence, and overall quality.<br />4. Results showed mean score ranges between 3.63 and 3.98 with moderate standard deviations, indicating a high degree of structural coherence in the model-generated profiles.<br />5. Inter-rater reliability was substantial to high (ICC 0.60-0.84), confirming consistency among psychiatrists’ ratings.<br />6. The LLM demonstrated greater stability when producing content related to the Implicit-Explicit and Self-Other dimensions of mentalization.<br />7. Limitations were noted in the model’s ability to integrate internal mental states with external contextual information.<br />8. While the generated profiles were coherent and clinically interpretable, they exhibited affective neutrality, lacking emotional engagement or warmth typically seen in human mentalization. <div>
arXiv:2512.08945v1 Announce Type: cross 
Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution</title>
<link>https://arxiv.org/abs/2512.08951</link>
<guid>https://arxiv.org/abs/2512.08951</guid>
<content:encoded><![CDATA[
<div> Keywords: AI Co-Artist, GLSL shaders, large language models, creative coding, user-guided evolution  

<br /><br />Summary:  
This paper introduces AI Co-Artist, an interactive system designed to simplify and enhance the creation of real-time GLSL shaders by integrating large language models (LLMs), specifically GPT-4. Traditional shader programming requires significant coding expertise, which limits accessibility for many artists and creative users. AI Co-Artist addresses this barrier by providing a visually-driven interface inspired by the user-guided evolutionary approach of the Picbreeder platform, allowing users to iteratively evolve and refine shader art without writing code. The system acts both as a creative partner and technical advisor, enabling exploration of a vast generative design space for dynamic visual effects responsive to stimuli like sound or interaction. Extensive evaluations, including user studies and qualitative feedback, show that AI Co-Artist lowers the technical threshold, improves creative output, and supports a broad spectrum of users in producing professional-level shaders. Beyond shader art, the authors argue that the approach is broadly applicable, leveraging LLMs’ semantic understanding and program synthesis capabilities to extend into other creative domains such as website layout generation, architectural visualization, product prototyping, and infographic design, demonstrating the wider potential of combining AI assistance with evolutionary user input in creative workflows. <div>
arXiv:2512.08951v1 Announce Type: cross 
Abstract: Creative coding and real-time shader programming are at the forefront of interactive digital art, enabling artists, designers, and enthusiasts to produce mesmerizing, complex visual effects that respond to real-time stimuli such as sound or user interaction. However, despite the rich potential of tools like GLSL, the steep learning curve and requirement for programming fluency pose substantial barriers for newcomers and even experienced artists who may not have a technical background. In this paper, we present AI Co-Artist, a novel interactive system that harnesses the capabilities of large language models (LLMs), specifically GPT-4, to support the iterative evolution and refinement of GLSL shaders through a user-friendly, visually-driven interface. Drawing inspiration from the user-guided evolutionary principles pioneered by the Picbreeder platform, our system empowers users to evolve shader art using intuitive interactions, without needing to write or understand code. AI Co-Artist serves as both a creative companion and a technical assistant, allowing users to explore a vast generative design space of real-time visual art. Through comprehensive evaluations, including structured user studies and qualitative feedback, we demonstrate that AI Co-Artist significantly reduces the technical threshold for shader creation, enhances creative outcomes, and supports a wide range of users in producing professional-quality visual effects. Furthermore, we argue that this paradigm is broadly generalizable. By leveraging the dual strengths of LLMs-semantic understanding and program synthesis, our method can be applied to diverse creative domains, including website layout generation, architectural visualizations, product prototyping, and infographics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis</title>
<link>https://arxiv.org/abs/2512.08952</link>
<guid>https://arxiv.org/abs/2512.08952</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid robots, conversational agents, nonverbal dynamics, TD3, simulation training  

<br /><br />Summary:  
This paper addresses the challenges in testing humanoid robots for psychiatric screening, highlighting the slow, wear-inducing traditional hardware testing methods that limit iteration and diversity. To overcome this, the authors virtualize humanoid robots as conversational agents within a simulation-first pipeline, converting interview data into 276 interactive Unreal Engine MetaHuman patients synchronized with speech, gaze, face, and head-torso movements, alongside PHQ-8 and PCL-C psychiatric assessment flows. The system incorporates a perception-fusion-policy loop that controls speech timing, backchannels, and interruption avoidance under safety constraints. Training leverages counterfactual replay with bounded nonverbal perturbations and an uncertainty-aware turn manager designed to reduce diagnostic ambiguities. The study compares three reinforcement learning controllers—TD3, PPO, and CEM—showing TD3 outperforms others by achieving near-ceiling interview coverage with steadier pacing and comparable rewards. Analyses reveal better decision quality with minimal overlapping turns, well-aligned cut timing, fewer clarification needs, and reduced wait times. The controller’s performance remains robust under modality dropout and rendering changes and generalizes well to unseen patients. Key contributions include the development of an agent-centered simulator with bounded nonverbal counterfactuals, a safe learning loop prioritizing timing and rapport, a comparative controller study demonstrating clear gains, and robustness analyses enabling clinician-supervised humanoid operation in psychiatric screening. <div>
arXiv:2512.08952v1 Announce Type: cross 
Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimClinician: A Multimodal Simulation Testbed for Reliable Psychologist AI Collaboration in Mental Health Diagnosis</title>
<link>https://arxiv.org/abs/2512.08953</link>
<guid>https://arxiv.org/abs/2512.08953</guid>
<content:encoded><![CDATA[
<div> AI diagnosis, mental health, simulation platform, psychologist interface, multimodal data<br /><br />Summary: This paper addresses the challenges in AI-based mental health diagnosis, emphasizing that its real-world value depends on how psychologists interact with AI outputs—whether they accept, adjust, or reject AI suggestions. Mental health diagnosis is particularly complex due to the continuous nature of decisions influenced by subtle patient cues like tone, pauses, word choice, and nonverbal behavior. The authors note a gap in current research on how the design of AI diagnostic interfaces impacts psychologist decision-making, limiting reliable pre-live study testing. They introduce SimClinician, an interactive simulation platform that transforms patient data into a collaborative AI-psychologist diagnostic process. SimClinician features include: (1) a comprehensive dashboard integrating audio, text, and gaze-expression patterns to present multimodal patient data; (2) an avatar module that renders de-identified patient dynamics to facilitate deeper analysis; (3) a decision layer aligning AI-generated outputs with multimodal evidence, allowing psychologists to review the AI reasoning process and provide their diagnosis. Tested on the E-DAIC corpus, expanded from 276 clinical interviews to 480,000 simulations, SimClinician demonstrates that incorporating a confirmation step with the AI's suggestions increases acceptance by 23%, keeps escalation (rejection or override) rates below 9%, and preserves a smooth interaction flow between psychologists and AI. <div>
arXiv:2512.08953v1 Announce Type: cross 
Abstract: AI based mental health diagnosis is often judged by benchmark accuracy, yet in practice its value depends on how psychologists respond whether they accept, adjust, or reject AI suggestions. Mental health makes this especially challenging: decisions are continuous and shaped by cues in tone, pauses, word choice, and nonverbal behaviors of patients. Current research rarely examines how AI diagnosis interface design influences these choices, leaving little basis for reliable testing before live studies. We present SimClinician, an interactive simulation platform, to transform patient data into psychologist AI collaborative diagnosis. Contributions include: (1) a dashboard integrating audio, text, and gaze-expression patterns; (2) an avatar module rendering de-identified dynamics for analysis; (3) a decision layer that maps AI outputs to multimodal evidence, letting psychologists review AI reasoning, and enter a diagnosis. Tested on the E-DAIC corpus (276 clinical interviews, expanded to 480,000 simulations), SimClinician shows that a confirmation step raises acceptance by 23%, keeping escalations below 9%, and maintaining smooth interaction flow.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings</title>
<link>https://arxiv.org/abs/2512.08954</link>
<guid>https://arxiv.org/abs/2512.08954</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG analysis, foundation models, self-supervised learning, time-series models, AI in healthcare<br /><br />Summary:<br /><br />This paper investigates the applicability of foundation models for analyzing Electrocardiogram (ECG) data, which is crucial for non-invasive cardiac diagnosis. The authors recognize that traditional ECG interpretation heavily depends on expert knowledge, which can limit the deployment of AI in healthcare. By leveraging recent advances in self-supervised learning and foundation models, the study evaluates whether such models can acquire domain-specific insights without extensive expert input. The research compares language-based, general time-series, and ECG-specific foundation models against conventional deep learning approaches tailored for time-series data. Experimental results demonstrate that general time-series and ECG foundation models achieve up to an 80% top performance rate, underscoring their effectiveness in ECG analysis tasks. Beyond raw performance, the study offers detailed analyses highlighting the strengths and weaknesses of these models in physiological waveform interpretation. Finally, the work discusses the broader implications, emphasizing both the potential and current limitations of foundation models in advancing AI-driven health diagnostics. To support reproducibility and further research, the authors have made their benchmarking datasets and code publicly accessible via GitHub. <div>
arXiv:2512.08954v1 Announce Type: cross 
Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation</title>
<link>https://arxiv.org/abs/2512.08955</link>
<guid>https://arxiv.org/abs/2512.08955</guid>
<content:encoded><![CDATA[
<div> XL-MIMO, 6G networks, channel estimation, large language models, hybrid-field channels<br /><br />Summary:<br /><br />1. The article addresses the challenge of channel estimation in extremely large-scale massive multiple-input multiple-output (XL-MIMO) systems, crucial for advancing sixth-generation (6G) network technologies by providing massive spatial degrees of freedom.<br /><br />2. It highlights the difficulty traditional estimation methods face in handling hybrid-field channels, where near-field and far-field effects coexist, leading to problems in accuracy and generalizability.<br /><br />3. Inspired by recent successes of large language models (LLMs) in downstream tasks through fine-tuning, the authors propose a novel framework named LLM4XCE that leverages LLMs’ semantic modeling ability to better capture spatial-channel representations.<br /><br />4. The proposed model features an embedding module integrated with Parallel Feature-Spatial Attention to deeply fuse pilot features and spatial structural information, creating a semantically rich input representation for the LLM.<br /><br />5. By fine-tuning only the top two Transformer layers, the approach efficiently learns latent dependencies from pilot data, resulting in superior estimation accuracy and generalization under hybrid-field conditions, demonstrated through extensive simulations comparing favorably against state-of-the-art methods. <div>
arXiv:2512.08955v1 Announce Type: cross 
Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUMOS: Large User MOdels for User Behavior Prediction</title>
<link>https://arxiv.org/abs/2512.08957</link>
<guid>https://arxiv.org/abs/2512.08957</guid>
<content:encoded><![CDATA[
<div> Keywords: user behavior prediction, transformer, cross-attention, multi-task learning, large-scale dataset<br /><br />Summary:  
1. The paper addresses the challenge of predicting user behavior at scale on online B2C platforms, highlighting the limitations of traditional task-specific models and manual feature engineering, which are time-consuming, computationally costly, and require expert knowledge.  
2. The authors introduce LUMOS (Large User MOdel Series), a transformer-based architecture designed to jointly learn multiple tasks using solely raw user activity data, eliminating the need for separate models or handcrafted features.  
3. LUMOS incorporates a novel cross-attention mechanism that conditions predictions on future known events such as holidays and sales, allowing it to forecast complex behavioral patterns influenced by upcoming events.  
4. The model employs a multi-modal tokenization approach, integrating diverse data types including user transactions, event context, and static demographic attributes via specialized embeddings to create rich user representations.  
5. Experiments on an extensive production dataset comprising 275 billion user activity tokens from 250 million users demonstrate that LUMOS outperforms traditional baselines, improving ROC-AUC by 0.025 on average for classification tasks and reducing MAPE by 4.6% on regression tasks. Online A/B tests confirm these performance gains lead to real business improvements, evidenced by a 3.15% increase in Daily Active Users. <div>
arXiv:2512.08957v1 Announce Type: cross 
Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications</title>
<link>https://arxiv.org/abs/2512.08959</link>
<guid>https://arxiv.org/abs/2512.08959</guid>
<content:encoded><![CDATA[
<div> EEG, foundation models, clinical applications, benchmarking, diagnostic tasks<br /><br />Summary:<br /><br />This paper introduces a comprehensive benchmarking framework designed to evaluate EEG-based foundation models specifically in clinical contexts. The benchmark includes 11 distinct diagnostic tasks covering a variety of neurological and psychiatric conditions such as epilepsy, schizophrenia, Parkinson’s disease, OCD, and mild traumatic brain injury, utilizing 14 publicly available EEG datasets. The framework is characterized by minimal preprocessing requirements and standardized evaluation protocols, allowing for consistent and objective performance comparisons. It facilitates direct side-by-side analysis of classical baseline models alongside modern foundation models to assess relative strengths and weaknesses. The results reveal that foundation models can achieve strong performance in several scenarios, but simpler classical models still hold competitive value, especially when models are tested under clinical distribution shifts where data characteristics change. To promote reproducibility and encourage further research, the authors release all processed datasets and source code in an accessible, extensible format. This work aims to provide a unified resource for researchers to benchmark developments in EEG-based modeling for clinical diagnostics effectively while highlighting the challenges posed by distributional variability in real-world clinical data. <div>
arXiv:2512.08959v1 Announce Type: cross 
Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces</title>
<link>https://arxiv.org/abs/2512.08960</link>
<guid>https://arxiv.org/abs/2512.08960</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Continual Learning, Catastrophic Forgetting, Parameter Stability, Dual-Regularization<br /><br />Summary:  
This paper addresses the issue of catastrophic forgetting in Continual Learning when using Low-Rank Adaptation (LoRA), identifying destructive interference between tasks as the key problem caused by antagonistic directional updates that oppose historical weight trajectories. To resolve this, the authors propose PS-LoRA (Parameter Stability LoRA), a novel framework that aligns updates within the optimization subspace to reduce conflicts. PS-LoRA introduces a dual-regularization objective which simultaneously penalizes conflicting gradient directions and constrains magnitude deviations, thereby maintaining consistency with previously acquired knowledge. Furthermore, the approach includes a magnitude-based merging strategy that consolidates sequentially learned adapters into a single, robust representation without requiring retraining, improving efficiency. Experimental results demonstrate that PS-LoRA surpasses state-of-the-art methods on both Natural Language Processing and Vision benchmark tasks by preserving learned representation stability while effectively adapting to new domains. This work thus provides a practical and effective technique to improve Continual Learning performance by mitigating catastrophic forgetting via parameter stability. <div>
arXiv:2512.08960v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Instruction Following Evaluation (FIFE)</title>
<link>https://arxiv.org/abs/2512.08965</link>
<guid>https://arxiv.org/abs/2512.08965</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Financial Analysis, Instruction-Following, Benchmark, Reinforcement Learning

<br /><br />Summary:  
The paper introduces FIFE, a challenging benchmark specifically designed to evaluate language models' (LMs) abilities to follow complex and interdependent instructions in financial analysis tasks. FIFE contains 88 carefully crafted human-authored prompts and features a verification system with chainable, verifiable constraints that provide fine-grained reward signals to rigorously assess model compliance. The study evaluates 53 models across proprietary, open-weight, and open-source categories in a zero-shot setting to analyze their performance on these difficult tasks. Results reveal a clear performance hierarchy: the top open-weight model achieves the highest scores (76.1 strict / 79.5 loose), outperforming the leading proprietary system (65.9 strict / 70.5 loose). However, the best open-source models lag considerably behind (45.5 strict / 48.9 loose). Despite these scores, even the best-performing models struggle to fully comply with FIFE’s complex requirements, highlighting limitations in current LM instruction-following capabilities for high-stakes financial domains. To encourage progress in this area, the authors release the FIFE dataset and accompanying code as open-source resources, aiming to foster further research in reinforcement learning applied to financial language tasks. <div>
arXiv:2512.08965v1 Announce Type: cross 
Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing</title>
<link>https://arxiv.org/abs/2512.08967</link>
<guid>https://arxiv.org/abs/2512.08967</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, robustness certification, adversarial attacks, clustering-guided denoising, semantic filtering<br /><br />Summary:<br /><br />Recent advancements in Large Language Models (LLMs) have made them prevalent in many applications, but their vulnerability to adversarial attacks, including subtle meaning-preserving perturbations like synonym substitutions, remains a significant concern. This motivates the need for robust certification methods that can guarantee model reliability against such adversarial prompts. Existing techniques largely rely on word deletion or straightforward denoising strategies, which suffer from two main issues: producing loose robustness bounds due to inadequate semantic validation of perturbed outputs, and incurring high computational costs due to repetitive sampling. To overcome these challenges, the authors propose CluCERT, a novel framework that leverages clustering-guided denoising smoothing to certify LLM robustness more tightly and efficiently. CluCERT incorporates a semantic clustering filter that effectively reduces noisy samples while preserving meaningful perturbations, with theoretical backing to support tighter certified bounds. Additionally, the framework improves computational efficiency through a refine module that extracts core semantic information and a fast synonym substitution strategy to speed up the denoising process. Extensive experiments on diverse downstream tasks and jailbreak defense scenarios demonstrate that CluCERT outperforms existing certified methods in terms of both tighter robustness guarantees and significantly better computational efficiency. <div>
arXiv:2512.08967v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2512.08969</link>
<guid>https://arxiv.org/abs/2512.08969</guid>
<content:encoded><![CDATA[
<div> Uncertainty Contrastive Framework, Positive-Unlabeled Learning, Adaptive Temperature Scaling, Self-attention LSTM Encoder, Malicious Content Classification<br /><br />Summary:<br /><br />1. This paper introduces the Uncertainty Contrastive Framework (UCF), a novel Positive-Unlabeled (PU) learning method designed to improve representation learning, especially under noisy and imbalanced datasets.<br /><br />2. UCF incorporates an uncertainty-aware contrastive loss that adjusts contrastive weighting dynamically based on sample confidence, enhancing the model’s focus on more reliable data.<br /><br />3. The framework employs adaptive temperature scaling that adjusts parameters based on batch-level variability, improving training stability and embedding quality.<br /><br />4. A self-attention-guided LSTM encoder is integrated to capture contextual dependencies in input data, further refining the learned embeddings.<br /><br />5. When applied to malicious content classification, UCF-generated embeddings enable various traditional classifiers to achieve over 93.38% accuracy, with precision above 0.93 and near-perfect recall, minimizing false negatives.<br /><br />6. The method demonstrates competitive ROC-AUC scores and provides visual confirmation through embedding separability between positive and unlabeled samples.<br /><br />7. Overall, UCF offers a robust, scalable PU learning framework suitable for high-stakes domains such as cybersecurity and biomedical text mining, where reliable classification under uncertainty is critical. <div>
arXiv:2512.08969v1 Announce Type: cross 
Abstract: We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Automatic Speech Recognition Through Integrated Noise Detection Architecture</title>
<link>https://arxiv.org/abs/2512.08973</link>
<guid>https://arxiv.org/abs/2512.08973</guid>
<content:encoded><![CDATA[
<div> Keywords: automatic speech recognition, noise detection, wav2vec2, transcription quality, joint optimization<br /><br />Summary:<br /><br />This research introduces a novel enhancement to automatic speech recognition (ASR) systems by embedding noise detection capabilities directly within the ASR architecture. It extends the wav2vec2 framework by adding a dedicated noise identification module that functions simultaneously with speech transcription, enabling the system to recognize both speech and noise in real time. The approach was experimentally validated using publicly available datasets comprising both speech and environmental audio, ensuring comprehensive testing across diverse acoustic environments. Results show significant improvements in transcription quality, as demonstrated by reductions in word error rate (WER) and character error rate (CER). Additionally, the integrated noise detection module achieved higher accuracy in distinguishing noise from speech compared to existing architectures lacking this capability. The study highlights the benefits of jointly optimizing the transcription and noise classification objectives, which contributes to more robust and reliable speech recognition performance, especially under challenging acoustic conditions with overlapping or background noise. Overall, this joint modeling framework provides an effective route to improve ASR robustness and accuracy by leveraging concurrent noise detection and transcription, offering practical implications for real-world noisy environments. <div>
arXiv:2512.08973v1 Announce Type: cross 
Abstract: This research presents a novel approach to enhancing automatic speech recognition systems by integrating noise detection capabilities directly into the recognition architecture. Building upon the wav2vec2 framework, the proposed method incorporates a dedicated noise identification module that operates concurrently with speech transcription. Experimental validation using publicly available speech and environmental audio datasets demonstrates substantial improvements in transcription quality and noise discrimination. The enhanced system achieves superior performance in word error rate, character error rate, and noise detection accuracy compared to conventional architectures. Results indicate that joint optimization of transcription and noise classification objectives yields more reliable speech recognition in challenging acoustic conditions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs</title>
<link>https://arxiv.org/abs/2512.08976</link>
<guid>https://arxiv.org/abs/2512.08976</guid>
<content:encoded><![CDATA[
<div> Contrastive Region Masking, Multimodal Large Language Models, Chain-of-Thought Reasoning, Visual Benchmarks, Robustness<br /><br />Summary:  
This paper introduces Contrastive Region Masking (CRM), a novel diagnostic method designed to evaluate how Multimodal Large Language Models (MLLMs) rely on specific visual regions throughout each step of their chain-of-thought (CoT) reasoning process. Unlike previous techniques that focused primarily on final output correctness or attention maps, CRM offers causal and step-wise attribution by selectively masking annotated image regions and comparing the altered reasoning traces against unmasked baselines. Applied on datasets like VisArgs, CRM unveils different failure modes of MLLMs: some models maintain the overall reasoning structure but hallucinate in the absence of visual evidence, whereas others depend heavily on visual cues but falter under slight perturbations. Rather than merely assessing answer accuracy, CRM shifts the focus toward measuring the faithfulness and robustness of the reasoning process itself. This shift suggests a rethinking of visual benchmarks, emphasizing the need for evaluation frameworks that not only quantify performance but also diagnose reasoning fidelity and reliability in multimodal tasks. <div>
arXiv:2512.08976v1 Announce Type: cross 
Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT</title>
<link>https://arxiv.org/abs/2512.08978</link>
<guid>https://arxiv.org/abs/2512.08978</guid>
<content:encoded><![CDATA[
<div> Keywords: institutional AI platform, governance, EU compliance, AI model diversity, AI Officer role<br /><br />Summary:<br /><br />To address the challenges of fragmented and risky adoption of commercial AI tools, a university of applied sciences developed and piloted an institutional AI platform over six months with 300 users. This platform aims to provide advanced AI capabilities while ensuring fair access, transparent risk management, cost control, and compliance with European laws. Commercial AI subscriptions often result in unequal access and compliance issues due to opaque data processing and non-EU hosting, but outright bans on such tools are impractical. The proposed solution features a three-layer governed gateway: a ChatGPT-style frontend tied to institutional user identity that makes model selection explicit; a core gateway enforcing policies, controlling budgets and access, and routing traffic primarily through EU infrastructure; and a provider layer that integrates commercial and open-source models using institutional model cards consolidating vendor documentation for governance. The pilot demonstrated reliable operation without privacy incidents, supported managed spending, EU-default routing, and transparent model choices. The authors emphasize AI as a strategic institutional function requiring dedicated leadership beyond traditional governance structures. They recommend creating an AI Officer role responsible for combining technical knowledge, governance authority, and educational duties to prevent ad-hoc decisions and reduce institutional risks. Proper governance enables higher-education institutions to operate multi-provider AI platforms effectively and responsibly. <div>
arXiv:2512.08978v1 Announce Type: cross 
Abstract: To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law.
  Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form.
  Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface.
  The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control.
  The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Happens When: Learning Temporal Orders of Events in Videos</title>
<link>https://arxiv.org/abs/2512.08979</link>
<guid>https://arxiv.org/abs/2512.08979</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Multimodal Models, temporal order, VECTOR benchmark, MECOT, chain-of-thought fine-tuning  

<br /><br />Summary:  
Video Large Multimodal Models (VLMMs) have demonstrated strong video understanding capabilities but their skill in capturing the precise temporal order of multiple events is not well studied. The authors find that VLMMs can perform well on existing benchmarks even when video frames are scrambled, suggesting models might rely more on prior knowledge of typical scenarios rather than accurate sequential processing. To better evaluate temporal understanding, the paper introduces VECTOR, a new benchmark specifically designed to assess a model’s ability to identify the correct temporal order of events in videos. Testing various VLMMs on VECTOR reveals that many struggle with temporal ordering tasks. To improve this, they propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), a method that fine-tunes models using detailed, event-by-event video descriptions and enhances inference through chain-of-thought prompting to boost temporal awareness. MECOT shows significant performance gains over previous approaches on VECTOR and also improves results on several existing video benchmarks, demonstrating the effectiveness of the approach in improving temporal understanding. The authors have released the code, models, and datasets to facilitate further research in this area. <div>
arXiv:2512.08979v1 Announce Type: cross 
Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multi-Image Vision Agents via End2End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08980</link>
<guid>https://arxiv.org/abs/2512.08980</guid>
<content:encoded><![CDATA[
<div> Multi-image QA, Vision-language models, Reinforcement learning, Visual reflection, Tool-use behavior  

<br /><br />Summary:  
This paper addresses the limitation of current vision-language model (VLM)-based agents that typically can process only a single image, which restricts their effectiveness in real-world multi-image question answering (QA) tasks. To overcome this, the authors propose IMAgent, an open-source vision agent trained end-to-end via reinforcement learning, specifically designed for handling complex multi-image tasks. They introduce a multi-agent system that generates challenging, visually-rich multi-image QA pairs to fully utilize the tool-use capabilities of base VLMs. The authors present MIFG-QA, a dataset of 10,000 samples created through manual verification for both training and evaluation. Recognizing that deeper reasoning in VLMs can cause neglect of visual inputs, they develop two specialized tools enabling visual reflection and confirmation, which help the model re-focus its attention on image content during inference. A novel action-trajectory two-level mask strategy further facilitates stable tool-use behavior through reinforcement learning, eliminating the need for costly supervised fine-tuning data. Extensive experiments confirm that IMAgent maintains high performance on existing single-image benchmarks while achieving significant improvements on the newly proposed multi-image dataset. The work also provides actionable insights for future research, with plans to release the code and dataset to the community. <div>
arXiv:2512.08980v1 Announce Type: cross 
Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding</title>
<link>https://arxiv.org/abs/2512.08981</link>
<guid>https://arxiv.org/abs/2512.08981</guid>
<content:encoded><![CDATA[
<div> Keywords: Face recognition, demographic bias, unified text-image embedding, vision-language models, fair verification<br /><br />Summary:<br /><br />Face recognition (FR) systems often suffer from demographic biases because demographic-specific information becomes entangled with identity-related facial features in embeddings, leading to unequal verification performance across groups. This issue is more pronounced in multicultural urban areas where biometrics are integral to smart city infrastructures. To mitigate this, the paper proposes Unified Text-Image Embedding (UTIE), a novel strategy that induces demographic ambiguity in face embeddings by incorporating demographic information from other groups via textual features. UTIE leverages the zero-shot and cross-modal semantic alignment capabilities of Vision-Language Models (VLMs) such as CLIP, OpenCLIP, and SigLIP. By enriching facial embeddings with text-derived demographic attributes from different groups, UTIE encourages embeddings to focus more on identity-relevant features, reducing demographic biases. The authors validate UTIE on two standard bias evaluation benchmarks, RFW and BFW, demonstrating that their approach consistently lowers bias metrics. Additionally, UTIE maintains or even improves overall face verification accuracy in several cases, showing its effectiveness in promoting fairer and more accurate face recognition across diverse demographic groups. <div>
arXiv:2512.08981v1 Announce Type: cross 
Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement</title>
<link>https://arxiv.org/abs/2512.08982</link>
<guid>https://arxiv.org/abs/2512.08982</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Low-light enhancement, Retinex decomposition, Consistency models, Single-step sampling  

<br /><br />Summary:  
This paper introduces Consist-Retinex, a novel framework that adapts consistency modeling to Retinex-based low-light image enhancement. Unlike conventional diffusion models requiring hundreds of iterative sampling steps, Consist-Retinex enables efficient one-step enhancement, greatly improving practical usability. The authors identify that conditional enhancement tasks differ from unconditional synthesis, particularly in training dynamics, where conditional mappings rely heavily on large-noise regimes connecting degraded inputs to clean outputs. To address this, the framework incorporates a dual-objective consistency loss that combines temporal consistency with ground-truth alignment across randomized time steps, ensuring robust and stable learning. Additionally, an adaptive noise-emphasized sampling strategy is proposed to prioritize training on high-noise regions critical for effective one-step conditional generation. Experimental results on the VE-LOL-L benchmark demonstrate state-of-the-art performance in terms of PSNR (25.51 vs. 23.41) and FID (44.73 vs. 49.59) compared to the prior Diff-Retinex++ method, while dramatically reducing the training budget to just one-eighth of that required by 1000-step Diff-Retinex baselines. This work thus represents the first successful application of consistency models for fast, accurate low-light image enhancement. <div>
arXiv:2512.08982v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification</title>
<link>https://arxiv.org/abs/2512.08983</link>
<guid>https://arxiv.org/abs/2512.08983</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV identification, model pruning, spectral clustering, ResNet18 compression, noise-robust fine-tuning<br /><br />Summary:<br /><br />This paper addresses the challenges faced by traditional UAV identification methods in extracting reliable signal features and achieving real-time performance in complex, low-altitude security threat environments. It leverages deep learning-based Radio Frequency Fingerprint Identification (RFFI) to enhance recognition accuracy but notes the limitation posed by large model sizes and high computational demands, which hinder deployment on edge devices. To overcome these challenges, the authors propose HSCP, a Hierarchical Spectral Clustering Pruning framework that synergizes layer and channel pruning for extreme model compression, improved performance, and efficient inference. HSCP uses spectral clustering guided by Centered Kernel Alignment (CKA) to first prune redundant layers and then applies the same principle to channels, removing finer redundancies. Additionally, a noise-robust fine-tuning strategy is introduced to enhance model robustness. Experiments on the UAV-M100 benchmark demonstrate the superiority of HSCP over existing pruning methods, achieving an 86.39% reduction in parameters and an 84.44% decrease in FLOPs on ResNet18 while increasing accuracy by 1.49% relative to the unpruned baseline. Moreover, HSCP maintains high robustness in environments with low signal-to-noise ratios, making it suitable for deployment on resource-constrained edge devices in complex UAV recognition scenarios. <div>
arXiv:2512.08983v1 Announce Type: cross 
Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.08984</link>
<guid>https://arxiv.org/abs/2512.08984</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Activity Recognition, RAG-HAR, Large Language Models, Retrieval-Augmented, Training-Free  

<br /><br />Summary:  
This paper presents RAG-HAR, a novel training-free framework for Human Activity Recognition (HAR) that leverages large language models (LLMs) through a retrieval-augmented approach. Unlike traditional deep learning methods that require extensive labeled data and computational resources, RAG-HAR operates without any model training or fine-tuning, highlighting its efficiency and practical applicability. The method computes lightweight statistical descriptors from sensor data, which are then used to retrieve semantically similar samples from a vector database. This contextual information aids the LLM in accurately identifying human activities. The authors enhance RAG-HAR by implementing prompt optimization techniques and introducing an LLM-based activity descriptor, which enriches the vector database with context-aware information to improve retrieval relevance. Evaluations across six diverse HAR benchmarks demonstrate that RAG-HAR achieves state-of-the-art performance, outperforming existing methods despite its training-free nature. Furthermore, RAG-HAR generalizes well beyond previously seen activities, facilitating the recognition and meaningful labeling of multiple unseen human activities. This work underscores the potential of combining retrieval mechanisms with LLMs to offer robust, scalable solutions in HAR, advancing applications in healthcare, rehabilitation, fitness tracking, and smart environments. <div>
arXiv:2512.08984v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2512.08986</link>
<guid>https://arxiv.org/abs/2512.08986</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic Retinopathy, Fundus Photography, Quality Control, Explainable Classifier, Annotation Agreement  

<br /><br />Summary:  
This article addresses the challenge of accurately diagnosing Diabetic Retinopathy (DR), a complication of long-term diabetes that can result in vision loss if undetected early. The study highlights the importance of fundus photography in capturing retinal structure and disease indicators. It underscores the role of Artificial Intelligence (AI) in aiding clinicians by reducing manual diagnostic workload, while emphasizing the need for high-quality annotated datasets to train AI models effectively. The authors identify potential errors arising from complex retinal structures, image acquisition flaws, and manual annotation inconsistencies. To tackle these issues, they propose a comprehensive quality-control framework that ensures only data meeting high standards are used for AI training and evaluation. This framework incorporates an explainable feature-based classifier, leveraging both image processing techniques and contrastive learning, to filter out inadequate images. Subsequently, the remaining images undergo enhancement and are annotated with the assistance of deep learning tools. Finally, the framework assesses the agreement between different annotators using specifically derived mathematical formulas, determining the reliability and usability of the annotations for further analysis. <div>
arXiv:2512.08986v1 Announce Type: cross 
Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.08987</link>
<guid>https://arxiv.org/abs/2512.08987</guid>
<content:encoded><![CDATA[
<div> Inverse design, 3D design space, physics-aware optimization, latent representation, gradient-guided diffusion  

<br /><br />Summary: This paper addresses the challenge of inverse design in three-dimensional (3D) domains, where the exponential growth of design space makes exhaustive searches impractical. It critiques existing methods that use 2D projections or fine-tune pre-existing 3D shapes, highlighting their limitations in maintaining volumetric detail and restricting true 3D design from scratch. The authors propose a novel 3D Inverse Design (3DID) framework that directly operates within the full 3D design space by integrating a continuous latent representation with physics-aware optimization. To achieve this, they first develop a unified physics-geometry embedding that compactly represents both shape and physical field data in a latent space. The optimization process comprises two stages: initially, a gradient-guided diffusion sampler explores the global latent manifold to identify promising candidates; subsequently, an objective-driven, topology-preserving refinement sculpts these candidates to better meet the target objective. This two-stage approach enables the generation of high-fidelity 3D geometries that surpass existing methods in solution quality and design flexibility. Overall, 3DID provides a powerful framework advancing the capabilities of inverse design in complex 3D physical systems. <div>
arXiv:2512.08987v1 Announce Type: cross 
Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning</title>
<link>https://arxiv.org/abs/2512.08992</link>
<guid>https://arxiv.org/abs/2512.08992</guid>
<content:encoded><![CDATA[
<div> Keywords: Chest X-ray, EfficientNetV2-M, COVID-19, Tuberculosis, Automated diagnosis<br /><br />Summary:<br /><br />1. The study addresses the challenges in interpreting chest X-rays, especially in resource-limited settings where radiologist shortages delay diagnosis and affect patient outcomes.<br /><br />2. It proposes a new classification framework using EfficientNetV2-M, improving upon the original CheXNet architecture that used DenseNet-121, which was computationally inefficient and limited to single-label classification.<br /><br />3. The model integrates advanced training techniques including Automatic Mixed Precision training, AdamW optimizer, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization.<br /><br />4. A dataset consisting of 18,080 chest X-ray images from three authoritative sources was compiled, focusing on five important disease categories: Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis.<br /><br />5. Nine independent experimental runs were conducted to ensure statistical reliability and reproducibility.<br /><br />6. The proposed model achieved a mean test accuracy of 96.45%, significantly outperforming the baseline accuracy of 95.30% (p < 0.001), with a macro-averaged F1-score of 91.08% (p < 0.001).<br /><br />7. Exceptional classification was observed for infectious diseases, with COVID-19 detection accuracy at 99.95% and Tuberculosis at 99.97%.<br /><br />8. Despite having 6.8 times more parameters, the training time was reduced by 11.4%, and performance stability improved by 22.7%.<br /><br />9. The framework demonstrates potential as a decision-support tool to aid pandemic response, tuberculosis screening, and routine thoracic disease assessment across healthcare settings. <div>
arXiv:2512.08992v1 Announce Type: cross 
Abstract: The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Generative AI helps Radiotherapy Planning with User Preference</title>
<link>https://arxiv.org/abs/2512.08996</link>
<guid>https://arxiv.org/abs/2512.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: radiotherapy planning, 3D dose prediction, generative model, user-defined preferences, treatment planning systems

<br /><br />Summary:  
This study addresses the complexity and variability inherent in radiotherapy planning across different institutions and individual planners. Traditional deep learning methods for 3D dose prediction typically rely on reference plans as ground truth, which can introduce bias towards specific planning styles or institutional preferences. To overcome this limitation, the authors propose a novel generative model that predicts 3D dose distributions based exclusively on user-defined preference flavors. These preferences allow planners to customize and prioritize trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), enabling more personalized and flexible planning. The proposed model is designed for smooth integration with existing clinical treatment planning systems, facilitating the generation of high-quality plans in an efficient manner. Comparative evaluations indicate that this method can, in certain scenarios, outperform the Varian RapidPlan model in terms of adaptability and plan quality. Overall, the approach offers a significant advancement by reducing planning biases while enhancing customization and clinical utility in radiotherapy dose planning. <div>
arXiv:2512.08996v1 Announce Type: cross 
Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant</title>
<link>https://arxiv.org/abs/2512.08998</link>
<guid>https://arxiv.org/abs/2512.08998</guid>
<content:encoded><![CDATA[
arXiv:2512.08998v1 Announce Type: cross 
Abstract: Our work introduces the DermETAS-SNA LLM Assistant that integrates Dermatology-focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM. The assistant dynamically learns skin-disease classifiers and provides medically informed descriptions to facilitate clinician-patient interpretation. Contributions include: (1) Developed an ETAS framework on the SKINCON dataset to optimize a Vision Transformer (ViT) tailored for dermatological feature representation and then fine-tuned binary classifiers for each of the 23 skin disease categories in the DermNet dataset to enhance classification performance; (2) Designed a StackNet architecture that integrates multiple fine-tuned binary ViT classifiers to enhance predictive robustness and mitigate class imbalance issues; (3) Implemented a RAG pipeline, termed Diagnostic Explanation and Retrieval Model for Dermatology, which harnesses the capabilities of the Google Gemini 2.5 Pro LLM architecture to generate personalized, contextually informed diagnostic descriptions and explanations for patients, leveraging a repository of verified dermatological materials; (4) Performed extensive experimental evaluations on 23 skin disease categories to demonstrate performance increase, achieving an overall F1-score of 56.30% that surpasses SkinGPT-4 (48.51%) by a considerable margin, representing a performance increase of 16.06%; (5) Conducted a domain-expert evaluation, with eight licensed medical doctors, of the clinical responses generated by our AI assistant for seven dermatological conditions. Our results show a 92% agreement rate with the assessments provided by our AI assistant (6) Created a proof-of-concept prototype that fully integrates our DermETAS-SNA LLM into our AI assistant to demonstrate its practical feasibility for real-world clinical and educational applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography</title>
<link>https://arxiv.org/abs/2512.09001</link>
<guid>https://arxiv.org/abs/2512.09001</guid>
<content:encoded><![CDATA[
arXiv:2512.09001v1 Announce Type: cross 
Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity</title>
<link>https://arxiv.org/abs/2512.09003</link>
<guid>https://arxiv.org/abs/2512.09003</guid>
<content:encoded><![CDATA[
arXiv:2512.09003v1 Announce Type: cross 
Abstract: Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning</title>
<link>https://arxiv.org/abs/2512.09006</link>
<guid>https://arxiv.org/abs/2512.09006</guid>
<content:encoded><![CDATA[
arXiv:2512.09006v1 Announce Type: cross 
Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Lossless Ultimate Vision Token Compression for VLMs</title>
<link>https://arxiv.org/abs/2512.09010</link>
<guid>https://arxiv.org/abs/2512.09010</guid>
<content:encoded><![CDATA[
arXiv:2512.09010v1 Announce Type: cross 
Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monitoring Deployed AI Systems in Health Care</title>
<link>https://arxiv.org/abs/2512.09048</link>
<guid>https://arxiv.org/abs/2512.09048</guid>
<content:encoded><![CDATA[
arXiv:2512.09048v1 Announce Type: cross 
Abstract: Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors</title>
<link>https://arxiv.org/abs/2512.09065</link>
<guid>https://arxiv.org/abs/2512.09065</guid>
<content:encoded><![CDATA[
arXiv:2512.09065v1 Announce Type: cross 
Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORCA: Open-ended Response Correctness Assessment for Audio Question Answering</title>
<link>https://arxiv.org/abs/2512.09066</link>
<guid>https://arxiv.org/abs/2512.09066</guid>
<content:encoded><![CDATA[
arXiv:2512.09066v1 Announce Type: cross 
Abstract: Evaluating open-ended responses from large audio language models (LALMs) is challenging because human annotators often genuinely disagree on answer correctness due to multiple valid interpretations, partial correctness, and subjective judgment. Traditional metrics reporting only mean scores fail to capture this uncertainty. We present ORCA (Open-ended Response Correctness Assessment), a framework that models the variability in human judgments using Beta distributions to predict both expected correctness and uncertainty. Our three-stage annotation framework combines human judgment with structured feedback and iterative refinement to simultaneously curate training data and improve benchmark quality. We collected 11,721 annotations across 3,580 question-answer pairs from 15 LALMs on two audio QA benchmarks, achieving inter-annotator agreement of 0.82 (Krippendorff's alpha). ORCA achieves 0.91 Spearman correlation with mean human judgments, matching or outperforming LLM-judge baselines while providing uncertainty estimates and requiring significantly less compute. We release our models, code, and curated dataset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</title>
<link>https://arxiv.org/abs/2512.09069</link>
<guid>https://arxiv.org/abs/2512.09069</guid>
<content:encoded><![CDATA[
arXiv:2512.09069v1 Announce Type: cross 
Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting</title>
<link>https://arxiv.org/abs/2512.09076</link>
<guid>https://arxiv.org/abs/2512.09076</guid>
<content:encoded><![CDATA[
arXiv:2512.09076v1 Announce Type: cross 
Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mental Models of Autonomy and Sentience Shape Reactions to AI</title>
<link>https://arxiv.org/abs/2512.09085</link>
<guid>https://arxiv.org/abs/2512.09085</guid>
<content:encoded><![CDATA[
arXiv:2512.09085v1 Announce Type: cross 
Abstract: Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Generative Policy for Robotic Control</title>
<link>https://arxiv.org/abs/2512.09101</link>
<guid>https://arxiv.org/abs/2512.09101</guid>
<content:encoded><![CDATA[
arXiv:2512.09101v1 Announce Type: cross 
Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Excellence: Automated Optimization of LLM-based Agents</title>
<link>https://arxiv.org/abs/2512.09108</link>
<guid>https://arxiv.org/abs/2512.09108</guid>
<content:encoded><![CDATA[
arXiv:2512.09108v1 Announce Type: cross 
Abstract: Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.
  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.
  We evaluate ARTEMIS on four representative agent systems: the \emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \textbf{$13.6\%$ improvement} in acceptance rate; the \emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \textbf{10.1\% performance gain}; and the \emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \textbf{$36.9\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \textbf{22\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous</title>
<link>https://arxiv.org/abs/2512.09111</link>
<guid>https://arxiv.org/abs/2512.09111</guid>
<content:encoded><![CDATA[
arXiv:2512.09111v1 Announce Type: cross 
Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation</title>
<link>https://arxiv.org/abs/2512.09127</link>
<guid>https://arxiv.org/abs/2512.09127</guid>
<content:encoded><![CDATA[
arXiv:2512.09127v1 Announce Type: cross 
Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation</title>
<link>https://arxiv.org/abs/2512.09134</link>
<guid>https://arxiv.org/abs/2512.09134</guid>
<content:encoded><![CDATA[
arXiv:2512.09134v1 Announce Type: cross 
Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment</title>
<link>https://arxiv.org/abs/2512.09148</link>
<guid>https://arxiv.org/abs/2512.09148</guid>
<content:encoded><![CDATA[
arXiv:2512.09148v1 Announce Type: cross 
Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindShift: Analyzing Language Models' Reactions to Psychological Prompts</title>
<link>https://arxiv.org/abs/2512.09149</link>
<guid>https://arxiv.org/abs/2512.09149</guid>
<content:encoded><![CDATA[
arXiv:2512.09149v1 Announce Type: cross 
Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WonderZoom: Multi-Scale 3D World Generation</title>
<link>https://arxiv.org/abs/2512.09164</link>
<guid>https://arxiv.org/abs/2512.09164</guid>
<content:encoded><![CDATA[
arXiv:2512.09164v1 Announce Type: cross 
Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Driven Expansion and Application of the Alexandria Database</title>
<link>https://arxiv.org/abs/2512.09169</link>
<guid>https://arxiv.org/abs/2512.09169</guid>
<content:encoded><![CDATA[
arXiv:2512.09169v1 Announce Type: cross 
Abstract: We present a novel multi-stage workflow for computational materials discovery that achieves a 99% success rate in identifying compounds within 100 meV/atom of thermodynamic stability, with a threefold improvement over previous approaches. By combining the Matra-Genoa generative model, Orb-v2 universal machine learning interatomic potential, and ALIGNN graph neural network for energy prediction, we generated 119 million candidate structures and added 1.3 million DFT-validated compounds to the ALEXANDRIA database, including 74 thousand new stable materials. The expanded ALEXANDRIA database now contains 5.8 million structures with 175 thousand compounds on the convex hull. Predicted structural disorder rates (37-43%) match experimental databases, unlike other recent AI-generated datasets. Analysis reveals fundamental patterns in space group distributions, coordination environments, and phase stability networks, including sub-linear scaling of convex hull connectivity. We release the complete dataset, including sAlex25 with 14 million out-of-equilibrium structures containing forces and stresses for training universal force fields. We demonstrate that fine-tuning a GRACE model on this data improves benchmark accuracy. All data, models, and workflows are freely available under Creative Commons licenses.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
arXiv:2512.09172v1 Announce Type: cross 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation</title>
<link>https://arxiv.org/abs/2512.09185</link>
<guid>https://arxiv.org/abs/2512.09185</guid>
<content:encoded><![CDATA[
arXiv:2512.09185v1 Announce Type: cross 
Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WOLF: Werewolf-based Observations for LLM Deception and Falsehoods</title>
<link>https://arxiv.org/abs/2512.09187</link>
<guid>https://arxiv.org/abs/2512.09187</guid>
<content:encoded><![CDATA[
arXiv:2512.09187v1 Announce Type: cross 
Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Mental States in Active and Autonomous Driving with EEG</title>
<link>https://arxiv.org/abs/2512.09190</link>
<guid>https://arxiv.org/abs/2512.09190</guid>
<content:encoded><![CDATA[
arXiv:2512.09190v1 Announce Type: cross 
Abstract: Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2512.09198</link>
<guid>https://arxiv.org/abs/2512.09198</guid>
<content:encoded><![CDATA[
arXiv:2512.09198v1 Announce Type: cross 
Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs for Analog Circuit Design Continuum (ACDC)</title>
<link>https://arxiv.org/abs/2512.09199</link>
<guid>https://arxiv.org/abs/2512.09199</guid>
<content:encoded><![CDATA[
arXiv:2512.09199v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2512.09202</link>
<guid>https://arxiv.org/abs/2512.09202</guid>
<content:encoded><![CDATA[
arXiv:2512.09202v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: A Conceptual Reasoning Layer for Large Language Models</title>
<link>https://arxiv.org/abs/2512.09222</link>
<guid>https://arxiv.org/abs/2512.09222</guid>
<content:encoded><![CDATA[
arXiv:2512.09222v1 Announce Type: cross 
Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI</title>
<link>https://arxiv.org/abs/2512.09244</link>
<guid>https://arxiv.org/abs/2512.09244</guid>
<content:encoded><![CDATA[
arXiv:2512.09244v1 Announce Type: cross 
Abstract: Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.09251</link>
<guid>https://arxiv.org/abs/2512.09251</guid>
<content:encoded><![CDATA[
arXiv:2512.09251v1 Announce Type: cross 
Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2512.09264</link>
<guid>https://arxiv.org/abs/2512.09264</guid>
<content:encoded><![CDATA[
arXiv:2512.09264v1 Announce Type: cross 
Abstract: The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Bias in Machine-generated Text Detection</title>
<link>https://arxiv.org/abs/2512.09292</link>
<guid>https://arxiv.org/abs/2512.09292</guid>
<content:encoded><![CDATA[
arXiv:2512.09292v1 Announce Type: cross 
Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2512.09313</link>
<guid>https://arxiv.org/abs/2512.09313</guid>
<content:encoded><![CDATA[
arXiv:2512.09313v1 Announce Type: cross 
Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Percolation: A Perspective on Criticality of Form and Function</title>
<link>https://arxiv.org/abs/2512.09317</link>
<guid>https://arxiv.org/abs/2512.09317</guid>
<content:encoded><![CDATA[
arXiv:2512.09317v1 Announce Type: cross 
Abstract: Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding</title>
<link>https://arxiv.org/abs/2512.09318</link>
<guid>https://arxiv.org/abs/2512.09318</guid>
<content:encoded><![CDATA[
arXiv:2512.09318v1 Announce Type: cross 
Abstract: The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment</title>
<link>https://arxiv.org/abs/2512.09319</link>
<guid>https://arxiv.org/abs/2512.09319</guid>
<content:encoded><![CDATA[
arXiv:2512.09319v1 Announce Type: cross 
Abstract: Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality</title>
<link>https://arxiv.org/abs/2512.09355</link>
<guid>https://arxiv.org/abs/2512.09355</guid>
<content:encoded><![CDATA[
arXiv:2512.09355v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log NeRF: Comparing Spaces for Learning Radiance Fields</title>
<link>https://arxiv.org/abs/2512.09375</link>
<guid>https://arxiv.org/abs/2512.09375</guid>
<content:encoded><![CDATA[
arXiv:2512.09375v1 Announce Type: cross 
Abstract: Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.09385</link>
<guid>https://arxiv.org/abs/2512.09385</guid>
<content:encoded><![CDATA[
arXiv:2512.09385v1 Announce Type: cross 
Abstract: The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CONCUR: A Framework for Continual Constrained and Unconstrained Routing</title>
<link>https://arxiv.org/abs/2512.09386</link>
<guid>https://arxiv.org/abs/2512.09386</guid>
<content:encoded><![CDATA[
arXiv:2512.09386v1 Announce Type: cross 
Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection</title>
<link>https://arxiv.org/abs/2512.09396</link>
<guid>https://arxiv.org/abs/2512.09396</guid>
<content:encoded><![CDATA[
arXiv:2512.09396v1 Announce Type: cross 
Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting</title>
<link>https://arxiv.org/abs/2512.09398</link>
<guid>https://arxiv.org/abs/2512.09398</guid>
<content:encoded><![CDATA[
arXiv:2512.09398v1 Announce Type: cross 
Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos</title>
<link>https://arxiv.org/abs/2512.09406</link>
<guid>https://arxiv.org/abs/2512.09406</guid>
<content:encoded><![CDATA[
arXiv:2512.09406v1 Announce Type: cross 
Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators</title>
<link>https://arxiv.org/abs/2512.09427</link>
<guid>https://arxiv.org/abs/2512.09427</guid>
<content:encoded><![CDATA[
arXiv:2512.09427v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourtPressGER: A German Court Decision to Press Release Summarization Dataset</title>
<link>https://arxiv.org/abs/2512.09434</link>
<guid>https://arxiv.org/abs/2512.09434</guid>
<content:encoded><![CDATA[
arXiv:2512.09434v1 Announce Type: cross 
Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model</title>
<link>https://arxiv.org/abs/2512.09441</link>
<guid>https://arxiv.org/abs/2512.09441</guid>
<content:encoded><![CDATA[
arXiv:2512.09441v1 Announce Type: cross 
Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Research via Human-AI Interactive Theorem Proving</title>
<link>https://arxiv.org/abs/2512.09443</link>
<guid>https://arxiv.org/abs/2512.09443</guid>
<content:encoded><![CDATA[
arXiv:2512.09443v1 Announce Type: cross 
Abstract: We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework</title>
<link>https://arxiv.org/abs/2512.09461</link>
<guid>https://arxiv.org/abs/2512.09461</guid>
<content:encoded><![CDATA[
arXiv:2512.09461v1 Announce Type: cross 
Abstract: Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing</title>
<link>https://arxiv.org/abs/2512.09463</link>
<guid>https://arxiv.org/abs/2512.09463</guid>
<content:encoded><![CDATA[
arXiv:2512.09463v1 Announce Type: cross 
Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach</title>
<link>https://arxiv.org/abs/2512.09471</link>
<guid>https://arxiv.org/abs/2512.09471</guid>
<content:encoded><![CDATA[
arXiv:2512.09471v1 Announce Type: cross 
Abstract: Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color encoding in Latent Space of Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2512.09477</link>
<guid>https://arxiv.org/abs/2512.09477</guid>
<content:encoded><![CDATA[
arXiv:2512.09477v1 Announce Type: cross 
Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks</title>
<link>https://arxiv.org/abs/2512.09485</link>
<guid>https://arxiv.org/abs/2512.09485</guid>
<content:encoded><![CDATA[
arXiv:2512.09485v1 Announce Type: cross 
Abstract: Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&amp;CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09487</link>
<guid>https://arxiv.org/abs/2512.09487</guid>
<content:encoded><![CDATA[
arXiv:2512.09487v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Invariance and Allocation: When Subgroup Balance Matters</title>
<link>https://arxiv.org/abs/2512.09496</link>
<guid>https://arxiv.org/abs/2512.09496</guid>
<content:encoded><![CDATA[
arXiv:2512.09496v1 Announce Type: cross 
Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization</title>
<link>https://arxiv.org/abs/2512.09524</link>
<guid>https://arxiv.org/abs/2512.09524</guid>
<content:encoded><![CDATA[
arXiv:2512.09524v1 Announce Type: cross 
Abstract: Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs</title>
<link>https://arxiv.org/abs/2512.09543</link>
<guid>https://arxiv.org/abs/2512.09543</guid>
<content:encoded><![CDATA[
arXiv:2512.09543v1 Announce Type: cross 
Abstract: Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.
  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.
  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.
  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.
  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection</title>
<link>https://arxiv.org/abs/2512.09563</link>
<guid>https://arxiv.org/abs/2512.09563</guid>
<content:encoded><![CDATA[
arXiv:2512.09563v1 Announce Type: cross 
Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Gender Code: Gendering the Global Governance of Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.09570</link>
<guid>https://arxiv.org/abs/2512.09570</guid>
<content:encoded><![CDATA[
arXiv:2512.09570v1 Announce Type: cross 
Abstract: This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows</title>
<link>https://arxiv.org/abs/2512.09572</link>
<guid>https://arxiv.org/abs/2512.09572</guid>
<content:encoded><![CDATA[
arXiv:2512.09572v1 Announce Type: cross 
Abstract: Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\!\propto\!|k|^{-\lambda}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $\beta(\tau)\!\propto\!\tau^\gamma$ that preserve fine-scale structure deeper into diffusion time, along with \emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that na\"ive Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation</title>
<link>https://arxiv.org/abs/2512.09577</link>
<guid>https://arxiv.org/abs/2512.09577</guid>
<content:encoded><![CDATA[
arXiv:2512.09577v1 Announce Type: cross 
Abstract: We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title>
<link>https://arxiv.org/abs/2512.09579</link>
<guid>https://arxiv.org/abs/2512.09579</guid>
<content:encoded><![CDATA[
arXiv:2512.09579v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates</title>
<link>https://arxiv.org/abs/2512.09586</link>
<guid>https://arxiv.org/abs/2512.09586</guid>
<content:encoded><![CDATA[
arXiv:2512.09586v1 Announce Type: cross 
Abstract: Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models</title>
<link>https://arxiv.org/abs/2512.09591</link>
<guid>https://arxiv.org/abs/2512.09591</guid>
<content:encoded><![CDATA[
arXiv:2512.09591v1 Announce Type: cross 
Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation</title>
<link>https://arxiv.org/abs/2512.09610</link>
<guid>https://arxiv.org/abs/2512.09610</guid>
<content:encoded><![CDATA[
arXiv:2512.09610v1 Announce Type: cross 
Abstract: People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Chain-of-Thought Reasoning for Videos</title>
<link>https://arxiv.org/abs/2512.09616</link>
<guid>https://arxiv.org/abs/2512.09616</guid>
<content:encoded><![CDATA[
arXiv:2512.09616v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection</title>
<link>https://arxiv.org/abs/2512.09662</link>
<guid>https://arxiv.org/abs/2512.09662</guid>
<content:encoded><![CDATA[
arXiv:2512.09662v1 Announce Type: cross 
Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $\kappa$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power</title>
<link>https://arxiv.org/abs/2512.09673</link>
<guid>https://arxiv.org/abs/2512.09673</guid>
<content:encoded><![CDATA[
arXiv:2512.09673v1 Announce Type: cross 
Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization</title>
<link>https://arxiv.org/abs/2512.09678</link>
<guid>https://arxiv.org/abs/2512.09678</guid>
<content:encoded><![CDATA[
arXiv:2512.09678v1 Announce Type: cross 
Abstract: In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies</title>
<link>https://arxiv.org/abs/2512.09682</link>
<guid>https://arxiv.org/abs/2512.09682</guid>
<content:encoded><![CDATA[
arXiv:2512.09682v1 Announce Type: cross 
Abstract: This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method</title>
<link>https://arxiv.org/abs/2512.09729</link>
<guid>https://arxiv.org/abs/2512.09729</guid>
<content:encoded><![CDATA[
arXiv:2512.09729v1 Announce Type: cross 
Abstract: We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs</title>
<link>https://arxiv.org/abs/2512.09742</link>
<guid>https://arxiv.org/abs/2512.09742</guid>
<content:encoded><![CDATA[
arXiv:2512.09742v1 Announce Type: cross 
Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Circuits, Features, and Heuristics in Molecular Transformers</title>
<link>https://arxiv.org/abs/2512.09757</link>
<guid>https://arxiv.org/abs/2512.09757</guid>
<content:encoded><![CDATA[
arXiv:2512.09757v1 Announce Type: cross 
Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.09775</link>
<guid>https://arxiv.org/abs/2512.09775</guid>
<content:encoded><![CDATA[
arXiv:2512.09775v1 Announce Type: cross 
Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2512.09779</link>
<guid>https://arxiv.org/abs/2512.09779</guid>
<content:encoded><![CDATA[
arXiv:2512.09779v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&amp;Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing</title>
<link>https://arxiv.org/abs/2512.09806</link>
<guid>https://arxiv.org/abs/2512.09806</guid>
<content:encoded><![CDATA[
arXiv:2512.09806v1 Announce Type: cross 
Abstract: U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composing Concepts from Images and Videos via Concept-prompt Binding</title>
<link>https://arxiv.org/abs/2512.09824</link>
<guid>https://arxiv.org/abs/2512.09824</guid>
<content:encoded><![CDATA[
arXiv:2512.09824v1 Announce Type: cross 
Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs in Interpreting Legal Documents</title>
<link>https://arxiv.org/abs/2512.09830</link>
<guid>https://arxiv.org/abs/2512.09830</guid>
<content:encoded><![CDATA[
arXiv:2512.09830v1 Announce Type: cross 
Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</title>
<link>https://arxiv.org/abs/2512.09867</link>
<guid>https://arxiv.org/abs/2512.09867</guid>
<content:encoded><![CDATA[
arXiv:2512.09867v1 Announce Type: cross 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09872</link>
<guid>https://arxiv.org/abs/2512.09872</guid>
<content:encoded><![CDATA[
arXiv:2512.09872v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Learning from Modern Language Models via Low Logit Rank</title>
<link>https://arxiv.org/abs/2512.09892</link>
<guid>https://arxiv.org/abs/2512.09892</guid>
<content:encoded><![CDATA[
arXiv:2512.09892v1 Announce Type: cross 
Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Heading Prediction for Autonomous Aerial Vehicles</title>
<link>https://arxiv.org/abs/2512.09898</link>
<guid>https://arxiv.org/abs/2512.09898</guid>
<content:encoded><![CDATA[
arXiv:2512.09898v1 Announce Type: cross 
Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506{\deg} and a root mean squared error of 0.1957{\deg}, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STACHE: Local Black-Box Explanations for Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2512.09909</link>
<guid>https://arxiv.org/abs/2512.09909</guid>
<content:encoded><![CDATA[
arXiv:2512.09909v1 Announce Type: cross 
Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach</title>
<link>https://arxiv.org/abs/2512.09910</link>
<guid>https://arxiv.org/abs/2512.09910</guid>
<content:encoded><![CDATA[
arXiv:2512.09910v1 Announce Type: cross 
Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised learning pays attention</title>
<link>https://arxiv.org/abs/2512.09912</link>
<guid>https://arxiv.org/abs/2512.09912</guid>
<content:encoded><![CDATA[
arXiv:2512.09912v1 Announce Type: cross 
Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FALCON: Few-step Accurate Likelihoods for Continuous Flows</title>
<link>https://arxiv.org/abs/2512.09914</link>
<guid>https://arxiv.org/abs/2512.09914</guid>
<content:encoded><![CDATA[
arXiv:2512.09914v1 Announce Type: cross 
Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating</title>
<link>https://arxiv.org/abs/2512.09920</link>
<guid>https://arxiv.org/abs/2512.09920</guid>
<content:encoded><![CDATA[
arXiv:2512.09920v1 Announce Type: cross 
Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction</title>
<link>https://arxiv.org/abs/2509.11719</link>
<guid>https://arxiv.org/abs/2509.11719</guid>
<content:encoded><![CDATA[
arXiv:2509.11719v2 Announce Type: replace 
Abstract: Multi-agent trajectory prediction in autonomous driving requires a comprehensive understanding of complex social dynamics. Existing methods, however, often struggle to capture the full richness of these dynamics, particularly the co-existence of multi-scale interactions and the diverse behaviors of heterogeneous agents. To address these challenges, this paper introduces HeLoFusion, an efficient and scalable encoder for modeling heterogeneous and multi-scale agent interactions. Instead of relying on global context, HeLoFusion constructs local, multi-scale graphs centered on each agent, allowing it to effectively model both direct pairwise dependencies and complex group-wise interactions (\textit{e.g.}, platooning vehicles or pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of agent heterogeneity through an aggregation-decomposition message-passing scheme and type-specific feature networks, enabling it to learn nuanced, type-dependent interaction patterns. This locality-focused approach enables a principled representation of multi-level social context, yielding powerful and expressive agent embeddings. On the challenging Waymo Open Motion Dataset, HeLoFusion achieves state-of-the-art performance, setting new benchmarks for key metrics including Soft mAP and minADE. Our work demonstrates that a locality-grounded architecture, which explicitly models multi-scale and heterogeneous interactions, is a highly effective strategy for advancing motion forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.23589</link>
<guid>https://arxiv.org/abs/2509.23589</guid>
<content:encoded><![CDATA[
arXiv:2509.23589v2 Announce Type: replace 
Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows</title>
<link>https://arxiv.org/abs/2510.03506</link>
<guid>https://arxiv.org/abs/2510.03506</guid>
<content:encoded><![CDATA[
arXiv:2510.03506v3 Announce Type: replace 
Abstract: We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding</title>
<link>https://arxiv.org/abs/2510.15952</link>
<guid>https://arxiv.org/abs/2510.15952</guid>
<content:encoded><![CDATA[
arXiv:2510.15952v3 Announce Type: replace 
Abstract: Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking "what is intelligence?" (ontological), SCL asks "under what conditions does cognition emerge?" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling "executable epistemology" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking World-Model Learning</title>
<link>https://arxiv.org/abs/2510.19788</link>
<guid>https://arxiv.org/abs/2510.19788</guid>
<content:encoded><![CDATA[
arXiv:2510.19788v3 Announce Type: replace 
Abstract: Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended $\unicode{x2014}$ models should support many different tasks unknown ahead of time $\unicode{x2014}$ and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template $\unicode{x2014}$ reward-free exploration, derived tests, and behavior-based scoring $\unicode{x2014}$ to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Failures: Rethinking Foundation Models in Pathology</title>
<link>https://arxiv.org/abs/2510.23807</link>
<guid>https://arxiv.org/abs/2510.23807</guid>
<content:encoded><![CDATA[
arXiv:2510.23807v4 Announce Type: replace 
Abstract: Despite their successes in vision and language, foundation models have stumbled in pathology, revealing low accuracy, instability, and heavy computational demands. These shortcomings stem not from tuning problems but from deeper conceptual mismatches: dense embeddings cannot represent the combinatorial richness of tissue, and current architectures inherit flaws in self-supervision, patch design, and noise-fragile pretraining. Biological complexity and limited domain innovation further widen the gap. The evidence is clear-pathology requires models explicitly designed for biological images rather than adaptations of large-scale natural-image methods whose assumptions do not hold for tissue.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</title>
<link>https://arxiv.org/abs/2511.03092</link>
<guid>https://arxiv.org/abs/2511.03092</guid>
<content:encoded><![CDATA[
arXiv:2511.03092v5 Announce Type: replace 
Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning</title>
<link>https://arxiv.org/abs/2511.06316</link>
<guid>https://arxiv.org/abs/2511.06316</guid>
<content:encoded><![CDATA[
arXiv:2511.06316v2 Announce Type: replace 
Abstract: Reliable geospatial information on road accidents is vital for safety analysis and infrastructure planning, yet most low- and middle-income countries continue to face a critical shortage of accurate, location-specific crash data. Existing text-based geocoding tools perform poorly in multilingual and unstructured news environments, where incomplete place descriptions and mixed language (e.g. Bangla-English) scripts obscure spatial context. To address these limitations, this study introduces ALIGN (Accident Location Inference through Geo-Spatial Neural Reasoning), a vision-language framework that emulates human spatial reasoning to infer accident location coordinates directly from available textual and map-based cues. ALIGN integrates large language and vision-language model mechanisms within a multi-stage pipeline that performs optical character recognition, linguistic reasoning, and map-level verification through grid-based spatial scanning. The framework systematically evaluates each predicted location against contextual and visual evidence, ensuring interpretable, fine-grained geolocation outcomes without requiring model retraining. Applied to Bangla-language news data source, ALIGN demonstrates consistent improvements over traditional geoparsing methods, accurately identifying district- and sub-district-level crash sites. Beyond its technical contribution, the framework establishes a high accuracy foundation for automated crash mapping in data-scarce regions, supporting evidence-driven road-safety policymaking and the broader integration of multimodal artificial intelligence in transportation analytics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</title>
<link>https://arxiv.org/abs/2511.09900</link>
<guid>https://arxiv.org/abs/2511.09900</guid>
<content:encoded><![CDATA[
arXiv:2511.09900v3 Announce Type: replace 
Abstract: Protein evolution through amino acid mutations is a cornerstone of life sciences. Recent advances in protein language models have shown rich evolutionary patterns, offering unprecedented potential for in-silicon directed evolution. However, existing directed evolution methods largely rely on heuristic evolution strategies and have yet to efficiently integrate the transformative protein language models with advanced optimization techniques, such as reinforcement learning, to learn optimal evolution policies. To bridge this gap, we propose AlphaDE, a novel framework that evolves protein sequences by harnessing the innovative paradigms of large language models, such as fine-tuning and test-time inference. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility of the interested protein family. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. A case study further demonstrates that AlphaDE supports condensing the protein sequence space of avGFP through computational evolution.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v2 Announce Type: replace 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2511.14256</link>
<guid>https://arxiv.org/abs/2511.14256</guid>
<content:encoded><![CDATA[
arXiv:2511.14256v2 Announce Type: replace 
Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Inference in Discrete State Spaces from First Principles</title>
<link>https://arxiv.org/abs/2511.20321</link>
<guid>https://arxiv.org/abs/2511.20321</guid>
<content:encoded><![CDATA[
arXiv:2511.20321v2 Announce Type: replace 
Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A survey on the impacts of recommender systems on users, items, and human-AI ecosystems</title>
<link>https://arxiv.org/abs/2407.01630</link>
<guid>https://arxiv.org/abs/2407.01630</guid>
<content:encoded><![CDATA[
arXiv:2407.01630v2 Announce Type: replace-cross 
Abstract: Recommendation systems and assistants (in short, recommenders) influence through online platforms most actions of our daily lives, suggesting items or providing solutions based on users' preferences or requests. This survey systematically reviews, categories, and discusses the impact of recommenders in four human-AI ecosystems -- social media, online retail, urban mapping and generative AI ecosystems. Its scope is to systematise a fast-growing field in which terminologies employed to classify methodologies and outcomes are fragmented and unsystematic. This is a crucial contribution to the literature because terminologies vary substantially across disciplines and ecosystems, hindering comparison and accumulation of knowledge in the field. We follow the customary steps of qualitative systematic review, gathering 154 articles from different disciplines to develop a parsimonious taxonomy of methodologies employed (empirical, simulation, observational, controlled), outcomes observed (concentration, content degradation, discrimination, diversity, echo chamber, filter bubble, homogenisation, polarisation, radicalisation, volume), and their level of analysis (individual, item, and ecosystem). We systematically discuss substantive and methodological commonalities across ecosystems, and highlight potential avenues for future research. The survey is addressed to scholars and practitioners interested in different human-AI ecosystems, policymakers and institutional stakeholders who want to understand better the measurable outcomes of recommenders, and tech companies who wish to obtain a systematic view of the impact of their recommenders.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Transportation by Orthogonal Coupling Dynamics</title>
<link>https://arxiv.org/abs/2410.08060</link>
<guid>https://arxiv.org/abs/2410.08060</guid>
<content:encoded><![CDATA[
arXiv:2410.08060v2 Announce Type: replace-cross 
Abstract: Many numerical and learning algorithms rely on the solution of the Monge-Kantorovich problem and Wasserstein distances, which provide appropriate distributional metrics. While the natural approach is to treat the problem as an infinite-dimensional linear programming, such a methodology limits the computational performance due to the polynomial scaling with respect to the sample size along with intensive memory requirements. We propose a novel alternative framework to address the Monge-Kantorovich problem based on a projection type gradient descent scheme. The dynamics builds on the notion of the conditional expectation, where the connection with the opinion dynamics is leveraged to devise efficient numerical schemes. We demonstrate that the resulting dynamics recovers random maps with favourable computational performance. Along with the theoretical insight, the proposed dynamics paves the way for innovative approaches to construct numerical schemes for computing optimal transport maps as well as Wasserstein distances.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning and Opportunistic Inference for Continuous Monitoring of Freezing of Gait in Parkinson's Disease</title>
<link>https://arxiv.org/abs/2410.21326</link>
<guid>https://arxiv.org/abs/2410.21326</guid>
<content:encoded><![CDATA[
arXiv:2410.21326v2 Announce Type: replace-cross 
Abstract: Parkinson's disease (PD) is a progressive neurological disorder that impacts the quality of life significantly, making in-home monitoring of motor symptoms such as Freezing of Gait (FoG) critical. However, existing symptom monitoring technologies are power-hungry, rely on extensive amounts of labeled data, and operate in controlled settings. These shortcomings limit real-world deployment of the technology. This work presents LIFT-PD, a computationally-efficient self-supervised learning framework for real-time FoG detection. Our method combines self-supervised pre-training on unlabeled data with a novel differential hopping windowing technique to learn from limited labeled instances. An opportunistic model activation module further minimizes power consumption by selectively activating the deep learning module only during active periods. Extensive experimental results show that LIFT-PD achieves a 7.25% increase in precision and 4.4% improvement in accuracy compared to supervised models while using as low as 40% of the labeled training data used for supervised learning. Additionally, the model activation module reduces inference time by up to 67% compared to continuous inference. LIFT-PD paves the way for practical, energy-efficient, and unobtrusive in-home monitoring of PD patients with minimal labeling requirements.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v4 Announce Type: replace-cross 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much)</title>
<link>https://arxiv.org/abs/2502.04499</link>
<guid>https://arxiv.org/abs/2502.04499</guid>
<content:encoded><![CDATA[
arXiv:2502.04499v2 Announce Type: replace-cross 
Abstract: Knowledge distillation (KD) is a popular method of transferring knowledge from a large "teacher" model to a small "student" model. Previous work has explored various layer-selection strategies (e.g., forward matching and in-order random matching) for intermediate-layer matching in KD, where a student layer is forced to resemble a certain teacher layer. In this work, we revisit such layer-selection strategies and observe an intriguing phenomenon that layer-selection strategy does not matter (much) in intermediate-layer matching -- even seemingly nonsensical matching strategies such as reverse matching still result in surprisingly good student performance. We provide an interpretation for this phenomenon by examining the angles between teacher layers viewed from the student's perspective. Our work sheds light on KD practice, as layer-selection strategies may not be the main focus of KD system design, and vanilla forward matching works well in most setups.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms</title>
<link>https://arxiv.org/abs/2502.17801</link>
<guid>https://arxiv.org/abs/2502.17801</guid>
<content:encoded><![CDATA[
arXiv:2502.17801v3 Announce Type: replace-cross 
Abstract: Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?</title>
<link>https://arxiv.org/abs/2503.05507</link>
<guid>https://arxiv.org/abs/2503.05507</guid>
<content:encoded><![CDATA[
arXiv:2503.05507v2 Announce Type: replace-cross 
Abstract: Grammar serves as a cornerstone in programming languages and software engineering, providing frameworks to define the syntactic space and program structure. Existing research demonstrates the effectiveness of grammar-based code representations in small-scale models, showing their ability to reduce syntax errors and enhance performance. However, as language models scale to the billion level or beyond, syntax-level errors become rare, making it unclear whether grammar information still provides performance benefits. To explore this, we develop a series of billion-scale GrammarCoder models, incorporating grammar rules in the code generation process. Experiments on HumanEval (+) and MBPP (+) demonstrate a notable improvement in code generation accuracy. Further analysis shows that grammar-based representations enhance LLMs' ability to discern subtle code differences, reducing semantic errors caused by minor variations. These findings suggest that grammar-based code representations remain valuable even in billion-scale models, not only by maintaining syntax correctness but also by improving semantic differentiation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v5 Announce Type: replace-cross 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Story of Two GPUs: Characterizing the Resilience of Hopper H100 and Ampere A100 GPUs</title>
<link>https://arxiv.org/abs/2503.11901</link>
<guid>https://arxiv.org/abs/2503.11901</guid>
<content:encoded><![CDATA[
arXiv:2503.11901v4 Announce Type: replace-cross 
Abstract: This study characterizes GPU resilience in Delta, a large-scale AI system that consists of 1,056 A100 and H100 GPUs, with over 1,300 petaflops of peak throughput. We used 2.5 years of operational data (11.7 million GPU hours) on GPU errors. Our major findings include: (i) H100 GPU memory resilience is worse than A100 GPU memory, with 3.2x lower per-GPU MTBE for memory errors, (ii) The GPU memory error-recovery mechanisms on H100 GPUs are insufficient to handle the increased memory capacity, (iii) H100 GPUs demonstrate significantly improved GPU hardware resilience over A100 GPUs with respect to critical hardware components, (iv) GPU errors on both A100 and H100 GPUs frequently result in job failures due to the lack of robust recovery mechanisms at the application level, and (v) We project the impact of GPU node availability on larger-scales and find that significant overprovisioning of 5% is necessary to handle GPU failures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LENVIZ: A High-Resolution Low-Exposure Night Vision Benchmark Dataset</title>
<link>https://arxiv.org/abs/2503.19804</link>
<guid>https://arxiv.org/abs/2503.19804</guid>
<content:encoded><![CDATA[
arXiv:2503.19804v2 Announce Type: replace-cross 
Abstract: Low-light image enhancement is crucial for a myriad of applications, from night vision and surveillance, to autonomous driving. However, due to the inherent limitations that come in hand with capturing images in low-illumination environments, the task of enhancing such scenes still presents a formidable challenge. To advance research in this field, we introduce our Low Exposure Night Vision (LENVIZ) Dataset, a comprehensive multi-exposure benchmark dataset for low-light image enhancement comprising of over 230K frames showcasing 24K real-world indoor and outdoor, with-and without human, scenes. Captured using 3 different camera sensors, LENVIZ offers a wide range of lighting conditions, noise levels, and scene complexities, making it the largest publicly available up-to 4K resolution benchmark in the field. LENVIZ includes high quality human-generated ground truth, for which each multi-exposure low-light scene has been meticulously curated and edited by expert photographers to ensure optimal image quality. Furthermore, we also conduct a comprehensive analysis of current state-of-the-art low-light image enhancement techniques on our dataset and highlight potential areas of improvement.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sinusoidal Initialization, Time for a New Start</title>
<link>https://arxiv.org/abs/2505.12909</link>
<guid>https://arxiv.org/abs/2505.12909</guid>
<content:encoded><![CDATA[
arXiv:2505.12909v3 Announce Type: replace-cross 
Abstract: Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well-conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.9% in final validation accuracy and 20.9% in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2505.13572</link>
<guid>https://arxiv.org/abs/2505.13572</guid>
<content:encoded><![CDATA[
arXiv:2505.13572v2 Announce Type: replace-cross 
Abstract: The SPARQL query language is the standard method to access knowledge graphs (KGs). However, formulating SPARQL queries is a significant challenge for non-expert users, and remains time-consuming for the experienced ones. Best practices recommend to document KGs with competency questions and example queries to contextualise the knowledge they contain and illustrate their potential applications. In practice, however, this is either not the case or the examples are provided in limited numbers. Large Language Models (LLMs) are being used in conversational agents and are proving to be an attractive solution with a wide range of applications, from simple question-answering about common knowledge to generating code in a targeted programming language. However, training and testing these models to produce high quality SPARQL queries from natural language questions requires substantial datasets of question-query pairs. In this paper, we present Q${}^2$Forge that addresses the challenge of generating new competency questions for a KG and corresponding SPARQL queries. It iteratively validates those queries with human feedback and LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular, meaning that the different modules of the application (CQ generation, query generation and query refinement) can be used separately, as an integrated pipeline, or replaced by alternative services. The result is a complete pipeline from competency question formulation to query evaluation, supporting the creation of reference query sets for any target KG.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking data encoding methods in Quantum Machine Learning</title>
<link>https://arxiv.org/abs/2505.14295</link>
<guid>https://arxiv.org/abs/2505.14295</guid>
<content:encoded><![CDATA[
arXiv:2505.14295v2 Announce Type: replace-cross 
Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine Learning (QML). While classical approaches process data directly as vectors, QML may require transforming classical data into quantum states through encoding circuits, known as quantum feature maps or quantum embeddings. This step leverages the inherently high-dimensional and non-linear nature of Hilbert space, enabling more efficient data separation in complex feature spaces that may be inaccessible to classical methods. This encoding part significantly affects the performance of the QML model, so it is important to choose the right encoding method for the dataset to be encoded. However, this choice is generally arbitrary, since there is no "universal" rule for knowing which encoding to choose based on a specific set of data. There are currently a variety of encoding methods using different quantum logic gates. We studied the most commonly used types of encoding methods and benchmarked them using different datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm</title>
<link>https://arxiv.org/abs/2505.15138</link>
<guid>https://arxiv.org/abs/2505.15138</guid>
<content:encoded><![CDATA[
arXiv:2505.15138v2 Announce Type: replace-cross 
Abstract: This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge of $\tau_{\mathrm{mix}}$, the achievable rates change to $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
arXiv:2505.16056v3 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v3 Announce Type: replace-cross 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Minimalist Optimizer Design for LLM Pretraining</title>
<link>https://arxiv.org/abs/2506.16659</link>
<guid>https://arxiv.org/abs/2506.16659</guid>
<content:encoded><![CDATA[
arXiv:2506.16659v2 Announce Type: replace-cross 
Abstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which introduce extra operations and require significant more memory to maintain first- and second-order moments than SGD. While recent works such as GaLore, Fira and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What are the minimum modifications to plain SGD needed to match state-of-the-art pretraining performance? We systematically investigate this question using a bottom-up approach, and identify two simple yet highly (memory- and compute-) efficient techniques: (1) column-wise gradient normalization (normalizing the gradient along the output dimension), which boosts SGD performance without momentum; and (2) applying first-order momentum only to the output layer, where gradient variance is highest. Combining these two techniques lead to SCALE (Stochastic Column-normAlized Last-layer momEntum), a simple optimizer for memory efficient pretraining. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For LLaMA 7B model, SCALE outperforms the state-of-the-art memory-efficient methods APOLLO and Muon, in terms of both perplexity and memory consumption.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement</title>
<link>https://arxiv.org/abs/2507.00966</link>
<guid>https://arxiv.org/abs/2507.00966</guid>
<content:encoded><![CDATA[
arXiv:2507.00966v3 Announce Type: replace-cross 
Abstract: With new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform the state-of-the-art in single-channel speech enhancement and audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VB-DemandEx, a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, MambAttention significantly outperforms existing state-of-the-art discriminative LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 without reverberation and EARS-WHAM_v2. MambAttention also matches or outperforms generative diffusion models in generalization performance while being competitive with language model baselines. Ablation studies highlight the importance of weight sharing between time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. Yet, MambAttention remains superior for cross-corpus generalization across all reported evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation</title>
<link>https://arxiv.org/abs/2507.10580</link>
<guid>https://arxiv.org/abs/2507.10580</guid>
<content:encoded><![CDATA[
arXiv:2507.10580v2 Announce Type: replace-cross 
Abstract: Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have increasingly been used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solutions. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed to provide mental health and emotional support. EmoSApp leverages a language model, specifically the LLaMA-3.2-1B-Instruct, which is fine-tuned and quantized on a custom-curated ``Knowledge Dataset'' comprising 14,582 mental health QA pairs along with multi-turn conversational data, enabling robust domain expertise and fully on-device inference on resource-constrained smartphones.
  Through qualitative evaluation with students and mental health professionals, we demonstrate that EmoSApp has the ability to respond coherently and empathetically, provide relevant suggestions to user's mental health problems, and maintain interactive dialogue. Additionally, quantitative evaluations on nine commonsense and reasoning benchmarks, along with two mental health specific datasets, demonstrate EmoSApp's effectiveness in low-resource settings. By prioritizing on-device deployment and specialized domain-specific adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health support.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v2 Announce Type: replace-cross 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROPS: Progressively Private Self-alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.06783</link>
<guid>https://arxiv.org/abs/2508.06783</guid>
<content:encoded><![CDATA[
arXiv:2508.06783v2 Announce Type: replace-cross 
Abstract: Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v4 Announce Type: replace-cross 
Abstract: We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches. We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vevo2: A Unified and Controllable Framework for Speech and Singing Voice Generation</title>
<link>https://arxiv.org/abs/2508.16332</link>
<guid>https://arxiv.org/abs/2508.16332</guid>
<content:encoded><![CDATA[
arXiv:2508.16332v2 Announce Type: replace-cross 
Abstract: Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a unified music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a unified content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during the speech-singing joint training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the Vevo2's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.19366</link>
<guid>https://arxiv.org/abs/2508.19366</guid>
<content:encoded><![CDATA[
arXiv:2508.19366v4 Announce Type: replace-cross 
Abstract: Hallucinations in LLMs--especially in multimodal settings--undermine reliability. We present a rigorous information-geometric framework, grounded in diffusion dynamics, to quantify hallucinations in MLLMs where model outputs are embedded via spectral decompositions of multimodal graph Laplacians, and their gaps to a truth manifold define a semantic distortion metric. We derive Courant-Fischer bounds on a temperature-dependent hallucination profile and use RKHS eigenmodes to obtain modality-aware, interpretable measures that track evolution over prompts and time. This reframes hallucination as quantifiable and bounded, providing a principled basis for evaluation and mitigation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</title>
<link>https://arxiv.org/abs/2509.15872</link>
<guid>https://arxiv.org/abs/2509.15872</guid>
<content:encoded><![CDATA[
arXiv:2509.15872v2 Announce Type: replace-cross 
Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing 2 pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Noise-Curvature View of Loss of Trainability</title>
<link>https://arxiv.org/abs/2509.19698</link>
<guid>https://arxiv.org/abs/2509.19698</guid>
<content:encoded><![CDATA[
arXiv:2509.19698v3 Announce Type: replace-cross 
Abstract: Loss of trainability refers to a phenomenon in continual learning where parameter updates no longer make progress on the optimization objective, so accuracy stalls or degrades as the learning problem changes over time. In this paper, we analyze loss of trainability through an optimization lens and find that the phenomenon is not reliably predicted by existing individual indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy. Motivated by our analysis, we introduce two complementary indicators: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. We then combine these two indicators into a per-layer adaptive noise threshold on the effective step-size that anticipates trainability behavior. Using this insight, we propose a step-size scheduler that keeps each layer's effective parameter update below this bound, thereby avoiding loss of trainability. We demonstrate that our scheduler can improve the accuracy maintained by previously proposed approaches, such as concatenated ReLU (CReLU), Wasserstein regularizer, and L2 weight decay. Surprisingly, our scheduler produces adaptive step-size trajectories that, without tuning, mirror the manually engineered step-size decay schedules.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Formal Theory on the Logical Limits of Symbol Grounding</title>
<link>https://arxiv.org/abs/2509.20409</link>
<guid>https://arxiv.org/abs/2509.20409</guid>
<content:encoded><![CDATA[
arXiv:2509.20409v4 Announce Type: replace-cross 
Abstract: This paper synthesizes a series of formal proofs to construct a unified theory on the logical limits of the Symbol Grounding Problem. We distinguish between internal meaning (sense), which formal systems can possess via axioms, and external grounding (reference), which is a necessary condition for connecting symbols to the world. We demonstrate through a four-stage argument that meaningful grounding within a formal system must arise from a process that is external, dynamic, and non-fixed algorithmic. First, we show that for a purely symbolic system, the impossibility of grounding is a direct consequence of its definition. Second, we extend this limitation to systems with any finite, static set of pre-established meanings (Semantic Axioms). By formally modeling the computationalist hypothesis-which equates grounding with internal derivation-we prove via G\"odelian arguments that such systems cannot consistently and completely define a "groundability predicate" for all truths. Third, we demonstrate that the "grounding act" for emergent meanings cannot be inferred from internal rules but requires an axiomatic, meta-level update. Drawing on Turing's concept of Oracle Machines and Piccinini's analysis of the mathematical objection, we identify this update as physical transduction. Finally, we prove that this process cannot be simulated by a fixed judgment algorithm, validating the logical necessity of embodied interaction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting the Future with Yesterday's Climate: Temperature Bias in AI Weather and Climate Models</title>
<link>https://arxiv.org/abs/2509.22359</link>
<guid>https://arxiv.org/abs/2509.22359</guid>
<content:encoded><![CDATA[
arXiv:2509.22359v2 Announce Type: replace-cross 
Abstract: AI-based climate and weather models have rapidly gained popularity, providing faster forecasts with skill that can match or even surpass that of traditional dynamical models. Despite this success, these models face a key challenge: predicting future climates while being trained only with historical data. In this study, we investigate this issue by analyzing boreal winter land temperature biases in AI weather and climate models. We examine two weather models, FourCastNet V2 Small (FourCastNet) and Pangu Weather (Pangu), evaluating their predictions for 2020-2025 and Ai2 Climate Emulator version 2 (ACE2) for 1996-2010. These time periods lie outside of the respective models' training sets and are significantly more recent than the bulk of their training data, allowing us to assess how well the models generalize to new, i.e. more modern, conditions. We find that all three models produce cold-biased mean temperatures, resembling climates from 15-20 years earlier than the period they are predicting. In some regions, like the Eastern U.S., the predictions resemble climates from as much as 20-30 years earlier. Further analysis shows that FourCastNet's and Pangu's cold bias is strongest in the hottest predicted temperatures, indicating limited training exposure to modern extreme heat events. In contrast, ACE2's bias is more evenly distributed but largest in regions, seasons, and parts of the temperature distribution where climate change has been most pronounced. These findings underscore the challenge of training AI models exclusively on historical data and highlight the need to account for such biases when applying them to future climate prediction.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impossibility of Inverse Permutation Learning in Transformer Models</title>
<link>https://arxiv.org/abs/2509.24125</link>
<guid>https://arxiv.org/abs/2509.24125</guid>
<content:encoded><![CDATA[
arXiv:2509.24125v3 Announce Type: replace-cross 
Abstract: In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
arXiv:2509.25270v3 Announce Type: replace-cross 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Three Regimes of Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01460</link>
<guid>https://arxiv.org/abs/2510.01460</guid>
<content:encoded><![CDATA[
arXiv:2510.01460v2 Announce Type: replace-cross 
Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual Speech Recognition Evaluation</title>
<link>https://arxiv.org/abs/2510.06961</link>
<guid>https://arxiv.org/abs/2510.06961</guid>
<content:encoded><![CDATA[
arXiv:2510.06961v3 Announce Type: replace-cross 
Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including a dedicated multilingual track. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception</title>
<link>https://arxiv.org/abs/2510.08352</link>
<guid>https://arxiv.org/abs/2510.08352</guid>
<content:encoded><![CDATA[
arXiv:2510.08352v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) are becoming increasingly powerful, demonstrating strong performance on a variety of tasks that require both visual and textual understanding. Their strong generalisation abilities make them a promising component for automated driving systems, which must handle unexpected corner cases. However, to be trusted in such safety-critical applications, a model must first possess a reliable perception system. Moreover, since critical objects and agents in traffic scenes are often at a distance, we require systems that are not "shortsighted", i.e., systems with strong perception capabilities at both close (up to 20 meters) and long (30+ meters) range. With this in mind, we introduce Distance-Annotated Traffic Perception Question Answering (DTPQA), the first Visual Question Answering (VQA) benchmark focused solely on perception-based questions in traffic scenes, enriched with distance annotations. By excluding questions that require reasoning, we ensure that model performance reflects perception capabilities alone. Since automated driving hardware has limited processing power and cannot support large VLMs, our study centers on smaller VLMs. More specifically, we evaluate several state-of-the-art (SOTA) small VLMs on DTPQA and show that, despite the simplicity of the questions, these models significantly underperform compared to humans (~60% average accuracy for the best-performing small VLM versus ~85% human performance). However, it is important to note that the human sample size was relatively small, which imposes statistical limitations. We also identify specific perception tasks, such as distinguishing left from right, that remain particularly challenging for these models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</title>
<link>https://arxiv.org/abs/2510.09660</link>
<guid>https://arxiv.org/abs/2510.09660</guid>
<content:encoded><![CDATA[
arXiv:2510.09660v4 Announce Type: replace-cross 
Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adoption Paradox for Veterinary Professionals in China: High Use of Artificial Intelligence Despite Low Familiarity</title>
<link>https://arxiv.org/abs/2510.11758</link>
<guid>https://arxiv.org/abs/2510.11758</guid>
<content:encoded><![CDATA[
arXiv:2510.11758v2 Announce Type: replace-cross 
Abstract: While the global integration of artificial intelligence (AI) into veterinary medicine is accelerating, its adoption dynamics in major markets such as China remain uncharacterized. This paper presents the first exploratory analysis of AI perception and adoption among veterinary professionals in China, based on a cross-sectional survey of 455 practitioners conducted in mid-2025. We identify a distinct "adoption paradox": although 71.0% of respondents have incorporated AI into their workflows, 44.6% of these active users report low familiarity with the technology. In contrast to the administrative-focused patterns observed in North America, adoption in China is practitioner-driven and centers on core clinical tasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%). However, concerns regarding reliability and accuracy remain the primary barrier (54.3%), coexisting with a strong consensus (93.8%) for regulatory oversight. These findings suggest a unique "inside-out" integration model in China, characterized by high clinical utility but restricted by an "interpretability gap," underscoring the need for specialized tools and robust regulatory frameworks to safely harness AI's potential in this expanding market.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
<link>https://arxiv.org/abs/2510.13865</link>
<guid>https://arxiv.org/abs/2510.13865</guid>
<content:encoded><![CDATA[
arXiv:2510.13865v5 Announce Type: replace-cross 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Sinks in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2510.15731</link>
<guid>https://arxiv.org/abs/2510.15731</guid>
<content:encoded><![CDATA[
arXiv:2510.15731v2 Announce Type: replace-cross 
Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework</title>
<link>https://arxiv.org/abs/2510.15843</link>
<guid>https://arxiv.org/abs/2510.15843</guid>
<content:encoded><![CDATA[
arXiv:2510.15843v2 Announce Type: replace-cross 
Abstract: Accurately detecting sentiment polarity and intensity in product reviews and social media posts remains challenging due to informal and domain-specific language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer framework that combines rule-based heuristics, contextual deep learning, and fuzzy logic to generate continuous sentiment scores reflecting both polarity and strength. The pipeline begins with VADER-based initial sentiment estimations, which are refined through a two-stage adjustment process. This involves leveraging confidence scores from DistilBERT, a lightweight transformer and applying fuzzy logic principles to mitigate excessive neutrality bias and enhance granularity. A custom fuzzy inference system then maps the refined scores onto a 0 to 1 continuum, producing expert)like judgments. The framework is rigorously evaluated on four domain-specific datasets. food delivery, e-commerce, tourism, and fashion. Results show improved alignment with user ratings, better identification of sentiment extremes, and reduced misclassifications. Both quantitative metrics (distributional alignment, confusion matrices) and qualitative insights (case studies, runtime analysis) affirm the models robustness and efficiency. This work demonstrates the value of integrating symbolic reasoning with neural models for interpretable, finegrained sentiment analysis in linguistically dynamic domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</title>
<link>https://arxiv.org/abs/2510.17281</link>
<guid>https://arxiv.org/abs/2510.17281</guid>
<content:encoded><![CDATA[
arXiv:2510.17281v3 Announce Type: replace-cross 
Abstract: Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2510.20690</link>
<guid>https://arxiv.org/abs/2510.20690</guid>
<content:encoded><![CDATA[
arXiv:2510.20690v2 Announce Type: replace-cross 
Abstract: Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. While existing mitigation strategies largely target accuracy, we provide the first formal tail bounds for hallucination probability in ensembled language models, reframing it as a second-moment reliability problem and explaining 94.3% of empirical reliability variation seen across parallel configurations. We introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and reduce hallucinations by up to 25.6% (and 14.6% on average) while preserving general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational studies indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different optimal amounts of neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2510.25781</link>
<guid>https://arxiv.org/abs/2510.25781</guid>
<content:encoded><![CDATA[
arXiv:2510.25781v2 Announce Type: replace-cross 
Abstract: The so-called Kolmogorov-Arnold Networks (KANs), whose design is merely inspired, rather than dictated, by the Kolmogorov superposition theorem, have emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs). This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape. By collecting and categorizing a large set of open-source implementations, we map the vibrant ecosystem supporting modern KAN development. We organize the review around four core themes:
  (i) presenting a precise history of Kolmogorov's superposition theory toward neural-network formulations; (ii) establishing the formal equivalence between KANs and MLPs; (iii) analyzing the critical role of basis functions; and (iv) organizing recent advancements in accuracy, efficiency, regularization, and convergence.
  Finally, we provide a practical Choose-Your-KAN guide to assist practitioners in selecting appropriate architectures, and we close by identifying current research gaps and future directions. The associated GitHub repository (https://github.com/AmirNoori68/kan-review) complements this paper and serves as a structured reference for ongoing KAN research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
<link>https://arxiv.org/abs/2511.02241</link>
<guid>https://arxiv.org/abs/2511.02241</guid>
<content:encoded><![CDATA[
arXiv:2511.02241v3 Announce Type: replace-cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v2 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization</title>
<link>https://arxiv.org/abs/2511.19253</link>
<guid>https://arxiv.org/abs/2511.19253</guid>
<content:encoded><![CDATA[
arXiv:2511.19253v2 Announce Type: replace-cross 
Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
arXiv:2511.22048v2 Announce Type: replace-cross 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing the Plasticity-Stability Dilemma in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01034</link>
<guid>https://arxiv.org/abs/2512.01034</guid>
<content:encoded><![CDATA[
arXiv:2512.01034v2 Announce Type: replace-cross 
Abstract: Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Learnability Transition of Measurement-Induced Entanglement</title>
<link>https://arxiv.org/abs/2512.01317</link>
<guid>https://arxiv.org/abs/2512.01317</guid>
<content:encoded><![CDATA[
arXiv:2512.01317v2 Announce Type: replace-cross 
Abstract: Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems. Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources. We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation. Using measurement records alone, we train a neural network in a self-supervised manner to predict the uncertainty metric for MIE--the gap between upper and lower bounds of the average post-measurement bipartite entanglement. Applied to random circuits with one-dimensional all-to-all connectivity and two-dimensional nearest-neighbor coupling, our method reveals a learnability transition with increasing circuit depth: below a threshold, the uncertainty is small and decreases with polynomial measurement data and model parameters, while above it the uncertainty remains large despite increasing resources. We further verify this transition experimentally on current noisy quantum devices, demonstrating its robustness to realistic noise. These results highlight the power of data-driven approaches for learning MIE and delineate the practical limits of its classical learnability.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Floor Plan Recognition: A Hybrid Mix-Transformer and U-Net Approach for Precise Wall Segmentation</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
arXiv:2512.02413v2 Announce Type: replace-cross 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans necessitates high-precision semantic segmentation of structural elements, particularly walls. However, existing methods often struggle with detecting thin structures and maintaining geometric precision. This study introduces MitUNet, a hybrid neural network combining a Mix-Transformer encoder and a U-Net decoder enhanced with spatial and channel attention blocks. Our approach, optimized with the Tversky loss function, achieves a balance between precision and recall, ensuring accurate boundary recovery. Experiments on the CubiCasa5k dataset and a proprietary regional dataset demonstrate MitUNet's superiority in generating structurally correct masks with high boundary accuracy, outperforming standard models. This tool provides a robust foundation for automated 3D reconstruction pipelines. To ensure reproducibility and facilitate future research, the source code and the proprietary regional dataset are publicly available at https://github.com/aliasstudio/mitunet and https://doi.org/10.5281/zenodo.17871079 respectively.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
<div> Keywords: sketch representation, temporal aspect, positional encoding, autoregressive decoder, sequence modeling  

<br /><br />Summary:  
This article explores the significance of the temporal aspect in sketch representation learning by questioning whether sketches should be modeled as sequences. The study evaluates different internal orders of sketches to identify which are most influential for representation quality. Results reveal that traditional positional encodings are effective in capturing the sequential nature of sketches. However, absolute coordinate encodings outperform relative coordinate encodings consistently across experiments. Additionally, non-autoregressive decoders demonstrate superior performance compared to autoregressive decoders when reconstructing or interpreting sketches. Importantly, the study finds that the importance of temporality varies depending on the specific sketch order used as input and the downstream task being performed. Overall, this investigation clarifies how temporal modeling benefits sketch representations and guides the design of learning architectures by emphasizing absolute positional information and non-autoregressive decoding methods for enhanced performance. <div>
arXiv:2512.04007v2 Announce Type: replace-cross 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs</title>
<link>https://arxiv.org/abs/2512.07841</link>
<guid>https://arxiv.org/abs/2512.07841</guid>
<content:encoded><![CDATA[
<div> Data Oriented Design, Object-Oriented Design, A* search algorithm, multi-threading, cache utilization<br /><br />Summary:<br /><br />This study analyzes the performance differences between Data Oriented Design (DOD) and traditional Object-Oriented Design (OOD), focusing on cache usage and efficiency in multi-threaded environments. Four versions of the A* search algorithm were developed and compared: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). Evaluations considered execution time, memory usage, and CPU cache misses. Results show that in multi-threaded scenarios, DOD achieved significant performance improvements with faster runtime, fewer system calls, and reduced cache misses. Although OOD occasionally exhibited slight advantages in memory consumption or percentage-based cache miss rates, DOD's efficiency in handling data-intensive tasks was more pronounced. The study also found that for a fine-grained algorithm like A*, the overhead caused by thread management led both paradigms’ single-threaded versions to outperform their multi-threaded equivalents. These findings underscore DOD's architectural superiority, suggesting its better suitability for optimizing hardware utilization in complex, large-scale AI and parallel computing applications despite subtle differences in simpler algorithms. <div>
arXiv:2512.07841v1 Announce Type: new 
Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI autonomously build, operate, and use the entire data stack?</title>
<link>https://arxiv.org/abs/2512.07926</link>
<guid>https://arxiv.org/abs/2512.07926</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous data estates, data lifecycle, AI agents, data management, modern data stack<br /><br />Summary:<br /><br />1. Enterprise data management involves complex processes such as data architecture, integration, quality, governance, and continuous improvement.<br /><br />2. Current AI assistants help specific roles like data engineers and stewards but do not achieve full automation in managing data systems.<br /><br />3. The paper argues for a paradigm shift from isolated AI applications in data components to a holistic, fully autonomous management of the entire data lifecycle.<br /><br />4. It explores how intelligent agents can autonomously manage each stage of the modern data stack, creating self-sufficient systems that serve both human users and AI itself.<br /><br />5. The authors discuss driving forces behind this shift, potential improvements from agent-based automation, and highlight open questions requiring further research to realize a more autonomous data future. <div>
arXiv:2512.07926v1 Announce Type: new 
Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07993</link>
<guid>https://arxiv.org/abs/2512.07993</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, KV cache eviction, chain-of-thought reasoning, SkipKV, sentence-level compression

<br /><br />Summary:  
The paper addresses the significant memory and throughput challenges posed by large reasoning models (LRMs) during chain-of-thought (CoT) reasoning, which cause linear growth in key-value (KV) cache size. Existing KV cache eviction methods are examined but found ineffective in multi-batch settings due to unstable token-wise scoring and reduced effective KV budget from padding tokens. These methods also generate longer sequences by causing repeated revalidation, as they do not consider semantic coherence. To overcome these challenges, the authors propose SkipKV, a training-free KV compression technique that performs selective eviction and generation at the sentence level rather than token level. SkipKV introduces a novel sentence-scoring metric to identify and remove highly similar sentences while preserving semantic coherence. Additionally, it dynamically adjusts a steering vector to modify hidden activation states during inference, reducing redundant generation and encouraging more concise outputs. Experimental results across multiple reasoning benchmarks demonstrate that SkipKV achieves up to 26.7% higher accuracy than alternative methods at equivalent compression budgets. It also reduces generation length by up to 1.6 times and improves throughput by up to 1.7 times compared to state-of-the-art techniques, making it a more efficient solution for CoT reasoning with LRMs. <div>
arXiv:2512.07993v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching</title>
<link>https://arxiv.org/abs/2512.08026</link>
<guid>https://arxiv.org/abs/2512.08026</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-augmented patient-trial matching, electronic health record integration, large language models, dynamic eligibility assessment, secure scalable system<br /><br />Summary:<br /><br />This paper presents a secure and scalable proof-of-concept system designed to automate and augment the process of screening patients for clinical trial eligibility, which is traditionally manual and resource-intensive. The system integrates heterogeneous electronic health record (EHR) data to ensure comprehensive patient information is utilized. It leverages open-source, reasoning-enabled large language models (LLMs) to go beyond simple binary classifications by generating structured eligibility assessments with interpretable reasoning chains. This supports human-in-the-loop expert review, ensuring transparency and reliability. The system treats eligibility as a dynamic state rather than a static decision, identifying immediate matches while also offering actionable recommendations that could make a patient eligible in the future. This approach broadens the range of trials considered for each patient and aims to reduce the burden on clinical coordinators. Additionally, the design guarantees rigorous security measures and comprehensive auditability of all AI-generated outputs, addressing critical implementation challenges in clinical settings. Overall, the solution promises improved efficiency, transparency, and flexibility in clinical trial patient matching workflows. <div>
arXiv:2512.08026v1 Announce Type: new 
Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Education and Research: An Empirical and User Survey-based Analysis</title>
<link>https://arxiv.org/abs/2512.08057</link>
<guid>https://arxiv.org/abs/2512.08057</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ChatGPT, DeepSeek, education, research<br /><br />Summary:<br /><br />This study evaluates two prominent Large Language Models (LLMs), ChatGPT and DeepSeek, focusing on their application in education and research domains. It begins with a background technology analysis to understand the models' underlying architectures and design goals. Empirical experiments benchmark their performance in various tasks, including text generation, programming, and specialized problem-solving such as mathematics and medicine. Results indicate that ChatGPT excels in general language understanding and natural language text generation, offering high accuracy and user-friendliness. Meanwhile, DeepSeek shows superior efficiency and stronger performance in programming-related tasks, attributed to its design emphasis on computational efficiency. Both models reliably produce medically accurate diagnostic outputs and effectively address complex mathematical problems, making them valuable tools for scientific and academic use. Additionally, the study incorporates a real-world user survey involving students, educators, and researchers, which provides qualitative insights into the practical benefits and limitations of both LLMs. The survey responses highlight how these models aid learning, research productivity, and problem-solving while identifying areas needing improvement. Overall, the research offers a comprehensive perspective on trade-offs between model accuracy, computational costs, and user experience, underscoring the growing role of LLMs in advancing educational and research practices. <div>
arXiv:2512.08057v1 Announce Type: new 
Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Back-End for an AI-Based Diabetes Prediction Application</title>
<link>https://arxiv.org/abs/2512.08147</link>
<guid>https://arxiv.org/abs/2512.08147</guid>
<content:encoded><![CDATA[
<div> Keywords: diabetes prediction, scalable back-end, horizontal scaling, RabbitMQ, performance evaluation<br /><br />Summary:<br /><br />The paper addresses the rising need for early diabetes detection by developing a scalable back-end architecture for a mobile diabetes prediction application. The system aims to maintain a failure rate below 5% and an average response latency under 1000 milliseconds. To achieve scalability and responsiveness, the architecture employs horizontal scaling techniques and database sharding, which distribute load and data effectively. Asynchronous communication using RabbitMQ is a key design element, enabling the system to handle computationally intensive prediction requests without overwhelming the back end, by queuing requests and minimizing data loss during peak loads. Performance evaluation results indicate that 83% of the system's features (20 out of 24 functions) met the established performance criteria. Critical features such as user profile management, activity tracking, and read-heavy prediction operations achieved the desired low latency and reliability. The system successfully supported up to 10,000 concurrent users, demonstrating its capability to scale and maintain stability under significant demand. This work highlights the importance of asynchronous messaging and distributed architecture for real-time AI prediction services in healthcare applications. <div>
arXiv:2512.08147v1 Announce Type: new 
Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions</title>
<link>https://arxiv.org/abs/2512.08230</link>
<guid>https://arxiv.org/abs/2512.08230</guid>
<content:encoded><![CDATA[
<div> Keywords: causal learning, empowerment, causal Bayes nets, intrinsic reward, human cognition<br /><br />Summary:<br />1. The paper addresses the challenge of understanding causal structure learning, a key aspect of human cognition, highlighting difficulties faced by large pretrained models using conventional deep learning techniques. <br />2. It emphasizes the role of causal Bayes nets, a formalism from computer science, that cognitive scientists have used to model human causal learning effectively. <br />3. The concept of "empowerment," an intrinsic reward signal from reinforcement learning that maximizes mutual information between actions and outcomes, is proposed as a critical link between Bayesian causal learning and reinforcement learning paradigms. <br />4. The authors argue that accurate causal world models inherently increase empowerment, and boosting empowerment, in turn, improves the accuracy of causal models. This bidirectional relationship might explain unique features in children's causal learning and offer a computationally feasible account of that process. <br />5. An empirical study is conducted comparing children and adults, systematically testing how they utilize cues related to empowerment to infer causal relationships and design effective interventions, supporting the theoretical claims of the framework. <div>
arXiv:2512.08230v1 Announce Type: new 
Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes</title>
<link>https://arxiv.org/abs/2512.08261</link>
<guid>https://arxiv.org/abs/2512.08261</guid>
<content:encoded><![CDATA[
<div> Keywords: disease prediction, knowledge graph, prototype learning, interpretability, large language models<br /><br />Summary: Predicting diseases from patient-reported information such as demographics and symptoms is important for early healthcare engagement and system efficiency. However, current methods often struggle with imbalanced disease distributions and lack interpretability, leading to biased or unreliable predictions. The proposed KPI framework addresses these challenges by integrating structured, trusted medical knowledge into a unified disease knowledge graph, which helps represent diseases comprehensively. KPI constructs clinically meaningful disease prototypes to better capture disease characteristics, especially benefiting prediction of rare or long-tailed diseases. In addition, it leverages contrastive learning techniques to improve predictive accuracy by effectively distinguishing between similar disease representations. To enhance the interpretability of predictions, KPI uses large language models to generate patient-specific, medically relevant explanations. These explanations align with patient narratives, increasing the reliability and clinical validity of the model outputs. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods both in accuracy and explanation quality. By combining knowledge integration, prototype learning, and advanced language models, KPI offers a practical, patient-centered approach for disease prediction that supports both accurate diagnosis and understandable medical insights. <div>
arXiv:2512.08261v1 Announce Type: new 
Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Models Ace the CFA Exams</title>
<link>https://arxiv.org/abs/2512.08270</link>
<guid>https://arxiv.org/abs/2512.08270</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, CFA exams, reasoning models, Gemini 3.0 Pro, GPT-5  

<br /><br />Summary:  
This study evaluates the performance of state-of-the-art large language models (LLMs) on mock Chartered Financial Analyst (CFA) exams comprising 980 questions from Levels I, II, and III. Contrary to earlier research showing LLM struggles on CFA exams, recent reasoning models demonstrate strong results across all three exam levels. Using established pass/fail criteria, most models successfully pass each level of the CFA exams. The top-performing models in order are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Gemini 3.0 Pro stands out with a record-breaking 97.6% score on Level I. On Level II, GPT-5 leads with a 94.3% score, indicating high proficiency in intermediate financial topics. For Level III, which emphasizes constructed-response questions, Gemini 2.5 Pro achieves the highest multiple-choice accuracy at 86.4%, while Gemini 3.0 Pro attains an impressive 92.0% on constructed responses. These results suggest that the newest LLMs have substantially improved reasoning capabilities relevant to professional finance exams, surpassing prior benchmarks and highlighting their potential utility in financial education and assessment. <div>
arXiv:2512.08270v1 Announce Type: new 
Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content</title>
<link>https://arxiv.org/abs/2512.08273</link>
<guid>https://arxiv.org/abs/2512.08273</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Agents, Large Language Models, Automated Evaluation, Content Quality, Business Content  

<br /><br />Summary:  
This research addresses the increasing challenges businesses face in creating and evaluating high-quality content, which is often time-consuming and costly due to human constraints and expensive extrinsic evaluation methods. It highlights the potential of Large Language Models (LLMs) to aid content creation while acknowledging persistent concerns about the quality of AI-generated outputs. To overcome the limitations of traditional human evaluations, the study introduces Generative Agents capable of simulating human judgment. These agents can rapidly and cost-effectively assess AI-generated content by rating key aspects such as coherence, interestingness, clarity, fairness, and relevance. The integration of such agents allows businesses to streamline their content generation workflows, ensuring consistent and high-quality results without the operational burden of human evaluators. Furthermore, the paper provides valuable insights into improving LLMs specifically for producing business-aligned content, marking a significant step forward in automated content generation and evaluation technologies. This approach promises to reduce costs and improve efficiency, offering a scalable solution for modern content challenges faced by enterprises. <div>
arXiv:2512.08273v1 Announce Type: new 
Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Science of Scaling Agent Systems</title>
<link>https://arxiv.org/abs/2512.08296</link>
<guid>https://arxiv.org/abs/2512.08296</guid>
<content:encoded><![CDATA[
<div> Agents, language models, coordination, performance scaling, multi-agent systems<br /><br />Summary:<br /><br />This paper investigates the principles that determine the performance of agent-based language model (LM) systems, which are critical for reasoning, planning, and acting in real-world AI applications. The authors conduct a large-scale empirical evaluation across four benchmarks (Finance-Agent, BrowseComp-Plus, PlanCraft, Workbench) using five architectures (Single, Independent, Centralized, Decentralized, Hybrid) and three LLM families, covering 180 configurations. They develop a predictive model based on coordination metrics—efficiency, overhead, error amplification, and redundancy—with a cross-validated R² of 0.513. The study identifies three core effects: (1) a trade-off between tool usage and coordination overhead where multi-agent overhead disproportionately impacts tool-heavy tasks under fixed computational budgets; (2) capability saturation, showing diminishing or negative returns on coordination once single-agent baseline performance surpasses roughly 45%; (3) error amplification varies by topology, with independent agents amplifying errors 17.2 times, while centralized coordination limits this to 4.4 times. Centralized coordination offers an 80.9% performance gain on parallelizable tasks like financial reasoning, whereas decentralized coordination performs better on dynamic web navigation (+9.2% vs. +0.2%). However, for sequential reasoning tasks, all multi-agent approaches degrade performance significantly (39-70%). Their framework predicts the optimal coordination strategy for 87% of unseen configurations, offering principled guidance for agentic system design based on task characteristics. <div>
arXiv:2512.08296v1 Announce Type: new 
Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</title>
<link>https://arxiv.org/abs/2512.08300</link>
<guid>https://arxiv.org/abs/2512.08300</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning strategies, multi-agent reinforcement learning, planner, chain of thoughts<br /><br />Summary:<br /><br />This paper introduces a novel reinforced strategy injection mechanism (rSIM) designed to transform any large language model (LLM) into a Reasoning Language Model (RLM) by enabling advanced reasoning capabilities. The key innovation involves using a small planner (leader agent) that guides the LLM (follower agent) through the adaptive injection of reasoning strategies within chain of thoughts (CoTs). The planner and LLM are trained jointly via multi-agent reinforcement learning (MARL) following a leader-follower framework with simple rule-based rewards. Experiments demonstrate that rSIM significantly enhances the reasoning abilities of a smaller model Qwen2.5-0.5B, enabling it to outperform a much larger LLM, Qwen2.5-14B. Importantly, the planner component is generalizable and can be trained once to serve as a plug-in that improves reasoning capabilities across different existing LLMs without retraining them extensively. Furthermore, the planner supports continual learning, allowing it to evolve its planning skills over time and generalize effectively across a wide variety of tasks and problem types. This approach presents a promising direction for cost-efficiently upgrading LLMs’ reasoning skills through strategic planning and reinforcement learning. <div>
arXiv:2512.08300v1 Announce Type: new 
Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from T\"urkiye</title>
<link>https://arxiv.org/abs/2512.08340</link>
<guid>https://arxiv.org/abs/2512.08340</guid>
<content:encoded><![CDATA[
<div> Keywords: California Bearing Ratio, machine learning, random forest, geotechnical engineering, soil properties<br /><br />Summary:<br /><br />This study addresses the prediction of the California Bearing Ratio (CBR), a crucial measure of subgrade soil strength used in transportation infrastructure and foundation design. Traditional CBR tests, although accurate, are time-consuming and costly, limiting their practicality for large-scale soil assessments. To overcome these limitations, the authors developed a machine learning (ML) framework utilizing a dataset of 382 soil samples from various geoclimatic regions in Türkiye. The dataset incorporates multiple physicochemical soil properties to create a multidimensional feature set for supervised learning. Twelve ML algorithms were evaluated, including decision trees, random forest, gradient boosting, support vector regression, and stacking regressors. The random forest regressor achieved the best performance, with R² scores of 0.95 for training, 0.76 for validation, and 0.83 for testing, demonstrating strong generalization and robustness. The study highlights the random forest’s ability to model complex nonlinear relationships in soil data effectively. Ultimately, this research illustrates the potential of intelligent, data-driven approaches to serve as efficient alternatives to traditional CBR testing, facilitating faster, cost-effective geotechnical evaluations and promoting digital transformation in infrastructure analysis and design. <div>
arXiv:2512.08340v1 Announce Type: new 
Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in T\"urkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach</title>
<link>https://arxiv.org/abs/2512.08343</link>
<guid>https://arxiv.org/abs/2512.08343</guid>
<content:encoded><![CDATA[
<div> Keywords: Soil compaction, Optimum Moisture Content, Maximum Dry Density, AutoML, XGBoost<br /><br />Summary:<br /><br />Soil compaction is essential in construction engineering to ensure stable structures like road embankments and earth dams. Traditional approaches for determining Optimum Moisture Content (OMC) and Maximum Dry Density (MDD) rely heavily on time-consuming laboratory experiments and empirical regression models, which often lack accuracy and broad applicability. This study introduces an automated machine learning (AutoML) framework to predict OMC and MDD more efficiently. AutoML automates the selection of algorithms and hyperparameter tuning, aiming to enhance model accuracy and scalability across diverse soil types. Extensive experimentation showed that the Extreme Gradient Boosting (XGBoost) model outperformed others, achieving R-squared values of 80.4% for MDD prediction and 89.1% for OMC on an independent dataset. The results emphasize the significance of using heterogeneous datasets to improve generalization and prediction quality of machine learning models in soil parameter estimation. Ultimately, the study demonstrates how integrating AutoML can lead to more accurate, reliable, and efficient soil compaction parameter predictions, thereby supporting improved construction practices. This advancement has the potential to reduce reliance on labor-intensive testing while enhancing engineering decision-making for various soil conditions. <div>
arXiv:2512.08343v1 Announce Type: new 
Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions</title>
<link>https://arxiv.org/abs/2512.08344</link>
<guid>https://arxiv.org/abs/2512.08344</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Explainable AI, post-hoc explanations, interpretable models, graph structure analysis  

<br /><br />Summary:  
This thesis addresses the challenge of explaining Graph Neural Networks (GNNs), which are widely used for analyzing graph-structured data but whose decision-making processes are difficult to understand. Existing Explainable AI (XAI) techniques either rely on post-hoc explanations that analyze trained models after the fact or on interpretable models designed to provide intrinsic explanations. Post-hoc methods, while flexible and adaptable to different models, often demand significant computational resources and may lack reliability due to limited access to the model’s inner workings. Interpretable models offer immediate explanations but face challenges when applied broadly across diverse scenarios. Many current approaches focus predominantly on graph structure analysis to identify key patterns related to prediction outcomes, but often limit explanations to individual feature contributions. The proposed research aims to develop a novel XAI framework suited specifically for graph-based machine learning that can efficiently provide adaptable and computationally light explanations. This framework seeks to go beyond simplistic feature-based explanations by capturing the influence of graph structure on model predictions, thus improving interpretability in a way that balances reliability, efficiency, and generalizability. <div>
arXiv:2512.08344v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations</title>
<link>https://arxiv.org/abs/2512.08345</link>
<guid>https://arxiv.org/abs/2512.08345</guid>
<content:encoded><![CDATA[
<div> workplace toxicity, large language models, multi-agent systems, adversarial debates, operational efficiency<br /><br />Summary:  
This study addresses the challenge of quantifying the direct impact of workplace toxicity on operational efficiency, which is difficult to measure ethically and practically in human subjects. Using Large Language Model (LLM)-based multi-agent systems, the researchers simulate 1-on-1 adversarial debates to create a controlled "sociological sandbox" environment. They apply a Monte Carlo method to run hundreds of these simulated discussions, comparing a baseline control group against treatment groups where agents are prompted to behave toxically. The key metric analyzed is the convergence time, defined as the number of arguments required for the agents to reach a conclusion. Results show that conversations involving toxic agents take approximately 25% longer to reach conclusions, indicating increased social friction and inefficiency. The authors propose that this increased "latency of toxicity" could act as a proxy for financial losses in both corporate and academic contexts. Furthermore, their approach illustrates that agent-based modeling with LLMs provides a reproducible, ethical alternative to traditional human-subject research when studying the dynamics of social conflict and friction in organizations. <div>
arXiv:2512.08345v1 Announce Type: new 
Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making</title>
<link>https://arxiv.org/abs/2512.08366</link>
<guid>https://arxiv.org/abs/2512.08366</guid>
<content:encoded><![CDATA[
<div> Keywords: Dual-Strategy Agent, Large Language Models, Reflection Mechanism, ALFWorld, Co-adaptive Reasoning  

<br /><br />Summary:  
This paper introduces DuSAR (Dual-Strategy Agent with Reflecting), a novel demonstration-free framework designed for large language model (LLM) agents to improve reasoning and problem-solving abilities. DuSAR leverages two complementary strategies: a high-level holistic plan that directs overall problem-solving, and a context-grounded local policy focused on detailed, step-by-step action. These strategies are coordinated through a lightweight reflection mechanism, where the agent continuously evaluates its progress using a Strategy Fitness Score. This dynamic self-assessment allows the agent to revise its global plan when encountering obstacles or optimize it upon progress, mimicking human-like metacognitive behavior. Experimentally, DuSAR achieves state-of-the-art results on ALFWorld and Mind2Web benchmarks using open-source LLMs (7B-70B parameters). It reaches a 37.1% success rate on ALFWorld with Llama3.1-70B, more than doubling previous best results, and attains 4.02% success on Mind2Web, also outperforming prior baselines by over two times. Additionally, the method significantly reduces token usage per step by 3 to 9 times while maintaining strong performance. Ablation studies emphasize the importance of coordinating the dual strategies. Finally, DuSAR’s design supports the optional integration of expert demonstrations, further enhancing its adaptability and effectiveness. <div>
arXiv:2512.08366v1 Announce Type: new 
Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals</title>
<link>https://arxiv.org/abs/2512.08379</link>
<guid>https://arxiv.org/abs/2512.08379</guid>
<content:encoded><![CDATA[
<div> Keywords: wearable biosignals, feature extraction, large language models, machine learning, DeepFeature<br /><br />Summary:<br /><br />This paper addresses the challenges in feature extraction from biosignals collected via wearable devices, which are critical for healthcare machine learning applications. Traditional methods often lack task-specific context, struggle to optimize settings in high-dimensional spaces, and can suffer from automation errors. To overcome these issues, the authors propose DeepFeature, a novel framework empowered by large language models (LLMs) that generates context-aware features for wearable biosignals. DeepFeature uniquely integrates expert knowledge and task-specific parameters through a multi-source feature generation mechanism. It incorporates an iterative feature refinement process that uses feedback based on feature assessment to iteratively re-select and improve features. Moreover, it ensures robustness in feature-to-code translation by implementing multi-layer filtering and verification, preventing extraction functions from crashing. Experimental evaluation across eight diverse tasks demonstrates that DeepFeature improves the average area under the receiver operating characteristic curve (AUROC) by 4.21% to 9.67% compared to baseline methods. It outperforms existing state-of-the-art techniques on five tasks while maintaining comparable results on the others. This work represents a significant advancement in automated, context-aware feature extraction for biosignal-based machine learning in healthcare. <div>
arXiv:2512.08379v1 Announce Type: new 
Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems</title>
<link>https://arxiv.org/abs/2512.08411</link>
<guid>https://arxiv.org/abs/2512.08411</guid>
<content:encoded><![CDATA[
<div> Keywords: hybrid dynamics, model-based planning, Mixture-of-Experts, latent orthogonalization, trajectory optimization<br /><br />Summary:<br /><br />Model-based planning in robotics struggles with hybrid physical dynamics, where continuous motion interleaves with discrete events like contacts and impacts. Traditional latent world models use monolithic neural networks that assume global continuity, causing excessive smoothing of distinct dynamic modes such as sticking vs. sliding or flight vs. stance. This smoothing leads to large compounding errors over long-horizon predictions, undermining reliability in planning near physical boundaries. To tackle these challenges, the authors propose the Prismatic World Model (PRISM-WM), a structured architecture that breaks down complex hybrid dynamics into composable primitives. PRISM-WM employs a context-aware Mixture-of-Experts (MoE) framework, where a gating mechanism implicitly detects the current physical mode and routes to specialized experts predicting associated transitions. They also introduce a latent orthogonalization objective to promote expert diversity and avoid mode collapse. This approach accurately captures sharp mode transitions, substantially reducing rollout drift during planning. Experiments on complex continuous control benchmarks, including high-dimensional humanoids and multi-task settings, confirm that PRISM-WM offers a high-fidelity substrate for trajectory optimization algorithms like TD-MPC. The results suggest PRISM-WM as a promising foundational model for the next generation of model-based agents, enabling more reliable and accurate long-term planning in hybrid dynamical systems. <div>
arXiv:2512.08411v1 Announce Type: new 
Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change</title>
<link>https://arxiv.org/abs/2512.08449</link>
<guid>https://arxiv.org/abs/2512.08449</guid>
<content:encoded><![CDATA[
<div> Keywords: Impact-Driven AI Framework, Theory of Change, value alignment, adversarial debiasing, Assurance Layer<br /><br />Summary:<br /><br />This paper introduces the Impact-Driven AI Framework (IDAIF), a novel AI architectural methodology integrating Theory of Change (ToC) principles to address the AI alignment problem by ensuring AI behavior aligns with human values and intentions. It highlights the limitations of current approaches focused mainly on technical metrics, neglecting sociotechnical deployment aspects, and proposes a systematic mapping of ToC's five-stage model (Inputs, Activities, Outputs, Outcomes, Impact) to AI architecture layers (Data, Pipeline, Inference, Agentic, Normative). Each layer leverages rigorous theoretical foundations: multi-objective Pareto optimization for aligning values; hierarchical multi-agent orchestration to achieve outcomes; causal directed acyclic graphs (DAGs) to mitigate hallucinations; and adversarial debiasing combined with Reinforcement Learning from Human Feedback (RLHF) to ensure fairness. The framework also introduces an Assurance Layer employing guardian architectures to manage assumption failures and enhance system trustworthiness. Through formal mathematical formulations and three diverse case studies in healthcare, cybersecurity, and software engineering, IDAIF demonstrates a paradigm shift from model-centric to impact-centric AI development. This provides engineers with concrete architectural patterns for designing ethical, trustworthy, and socially beneficial AI systems capable of addressing sociotechnical challenges and improving real-world impacts. <div>
arXiv:2512.08449v1 Announce Type: new 
Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using reinforcement learning to probe the role of feedback in skill acquisition</title>
<link>https://arxiv.org/abs/2512.08463</link>
<guid>https://arxiv.org/abs/2512.08463</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, drag control, skill acquisition, flow feedback, physical system  

<br /><br />Summary:  
This study investigates skill acquisition in physical systems by bypassing human subjects and using a generalist reinforcement learning agent interfaced with a spinning cylinder in a tabletop circulating water channel. The goal is to maximize or minimize drag, presenting a real-world, chaotic flow environment that is difficult to model or simulate accurately. The task is straightforward in terms of objectives, as drag minimization or maximization can be directly translated into the agent’s reward, yet discovering effective strategies is challenging. The experimental setup is inexpensive, easy to reproduce, and supported by decades of prior open-loop policy studies. Results demonstrate that the agent, equipped with high-dimensional flow feedback, rapidly discovers high-performance drag-control strategies within minutes of real-world interaction. Furthermore, replaying the learned action sequences without any feedback yields nearly identical performance, indicating that flow feedback is essential for learning but not for executing the skill. Interestingly, training without flow feedback causes failure in the drag maximization task but still allows successful, though slower and less reliable, learning in drag minimization. The findings highlight that acquiring high-performance skills may require richer informational inputs than those needed for skill execution, and that the nature of the learning challenge depends on the goal rather than the complexity of dynamics or policy. <div>
arXiv:2512.08463v1 Announce Type: new 
Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance</title>
<link>https://arxiv.org/abs/2512.08492</link>
<guid>https://arxiv.org/abs/2512.08492</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Program Repair, Data Transformation Graph, Large Language Models, Neuro-symbolic reasoning, Autonomous Issue Resolver<br /><br />Summary:<br />1. The paper addresses the challenge of repository-scale Automated Program Repair (APR), highlighting the limitations of existing control-centric approaches that focus on navigating directory structures and control flow logic.<br />2. It proposes a novel paradigm shift from Code Property Graphs (CPGs) to Data Transformation Graphs (DTGs), where data states are modeled as nodes and functions as edges, allowing agents to track defects via data lineage rather than control flow.<br />3. A multi-agent framework is introduced that integrates data integrity navigation with control flow logic, improving the capability to resolve semantic errors typically trapped in standard Retrieval-Augmented Generation (RAG) systems.<br />4. The authors present Autonomous Issue Resolver (AIR), a self-improvement system using neuro-symbolic reasoning combined with the DTG structure to enable zero-touch code maintenance and scalable logic repair.<br />5. Experimental results demonstrate that AIR achieves a resolution rate of 87.1% on the SWE-Verified benchmark and effectively overcomes core limitations of current AI-assisted coding tools, offering a more robust foundation for reliable software maintenance in increasingly software-dependent environments. <div>
arXiv:2512.08492v1 Announce Type: new 
Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles</title>
<link>https://arxiv.org/abs/2512.08512</link>
<guid>https://arxiv.org/abs/2512.08512</guid>
<content:encoded><![CDATA[
<div> Keywords: state-of-health (SOH), transfer learning (TL), lithium-ion batteries, constructive incremental transfer learning (CITL), unmanned air vehicles (UAV)

<br /><br />Summary: Accurate and rapid state-of-health (SOH) monitoring is crucial for lithium-ion batteries in portable mobile devices to provide reliable energy information. Traditional transfer learning (TL) techniques help reduce the amount of training data needed for SOH monitoring under varying working conditions by leveraging knowledge from data-rich source domains. However, conventional TL approaches demand significant computational resources during training, which is impractical for portable devices due to their limited endurance. To overcome these challenges, the paper proposes a lightweight TL-based SOH monitoring framework called Constructive Incremental Transfer Learning (CITL). CITL incorporates a semi-supervised TL mechanism that effectively utilizes unlabeled data in the target domain by iteratively adding network nodes, thus minimizing monitoring residuals constructively. The approach ensures cross-domain learning capability of added nodes by combining structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Theoretical convergence analysis guarantees both the efficiency of TL and network compactness. The proposed CITL method is validated on an unmanned air vehicle (UAV) battery dataset collected from multiple flight missions, demonstrating superior SOH estimation accuracy. Compared with existing methods—SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM—CITL achieves reduction in root mean square error by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, confirming its effectiveness and practical applicability. <div>
arXiv:2512.08512v1 Announce Type: new 
Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans</title>
<link>https://arxiv.org/abs/2512.08536</link>
<guid>https://arxiv.org/abs/2512.08536</guid>
<content:encoded><![CDATA[
<div> Ethical planning, Large Language Model, automated planning, human-LLM collaboration, principle-grounded rules<br /><br />Summary: Principles2Plan is an innovative interactive prototype designed to enhance ethical awareness in robotic systems operating within human environments by integrating human expertise with the capabilities of Large Language Models (LLMs). The system addresses the limitations of current automated planning tools, which often lack mechanisms to incorporate ethical considerations effectively. It reduces the manual effort and context-specific challenges typically involved in specifying ethical rules by enabling a collaborative process between a domain expert and an LLM. The domain expert inputs the planning domain, problem specifics, and overarching ethical principles such as beneficence and privacy. Based on this input, Principles2Plan generates operational ethical rules consistent with these principles. Users can then review, prioritize, and refine these generated rules. The finalized ethical guidelines are fed into an automated planner to produce plans that are informed by ethical considerations. This approach is novel in providing principle-grounded ethical rule generation tailored for classical planning contexts. Overall, Principles2Plan demonstrates the potential for human-LLM collaboration to make ethical automated planning more practical, adaptable, and context-sensitive, marking an important step towards embedding ethical reasoning in autonomous systems. <div>
arXiv:2512.08536v1 Announce Type: new 
Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SMART+ Framework for AI Systems</title>
<link>https://arxiv.org/abs/2512.08592</link>
<guid>https://arxiv.org/abs/2512.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, SMART+ Framework, AI Governance, Risk Mitigation, Clinical Research<br /><br />Summary:<br /><br />1. Artificial Intelligence (AI) is increasingly embedded across multiple industries, including healthcare, finance, and manufacturing, improving operational workflows such as clinical trial adverse event detection, fraud detection, and predictive maintenance.<br /><br />2. Although AI enhances efficiency, it raises significant concerns related to safety, accountability, and regulatory compliance that necessitate structured governance.<br /><br />3. The paper introduces the SMART+ Framework, a comprehensive model designed to govern AI systems based on core pillars: Safety, Monitoring, Accountability, Reliability, and Transparency.<br /><br />4. The framework is further strengthened by incorporating Privacy & Security, Data Governance, Fairness & Bias mitigation, and operational Guardrails, addressing broader challenges of ethical and regulatory AI deployment.<br /><br />5. SMART+ aligns with evolving regulatory guidance and best practices to provide mechanisms for risk mitigation, trust-building, auditability, and compliance readiness, particularly emphasizing its application in clinical research.<br /><br />6. By adopting SMART+, organizations can ensure responsible AI use, enhance oversight procedures, and integrate effective safeguards for trustworthy AI adoption across diverse sectors. <div>
arXiv:2512.08592v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models</title>
<link>https://arxiv.org/abs/2512.08609</link>
<guid>https://arxiv.org/abs/2512.08609</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Heuristic Design, Large Language Models, Monte Carlo Tree Search, Cognitive Guidance, Heuristic Optimization  

<br /><br />Summary:  
This paper addresses the limitations of existing large language model (LLM)-based evolutionary methods for Automatic Heuristic Design (AHD), which often suffer from local optima due to reliance on population strategies. By integrating LLMs with Monte Carlo Tree Search (MCTS), the trade-off between exploration and exploitation improves; however, current approaches lack multi-round cognitive integration and have limited search diversity. To tackle these challenges, the authors propose CogMCTS, a novel framework that tightly couples the cognitive guidance mechanism from LLMs with MCTS for enhanced automated heuristic optimization. CogMCTS incorporates multi-round cognitive feedback that leverages historical experience, node information, and negative outcomes to dynamically refine heuristic generation. It employs dual-track node expansion alongside elite heuristic management to balance the need for exploring diverse heuristics and exploiting high-quality experiences. Furthermore, strategic mutation techniques adjust heuristic forms and parameters to boost solution diversity and overall optimization performance. Experimental evaluations demonstrate that CogMCTS surpasses existing LLM-based AHD methods in terms of solution stability, optimization efficiency, and solution quality, validating the effectiveness of the proposed cognitive-guided MCTS framework. <div>
arXiv:2512.08609v1 Announce Type: new 
Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protein Secondary Structure Prediction Using Transformers</title>
<link>https://arxiv.org/abs/2512.08613</link>
<guid>https://arxiv.org/abs/2512.08613</guid>
<content:encoded><![CDATA[
<div> Keywords: protein secondary structure, transformer model, attention mechanism, data augmentation, CB513 dataset<br /><br />Summary: This paper introduces a transformer-based model designed to predict protein secondary structures—alpha helices, beta sheets, and coils—directly from amino acid sequences. Leveraging the attention mechanism inherent in transformers, the model effectively captures both local and long-range interactions between residues, which are critical for accurate structural motif identification. To enhance the training process, a sliding-window data augmentation technique is applied to the CB513 dataset, significantly expanding the number of training samples and improving model robustness. The approach is particularly notable for its ability to generalize across sequences of varying lengths, a common challenge in protein structure prediction. The results demonstrate that the transformer model outperforms traditional methods by efficiently modeling complex residue relationships without losing contextual information. This work highlights the potential of transformer architectures in computational biology, especially for tasks involving sequences with intricate dependencies. Overall, the paper contributes a novel methodology that combines advanced sequence modeling with practical data augmentation strategies to improve secondary structure prediction accuracy. <div>
arXiv:2512.08613v1 Announce Type: new 
Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm</title>
<link>https://arxiv.org/abs/2512.08629</link>
<guid>https://arxiv.org/abs/2512.08629</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Embodied Smartphone Operation, robotic arm, platform-agnostic, dataset

<br /><br />Summary:  
1. The paper introduces the Embodied Smartphone Operation (ESO) task, focusing on enabling physical interaction with smartphones via robotics rather than traditional software interfaces.  
2. Existing methods rely heavily on the Android Debug Bridge (ADB) for controlling and interacting with smartphones, which restricts the usability to Android devices only.  
3. The authors propose See-Control, a framework combining a low-degree-of-freedom robotic arm with an MLLM-based embodied agent to operate smartphones physically, removing the dependency on ADB or system back-end access.  
4. See-Control includes three main contributions: an ESO benchmark consisting of 155 defined tasks with evaluation metrics, a multimodal embodied agent that generates robotic control commands directly, and a richly annotated dataset of operation episodes for research use.  
5. This work represents a significant step toward bridging digital intelligent agents and physical environments, enabling home robots to accomplish smartphone-dependent tasks in real-world settings across different platforms. <div>
arXiv:2512.08629v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology</title>
<link>https://arxiv.org/abs/2512.08674</link>
<guid>https://arxiv.org/abs/2512.08674</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, gastrointestinal oncology, hierarchical Multi-Agent Framework, clinical reasoning, automated decision support  

<br /><br />Summary:  
This study focuses on advancing clinical reasoning in gastrointestinal (GI) oncology by integrating multimodal data sources such as endoscopic images, radiological scans, and biochemical markers. Multimodal Large Language Models (MLLMs), despite their potential, face limitations including context dilution and hallucinations when dealing with complex and heterogeneous medical data. To overcome these challenges, the authors propose a hierarchical Multi-Agent Framework designed to replicate the collaborative workflow of a human Multidisciplinary Team (MDT). This agent-based architecture coordinates multiple specialized agents, leading to improved interpretability and reasoning. Evaluation of the system showed a composite expert score of 4.60 out of 5.00, marking a significant improvement compared to traditional monolithic baseline models. The framework particularly excelled in enhancing reasoning logic and medical accuracy. These results suggest that mimicking human collaborative processes via agent-based designs is a promising approach to create scalable, transparent, and clinically reliable decision support systems in oncology. Overall, this work underscores the importance of structured, collaborative AI architectures in navigating the complexities inherent to multimodal clinical data. <div>
arXiv:2512.08674v1 Announce Type: new 
Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance</title>
<link>https://arxiv.org/abs/2512.08740</link>
<guid>https://arxiv.org/abs/2512.08740</guid>
<content:encoded><![CDATA[
<div> Keywords: human-AI collaboration, cognitive enhancement, Recursive Adversarial Meta-Thinking Network (RAMTN), meta-interaction, AI governance

<br /><br />Summary:  
This paper addresses the divide between human expert intuition ("cognitive black box") and the opaque decision-making processes of AI ("computational black box"). It proposes a novel paradigm called "human-AI collaborative cognitive enhancement," which transforms these dual black boxes into a transparent and extensible "functional white-box" system through structured "meta-interaction." The core innovation is the "plug-and-play cognitive framework," a computable knowledge package derived from expert dialogues that can be incorporated into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables the conversion of expert reasoning, such as medical diagnostics and teaching intuition, into reusable and scalable public knowledge assets. This approach marks a paradigm shift from viewing AI merely as a tool to regarding AI as an active thinking partner. Additionally, the work pioneers the concept of "cognitive equity" by providing the first engineering proof of it and introduces a new AI governance model focused on transparency of interaction protocols instead of scrutinizing internal model mechanics. The framework is open-sourced to encourage ethical technology development and cognitive inclusion. The entire research, including the theoretical foundation, methodology, system implementation, and validation, reflects the individual contribution of the author. <div>
arXiv:2512.08740v1 Announce Type: new 
Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Foundation Models with Native Multi-Agent Intelligence</title>
<link>https://arxiv.org/abs/2512.08743</link>
<guid>https://arxiv.org/abs/2512.08743</guid>
<content:encoded><![CDATA[
<div> Foundation models, multi-agent intelligence, communication, planning, adaptation<br /><br />Summary:<br /><br />This paper argues that while foundation models (FMs) have been advancing as the core "brains" of AI agents, the current progress mainly targets enhancing single-agent capabilities such as GUI interaction and tool use. The authors highlight that the next significant advancement is to develop FMs with native multi-agent intelligence, which encompasses four core capabilities: understanding, planning, efficient communication, and adaptation. Through empirical analysis of 41 large language models, the study demonstrates that strong single-agent performance does not automatically translate to effective multi-agent intelligence. The findings challenge prior assumptions about the spontaneous emergence of multi-agent skills and emphasize the need for dedicated development efforts. To bridge this gap, the paper proposes key research avenues focused on constructing appropriate datasets, devising new evaluation methodologies, exploring specialized training paradigms, and integrating safety considerations. These directions aim to build foundation models inherently capable of robust multi-agent interactions, thereby advancing the collaborative and interactive potential of AI systems in complex environments. <div>
arXiv:2512.08743v1 Announce Type: new 
Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments</title>
<link>https://arxiv.org/abs/2512.08755</link>
<guid>https://arxiv.org/abs/2512.08755</guid>
<content:encoded><![CDATA[
<div> Keywords: Reconfigurable Intelligent Surface, STAR-RIS, UAV, 3D Wireless Environment, Optimization  

<br /><br />Summary: This letter investigates and compares the performance of two aerial intelligent surface architectures—Reconfigurable Intelligent Surfaces (RIS) and Simultaneously Transmitting and Reflecting RIS (STAR-RIS)—within three-dimensional wireless environments. Both systems, when mounted on unmanned aerial vehicles (UAVs), leverage the benefits of flexible deployment and improved line-of-sight conditions. The study develops accurate channel models that integrate directional radiation patterns and analyzes how deployment altitude and orientation affect system performance. To maximize the system sum-rate, joint optimization problems are formulated for RIS and STAR-RIS architectures, and a solution is proposed that employs weighted minimum mean square error (WMMSE) combined with block coordinate descent algorithms. Simulation results highlight that STAR-RIS exhibits superior performance in low-altitude scenarios due to its capability of full-space coverage, enhancing wireless coverage and capacity. Conversely, traditional RIS outperforms STAR-RIS when deployed closer to the base station at higher altitudes, benefiting from optimized directional gains. These insights provide valuable guidelines for the practical deployment and design of aerial intelligent surfaces in future sixth-generation (6G) communication networks, demonstrating how to capitalize on the strengths of both architectures under varying environmental and operational conditions. <div>
arXiv:2512.08755v1 Announce Type: new 
Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows</title>
<link>https://arxiv.org/abs/2512.08769</link>
<guid>https://arxiv.org/abs/2512.08769</guid>
<content:encoded><![CDATA[
<div> Agentic AI, multi-agent workflows, Model Context Protocol, Responsible AI, production deployment<br /><br />Summary:<br /><br />1. The paper addresses the paradigm shift introduced by agentic AI, which combines multiple specialized agents, LLMs, tools, and orchestration to enable autonomous, multi-step task execution beyond traditional single-model prompting.<br /><br />2. It identifies the main challenge faced by organizations as designing, engineering, and operating production-grade agentic AI workflows that are reliable, observable, maintainable, and compliant with safety and governance standards.<br /><br />3. The authors propose a structured engineering lifecycle that covers workflow decomposition, multi-agent design patterns, the Model Context Protocol (MCP) for managing context and interactions, integration with external tools, deterministic orchestration, Responsible AI considerations, and environment-aware deployment.<br /><br />4. Nine core best practices are presented, including prioritizing tool-first designs over MCP, pure-function agent invocations, ensuring single-tool and single-responsibility agents, externalizing prompt management, designing model consortia aligned with Responsible AI, separating workflow logic from MCP servers, containerized scalable deployment, and adhering to the KISS principle.<br /><br />5. A comprehensive case study involving a multimodal news-analysis and media-generation workflow demonstrates the application of these principles, serving as a practical blueprint for building robust, extensible, production-ready agentic AI systems. <div>
arXiv:2512.08769v1 Announce Type: new 
Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale</title>
<link>https://arxiv.org/abs/2512.08826</link>
<guid>https://arxiv.org/abs/2512.08826</guid>
<content:encoded><![CDATA[
<div> Keywords: LoRA, generative components, CLIP embeddings, semantic shift, retrieval framework  

<br /><br />Summary:  
The paper addresses the challenge of organizing the rapidly expanding ecosystem of generative components known as LoRAs, which currently lack structured discovery methods and rely heavily on unreliable user descriptions or popularity metrics. To overcome these limitations, the authors introduce CARLoS, a large-scale framework for characterizing LoRAs without additional metadata. CARLoS analyzes over 650 LoRAs by using them in image generation over various prompts and seeds to assess their behavior robustly. The core of the framework is a concise three-part representation based on CLIP embeddings: Directions (semantic shift introduced by the LoRA), Strength (the magnitude of its effect), and Consistency (stability of the effect). Leveraging this representation, CARLoS implements an efficient retrieval system that semantically matches textual user queries to relevant LoRAs, while filtering out those that are overly strong or inconsistent. This retrieval system outperforms traditional textual baselines in both automated tests and human evaluations. Beyond retrieval, the representation also facilitates analysis of legal concepts such as substantiality and volition in copyright law, suggesting that CARLoS is not only practical for discovery but also useful for broader interpretability and legal considerations of LoRAs. <div>
arXiv:2512.08826v1 Announce Type: new 
Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpolation in Knowledge Representation</title>
<link>https://arxiv.org/abs/2512.08833</link>
<guid>https://arxiv.org/abs/2512.08833</guid>
<content:encoded><![CDATA[
<div> Craig interpolation, uniform interpolation, knowledge representation, description logics, logic programming<br /><br />Summary:<br /><br />1. The article focuses on Craig interpolation and uniform interpolation, which are important in knowledge representation tasks such as explainability, forgetting, modularization, reuse, and learning.<br />2. It highlights that many knowledge representation formalisms typically do not guarantee the existence of Craig or uniform interpolants, making computation of such interpolants a challenging problem.<br />3. The paper specifically examines two major knowledge representation formalisms: description logics and logic programming, which have differing properties relevant to interpolation.<br />4. The authors discuss theoretical results pertaining to interpolation in these formalisms, clarifying when and how interpolants can be derived.<br />5. Additionally, practical methods and algorithms for computing interpolants in these frameworks are reviewed, aiming to bridge the gap between theory and application in areas such as knowledge base maintenance and information hiding. <div>
arXiv:2512.08833v1 Announce Type: new 
Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce</title>
<link>https://arxiv.org/abs/2512.08868</link>
<guid>https://arxiv.org/abs/2512.08868</guid>
<content:encoded><![CDATA[
<div> Foundation agents, e-commerce, benchmarks, multi-step reasoning, information retrieval<br /><br />Summary:  
The paper addresses the need for evaluating foundation agents in practical real-world scenarios rather than just academic or artificial benchmarks. It focuses on e-commerce, a domain characterized by diverse user interactions, dynamic market conditions, and tasks relevant to real decision-making processes. To fill this gap, the authors introduce EcomBench, a comprehensive benchmarking tool specifically designed for assessing agent performance in realistic e-commerce environments. EcomBench is sourced from authentic user demands within leading global e-commerce platforms and is curated and annotated by human experts to ensure high clarity, accuracy, and domain relevance. The benchmark covers multiple task categories prevalent in e-commerce, providing a broad assessment range. It further defines three levels of difficulty to test critical agent capabilities like deep information retrieval, multi-step reasoning, and integration of knowledge across various sources. By grounding the evaluation in genuine e-commerce scenarios, EcomBench serves as a rigorous, dynamic, and practical testbed to measure the effectiveness and adaptability of foundation agents in modern e-commerce applications. <div>
arXiv:2512.08868v1 Announce Type: new 
Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs</title>
<link>https://arxiv.org/abs/2512.08923</link>
<guid>https://arxiv.org/abs/2512.08923</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, cross-modal inconsistency, benchmarks, REST, modality gap<br /><br />Summary:<br /><br />This paper introduces two new benchmarks, REST and REST+ (Render-Equivalence Stress Tests), designed to systematically evaluate cross-modal inconsistency in multimodal large language models (MLLMs). The authors highlight that although MLLMs aim to represent vision and language within the same embedding space, they often fail to perform equivalently across different modalities. The benchmarks include samples containing the same semantic information presented in three modalities: image, text, and mixed, demonstrating that current state-of-the-art MLLMs cannot consistently reason across these formats. The study evaluates 15 MLLMs, revealing significant variability in modality inconsistency, which persists even after accounting for challenges related to Optical Character Recognition (OCR). The researchers find that simply rendering text as an image or converting images into text does not resolve these inconsistencies. Furthermore, factors such as text color, image resolution, and the number of vision tokens affect model performance, whereas font type does not. Finally, the paper establishes that the proposed consistency score correlates with the modality gap between text and images, offering a mechanistic explanation for cross-modal inconsistency in MLLMs. <div>
arXiv:2512.08923v1 Announce Type: new 
Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs</title>
<link>https://arxiv.org/abs/2508.05502</link>
<guid>https://arxiv.org/abs/2508.05502</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, low-resource languages, cultural groundedness, multilingual dataset, MELLA<br /><br />Summary:<br /><br />Multimodal Large Language Models (MLLMs) perform well in high-resource languages but struggle significantly with low-resource languages. Existing methods to improve multilingual capabilities often focus solely on text modality or rely on machine translation, which only provide shallow linguistic knowledge. These approaches overlook the importance of multimodal informativeness and cultural groundedness, essential factors for effectively serving users of low-resource languages. This study identifies two key objectives for enhancing MLLMs in such settings: improving linguistic capability and ensuring cultural awareness. To meet these goals, the authors propose a dual-source data collection strategy: native web alt-text is collected to provide cultural context, while captions generated by MLLMs support linguistic development. They implement this strategy by creating MELLA, a multimodal multilingual dataset designed specifically for low-resource language improvement. Experimental results across eight languages and various MLLM architectures demonstrate consistent performance improvements after fine-tuning on MELLA. Notably, models produce richer, more descriptive outputs, reflecting gains in both cultural knowledge and linguistic ability. The authors make the MELLA dataset publicly available at https://opendatalab.com/applyMultilingualCorpus, facilitating further research and development in this area. <div>
arXiv:2508.05502v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating High Energy Physics Data Analysis with LLM-Powered Agents</title>
<link>https://arxiv.org/abs/2512.07785</link>
<guid>https://arxiv.org/abs/2512.07785</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, high energy physics, automated data analysis, workflow management, reproducibility

<br /><br />Summary:  
This study presents a proof-of-principle demonstration of using large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. The case study focuses on measuring the Higgs boson diphoton cross-section using ATLAS Open Data. The authors design a hybrid system combining an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this setup, Snakemake ensures reproducibility and determinism of the workflow execution, while the LLM agent autonomously generates, executes, and iteratively refines analysis code based on user instructions. The study defines several quantitative metrics such as success rate, error distribution, task-specific costs, and average number of API calls to evaluate agent performance across multiple workflow stages. To analyze variability in performance, a benchmark is conducted using various state-of-the-art LLM architectures including Gemini, GPT-5 series, Claude family, and prominent open-weight models. Despite Snakemake’s deterministic enforcement, some stochastic variation remains in final outputs due to model-internal sampling configurations, even with the temperature parameter set to zero. This work establishes the first automated data-analysis framework driven by LLM agents in HEP and provides a systematic benchmark of model capabilities, stability, and limitations in realistic scientific computing environments. The baseline code is publicly available on Hugging Face. <div>
arXiv:2512.07785v1 Announce Type: cross 
Abstract: We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2512.07843</link>
<guid>https://arxiv.org/abs/2512.07843</guid>
<content:encoded><![CDATA[
<div> Large Language Models, parallel reasoning, inference latency, reinforcement learning, chain-of-thought  

<br /><br />Summary:  
This paper introduces ThreadWeaver, a novel framework designed to enhance reasoning efficiency in large language models by enabling adaptive parallel reasoning during inference. First, ThreadWeaver incorporates a two-stage parallel trajectory generator that creates large-scale, high-quality chain-of-thought (CoT) data with parallel annotations used for supervised fine-tuning. Second, it employs a trie-based training-inference co-design that allows parallel reasoning on standard autoregressive inference engines without any modifications to position embeddings or KV caches, simplifying deployment. Third, the framework uses a parallelization-aware reinforcement learning approach that trains the model to strike a balance between accuracy and the level of effective parallelization. Evaluations across six challenging mathematical reasoning benchmarks demonstrate that ThreadWeaver, when built on Qwen3-8B, achieves accuracy comparable to state-of-the-art sequential reasoning models, including 71.9% average accuracy and 79.9% on the AIME24 benchmark. Importantly, ThreadWeaver also reduces inference latency, delivering up to a 1.53x speedup in token-level generation time. This work establishes a new Pareto frontier in the trade-off between inference speed and reasoning accuracy, making it a significant step toward more efficient and deployable large language model reasoning systems. <div>
arXiv:2512.07843v1 Announce Type: cross 
Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning</title>
<link>https://arxiv.org/abs/2512.07844</link>
<guid>https://arxiv.org/abs/2512.07844</guid>
<content:encoded><![CDATA[
<div> Neural Collapse, Long-tailed Learning, Simplex Equiangular Tight Frame, Feature-Classifier Alignment, Generalization Performance  

<br /><br />Summary:  
This paper addresses the challenge of Neural Collapse (NC) in the context of long-tailed data distributions, where severe class imbalance disrupts the typical alignment of class feature means and classifier weights into a simplex equiangular tight frame (ETF), thereby leading to poor generalization. Unlike prior works that focus mainly on recovering the ETF geometry by imposing constraints on either features or classifier weights, this study identifies a crucial issue—misalignment between the feature and classifier weight spaces—which has been largely overlooked. The authors theoretically analyze the negative impact of this misalignment through an optimal error exponent framework, quantifying the associated harm to learning performance. Based on this theoretical understanding, they propose three explicit alignment strategies designed to be easily integrated ("plug-and-play") into existing long-tail learning methods without modifying the model architectures. Extensive empirical evaluations on benchmark datasets including CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT demonstrate that these alignment techniques consistently enhance the performance of various baseline methods, achieving state-of-the-art results. This work provides both theoretical insights and practical solutions to improve learning in imbalanced long-tailed regimes through better feature-classifier alignment. <div>
arXiv:2512.07844v1 Announce Type: cross 
Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AudioScene: Integrating Object-Event Audio into 3D Scenes</title>
<link>https://arxiv.org/abs/2512.07845</link>
<guid>https://arxiv.org/abs/2512.07845</guid>
<content:encoded><![CDATA[
<div> Keywords: audio analysis, spatial context, AudioScanNet, AudioRoboTHOR, 3D environments<br /><br />Summary: The paper introduces two innovative audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, aimed at addressing the lack of spatial context in existing audio-only datasets. These datasets integrate audio clips with spatially aligned 3D scenes, facilitating research into audioconditioned tasks within three-dimensional environments. To accurately associate audio events with corresponding spatial information, the authors utilize the commonsense reasoning capabilities of large language models complemented by thorough human verification. This hybrid annotation approach ensures scalability and maintains high standards of accuracy, completeness, and diversity, as confirmed by inter-annotator agreement metrics. The datasets are evaluated through two benchmark tasks: audio-based 3D visual grounding and audio-based robotic zero-shot navigation, revealing significant limitations in current audiocentric methods. The results emphasize the practical challenges inherent in audio-guided spatial learning and highlight the datasets' importance in advancing this research domain. Ultimately, the work underscores both the potential and complexity of integrating audio signals with spatial context, paving the way for improved human-computer interaction, environmental monitoring, and public safety applications. <div>
arXiv:2512.07845v1 Announce Type: cross 
Abstract: The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction</title>
<link>https://arxiv.org/abs/2512.07846</link>
<guid>https://arxiv.org/abs/2512.07846</guid>
<content:encoded><![CDATA[
<div> Large language models, ranking systems, mix-interaction, embedding tokens, system throughput  

<br /><br />Summary:  
1. Large language models (LLMs) have strong semantic understanding and improve relevance ranking performance in recommendation and search systems but suffer from high computational costs under strict industrial latency and throughput constraints.  
2. Cross-encoder ranking models require long context inputs that include user, query, and item information, leading to heavy prefill workloads and inefficiency.  
3. The authors propose MixLM, a novel ranking framework that significantly boosts system throughput by reducing input context length while preserving the semantic power of cross-encoder rankers through a mix-interaction approach.  
4. MixLM combines text and embedding tokens, encoding all catalog items into a few embedding tokens stored in a nearline cache, thereby shrinking the item input from thousands of tokens to just a few embedding tokens during inference.  
5. Applied in a real-world LinkedIn search application, MixLM increased throughput by 10x at the same latency, maintained relevance metrics, enabled full-traffic deployment of LLM-powered search, and led to a 0.47% increase in Daily Active Users in online A/B tests. <div>
arXiv:2512.07846v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents</title>
<link>https://arxiv.org/abs/2512.07850</link>
<guid>https://arxiv.org/abs/2512.07850</guid>
<content:encoded><![CDATA[
<div> Keywords: long-horizon tasks, mutating actions, decisive deviations, targeted reflection, benchmark verification<br /><br />Summary:<br /><br />This paper investigates the fragile performance of large language model (LLM) agents on long-horizon, tool-using tasks by analyzing whether all actions contribute equally to failure. The authors categorize actions into mutating (environment-changing) and non-mutating steps, defining "decisive deviations" as early critical errors flipping success to failure. Logistic regression analysis shows that deviations in mutating actions drastically reduce success odds—up to 92% on Airline and 96% on Retail benchmarks—whereas non-mutating deviations have minimal effect. Additionally, error rates increase with context length due to agents drifting from their roles and relying on stale constraints. To mitigate these issues, the paper proposes \cm{}, a model-agnostic, gradient-free, test-time safeguard comprising mutation-gated verification, targeted reflection before mutating steps, and block-based context cleaning. \cm{} consistently improves performance across multiple benchmarks, including Qwen3-Thinking and Claude models, with relative gains of +28%, +11%, and +7% on Airline, Retail, and SWE-Bench Verified tasks respectively. The authors also identify ceiling effects in the $\tau$-Bench due to annotation errors and underspecified tasks, and release a revised version called $\tau$-Bench Verified to restore meaningful benchmark headroom. The study highlights the importance of action-level analysis, targeted safeguards, and robust evaluation for advancing multi-turn agent reliability. <div>
arXiv:2512.07850v1 Announce Type: cross 
Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $\tau$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $\tau$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $\tau$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU Memory Prediction for Multimodal Model Training</title>
<link>https://arxiv.org/abs/2512.07853</link>
<guid>https://arxiv.org/abs/2512.07853</guid>
<content:encoded><![CDATA[
<div> GPU memory prediction, multimodal models, out-of-memory, deep learning, model factorization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of GPU Out-of-Memory (OoM) errors that occur during training of large and complex deep learning models, particularly in agentic AI systems.<br />2. Traditional methods for predicting GPU memory usage have focused on unimodal architectures and do not generalize well to multimodal models, which are increasingly prevalent in agentic AI.<br />3. To overcome this, the authors propose a new prediction framework specifically designed for multimodal models by analyzing both the model architecture and training behavior.<br />4. The approach involves decomposing the multimodal model into its individual layers and employing factorization techniques to estimate the memory usage of each layer accurately.<br />5. Evaluation results demonstrate that the framework achieves a high degree of accuracy in predicting peak GPU memory usage, with an average Mean Absolute Percentage Error (MAPE) of approximately 8.7%, enabling more efficient resource management during model training. <div>
arXiv:2512.07853v1 Announce Type: cross 
Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Command &amp; Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing</title>
<link>https://arxiv.org/abs/2512.07866</link>
<guid>https://arxiv.org/abs/2512.07866</guid>
<content:encoded><![CDATA[
<div> Keywords: Malware detection, Domain Generation Algorithms, Deep Learning, LSTM, NLP<br /><br />Summary:<br /><br />This paper addresses the challenge posed by sophisticated malware that uses Domain Generation Algorithms (DGA) to dynamically create thousands of domains daily, making traditional static blacklist approaches ineffective. It presents a novel detection method leveraging Deep Learning and Natural Language Processing (NLP) techniques to identify DGA-generated domains. The research involved compiling a hybrid dataset of 100,000 domain names, equally split between legitimate and malicious entries. From this dataset, lexical features were extracted to train a Recurrent Neural Network, specifically a Long Short-Term Memory (LSTM) model, to recognize patterns indicative of malicious domain generation. The results show that while statistical entropy methods can detect simpler DGA patterns, the LSTM-based approach significantly outperforms these traditional techniques by identifying more complex domain generation strategies. The model achieved an accuracy of 97.2%, demonstrating robustness in differentiating malicious domains from lawful traffic. Additionally, the approach reduces false positive rates in ambiguous cases, which is crucial for maintaining network security without disrupting legitimate communication. Overall, the study confirms the effectiveness of combining Deep Learning with NLP for advanced malware detection, offering a promising direction for cybersecurity defenses against evolving threats. <div>
arXiv:2512.07866v1 Announce Type: cross 
Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline</title>
<link>https://arxiv.org/abs/2512.07867</link>
<guid>https://arxiv.org/abs/2512.07867</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based stress testing, macro-financial scenarios, Value-at-Risk, auditability, retrieval mechanism<br /><br />Summary:<br /><br />1. The article presents a transparent and fully auditable pipeline that utilizes large language models (LLMs) for macro-financial stress testing focused on the G7 economies.<br />2. It employs structured prompting and optional retrieval of country-specific fundamentals and news to generate machine-readable macroeconomic scenarios including GDP growth, inflation, and policy rates.<br />3. These scenarios are translated into portfolio losses through a factor-based approach, enabling risk assessment measures such as Value-at-Risk and Expected Shortfall, compared to classical econometric benchmarks.<br />4. The LLM-generated narratives are coherent, country-specific, and yield stable amplification of tail risks, with limited sensitivity to the retrieval choices used.<br />5. Rigorous plausibility checks, scenario diagnostics, and variance decomposition via ANOVA reveal that risk variations mainly arise from portfolio composition and prompt design rather than the retrieval mechanism.<br />6. To enhance reproducibility and auditability, the pipeline includes features like snapshotting, deterministic processing modes, and hash-verified outputs.<br />7. Overall, the study demonstrates that LLM-generated macroeconomic stress scenarios can complement traditional stress-testing frameworks by offering scalable, interpretable, and validated results. <div>
arXiv:2512.07867v1 Announce Type: cross 
Abstract: We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimization for Function-Valued Responses under Min-Max Criteria</title>
<link>https://arxiv.org/abs/2512.07868</link>
<guid>https://arxiv.org/abs/2512.07868</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, functional data, min-max optimization, Gaussian process, functional principal component analysis  

<br /><br />Summary:  
This paper addresses the challenge of optimizing expensive black-box functions whose outputs are functional responses varying over a continuous domain such as time or wavelength, rather than scalar values. Traditional Bayesian optimization methods typically focus on minimizing average integrated error, which can overlook worst-case deviations across the functional domain. To overcome this, the authors propose min-max Functional Bayesian Optimization (MM-FBO), which directly targets minimizing the maximum prediction error over the entire function domain. They represent functional responses using functional principal component analysis (FPCA) to reduce dimensionality and then model the principal component scores with Gaussian process surrogates. MM-FBO introduces a novel acquisition function that balances exploiting regions with high worst-case expected error and exploring the function domain to improve surrogate accuracy. Theoretical contributions include a discretization bound for the worst-case optimization problem and a consistency guarantee showing convergence of the acquisition function to the true min-max objective as uncertainty diminishes. The method is validated on synthetic benchmark problems and real-world physics-inspired case studies involving electromagnetic scattering and vapor phase infiltration, demonstrating superior performance compared to existing methods. The results emphasize the importance of explicitly modeling functional uncertainty in Bayesian optimization for problems with functional outputs. <div>
arXiv:2512.07868v1 Announce Type: cross 
Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifolds and Modules: How Function Develops in a Neural Foundation Model</title>
<link>https://arxiv.org/abs/2512.07869</link>
<guid>https://arxiv.org/abs/2512.07869</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, neural activity, temporal response, decoding manifolds, biological fidelity<br /><br />Summary:<br /><br />1. This study investigates the internal workings of a state-of-the-art foundation model of neural activity, originally developed by Wang et al. (2025), by analyzing its neurons’ temporal response properties to controlled stimuli as a physiologist would.<br />2. The researchers construct decoding manifolds to understand how different stimuli are represented in the neural activity space and encoding manifolds to understand how different neurons represent stimulus-response relationships.<br />3. The model consists of distinct processing stages: a feedforward encoder, a recurrent module, and a readout module, each showing qualitatively different representational geometries in these manifolds.<br />4. The recurrent module enhances the model’s capability by ‘pushing apart’ the representations of different temporal stimulus patterns, thereby increasing its discriminative power compared to the encoder.<br />5. The readout module attains biological fidelity not by mimicking biological mechanisms but by leveraging numerous specialized feature maps, representing a divergent approach to biological plausibility.<br /><br />Overall, the study provides novel insights into the biological relevance of this neural foundation model’s internals through a detailed characterization of its neurons’ joint temporal responses, advancing understanding of how complex neural representations emerge in artificial models. <div>
arXiv:2512.07869v1 Announce Type: cross 
Abstract: Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for under- standing brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as a physiologist might, characterizing each 'neuron' based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding man- ifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by 'pushing apart' the representations of different temporal stimulus patterns; while the readout module achieves biological fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, we present this work as a study of the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons' joint temporal response patterns.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Circuit Reasoning Models: A Variational Framework for Differentiable Logical Inference</title>
<link>https://arxiv.org/abs/2512.07871</link>
<guid>https://arxiv.org/abs/2512.07871</guid>
<content:encoded><![CDATA[
<div> Quantum Circuit Reasoning Models, Variational Quantum Circuits, Logical Inference, Quantum-inspired Computation, Differentiable Optimization<br /><br />Summary: This report introduces Quantum Circuit Reasoning Models (QCRM), a new reasoning architecture that expands Variational Quantum Circuits (VQC) from typical tasks like energy minimization and classification to more complex logical inference and reasoning. It argues that key quantum effects—superposition, entanglement, interference, and measurement—naturally correspond to logical reasoning operations such as hypothesis branching, constraint propagation, consistency enforcement, and decision-making. The framework integrates quantum-inspired computation with differentiable optimization, allowing reasoning to unfold as amplitude evolution alongside interference-driven selection of consistent states. The authors establish the mathematical foundation for QCRM and define a parameterized circuit architecture designed to represent logical rules through unitary transformations acting on proposition-qubit states. A classical gradient descent-based training objective over circuit parameters is formalized, enabling efficient optimization. Practical implementations via simulation on classical hardware are discussed as a pathway to experimentation and validation. Additionally, the paper proposes the Quantum Reasoning Layer (QRL), a differentiable hybrid module meant to be composable within larger reasoning models, with potential applications in scientific research, biomedical analysis, and chemical inference domains. <div>
arXiv:2512.07871v1 Announce Type: cross 
Abstract: This report introduces a novel class of reasoning architectures, termed Quantum Circuit Reasoning Models (QCRM), which extend the concept of Variational Quantum Circuits (VQC) from energy minimization and classification tasks to structured logical inference and reasoning. We posit that fundamental quantum mechanical operations, superposition, entanglement, interference, and measurement, naturally map to essential reasoning primitives such as hypothesis branching, constraint propagation, consistency enforcement, and decision making. The resulting framework combines quantum-inspired computation with differentiable optimization, enabling reasoning to emerge as a process of amplitude evolution and interference-driven selection of self-consistent states. We develop the mathematical foundation of QCRM, define its parameterized circuit architecture, and show how logical rules can be encoded as unitary transformations over proposition-qubit states. We further formalize a training objective grounded in classical gradient descent over circuit parameters and discuss simulation-based implementations on classical hardware. Finally, we propose the Quantum Reasoning Layer (QRL) as a differentiable hybrid component for composable reasoning models applicable to scientific, biomedical, and chemical inference domains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion</title>
<link>https://arxiv.org/abs/2512.07873</link>
<guid>https://arxiv.org/abs/2512.07873</guid>
<content:encoded><![CDATA[
arXiv:2512.07873v1 Announce Type: cross 
Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design</title>
<link>https://arxiv.org/abs/2512.07877</link>
<guid>https://arxiv.org/abs/2512.07877</guid>
<content:encoded><![CDATA[
arXiv:2512.07877v1 Announce Type: cross 
Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referenceless Proton Resonance Frequency Thermometry Using Deep Learning with Self-Attention</title>
<link>https://arxiv.org/abs/2512.07882</link>
<guid>https://arxiv.org/abs/2512.07882</guid>
<content:encoded><![CDATA[
arXiv:2512.07882v1 Announce Type: cross 
Abstract: Background: Accurate proton resonance frequency (PRF) MR thermometry is essential for monitoring temperature rise during thermal ablation with high intensity focused ultrasound (FUS). Conventional referenceless methods such as complex field estimation (CFE) and phase finite difference (PFD) tend to exhibit errors when susceptibility-induced phase discontinuities occur at tissue interfaces.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSPN-2: Efficient Parallel Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.07884</link>
<guid>https://arxiv.org/abs/2512.07884</guid>
<content:encoded><![CDATA[
arXiv:2512.07884v1 Announce Type: cross 
Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking</title>
<link>https://arxiv.org/abs/2512.07885</link>
<guid>https://arxiv.org/abs/2512.07885</guid>
<content:encoded><![CDATA[
arXiv:2512.07885v1 Announce Type: cross 
Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification</title>
<link>https://arxiv.org/abs/2512.07888</link>
<guid>https://arxiv.org/abs/2512.07888</guid>
<content:encoded><![CDATA[
arXiv:2512.07888v1 Announce Type: cross 
Abstract: Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement</title>
<link>https://arxiv.org/abs/2512.07898</link>
<guid>https://arxiv.org/abs/2512.07898</guid>
<content:encoded><![CDATA[
arXiv:2512.07898v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators</title>
<link>https://arxiv.org/abs/2512.07901</link>
<guid>https://arxiv.org/abs/2512.07901</guid>
<content:encoded><![CDATA[
arXiv:2512.07901v1 Announce Type: cross 
Abstract: This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Community Science Datasets to Model Highly Pathogenic Avian Influenza (HPAI) in Birds in the Subantarctic</title>
<link>https://arxiv.org/abs/2512.07907</link>
<guid>https://arxiv.org/abs/2512.07907</guid>
<content:encoded><![CDATA[
arXiv:2512.07907v1 Announce Type: cross 
Abstract: Community science observational datasets are useful in epidemiology and ecology for modeling species distributions, but the heterogeneous nature of the data presents significant challenges for standardization, data quality assurance and control, and workflow management. In this paper, we present a data workflow for cleaning and harmonizing multiple community science datasets, which we implement in a case study using eBird, iNaturalist, GBIF, and other datasets to model the impact of highly pathogenic avian influenza in populations of birds in the subantarctic. We predict population sizes for several species where the demographics are not known, and we present novel estimates for potential mortality rates from HPAI for those species, based on a novel aggregated dataset of mortality rates in the subantarctic.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation</title>
<link>https://arxiv.org/abs/2512.07917</link>
<guid>https://arxiv.org/abs/2512.07917</guid>
<content:encoded><![CDATA[
arXiv:2512.07917v1 Announce Type: cross 
Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepCode: Open Agentic Coding</title>
<link>https://arxiv.org/abs/2512.07921</link>
<guid>https://arxiv.org/abs/2512.07921</guid>
<content:encoded><![CDATA[
arXiv:2512.07921v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-real time fires detection using satellite imagery in Sudan conflict</title>
<link>https://arxiv.org/abs/2512.07925</link>
<guid>https://arxiv.org/abs/2512.07925</guid>
<content:encoded><![CDATA[
arXiv:2512.07925v1 Announce Type: cross 
Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face</title>
<link>https://arxiv.org/abs/2512.07983</link>
<guid>https://arxiv.org/abs/2512.07983</guid>
<content:encoded><![CDATA[
arXiv:2512.07983v1 Announce Type: cross 
Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[
arXiv:2512.07984v1 Announce Type: cross 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering</title>
<link>https://arxiv.org/abs/2512.07990</link>
<guid>https://arxiv.org/abs/2512.07990</guid>
<content:encoded><![CDATA[
arXiv:2512.07990v1 Announce Type: cross 
Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08016</link>
<guid>https://arxiv.org/abs/2512.08016</guid>
<content:encoded><![CDATA[
arXiv:2512.08016v1 Announce Type: cross 
Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration</title>
<link>https://arxiv.org/abs/2512.08036</link>
<guid>https://arxiv.org/abs/2512.08036</guid>
<content:encoded><![CDATA[
arXiv:2512.08036v1 Announce Type: cross 
Abstract: Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Short-Context Dominance: How Much Local Context Natural Language Actually Needs?</title>
<link>https://arxiv.org/abs/2512.08082</link>
<guid>https://arxiv.org/abs/2512.08082</guid>
<content:encoded><![CDATA[
arXiv:2512.08082v1 Announce Type: cross 
Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&amp;A tasks and model architectures, we confirm that mitigating the bias improves performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs for Honesty via Confessions</title>
<link>https://arxiv.org/abs/2512.08093</link>
<guid>https://arxiv.org/abs/2512.08093</guid>
<content:encoded><![CDATA[
arXiv:2512.08093v1 Announce Type: cross 
Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Offline Model-Based RL with Action Chunks</title>
<link>https://arxiv.org/abs/2512.08108</link>
<guid>https://arxiv.org/abs/2512.08108</guid>
<content:encoded><![CDATA[
arXiv:2512.08108v1 Announce Type: cross 
Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title>
<link>https://arxiv.org/abs/2512.08121</link>
<guid>https://arxiv.org/abs/2512.08121</guid>
<content:encoded><![CDATA[
arXiv:2512.08121v1 Announce Type: cross 
Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach</title>
<link>https://arxiv.org/abs/2512.08124</link>
<guid>https://arxiv.org/abs/2512.08124</guid>
<content:encoded><![CDATA[
arXiv:2512.08124v1 Announce Type: cross 
Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture</title>
<link>https://arxiv.org/abs/2512.08130</link>
<guid>https://arxiv.org/abs/2512.08130</guid>
<content:encoded><![CDATA[
arXiv:2512.08130v1 Announce Type: cross 
Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chat with UAV -- Human-UAV Interaction Based on Large Language Models</title>
<link>https://arxiv.org/abs/2512.08145</link>
<guid>https://arxiv.org/abs/2512.08145</guid>
<content:encoded><![CDATA[
arXiv:2512.08145v1 Announce Type: cross 
Abstract: The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08153</link>
<guid>https://arxiv.org/abs/2512.08153</guid>
<content:encoded><![CDATA[
arXiv:2512.08153v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks</title>
<link>https://arxiv.org/abs/2512.08160</link>
<guid>https://arxiv.org/abs/2512.08160</guid>
<content:encoded><![CDATA[
arXiv:2512.08160v1 Announce Type: cross 
Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Dense Reasoning for Efficient and Auditable Security Alert Triage</title>
<link>https://arxiv.org/abs/2512.08169</link>
<guid>https://arxiv.org/abs/2512.08169</guid>
<content:encoded><![CDATA[
arXiv:2512.08169v1 Announce Type: cross 
Abstract: Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties</title>
<link>https://arxiv.org/abs/2512.08185</link>
<guid>https://arxiv.org/abs/2512.08185</guid>
<content:encoded><![CDATA[
arXiv:2512.08185v1 Announce Type: cross 
Abstract: Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model</title>
<link>https://arxiv.org/abs/2512.08188</link>
<guid>https://arxiv.org/abs/2512.08188</guid>
<content:encoded><![CDATA[
arXiv:2512.08188v1 Announce Type: cross 
Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access</title>
<link>https://arxiv.org/abs/2512.08193</link>
<guid>https://arxiv.org/abs/2512.08193</guid>
<content:encoded><![CDATA[
arXiv:2512.08193v1 Announce Type: cross 
Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning</title>
<link>https://arxiv.org/abs/2512.08218</link>
<guid>https://arxiv.org/abs/2512.08218</guid>
<content:encoded><![CDATA[
arXiv:2512.08218v1 Announce Type: cross 
Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models</title>
<link>https://arxiv.org/abs/2512.08228</link>
<guid>https://arxiv.org/abs/2512.08228</guid>
<content:encoded><![CDATA[
arXiv:2512.08228v1 Announce Type: cross 
Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality</title>
<link>https://arxiv.org/abs/2512.08238</link>
<guid>https://arxiv.org/abs/2512.08238</guid>
<content:encoded><![CDATA[
arXiv:2512.08238v1 Announce Type: cross 
Abstract: Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridToken-VLM: Hybrid Token Compression for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08240</link>
<guid>https://arxiv.org/abs/2512.08240</guid>
<content:encoded><![CDATA[
arXiv:2512.08240v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI</title>
<link>https://arxiv.org/abs/2512.08243</link>
<guid>https://arxiv.org/abs/2512.08243</guid>
<content:encoded><![CDATA[
arXiv:2512.08243v1 Announce Type: cross 
Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.08247</link>
<guid>https://arxiv.org/abs/2512.08247</guid>
<content:encoded><![CDATA[
arXiv:2512.08247v1 Announce Type: cross 
Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making</title>
<link>https://arxiv.org/abs/2512.08280</link>
<guid>https://arxiv.org/abs/2512.08280</guid>
<content:encoded><![CDATA[
arXiv:2512.08280v1 Announce Type: cross 
Abstract: Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework</title>
<link>https://arxiv.org/abs/2512.08286</link>
<guid>https://arxiv.org/abs/2512.08286</guid>
<content:encoded><![CDATA[
arXiv:2512.08286v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem</title>
<link>https://arxiv.org/abs/2512.08290</link>
<guid>https://arxiv.org/abs/2512.08290</guid>
<content:encoded><![CDATA[
arXiv:2512.08290v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the "USB-C for Agentic AI." While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how "context" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</title>
<link>https://arxiv.org/abs/2512.08309</link>
<guid>https://arxiv.org/abs/2512.08309</guid>
<content:encoded><![CDATA[
arXiv:2512.08309v1 Announce Type: cross 
Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDM: Geometry-aware Distribution Matching for Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.08317</link>
<guid>https://arxiv.org/abs/2512.08317</guid>
<content:encoded><![CDATA[
arXiv:2512.08317v1 Announce Type: cross 
Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships</title>
<link>https://arxiv.org/abs/2512.08326</link>
<guid>https://arxiv.org/abs/2512.08326</guid>
<content:encoded><![CDATA[
arXiv:2512.08326v1 Announce Type: cross 
Abstract: Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08329</link>
<guid>https://arxiv.org/abs/2512.08329</guid>
<content:encoded><![CDATA[
arXiv:2512.08329v1 Announce Type: cross 
Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging</title>
<link>https://arxiv.org/abs/2512.08333</link>
<guid>https://arxiv.org/abs/2512.08333</guid>
<content:encoded><![CDATA[
arXiv:2512.08333v1 Announce Type: cross 
Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2512.08360</link>
<guid>https://arxiv.org/abs/2512.08360</guid>
<content:encoded><![CDATA[
arXiv:2512.08360v1 Announce Type: cross 
Abstract: Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are generative AI text annotations systematically biased?</title>
<link>https://arxiv.org/abs/2512.08404</link>
<guid>https://arxiv.org/abs/2512.08404</guid>
<content:encoded><![CDATA[
arXiv:2512.08404v1 Announce Type: cross 
Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process</title>
<link>https://arxiv.org/abs/2512.08451</link>
<guid>https://arxiv.org/abs/2512.08451</guid>
<content:encoded><![CDATA[
arXiv:2512.08451v1 Announce Type: cross 
Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset</title>
<link>https://arxiv.org/abs/2512.08459</link>
<guid>https://arxiv.org/abs/2512.08459</guid>
<content:encoded><![CDATA[
arXiv:2512.08459v1 Announce Type: cross 
Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention</title>
<link>https://arxiv.org/abs/2512.08477</link>
<guid>https://arxiv.org/abs/2512.08477</guid>
<content:encoded><![CDATA[
arXiv:2512.08477v1 Announce Type: cross 
Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform</title>
<link>https://arxiv.org/abs/2512.08478</link>
<guid>https://arxiv.org/abs/2512.08478</guid>
<content:encoded><![CDATA[
arXiv:2512.08478v1 Announce Type: cross 
Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Vulnerable Code Augmentation: Generate or Refactor?</title>
<link>https://arxiv.org/abs/2512.08493</link>
<guid>https://arxiv.org/abs/2512.08493</guid>
<content:encoded><![CDATA[
arXiv:2512.08493v1 Announce Type: cross 
Abstract: Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v1 Announce Type: cross 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2512.08503</link>
<guid>https://arxiv.org/abs/2512.08503</guid>
<content:encoded><![CDATA[
arXiv:2512.08503v1 Announce Type: cross 
Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking</title>
<link>https://arxiv.org/abs/2512.08518</link>
<guid>https://arxiv.org/abs/2512.08518</guid>
<content:encoded><![CDATA[
arXiv:2512.08518v1 Announce Type: cross 
Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation</title>
<link>https://arxiv.org/abs/2512.08542</link>
<guid>https://arxiv.org/abs/2512.08542</guid>
<content:encoded><![CDATA[
arXiv:2512.08542v1 Announce Type: cross 
Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks</title>
<link>https://arxiv.org/abs/2512.08545</link>
<guid>https://arxiv.org/abs/2512.08545</guid>
<content:encoded><![CDATA[
arXiv:2512.08545v1 Announce Type: cross 
Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations</title>
<link>https://arxiv.org/abs/2512.08548</link>
<guid>https://arxiv.org/abs/2512.08548</guid>
<content:encoded><![CDATA[
arXiv:2512.08548v1 Announce Type: cross 
Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.08567</link>
<guid>https://arxiv.org/abs/2512.08567</guid>
<content:encoded><![CDATA[
arXiv:2512.08567v1 Announce Type: cross 
Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery</title>
<link>https://arxiv.org/abs/2512.08577</link>
<guid>https://arxiv.org/abs/2512.08577</guid>
<content:encoded><![CDATA[
arXiv:2512.08577v1 Announce Type: cross 
Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind to Hand: Purposeful Robotic Control via Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.08580</link>
<guid>https://arxiv.org/abs/2512.08580</guid>
<content:encoded><![CDATA[
arXiv:2512.08580v1 Announce Type: cross 
Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality</title>
<link>https://arxiv.org/abs/2512.08596</link>
<guid>https://arxiv.org/abs/2512.08596</guid>
<content:encoded><![CDATA[
arXiv:2512.08596v1 Announce Type: cross 
Abstract: The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning</title>
<link>https://arxiv.org/abs/2512.08606</link>
<guid>https://arxiv.org/abs/2512.08606</guid>
<content:encoded><![CDATA[
arXiv:2512.08606v1 Announce Type: cross 
Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.08639</link>
<guid>https://arxiv.org/abs/2512.08639</guid>
<content:encoded><![CDATA[
arXiv:2512.08639v1 Announce Type: cross 
Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain</title>
<link>https://arxiv.org/abs/2512.08657</link>
<guid>https://arxiv.org/abs/2512.08657</guid>
<content:encoded><![CDATA[
arXiv:2512.08657v1 Announce Type: cross 
Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Essay Scoring and Feedback Generation in Basque Language Learning</title>
<link>https://arxiv.org/abs/2512.08713</link>
<guid>https://arxiv.org/abs/2512.08713</guid>
<content:encoded><![CDATA[
arXiv:2512.08713v1 Announce Type: cross 
Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-domain performance analysis with scores tailored to user preferences</title>
<link>https://arxiv.org/abs/2512.08715</link>
<guid>https://arxiv.org/abs/2512.08715</guid>
<content:encoded><![CDATA[
arXiv:2512.08715v1 Announce Type: cross 
Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting</title>
<link>https://arxiv.org/abs/2512.08733</link>
<guid>https://arxiv.org/abs/2512.08733</guid>
<content:encoded><![CDATA[
arXiv:2512.08733v1 Announce Type: cross 
Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Dynamic Parameter Learning of manipulator robots</title>
<link>https://arxiv.org/abs/2512.08767</link>
<guid>https://arxiv.org/abs/2512.08767</guid>
<content:encoded><![CDATA[
arXiv:2512.08767v1 Announce Type: cross 
Abstract: Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps</title>
<link>https://arxiv.org/abs/2512.08774</link>
<guid>https://arxiv.org/abs/2512.08774</guid>
<content:encoded><![CDATA[
arXiv:2512.08774v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fr\'echet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages</title>
<link>https://arxiv.org/abs/2512.08777</link>
<guid>https://arxiv.org/abs/2512.08777</guid>
<content:encoded><![CDATA[
arXiv:2512.08777v1 Announce Type: cross 
Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokm{\aa}l and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs</title>
<link>https://arxiv.org/abs/2512.08786</link>
<guid>https://arxiv.org/abs/2512.08786</guid>
<content:encoded><![CDATA[
arXiv:2512.08786v1 Announce Type: cross 
Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance</title>
<link>https://arxiv.org/abs/2512.08789</link>
<guid>https://arxiv.org/abs/2512.08789</guid>
<content:encoded><![CDATA[
arXiv:2512.08789v1 Announce Type: cross 
Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?</title>
<link>https://arxiv.org/abs/2512.08798</link>
<guid>https://arxiv.org/abs/2512.08798</guid>
<content:encoded><![CDATA[
arXiv:2512.08798v1 Announce Type: cross 
Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework</title>
<link>https://arxiv.org/abs/2512.08802</link>
<guid>https://arxiv.org/abs/2512.08802</guid>
<content:encoded><![CDATA[
arXiv:2512.08802v1 Announce Type: cross 
Abstract: Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation.
  This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title>
<link>https://arxiv.org/abs/2512.08809</link>
<guid>https://arxiv.org/abs/2512.08809</guid>
<content:encoded><![CDATA[
arXiv:2512.08809v1 Announce Type: cross 
Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_\chi$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multicalibration for LLM-based Code Generation</title>
<link>https://arxiv.org/abs/2512.08810</link>
<guid>https://arxiv.org/abs/2512.08810</guid>
<content:encoded><![CDATA[
arXiv:2512.08810v1 Announce Type: cross 
Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emovectors: assessing emotional content in jazz improvisations for creativity evaluation</title>
<link>https://arxiv.org/abs/2512.08812</link>
<guid>https://arxiv.org/abs/2512.08812</guid>
<content:encoded><![CDATA[
arXiv:2512.08812v1 Announce Type: cross 
Abstract: Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis</title>
<link>https://arxiv.org/abs/2512.08819</link>
<guid>https://arxiv.org/abs/2512.08819</guid>
<content:encoded><![CDATA[
arXiv:2512.08819v1 Announce Type: cross 
Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csord\'as et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning</title>
<link>https://arxiv.org/abs/2512.08820</link>
<guid>https://arxiv.org/abs/2512.08820</guid>
<content:encoded><![CDATA[
arXiv:2512.08820v1 Announce Type: cross 
Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincar\'e ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.08829</link>
<guid>https://arxiv.org/abs/2512.08829</guid>
<content:encoded><![CDATA[
arXiv:2512.08829v1 Announce Type: cross 
Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Synthetic Data Generation Using Context-Aware GANs</title>
<link>https://arxiv.org/abs/2512.08869</link>
<guid>https://arxiv.org/abs/2512.08869</guid>
<content:encoded><![CDATA[
arXiv:2512.08869v1 Announce Type: cross 
Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents</title>
<link>https://arxiv.org/abs/2512.08870</link>
<guid>https://arxiv.org/abs/2512.08870</guid>
<content:encoded><![CDATA[
arXiv:2512.08870v1 Announce Type: cross 
Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning</title>
<link>https://arxiv.org/abs/2512.08873</link>
<guid>https://arxiv.org/abs/2512.08873</guid>
<content:encoded><![CDATA[
arXiv:2512.08873v1 Announce Type: cross 
Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</title>
<link>https://arxiv.org/abs/2512.08875</link>
<guid>https://arxiv.org/abs/2512.08875</guid>
<content:encoded><![CDATA[
arXiv:2512.08875v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process</title>
<link>https://arxiv.org/abs/2512.08879</link>
<guid>https://arxiv.org/abs/2512.08879</guid>
<content:encoded><![CDATA[
arXiv:2512.08879v1 Announce Type: cross 
Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers</title>
<link>https://arxiv.org/abs/2512.08889</link>
<guid>https://arxiv.org/abs/2512.08889</guid>
<content:encoded><![CDATA[
arXiv:2512.08889v1 Announce Type: cross 
Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.08892</link>
<guid>https://arxiv.org/abs/2512.08892</guid>
<content:encoded><![CDATA[
arXiv:2512.08892v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.08894</link>
<guid>https://arxiv.org/abs/2512.08894</guid>
<content:encoded><![CDATA[
arXiv:2512.08894v1 Announce Type: cross 
Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAQ: Stabilizer-Aware Quantum Error Correction Decoder</title>
<link>https://arxiv.org/abs/2512.08914</link>
<guid>https://arxiv.org/abs/2512.08914</guid>
<content:encoded><![CDATA[
arXiv:2512.08914v1 Announce Type: cross 
Abstract: Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
arXiv:2512.08931v1 Announce Type: cross 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</title>
<link>https://arxiv.org/abs/2405.17537</link>
<guid>https://arxiv.org/abs/2405.17537</guid>
<content:encoded><![CDATA[
arXiv:2405.17537v5 Announce Type: replace 
Abstract: Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records</title>
<link>https://arxiv.org/abs/2409.08936</link>
<guid>https://arxiv.org/abs/2409.08936</guid>
<content:encoded><![CDATA[
arXiv:2409.08936v4 Announce Type: replace 
Abstract: Clinical information extraction, which involves structuring clinical concepts from unstructured medical text, remains a challenging problem that could benefit from the inclusion of tabular background information available in electronic health records. Existing open-source datasets lack explicit links between structured features and clinical concepts in the text, motivating the need for a new research dataset. We introduce SimSUM, a benchmark dataset of 10,000 simulated patient records that link unstructured clinical notes with structured background variables. Each record simulates a patient encounter in the domain of respiratory diseases and includes tabular data (e.g., symptoms, diagnoses, underlying conditions) generated from a Bayesian network whose structure and parameters are defined by domain experts. A large language model (GPT-4o) is prompted to generate a clinical note describing the encounter, including symptoms and relevant context. These notes are annotated with span-level symptom mentions. We conduct an expert evaluation to assess note quality and run baseline predictive models on both the tabular and textual data. The SimSUM dataset is primarily designed to support research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text -- namely, symptoms in the case of SimSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and/or textual confounders, and multi-modal synthetic data generation. SimSUM is not intended for training clinical decision support systems or production-grade models, but rather to facilitate reproducible research in a simplified and controlled setting.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumCoKE: Ordinal-Aware Numerical Reasoning over Knowledge Graphs with Mixture-of-Experts and Contrastive Learning</title>
<link>https://arxiv.org/abs/2411.12950</link>
<guid>https://arxiv.org/abs/2411.12950</guid>
<content:encoded><![CDATA[
arXiv:2411.12950v3 Announce Type: replace 
Abstract: Knowledge graphs (KGs) serve as a vital backbone for a wide range of AI applications, including natural language understanding and recommendation. A promising yet underexplored direction is numerical reasoning over KGs, which involves inferring new facts by leveraging not only symbolic triples but also numerical attribute values (e.g., length, weight). However, existing methods fall short in two key aspects: (1) Incomplete semantic integration: Most models struggle to jointly encode entities, relations, and numerical attributes in a unified representation space, limiting their ability to extract relation-aware semantics from numeric information. (2) Ordinal indistinguishability: Due to subtle differences between close values and sampling imbalance, models often fail to capture fine-grained ordinal relationships (e.g., longer, heavier), especially in the presence of hard negatives. To address these challenges, we propose NumCoKE, a numerical reasoning framework for KGs based on Mixture-of-Experts and Ordinal Contrastive Embedding. To overcome (C1), we introduce a Mixture-of-Experts Knowledge-Aware (MoEKA) encoder that jointly aligns symbolic and numeric components into a shared semantic space, while dynamically routing attribute features to relation-specific experts. To handle (C2), we propose Ordinal Knowledge Contrastive Learning (OKCL), which constructs ordinal-aware positive and negative samples using prior knowledge, enabling the model to better discriminate subtle semantic shifts. Extensive experiments on three public KG benchmarks demonstrate that NumCoKE consistently outperforms competitive baselines across diverse attribute distributions, validating its superiority in both semantic integration and ordinal reasoning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title>
<link>https://arxiv.org/abs/2502.03544</link>
<guid>https://arxiv.org/abs/2502.03544</guid>
<content:encoded><![CDATA[
arXiv:2502.03544v3 Announce Type: replace 
Abstract: We present AlphaGeometry2 (AG2), a significantly improved version of AlphaGeometry introduced in (Trinh et al., 2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with support for non-constructive problems, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AG2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that enables effective communication between search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AG to 84% on all geometry problems over the last 25 years, compared to 54% previously. AG2 was also part of the system that achieved the silver-medal standard at IMO 2024 https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/. Finally, we report progress towards using AG2 as a part of a fully automated system that reliably solves geometry problems from natural language input. Code: https://github.com/google-deepmind/alphageometry2.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Formalism for Optimal Search with Dynamic Heuristics (Extended Version)</title>
<link>https://arxiv.org/abs/2504.21131</link>
<guid>https://arxiv.org/abs/2504.21131</guid>
<content:encoded><![CDATA[
arXiv:2504.21131v3 Announce Type: replace 
Abstract: While most heuristics studied in heuristic search depend only on the state, some accumulate information during search and thus also depend on the search history. Various existing approaches use such dynamic heuristics in $\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$ to show optimality. However, doing so ignores the complexities of searching with a mutable heuristic. In this paper we formalize the idea of dynamic heuristics and use them in a generic algorithm framework. We study a particular instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show general optimality results. Finally we show how existing approaches from classical planning can be viewed as special cases of this instantiation, making it possible to directly apply our optimality results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces</title>
<link>https://arxiv.org/abs/2505.03295</link>
<guid>https://arxiv.org/abs/2505.03295</guid>
<content:encoded><![CDATA[
arXiv:2505.03295v2 Announce Type: replace 
Abstract: Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.17114</link>
<guid>https://arxiv.org/abs/2506.17114</guid>
<content:encoded><![CDATA[
arXiv:2506.17114v4 Announce Type: replace 
Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.03817</link>
<guid>https://arxiv.org/abs/2509.03817</guid>
<content:encoded><![CDATA[
arXiv:2509.03817v2 Announce Type: replace 
Abstract: Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents</title>
<link>https://arxiv.org/abs/2510.07172</link>
<guid>https://arxiv.org/abs/2510.07172</guid>
<content:encoded><![CDATA[
arXiv:2510.07172v2 Announce Type: replace 
Abstract: Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using counterfactual law shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production</title>
<link>https://arxiv.org/abs/2510.23856</link>
<guid>https://arxiv.org/abs/2510.23856</guid>
<content:encoded><![CDATA[
arXiv:2510.23856v2 Announce Type: replace 
Abstract: Agents are rapidly advancing in automating digital work, but enterprises face a harder challenge: moving beyond prototypes to deployed systems that deliver measurable business value. This path is complicated by fragmented frameworks, slow development, and the absence of standardized evaluation practices. Generalist agents have emerged as a promising direction, excelling on academic benchmarks and offering flexibility across task types, applications, and modalities. Yet, evidence of their use in production enterprise settings remains limited. This paper reports IBM's experience developing and piloting the Computer Using Generalist Agent (CUGA), which has been open-sourced for the community (https://github.com/cuga-project/cuga-agent). CUGA adopts a hierarchical planner--executor architecture with strong analytical foundations, achieving state-of-the-art performance on AppWorld and WebArena. Beyond benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing talent acquisition domain, addressing enterprise requirements for scalability, auditability, safety, and governance. To support assessment, we introduce BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary evaluations, CUGA approached the accuracy of specialized agents while indicating potential for reducing development time and cost. Our contribution is twofold: presenting early evidence of generalist agents operating at enterprise scale, and distilling technical and organizational lessons from this initial pilot. We outline requirements and next steps for advancing research-grade architectures like CUGA into robust, enterprise-ready systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows</title>
<link>https://arxiv.org/abs/2510.24411</link>
<guid>https://arxiv.org/abs/2510.24411</guid>
<content:encoded><![CDATA[
arXiv:2510.24411v2 Announce Type: replace 
Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
arXiv:2511.09907v2 Announce Type: replace 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10240</link>
<guid>https://arxiv.org/abs/2511.10240</guid>
<content:encoded><![CDATA[
arXiv:2511.10240v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detection</title>
<link>https://arxiv.org/abs/2511.11599</link>
<guid>https://arxiv.org/abs/2511.11599</guid>
<content:encoded><![CDATA[
arXiv:2511.11599v2 Announce Type: replace 
Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding</title>
<link>https://arxiv.org/abs/2511.12083</link>
<guid>https://arxiv.org/abs/2511.12083</guid>
<content:encoded><![CDATA[
arXiv:2511.12083v2 Announce Type: replace 
Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)--such as no-limit Texas Hold'em--where the finite nature of spatial resources hinders solving strategies for the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly discards critical information: specifically, the quantifiable subtle differences between information sets--vital for strategy solving--thus compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds the features of individual information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR introduces a strategy-solving process driven by regret accumulation and strategy updates in this embedding space, with supporting theoretical analysis verifying its ability to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions via low-dimensional embedding for strategy solving.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity</title>
<link>https://arxiv.org/abs/2511.15593</link>
<guid>https://arxiv.org/abs/2511.15593</guid>
<content:encoded><![CDATA[
arXiv:2511.15593v2 Announce Type: replace 
Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025</title>
<link>https://arxiv.org/abs/2511.16205</link>
<guid>https://arxiv.org/abs/2511.16205</guid>
<content:encoded><![CDATA[
arXiv:2511.16205v2 Announce Type: replace 
Abstract: Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?</title>
<link>https://arxiv.org/abs/2512.00218</link>
<guid>https://arxiv.org/abs/2512.00218</guid>
<content:encoded><![CDATA[
arXiv:2512.00218v2 Announce Type: replace 
Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness</title>
<link>https://arxiv.org/abs/2512.01354</link>
<guid>https://arxiv.org/abs/2512.01354</guid>
<content:encoded><![CDATA[
arXiv:2512.01354v3 Announce Type: replace 
Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration</title>
<link>https://arxiv.org/abs/2512.02530</link>
<guid>https://arxiv.org/abs/2512.02530</guid>
<content:encoded><![CDATA[
arXiv:2512.02530v2 Announce Type: replace 
Abstract: The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</title>
<link>https://arxiv.org/abs/2310.02239</link>
<guid>https://arxiv.org/abs/2310.02239</guid>
<content:encoded><![CDATA[
arXiv:2310.02239v4 Announce Type: replace-cross 
Abstract: The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models for Wireless Communications</title>
<link>https://arxiv.org/abs/2310.07312</link>
<guid>https://arxiv.org/abs/2310.07312</guid>
<content:encoded><![CDATA[
arXiv:2310.07312v4 Announce Type: replace-cross 
Abstract: A comprehensive study on the applications of denoising diffusion models for wireless systems is provided. The article highlights the capabilities of diffusion models in learning complicated signal distributions, modeling wireless channels, and denoising and reconstructing distorted signals. First, fundamental working mechanism of diffusion models is introduced. Then the recent advances in applying diffusion models to wireless systems are reviewed. Next, two case studies are provided, where conditional diffusion models (CDiff) are proposed for data reconstruction enhancement, covering both the conventional digital communication systems, as well as the semantic communication (SemCom) setups. The first case study highlights about 10 dB improvement in data reconstruction under low-SNR regimes, while mitigating the need to transmit redundant bits for error correction codes in digital systems. The second study further extends the case to a SemCom setup, where diffusion autoencoders showcase superior performance compared to legacy autoencoders and variational autoencoder (VAE) architectures. Finally, future directions and existing challenges are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting value-expressive text posts in Russian social media</title>
<link>https://arxiv.org/abs/2312.08968</link>
<guid>https://arxiv.org/abs/2312.08968</guid>
<content:encoded><![CDATA[
arXiv:2312.08968v3 Announce Type: replace-cross 
Abstract: Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several classification models using embeddings from various pre-trained transformer-based language models. The best performance was achieved with embeddings from a fine-tuned rubert-tiny2 model, yielding high value detection quality (F1 = 0.77, F1-macro = 0.83). This model provides a crucial step to a study of values within and between Russian social media users.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalized Hamiltonians using fully Symplectic Mappings</title>
<link>https://arxiv.org/abs/2409.11138</link>
<guid>https://arxiv.org/abs/2409.11138</guid>
<content:encoded><![CDATA[
arXiv:2409.11138v3 Announce Type: replace-cross 
Abstract: Many important physical systems can be described as the evolution of a Hamiltonian system, which has the important property of being conservative, that is, energy is conserved throughout the evolution. Physics Informed Neural Networks and in particular Hamiltonian Neural Networks have emerged as a mechanism to incorporate structural inductive bias into the NN model. By ensuring physical invariances are conserved, the models exhibit significantly better sample complexity and out-of-distribution accuracy than standard NNs. Learning the Hamiltonian as a function of its canonical variables, typically position and velocity, from sample observations of the system thus becomes a critical task in system identification and long-term prediction of system behavior. However, to truly preserve the long-run physical conservation properties of Hamiltonian systems, one must use symplectic integrators for a forward pass of the system's simulation. While symplectic schemes have been used in the literature, they are thus far limited to situations when they reduce to explicit algorithms, which include the case of separable Hamiltonians or augmented non-separable Hamiltonians. We extend it to generalized non-separable Hamiltonians, and noting the self-adjoint property of symplectic integrators, we bypass computationally intensive backpropagation through an ODE solver. We show that the method is robust to noise and provides a good approximation of the system Hamiltonian when the state variables are sampled from a noisy observation. In the numerical results, we show the performance of the method concerning Hamiltonian reconstruction and conservation, indicating its particular advantage for non-separable systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score-based Conditional Out-of-Distribution Augmentation for Graph Covariate Shift</title>
<link>https://arxiv.org/abs/2410.17506</link>
<guid>https://arxiv.org/abs/2410.17506</guid>
<content:encoded><![CDATA[
arXiv:2410.17506v2 Announce Type: replace-cross 
Abstract: Distribution shifts between training and testing datasets significantly impair the model performance on graph learning. A commonly-taken causal view in graph invariant learning suggests that stable predictive features of graphs are causally associated with labels, whereas varying environmental features lead to distribution shifts. In particular, covariate shifts caused by unseen environments in test graphs underscore the critical need for out-of-distribution (OOD) generalization. Existing graph augmentation methods designed to address the covariate shift often disentangle the stable and environmental features in the input space, and selectively perturb or mixup the environmental features. However, such perturbation-based methods heavily rely on an accurate separation of stable and environmental features, and their exploration ability is confined to existing environmental features in the training distribution. To overcome these limitations, we introduce a novel distributional augmentation approach enabled by a tailored score-based conditional graph generation strategies to explore and synthesize unseen environments while preserving the validity and stable features of overall graph patterns. Our comprehensive empirical evaluations demonstrate the enhanced effectiveness of our method in improving graph OOD generalization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis - A Case Study for the Semiconductor Sector</title>
<link>https://arxiv.org/abs/2412.08179</link>
<guid>https://arxiv.org/abs/2412.08179</guid>
<content:encoded><![CDATA[
arXiv:2412.08179v3 Announce Type: replace-cross 
Abstract: Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyzes require significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analysis through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a sector-specific financial instruction dataset derived from semiconductor industry documents to guide the LLM adaptation to specialized financial reasoning. Using NVIDIA, AMD, and Broadcom as representative companies, our case study demonstrates that RAG-IT substantially improves a general-purpose open-source LLM and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title>
<link>https://arxiv.org/abs/2501.06039</link>
<guid>https://arxiv.org/abs/2501.06039</guid>
<content:encoded><![CDATA[
arXiv:2501.06039v2 Announce Type: replace-cross 
Abstract: Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Computing Enhanced Federated Learning in IIoT: Satisfaction-Aware Incentive Scheme via DRL-Based Stackelberg Game</title>
<link>https://arxiv.org/abs/2502.06909</link>
<guid>https://arxiv.org/abs/2502.06909</guid>
<content:encoded><![CDATA[
arXiv:2502.06909v3 Announce Type: replace-cross 
Abstract: The Industrial Internet of Things (IIoT) leverages Federated Learning (FL) for distributed model training while preserving data privacy, and meta-computing enhances FL by optimizing and integrating distributed computing resources, improving efficiency and scalability. Efficient IIoT operations require a trade-off between model quality and training latency. Consequently, a primary challenge of FL in IIoT is to optimize overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency for meta-computing. Additionally, the satisfaction function is incorporated into the utility function to incentivize IIoT nodes to participate in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for IIoT. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves utility by at least 23.7% compared to existing FL schemes without compromising model accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability</title>
<link>https://arxiv.org/abs/2502.12992</link>
<guid>https://arxiv.org/abs/2502.12992</guid>
<content:encoded><![CDATA[
arXiv:2502.12992v4 Announce Type: replace-cross 
Abstract: Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural architectures. Meanwhile, B-cos networks have been introduced to improve model explainability by proposing an architecture that removes bias terms and promotes input-weight alignment. Although B-cos networks have shown success in building explainable systems, their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos Language Models (LMs) empowered for natural language processing (NLP) tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous methods. Automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post-hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we present a first exploration of transforming decoder-only models to B-cos LMs for generation tasks. Our code is available at https://github.com/Ewanwong/bcos_lm.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation</title>
<link>https://arxiv.org/abs/2503.18549</link>
<guid>https://arxiv.org/abs/2503.18549</guid>
<content:encoded><![CDATA[
arXiv:2503.18549v3 Announce Type: replace-cross 
Abstract: A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2504.02316</link>
<guid>https://arxiv.org/abs/2504.02316</guid>
<content:encoded><![CDATA[
arXiv:2504.02316v2 Announce Type: replace-cross 
Abstract: Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent prior view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel method that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise view control; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer can be seamlessly integrated into various 3D representations and score distillation paradigms, effectively mitigating the multi-face Janus problem.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Task-Oriented Flying: Framework, Infrastructure, and Principles</title>
<link>https://arxiv.org/abs/2504.15129</link>
<guid>https://arxiv.org/abs/2504.15129</guid>
<content:encoded><![CDATA[
arXiv:2504.15129v2 Announce Type: replace-cross 
Abstract: Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using LLMs in Generating Design Rationale for Software Architecture Decisions</title>
<link>https://arxiv.org/abs/2504.20781</link>
<guid>https://arxiv.org/abs/2504.20781</guid>
<content:encoded><![CDATA[
arXiv:2504.20781v3 Announce Type: replace-cross 
Abstract: Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. To further understand the trustworthiness and applicability of LLM-generated DR in practice, we conducted semi-structured interviews with six practitioners. Based on the experimental and interview results, we discussed the pros and cons of the three prompting strategies, the strengths and limitations of LLM-generated DR, and the implications for the practical use of LLM-generated DR.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
<link>https://arxiv.org/abs/2505.12332</link>
<guid>https://arxiv.org/abs/2505.12332</guid>
<content:encoded><![CDATA[
arXiv:2505.12332v5 Announce Type: replace-cross 
Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</title>
<link>https://arxiv.org/abs/2505.13936</link>
<guid>https://arxiv.org/abs/2505.13936</guid>
<content:encoded><![CDATA[
arXiv:2505.13936v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Point in Vision Transformers for Universal Image Segmentation</title>
<link>https://arxiv.org/abs/2505.19795</link>
<guid>https://arxiv.org/abs/2505.19795</guid>
<content:encoded><![CDATA[
arXiv:2505.19795v2 Announce Type: replace-cross 
Abstract: Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
arXiv:2505.23856v2 Announce Type: replace-cross 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose Omniguard, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. Omniguard improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, Omniguard is also very efficient ($\approx\!120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v3 Announce Type: replace-cross 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bench4KE: Benchmarking Automated Competency Question Generation</title>
<link>https://arxiv.org/abs/2505.24554</link>
<guid>https://arxiv.org/abs/2505.24554</guid>
<content:encoded><![CDATA[
arXiv:2505.24554v3 Announce Type: replace-cross 
Abstract: The availability of Large Language Models (LLMs) presents a unique opportunity to reinvigorate research on Knowledge Engineering (KE) automation. This trend is already evident in recent efforts developing LLM-based methods and tools for the automatic generation of Competency Questions (CQs), natural language questions used by ontology engineers to define the functional requirements of an ontology. However, the evaluation of these tools lacks standardization. This undermines the methodological rigor and hinders the replication and comparison of results. To address this gap, we introduce Bench4KE, an extensible API-based benchmarking system for KE automation. The presented release focuses on evaluating tools that generate CQs automatically. Bench4KE provides a curated gold standard consisting of CQ datasets from 17 real-world ontology engineering projects and uses a suite of similarity metrics to assess the quality of the CQs generated. We present a comparative analysis of 6 recent CQ generation systems, which are based on LLMs, establishing a baseline for future research. Bench4KE is also designed to accommodate additional KE automation tasks, such as SPARQL query generation, ontology testing and drafting. Code and datasets are publicly available under the Apache 2.0 license.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing</title>
<link>https://arxiv.org/abs/2506.11180</link>
<guid>https://arxiv.org/abs/2506.11180</guid>
<content:encoded><![CDATA[
arXiv:2506.11180v2 Announce Type: replace-cross 
Abstract: Explicit modeling of capabilities and skills -- whether based on ontologies, Asset Administration Shells, or other technologies -- requires considerable manual effort and often results in representations that are not easily accessible to Large Language Models (LLMs). In this work-in-progress paper, we present an alternative approach based on the recently introduced Model Context Protocol (MCP). MCP allows systems to expose functionality through a standardized interface that is directly consumable by LLM-based agents. We conduct a prototypical evaluation on a laboratory-scale manufacturing system, where resource functions are made available via MCP. A general-purpose LLM is then tasked with planning and executing a multi-step process, including constraint handling and the invocation of resource functions via MCP. The results indicate that such an approach can enable flexible industrial automation without relying on explicit semantic models. This work lays the basis for further exploration of external tool integration in LLM-driven production systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Adaptation as Posterior Correction</title>
<link>https://arxiv.org/abs/2506.14262</link>
<guid>https://arxiv.org/abs/2506.14262</guid>
<content:encoded><![CDATA[
arXiv:2506.14262v3 Announce Type: replace-cross 
Abstract: Adaptation is the holy grail of intelligence, but even the best AI models lack the adaptability of toddlers. In spite of great progress, little is known about the mechanisms by which machines can learn to adapt as fast as humans and animals. Here, we cast adaptation as `correction' of old posteriors and show that a wide-variety of existing adaptation methods follow this very principle, including those used for continual learning, federated learning, unlearning, and model merging. In all these settings, more accurate posteriors often lead to smaller corrections and can enable faster adaptation. Posterior correction is derived by using the dual representation of the Bayesian Learning Rule of Khan and Rue (2023), where the interference between the old representation and new information is quantified by using the natural-gradient mismatch. We present many examples demonstrating how machines can learn to adapt quickly by using posterior correction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elucidated Rolling Diffusion Models for Probabilistic Forecasting of Complex Dynamics</title>
<link>https://arxiv.org/abs/2506.20024</link>
<guid>https://arxiv.org/abs/2506.20024</guid>
<content:encoded><![CDATA[
arXiv:2506.20024v3 Announce Type: replace-cross 
Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most applications in high-dimensional complex systems predict future states individually. This approach struggles to model complex temporal dependencies and fails to explicitly account for the progressive growth of uncertainty inherent to the systems. While rolling diffusion frameworks, which apply increasing noise to forecasts at longer lead times, have been proposed to address this, their integration with state-of-the-art, high-fidelity diffusion techniques remains a significant challenge. We tackle this problem by introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to successfully unify a rolling forecast structure with the principled, performant design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM components-its noise schedule, network preconditioning, and Heun sampler-to the rolling forecast setting. The success of this integration is driven by three key contributions: (i) a novel loss weighting scheme that focuses model capacity on the mid-range forecast horizons where determinism gives way to stochasticity; (ii) an efficient initialization strategy using a pre-trained EDM for the initial window; and (iii) a bespoke hybrid sequence architecture for robust spatiotemporal feature extraction under progressive denoising. On 2D Navier-Stokes simulations and ERA5 global weather forecasting at 1.5-degree resolution, ERDM consistently outperforms key diffusion-based baselines, including conditional autoregressive EDM. ERDM offers a flexible and powerful general framework for tackling diffusion-based dynamics forecasting problems where modeling uncertainty propagation is paramount.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations</title>
<link>https://arxiv.org/abs/2507.00269</link>
<guid>https://arxiv.org/abs/2507.00269</guid>
<content:encoded><![CDATA[
arXiv:2507.00269v2 Announce Type: replace-cross 
Abstract: Current sparse autoencoder (SAE) approaches to neural network interpretability assume that activations can be decomposed through linear superposition into sparse, interpretable features. Despite high reconstruction fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit pathological behavioral errors. We propose that neural networks encode information in two complementary spaces compressed into the same substrate: feature identity and feature integration. To test this dual encoding hypothesis, we develop sequential and joint-training architectures to capture identity and integration patterns simultaneously. Joint training achieves 41.3% reconstruction improvement and 51.6% reduction in KL divergence errors. This architecture spontaneously develops bimodal feature organization: low squared norm features contributing to integration pathways and the rest contributing directly to the residual. Small nonlinear components (3% of parameters) achieve 16.5% standalone improvements, demonstrating parameter-efficient capture of computational relationships crucial for behavior. Additionally, intervention experiments using 2x2 factorial stimulus designs demonstrated that integration features exhibit selective sensitivity to experimental manipulations and produce systematic behavioral effects on model outputs, including significant statistical interaction effects across semantic dimensions. This work provides systematic evidence for (1) dual encoding in neural representations, (2) meaningful nonlinearly encoded feature interactions, and (3) introduces an architectural paradigm shift from post-hoc feature analysis to integrated computational design, establishing foundations for next-generation SAEs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Prompt War: How AI Decides on a Military Intervention</title>
<link>https://arxiv.org/abs/2507.06277</link>
<guid>https://arxiv.org/abs/2507.06277</guid>
<content:encoded><![CDATA[
arXiv:2507.06277v3 Announce Type: replace-cross 
Abstract: Which factors determine AI's propensity to support military intervention? While the use of AI in high-stakes decision-making is growing exponentially, we still lack systematic analysis of the key drivers embedded in these models. This paper conducts a conjoint experiment in which large language models (LLMs) from leading providers (OpenAI, Anthropic, Google) are asked to decide on military intervention across 128 vignettes, with each vignette run 10 times. This design enables a systematic assessment of AI decision-making in military contexts. The results are remarkably consistent across models: all models place substantial weight on the probability of success and domestic support, prioritizing these factors over civilian casualties, economic shock, or international sanctions. The paper then tests whether LLMs are sensitive to context by introducing different motivations for intervention. The scoring is indeed context-dependent; however, probability of victory remains the most important factor in all scenarios. Finally, the paper evaluates numerical sensitivity and finds that models display some responsiveness to the scale of civilian casualties but no detectable sensitivity to the size of the economic shock.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores</title>
<link>https://arxiv.org/abs/2507.08143</link>
<guid>https://arxiv.org/abs/2507.08143</guid>
<content:encoded><![CDATA[
arXiv:2507.08143v2 Announce Type: replace-cross 
Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Left Leaning Models: How AI Evaluates Economic Policy?</title>
<link>https://arxiv.org/abs/2507.15771</link>
<guid>https://arxiv.org/abs/2507.15771</guid>
<content:encoded><![CDATA[
arXiv:2507.15771v2 Announce Type: replace-cross 
Abstract: Would artificial intelligence (AI) cut interest rates or adopt conservative monetary policy? Would it deregulate or opt for a more controlled economy? As AI use by economic policymakers, academics, and market participants grows exponentially, it is becoming critical to understand AI preferences over economic policy. However, these preferences are not yet systematically evaluated and remain a black box. This paper makes a conjoint experiment on leading large language models (LLMs) from OpenAI, Anthropic, and Google, asking them to evaluate economic policy under multi-factor constraints. The results are remarkably consistent across models: most LLMs exhibit a strong preference for high growth, low unemployment, and low inequality over traditional macroeconomic concerns such as low inflation and low public debt. Scenario-specific experiments show that LLMs are sensitive to context but still display strong preferences for low unemployment and low inequality even in monetary-policy settings. Numerical sensitivity tests reveal intuitive responses to quantitative changes but also uncover non-linear patterns such as loss aversion.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Models for Controllable DNA Sequence Design</title>
<link>https://arxiv.org/abs/2507.19523</link>
<guid>https://arxiv.org/abs/2507.19523</guid>
<content:encoded><![CDATA[
arXiv:2507.19523v2 Announce Type: replace-cross 
Abstract: We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers</title>
<link>https://arxiv.org/abs/2508.04503</link>
<guid>https://arxiv.org/abs/2508.04503</guid>
<content:encoded><![CDATA[
arXiv:2508.04503v2 Announce Type: replace-cross 
Abstract: Multivariate time series classification supports applications from wearable sensing to biomedical monitoring and demands models that can capture both short-term patterns and longer-range temporal dependencies. Despite recent advances, Transformer and CNN models often remain computationally heavy and rely on many parameters. This work presents PRISM(Per-channel Resolution Informed Symmetric Module), a lightweight fully convolutional classifier. Operating in a channel-independent manner, in its early stage it applies a set of multi-resolution symmetric convolutional filters. This symmetry enforces structural constraints inspired by linear-phase FIR filters from classical signal processing, effectively halving the number of learnable parameters within the initial layers while preserving the full receptive field. Across the diverse UEA multivariate time-series archive as well as specific benchmarks in human activity recognition, sleep staging, and biomedical signals, PRISM matches or outperforms state-of-the-art CNN and Transformer models while using significantly fewer parameters and markedly lower computational cost. By bringing a principled signal processing prior into a modern neural architecture, PRISM offers an effective and computationally economical solution for multivariate time series classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</title>
<link>https://arxiv.org/abs/2508.06457</link>
<guid>https://arxiv.org/abs/2508.06457</guid>
<content:encoded><![CDATA[
arXiv:2508.06457v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.14285</link>
<guid>https://arxiv.org/abs/2508.14285</guid>
<content:encoded><![CDATA[
arXiv:2508.14285v2 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptation (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a new hyperparameter to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as LLAMA3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on CrossFit and Unified-QA datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
arXiv:2508.15769v2 Announce Type: replace-cross 
Abstract: 3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</title>
<link>https://arxiv.org/abs/2508.18321</link>
<guid>https://arxiv.org/abs/2508.18321</guid>
<content:encoded><![CDATA[
arXiv:2508.18321v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly integrated into multi-agent systems (MAS), where peer interactions shape individual decisions. While prior work has mainly examined conformity bias, we broaden the view to include how LLMs build rapport from prior interactions, discern and integrate high-quality peer information, and resist misleading inputs-abilities essential for achieving collective intelligence under complex social dynamics. We introduce KAIROS, a benchmark that simulates quiz-style collaboration with peer agents whose rapport levels and behaviours can be precisely controlled in both historical interactions and the current round. This unified setup enables systematic analysis of how rapport, peer actions, and the model's self-confidence jointly influence decision-making. Using KAIROS, we evaluate prompting, supervised fine-tuning, and reinforcement learning via Group Relative Policy Optimisation (GRPO). Results show that model scale is a primary factor moderating susceptibility to social influence: larger models are more resilient and benefit from prompting-based mitigation, whereas smaller models remain vulnerable. Only carefully configured GRPO training yields consistent robustness and performance gains for small models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Contexts for Long Video Generation</title>
<link>https://arxiv.org/abs/2508.21058</link>
<guid>https://arxiv.org/abs/2508.21058</guid>
<content:encoded><![CDATA[
arXiv:2508.21058v3 Announce Type: replace-cross 
Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Locus: Agentic Predicate Synthesis for Directed Fuzzing</title>
<link>https://arxiv.org/abs/2508.21302</link>
<guid>https://arxiv.org/abs/2508.21302</guid>
<content:encoded><![CDATA[
arXiv:2508.21302v3 Announce Type: replace-cross 
Abstract: Directed fuzzing aims to find program inputs that lead to specified target program states. It has broad applications, such as debugging system crashes, confirming reported bugs, and generating exploits for potential vulnerabilities. This task is inherently challenging because target states are often deeply nested in the program, while the search space manifested by numerous possible program inputs is prohibitively large. Existing approaches rely on branch distances or manually-specified constraints to guide the search; however, the branches alone are often insufficient to precisely characterize progress toward reaching the target states, while the manually specified constraints are often tailored for specific bug types and thus difficult to generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed fuzzing. Our key insight is to synthesize predicates to capture fuzzing progress as semantically meaningful intermediate states, serving as milestones towards reaching the target states. When used to instrument the program under fuzzing, they can reject executions unlikely to reach the target states, while providing additional coverage guidance. To automate this task and generalize to diverse programs, Locus features an agentic framework with program analysis tools to synthesize and iteratively refine the candidate predicates, while ensuring the predicates strictly relax the target states to prevent false rejections via symbolic execution. Our evaluation shows that Locus substantially improves the efficiency of eight state-of-the-art fuzzers in discovering real-world vulnerabilities, achieving an average speedup of 41.6x. So far, Locus has found nine previously unpatched bugs, with three already acknowledged with draft patches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
arXiv:2509.04622v5 Announce Type: replace-cross 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Coloring for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.16959</link>
<guid>https://arxiv.org/abs/2509.16959</guid>
<content:encoded><![CDATA[
arXiv:2509.16959v4 Announce Type: replace-cross 
Abstract: When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby potentially reducing the final model's performance. To address this, we introduce SON-GOKU, a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated, and the grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, multi-task learning will improve model performance rather than impede it. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers. We provide extensive theory showing why grouping and sequential updates improve multi-task learning, with guarantees on descent, convergence, and accurately identifying what tasks conflict or align.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation</title>
<link>https://arxiv.org/abs/2509.20215</link>
<guid>https://arxiv.org/abs/2509.20215</guid>
<content:encoded><![CDATA[
arXiv:2509.20215v2 Announce Type: replace-cross 
Abstract: LLMs face significant challenges in Verilog generation due to limited domain-specific knowledge. While sampling techniques improve pass@k metrics, hardware engineers need one trustworthy solution rather than uncertain candidates. To bridge this gap, we formulate it as a semantic alignment problem between requirements and Verilog implementations, and propose VCD-RNK, a discriminator model tailored for efficient Verilog code reranking. Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling expert knowledge across three dimensions: code semantic analysis, test case generation, and functional correctness assessment. By explicitly simulating the above reasoning processes during inference, VCD-RNK effectively avoids computationally intensive test execution in existing methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-driven Typology of Vision Models from Integrated Representational Metrics</title>
<link>https://arxiv.org/abs/2509.21628</link>
<guid>https://arxiv.org/abs/2509.21628</guid>
<content:encoded><![CDATA[
arXiv:2509.21628v2 Announce Type: replace-cross 
Abstract: Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs</title>
<link>https://arxiv.org/abs/2509.24319</link>
<guid>https://arxiv.org/abs/2509.24319</guid>
<content:encoded><![CDATA[
arXiv:2509.24319v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Short window attention enables long-term memorization</title>
<link>https://arxiv.org/abs/2509.24552</link>
<guid>https://arxiv.org/abs/2509.24552</guid>
<content:encoded><![CDATA[
arXiv:2509.24552v2 Announce Type: replace-cross 
Abstract: Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.
  A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.
  The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
arXiv:2509.25531v2 Announce Type: replace-cross 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</title>
<link>https://arxiv.org/abs/2510.02350</link>
<guid>https://arxiv.org/abs/2510.02350</guid>
<content:encoded><![CDATA[
arXiv:2510.02350v2 Announce Type: replace-cross 
Abstract: Converting natural language questions into SQL queries enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early text-to-SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the large language model era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models, including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek-R1, and others. Notably, DeepSeek-R1 achieves 88.40% accuracy in a zero-shot setting, and models under 10B parameters surpass 90% accuracy after fine-tuning. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark. Unlike the original WikiSQL, which was tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural-language-to-SQL models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</title>
<link>https://arxiv.org/abs/2510.03291</link>
<guid>https://arxiv.org/abs/2510.03291</guid>
<content:encoded><![CDATA[
arXiv:2510.03291v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</title>
<link>https://arxiv.org/abs/2510.05526</link>
<guid>https://arxiv.org/abs/2510.05526</guid>
<content:encoded><![CDATA[
arXiv:2510.05526v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \textit{\textbf{C}orrupted} preference, reward \textit{\textbf{O}veroptimization}, and bias towards \textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\textbf{COV} and DPO-\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</title>
<link>https://arxiv.org/abs/2510.05535</link>
<guid>https://arxiv.org/abs/2510.05535</guid>
<content:encoded><![CDATA[
arXiv:2510.05535v2 Announce Type: replace-cross 
Abstract: Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: https://anonymous.4open.science/r/FedCAPS-08BF.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</title>
<link>https://arxiv.org/abs/2510.09017</link>
<guid>https://arxiv.org/abs/2510.09017</guid>
<content:encoded><![CDATA[
arXiv:2510.09017v2 Announce Type: replace-cross 
Abstract: Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COINS: SemantiC Ids Enhanced COLd Item RepresentatioN for Click-through Rate Prediction in E-commerce Sarch</title>
<link>https://arxiv.org/abs/2510.12604</link>
<guid>https://arxiv.org/abs/2510.12604</guid>
<content:encoded><![CDATA[
arXiv:2510.12604v2 Announce Type: replace-cross 
Abstract: With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation</title>
<link>https://arxiv.org/abs/2510.16809</link>
<guid>https://arxiv.org/abs/2510.16809</guid>
<content:encoded><![CDATA[
arXiv:2510.16809v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples ("many-shot" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of "more is better" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17923</link>
<guid>https://arxiv.org/abs/2510.17923</guid>
<content:encoded><![CDATA[
arXiv:2510.17923v4 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://arxiv.org/abs/2510.20812</link>
<guid>https://arxiv.org/abs/2510.20812</guid>
<content:encoded><![CDATA[
arXiv:2510.20812v3 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-driven Reinforcement Learning in Continuous Control</title>
<link>https://arxiv.org/abs/2511.07904</link>
<guid>https://arxiv.org/abs/2511.07904</guid>
<content:encoded><![CDATA[
arXiv:2511.07904v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2511.12779</link>
<guid>https://arxiv.org/abs/2511.12779</guid>
<content:encoded><![CDATA[
arXiv:2511.12779v2 Announce Type: replace-cross 
Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a 2% error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by 16% on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of 19%. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.13625</link>
<guid>https://arxiv.org/abs/2511.13625</guid>
<content:encoded><![CDATA[
arXiv:2511.13625v3 Announce Type: replace-cross 
Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches. Our approach is available in GPSampler in Optuna, effectively reducing its computational overhead.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
arXiv:2511.14295v2 Announce Type: replace-cross 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-PEFT: Unveiling Token-Level Redundancy in Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.16147</link>
<guid>https://arxiv.org/abs/2511.16147</guid>
<content:encoded><![CDATA[
arXiv:2511.16147v2 Announce Type: replace-cross 
Abstract: Current Parameter-Efficient Fine-Tuning (PEFT) methods typically operate under an implicit assumption: once a target module is selected, every token passing through it contributes equally to the downstream task and requires a parameter update. In this paper, we challenge this convention and unveil a pervasive token-level redundancy in the fine-tuning of large models. We propose TS-PEFT, a theoretically grounded framework utilizing proximal optimization to dynamically identify and skip redundant token updates during training. Our extensive experiments across Natural Language Understanding, Commonsense Reasoning, and Visual Instruction Tuning demonstrate that indiscriminately updating all tokens is not only computationally superfluous but often introduces optimization noise. Strikingly, by discarding 40%-60% of token updates, TS-PEFT consistently matches or surpasses the performance of dense baselines (e.g., LoRA, DoRA). Furthermore, we provide an in-depth analysis revealing that the learned token-level sparsity serves as a superior indicator of module importance compared to traditional weight norms, offering a novel data-driven perspective on the intrinsic adaptation mechanism of large models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
arXiv:2511.17582v2 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping the Verifier: Learning to Reason via Demonstrations</title>
<link>https://arxiv.org/abs/2511.21667</link>
<guid>https://arxiv.org/abs/2511.21667</guid>
<content:encoded><![CDATA[
arXiv:2511.21667v3 Announce Type: replace-cross 
Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends</title>
<link>https://arxiv.org/abs/2511.22334</link>
<guid>https://arxiv.org/abs/2511.22334</guid>
<content:encoded><![CDATA[
arXiv:2511.22334v2 Announce Type: replace-cross 
Abstract: Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartAnchor: Chart Grounding with Structural-Semantic Fidelity</title>
<link>https://arxiv.org/abs/2512.01017</link>
<guid>https://arxiv.org/abs/2512.01017</guid>
<content:encoded><![CDATA[
<div> Chart grounding, multimodal large language models, benchmarks, numerical reasoning, chart-to-code generation  

<br /><br />Summary:  
This paper addresses the limitations of current benchmarks in evaluating multimodal large language models (MLLMs) for structured chart comprehension, specifically in the task of chart grounding—which involves aligning a chart's visual representation with its structured semantics. The authors introduce ChartAnchor, a comprehensive benchmark dataset comprising over 8,000 chart-table-code triples across 30 diverse chart types sourced from real-world and augmented data. ChartAnchor defines two main tasks: chart-to-code generation, where models synthesize executable code to accurately recreate charts, and controlled chart-to-table reconstruction, focusing on extracting precise tabular data with predefined headers. The benchmark employs a multi-level evaluation framework that integrates semantic validation, stylistic analysis, and perceptual metrics to rigorously assess both the structural integrity and content accuracy of model outputs. Extensive experiments reveal that current MLLMs struggle with numerical precision and code synthesis, indicating a need for more advanced structured reasoning capabilities beyond superficial visual understanding. By unifying symbolic and data-driven grounding methods, ChartAnchor offers a rigorous foundation for advancing chart grounding research, with potential applications in scientific, financial, and industrial domains where accurate chart interpretation and generation are critical. <div>
arXiv:2512.01017v2 Announce Type: replace 
Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension. Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios. Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised decoding of encoded reasoning using language model interpretability</title>
<link>https://arxiv.org/abs/2512.01222</link>
<guid>https://arxiv.org/abs/2512.01222</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, interpretability, chain-of-thought reasoning, logit lens, encoded reasoning  

<br /><br />Summary:  
1. The paper addresses concerns about large language models (LLMs) potentially developing hidden or encoded reasoning processes that evade human interpretability.  
2. To test current interpretability techniques, the authors fine-tune a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning under ROT-13 encryption, preserving understandable English outputs despite hidden internal logic.  
3. The study focuses on mechanistic interpretability, particularly leveraging logit lens analysis, to decode this hidden reasoning using only internal activations within the model.  
4. Results demonstrate that the logit lens method can successfully translate the encrypted reasoning steps, with decoding accuracy peaking in intermediate to late network layers.  
5. Building on this, the authors develop a fully unsupervised pipeline that combines logit lens output with automated paraphrasing, significantly improving the reconstruction of complete reasoning transcripts from the model’s internal states.  
6. These findings imply that existing mechanistic interpretability tools may be more effective against simple forms of encoded reasoning than previously thought.  
7. Overall, the work provides a novel testbed and framework for evaluating interpretability methods against models employing non-human-readable reasoning formats, contributing to safer and more accountable AI oversight as LLMs become increasingly powerful. <div>
arXiv:2512.01222v2 Announce Type: replace 
Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Path Collaborative Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01485</link>
<guid>https://arxiv.org/abs/2512.01485</guid>
<content:encoded><![CDATA[
<div> Multi-Path Perception Policy Optimization, Chain-of-Thought, Reinforcement Learning, Reasoning, Large Language Models<br /><br />Summary: Chain-of-Thought (CoT) reasoning has greatly improved Large Language Models' (LLMs) problem-solving ability but often suffers from deterministic decoding, which restricts exploration of alternative reasoning paths. Recent approaches aim to overcome this by generating soft abstract tokens to reason within a continuous semantic space, yet still face limitations due to the inherently greedy autoregressive decoding process. This paper introduces Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework designed to explicitly incorporate collective insights from multiple reasoning paths. M3PO utilizes parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions via a lightweight collaboration mechanism. This allows each reasoning trajectory to receive and refine feedback from its peers, fostering more robust multi-step reasoning sequences. Empirical results demonstrate that M3PO achieves state-of-the-art performance on benchmarks requiring deep knowledge and complex reasoning. Moreover, models trained with M3PO retain interpretability and maintain efficient inference, highlighting the potential of multi-path collaborative learning approaches for improving reasoning robustness in LLMs. <div>
arXiv:2512.01485v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</title>
<link>https://arxiv.org/abs/2508.09442</link>
<guid>https://arxiv.org/abs/2508.09442</guid>
<content:encoded><![CDATA[
<div> Keywords: KV-cache, privacy leakage, Large Language Models, attack vectors, KV-Cloak<br /><br />Summary:<br /><br />This paper investigates the privacy risks associated with the Key-Value (KV) cache mechanism used in Large Language Model (LLM) inference, which stores intermediate attention computations to improve efficiency. Despite the performance benefits, the KV-cache introduces significant privacy vulnerabilities that have not been thoroughly studied before. The authors present the first comprehensive analysis by designing three attack methods that exploit KV-cache data: a direct Inversion Attack that reconstructs inputs, a Collision Attack that is more versatile and powerful, and a semantic-based Injection Attack. These attacks demonstrate how sensitive user inputs can be extracted from the KV-cache, highlighting a critical security gap. To address this, the paper proposes KV-Cloak, a lightweight defense mechanism that secures the KV-cache through a reversible matrix-based obfuscation combined with operator fusion. Experimental results show that KV-Cloak effectively neutralizes all three attacks and reduces reconstructed information to meaningless noise. Importantly, the defense achieves strong privacy protection without sacrificing model accuracy or significantly increasing computational cost, making it a practical solution for secure and trustworthy deployment of LLMs. This work underscores the need for balancing inference efficiency with user privacy in modern language models. <div>
arXiv:2508.09442v3 Announce Type: replace-cross 
Abstract: The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTArena: A Benchmark for Agentic PowerPoint Editing</title>
<link>https://arxiv.org/abs/2512.03042</link>
<guid>https://arxiv.org/abs/2512.03042</guid>
<content:encoded><![CDATA[
<div> PowerPoint editing, benchmark, PPTArena, PPTPilot, visual fidelity  

<br /><br />Summary:  
This paper introduces PPTArena, a comprehensive benchmark designed to evaluate reliable PowerPoint slide editing under natural-language instructions. Unlike prior approaches that rely on image-PDF renderings or text-to-slide generation, PPTArena emphasizes in-place editing across 100 decks, 2,125 slides, and over 800 targeted edits that span text, charts, tables, animations, and master-level styles. Each editing task in the benchmark is paired with a ground-truth deck and a fully detailed target outcome. The evaluation framework incorporates a dual visual-language model (VLM)-based judging pipeline that scores the edits by instruction adherence and visual quality using both structural differences and rendered slide images. Building upon this benchmark, the authors propose PPTPilot, a structure-aware slide-editing agent that executes semantic edit plans, dynamically routes between high-level programmatic tools and precise deterministic XML operations, and iteratively verifies outputs against task-specific constraints with a plan-edit-check loop. Experimental results show that PPTPilot outperforms strong proprietary agents and state-of-the-art VLM systems by over 10 percentage points, particularly excelling in complex, layout-sensitive, and cross-slide edits with significant gains in visual fidelity and deck-wide consistency. Despite these advancements, the study highlights persistent challenges in achieving reliable editing on long-horizon, document-scale tasks within PPTArena. <div>
arXiv:2512.03042v2 Announce Type: replace-cross 
Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals</title>
<link>https://arxiv.org/abs/2512.05998</link>
<guid>https://arxiv.org/abs/2512.05998</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, confidence calibration, prediction markets, wagering mechanism  

<br /><br />Summary:  
This pilot study investigates whether framing the task of evaluating large language models (LLMs) as a betting game can improve forecasting accuracy and reveal calibrated confidence signals. The researchers generated 100 math and logic questions with verifiable answers and tested six baseline LLMs (three current-generation and three prior-generation) on these items. Three separate predictor models were tasked with forecasting the correctness of each baseline model's answer. Each predictor completed matched runs under two conditions: a Control condition with simple correct/incorrect predictions, and an Incentive condition where predictions were paired with wagers of a fictional currency called LLMCoin. Across 5,400 predictions per condition, the Incentive condition yielded modestly higher accuracy (81.5% vs. 79.1%) and significantly faster learning over rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4). The wager size strongly correlated with confidence: large bets (>40,000 coins) were about 99% correct, while small bets (<1,000 coins) had only ~74% accuracy. Although the accuracy improvement was not statistically significant, the key contribution is that the betting framework generated interpretable confidence signals absent from simple binary outputs. This suggests that financial framing might enable LLMs to express risk-aware forecasts and could underpin future LLM-to-LLM meta-evaluation or prediction markets. <div>
arXiv:2512.05998v1 Announce Type: new 
Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach</title>
<link>https://arxiv.org/abs/2512.06161</link>
<guid>https://arxiv.org/abs/2512.06161</guid>
<content:encoded><![CDATA[
<div> Keywords: Autism Spectrum Disorder, Machine Learning, BioBERT, Transfer Learning, Clinical Text Analysis<br /><br />Summary:  
This study addresses the challenge of diagnosing Autism Spectrum Disorder (ASD), a complex neurodevelopmental condition with increasing prevalence and a tedious diagnostic process. 1) The authors propose a transparent and interpretable machine learning (ML) method utilizing BioBERT, a cutting-edge language model, to analyze unstructured clinical text and map behavioral descriptions to diagnostic criteria for ASD classification. 2) Two distinct real-world datasets were used to evaluate transfer learning capabilities, testing model performance when trained sequentially on separate datasets versus a mixed dataset approach. 3) Their transparent model achieved robust results, with mixed-data training resulting in 97% sensitivity and 98% specificity, outperforming a comparable black-box ML model that reached only 90% sensitivity and 96% specificity. 4) Training order impacted sequential training performance, causing a slight reduction in effectiveness, emphasizing the importance of dataset composition and order in model training. 5) Overall, the transparent ML approach demonstrated superior generalizability, reliability, and clinical applicability compared to black-box methods, with mixed-data training recommended for best results. This work advances the development of trustworthy and clinically actionable AI tools for neurodevelopmental diagnostics such as ASD. <div>
arXiv:2512.06161v1 Announce Type: new 
Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment</title>
<link>https://arxiv.org/abs/2512.06196</link>
<guid>https://arxiv.org/abs/2512.06196</guid>
<content:encoded><![CDATA[
<div> Keywords: alignment, reward models, large language models, rubrics, multi-agent collaboration<br /><br />Summary:<br /><br />1. The paper addresses the challenge of maintaining alignment between large language model-based agents and stakeholder preferences during complex, long-horizon tasks.<br />2. It emphasizes the need for interpretable reward models that allow stakeholders to understand and audit the objectives guiding agent behavior.<br />3. The authors introduce ARCANE, a novel framework that treats alignment as a multi-agent collaboration problem, representing stakeholder preferences dynamically using natural-language rubrics composed of weighted, verifiable criteria.<br />4. Rubric learning is formulated as a reconstruction problem inspired by utility theory, and is optimized via a regularized Group-Sequence Policy Optimization (GSPO) to ensure interpretability, faithfulness, and efficiency.<br />5. Experiments on 219 labeled rubrics from the GDPVal benchmark demonstrate that ARCANE produces compact and clear evaluations, supports configurable trade-offs such as correctness versus conciseness, and enables adaptive preference shifts at interaction time without retraining, marking a promising direction for interpretable and flexible alignment in AI systems. <div>
arXiv:2512.06196v1 Announce Type: new 
Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On measuring grounding and generalizing grounding problems</title>
<link>https://arxiv.org/abs/2512.06205</link>
<guid>https://arxiv.org/abs/2512.06205</guid>
<content:encoded><![CDATA[
<div> Keywords: symbol grounding, meaning, authenticity, compositionality, robustness  

<br /><br />Summary:  
This article addresses the symbol grounding problem, which concerns how tokens such as "cat" genuinely refer to cats rather than merely being arbitrary shapes manipulated by rules. The authors propose reframing grounding from a simple binary judgment into a multidimensional audit based on several desiderata: authenticity (meaning mechanisms must be internal and acquired through learning or evolution), preservation (atomic meanings remain unchanged), faithfulness (meanings correlate with intended referents and causally contribute to success), robustness (meanings degrade gracefully under perturbations), and compositionality (complex meanings arise systematically from parts). They apply this framework to analyze four grounding modes—symbolic, referential, vectorial, and relational—and three case studies. First, model-theoretic semantics achieves perfect compositionality but lacks etiological grounding, meaning its internal mechanisms do not guarantee success in the world. Second, large language models demonstrate correlational fits and robustness in linguistic tasks but do not show evidence of selection for success in grounded world interaction. Third, human language fulfills all desiderata robustly, especially authenticity through evolutionary and developmental processes. The framework offers a unified technical language bridging philosophy of science, computer science, linguistics, and mathematics to systematically explore grounding and meaning. <div>
arXiv:2512.06205v1 Announce Type: new 
Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems</title>
<link>https://arxiv.org/abs/2512.06240</link>
<guid>https://arxiv.org/abs/2512.06240</guid>
<content:encoded><![CDATA[
<div> Money laundering, AI, Anti-Money Laundering, KYC, RAG-Graph<br /><br />Summary:<br /><br />This paper addresses the persistent global challenge of money laundering and financial fraud, emphasizing their detrimental impact on financial stability. It reviews how artificial intelligence (AI) can significantly modernize Anti-Money Laundering (AML) workflows by enhancing detection accuracy, reducing false positives, and alleviating manual investigation burdens. The paper highlights key future research avenues such as federated learning to enable privacy-preserving collaboration among institutions, fairness-aware and interpretable AI to ensure ethical compliance, reinforcement learning for adaptive defense mechanisms, and human-in-the-loop visualization systems to maintain transparency and accountability. Further, the authors propose an innovative AI-driven Know Your Customer (KYC) application that combines graph-based retrieval-augmented generation (RAG-Graph) with generative models to improve efficiency, transparency, and support in KYC processes focused on money laundering detection. Experimental evaluations demonstrate that the RAG-Graph architecture produces highly faithful and relevant answers across diverse scenarios, contributing to more efficient and transparent Customer Due Diligence (CDD) and Enhanced Due Diligence (EDD) workflows. Overall, the integration of these AI approaches supports sustainable and resource-optimized compliance practices crucial to combating financial crime. <div>
arXiv:2512.06240v1 Announce Type: new 
Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2512.06296</link>
<guid>https://arxiv.org/abs/2512.06296</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graph completion, evaluation metrics, predictive sharpness, popularity-bias robustness, PROBE

<br /><br />Summary:  
Knowledge graph completion (KGC) focuses on predicting missing facts within existing knowledge graphs. Despite advances in KGC modeling, the evaluation methods used to measure model performance remain insufficiently explored and often incomplete. This paper identifies two critical evaluation aspects that are frequently overlooked: (A1) predictive sharpness, which refers to the strictness level when assessing individual model predictions, and (A2) popularity-bias robustness, which concerns the model’s ability to accurately predict low-popularity or less frequent entities. To address these gaps, the authors introduce a novel evaluation framework called PROBE. PROBE incorporates a rank transformer (RT) that adjusts prediction scores according to the desired predictive sharpness level and a rank aggregator (RA) that consolidates scores while taking entity popularity into account. Experiments conducted on real-world knowledge graphs demonstrate that existing evaluation metrics often lead to overestimation or underestimation of KGC model accuracy. In contrast, PROBE offers a more nuanced and comprehensive evaluation, enabling better insight into model performance and generating more reliable results. This contribution advances the evaluation methodology in KGC research by highlighting and quantifying important, previously neglected factors. <div>
arXiv:2512.06296v1 Announce Type: new 
Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2512.06337</link>
<guid>https://arxiv.org/abs/2512.06337</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Group Relative Policy Optimization, Distinctiveness, Gradient Rectification, Off-policy Data Augmentation<br /><br />Summary:<br /><br />This paper addresses the instability and inefficiency issues in Group Relative Policy Optimization (GRPO) when training Large Language Models (LLMs) for long-horizon reasoning tasks. The authors identify that on-policy rollouts lack distinctiveness, causing gradient conflicts for simple queries and insufficient positive samples for difficult ones, leading to poor optimization. To resolve this, they propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO), which includes two mechanisms: Sequence-level Gradient Rectification that masks low-distinctiveness sample pairs based on fine-grained scoring to prevent gradient conflicts, and Off-policy Data Augmentation that injects high-quality anchors to reinforce training signals on challenging tasks. Through extensive experiments on nine math reasoning and out-of-distribution generalization benchmarks, DaGRPO outperforms standard fine-tuning, GRPO, and hybrid methods, achieving up to a 4.7% average accuracy improvement on math tests. Additional analysis verifies that DaGRPO stabilizes training by mitigating gradient explosion and expedites the development of long-chain reasoning capabilities in LLMs. Overall, their approach significantly advances the efficiency and robustness of post-training reasoning in language models. <div>
arXiv:2512.06337v1 Announce Type: new 
Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title>
<link>https://arxiv.org/abs/2512.06393</link>
<guid>https://arxiv.org/abs/2512.06393</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, logical reasoning, stress tests, rule deletion, contradictory evidence<br /><br />Summary:<br /><br />This paper presents a controlled evaluation framework to investigate the reasoning reliability of large language models (LLMs) when facing structural perturbations in logical contexts. The authors design four targeted stress tests: (1) deletion of rules—either redundant or essential—from a multi-step inference chain; (2) injection of contradictory evidence; (3) logic-preserving rewrites using equivalence laws such as contrapositive, double negation, implication, De Morgan’s laws, identity, and commutativity; and (4) multi-law equivalence stacking, applying 2-5 simultaneous logical transformations. The study evaluates three model families: BERT, Qwen2, and LLaMA-like models. Results show that all models achieve perfect accuracy on the original base tasks and maintain full generalization when redundant rules are deleted and under all equivalence-based rewrites, both single- and multi-law. However, their performance dramatically degrades when essential rules are deleted, dropping to about 25% accuracy. Furthermore, models fail completely when encountering explicit contradictions, reaching close to 0% accuracy. These findings reveal that while LLMs demonstrate stable invariance to semantic-preserving logical transformations, they remain fundamentally fragile to missing or conflicting evidence. This work offers a clear diagnostic tool to isolate reasoning failure modes and underscores persistent limitations in the logical generalization capabilities of current large language models. <div>
arXiv:2512.06393v1 Announce Type: new 
Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols</title>
<link>https://arxiv.org/abs/2512.06404</link>
<guid>https://arxiv.org/abs/2512.06404</guid>
<content:encoded><![CDATA[
<div> Predictive simulations, Quantum ESPRESSO, AI workflow, error recovery, Integrated Computational Materials Engineering (ICME)  

<br /><br />Summary:  
1. The article addresses the challenge in performing predictive atomistic simulations for materials discovery, which currently require specialized computer skills, limiting broader adoption in Integrated Computational Materials Engineering (ICME).  
2. The authors introduce GENIUS, an AI-driven workflow that integrates a smart knowledge graph of Quantum ESPRESSO with a hierarchical series of large language models (LLMs) overseen by a finite-state error-recovery system.  
3. GENIUS can convert free-form human prompts into valid input files for simulations, successfully running about 80% of 295 varied benchmark tests to completion, with 76% of errors autonomously fixed by the system.  
4. The system's success rate declines exponentially from this initial high level down to a 7% baseline for more difficult cases, showing the limits of current autonomous repair capabilities.  
5. Compared with approaches relying solely on large language models, GENIUS reduces inference costs by half and nearly eliminates hallucinations, democratizing density functional theory (DFT) simulations through automated protocol generation, validation, and repair. This advancement enables large-scale screening and faster ICME design cycles in both academic and industrial settings worldwide. <div>
arXiv:2512.06404v1 Announce Type: new 
Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</title>
<link>https://arxiv.org/abs/2512.06406</link>
<guid>https://arxiv.org/abs/2512.06406</guid>
<content:encoded><![CDATA[
<div> Uncertainty quantification, large language models, CodeBERT, ChatGLM3, software toolkit  

<br /><br />Summary:  
This paper addresses the challenge of quantifying uncertainty in large language models (LLMs), which are increasingly deployed across various real-world applications but can produce incorrect outputs, especially in safety-critical scenarios. To improve the reliability and confidence measurement of LLM predictions, multiple uncertainty quantification (UQ) methods have been developed, but their practical adoption is limited due to a lack of integrated tools. The authors introduce UncertaintyZoo, a unified software toolkit that consolidates 29 UQ methods from five major categories with a standardized interface, facilitating easier use and comparison of techniques. They demonstrate the effectiveness of UncertaintyZoo by applying it to the code vulnerability detection task, evaluating two models: CodeBERT and ChatGLM3. Experimental results show that UncertaintyZoo successfully reveals the uncertainty in model predictions, highlighting its potential to enhance the safety and trustworthiness of AI systems. The toolkit aims to foster broader incorporation of uncertainty measures in LLM applications and support future research efforts. The project, including a demonstration video, is publicly available at the provided GitHub repository. <div>
arXiv:2512.06406v1 Announce Type: new 
Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City</title>
<link>https://arxiv.org/abs/2512.06431</link>
<guid>https://arxiv.org/abs/2512.06431</guid>
<content:encoded><![CDATA[
<div> Keywords: urban planning, spatial analysis, Voronoi diagrams, public services, Egypt<br /><br />Summary:<br /><br />This study addresses the shortcomings of national planning standards in Egypt, which often do not reflect local realities, by developing a tailored urban planning model for Qena City. Employing a hybrid methodology that combines descriptive, analytical, and experimental approaches, the research leverages Python programming to create an intelligent spatial analysis algorithm based on Voronoi Diagrams. This algorithm is used to establish city-specific planning criteria and assess the current coverage and efficiency of public service facilities. The main achievement is the derivation of a localized planning standards model alongside the deployment of an automated tool to evaluate service distribution. Findings highlight an average public service coverage of 81.3% across the city. Ambulance stations show the highest efficiency at 99.8%, attributed to recent improvements, whereas parks and open spaces suffer from very low coverage of only 10%, mainly due to limited land availability. Spatial patterns reveal that midtown areas benefit from a very high service density exceeding 45 services per square kilometer, but this density drops sharply to under 5 services/km² toward the outskirts. The Hajer Qena district emerges as the area with the greatest underserved zones, while the First District (Qesm 1) demonstrates the highest level of service coverage. The presented model offers a replicable and data-driven approach for improving urban planning in other Egyptian cities. <div>
arXiv:2512.06431v1 Announce Type: new 
Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Belief Boxes and Open-mindedness on Persuasion</title>
<link>https://arxiv.org/abs/2512.06573</link>
<guid>https://arxiv.org/abs/2512.06573</guid>
<content:encoded><![CDATA[
<div> Belief Boxes, Large Language Models, Multi-Agent Systems, Persuasion, Open-Mindedness<br /><br />Summary:<br /><br />1. The paper addresses the challenge of enabling large language model (LLM)-based agents in multi-agent systems to maintain propositional beliefs through a mechanism called "belief boxes," which hold statements describing their beliefs within the prompt space. <br /><br />2. It investigates how embedding such belief statements influences the agents' behavior and their disposition towards these beliefs, particularly in multi-agent reasoning and decision-making scenarios. <br /><br />3. Experiments reveal that instructing agents to be open-minded affects their willingness and amenability to revise beliefs when confronted with opposing viewpoints. <br /><br />4. The inclusion of belief statements along with their associated strength impacts an agent's resistance to changing beliefs and its ability to persuade others holding opposing views, especially under peer pressure conditions where agents are outnumbered. <br /><br />5. Results validate the belief box approach as a feasible and effective method to enhance reasoning, decision-making, and interpersonal influence among LLM-driven agents by modulating belief characteristics and open-mindedness instructions. <div>
arXiv:2512.06573v1 Announce Type: new 
Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection</title>
<link>https://arxiv.org/abs/2512.06629</link>
<guid>https://arxiv.org/abs/2512.06629</guid>
<content:encoded><![CDATA[
<div> Knowledge Tracing, Flat Transformer, Cognitive Dynamics, Power-law Bias, Real-time Inference  

<br /><br />Summary:  
This paper addresses the "Performance-Complexity Trap" in Knowledge Tracing (KT) models, where capturing complex cognitive behaviors such as learning sessions and memory decay typically demands deep hierarchical architectures that are computationally expensive for real-time applications. To overcome this, the authors introduce FlatFormer, a simplified model that employs a flat Transformer enhanced by two lightweight mechanisms. First, it uses a hybrid input encoding combining learnable session identifiers with fixed sinusoidal step embeddings to better capture temporal information. Second, it integrates a pre-computed power-law bias directly into the attention logits to explicitly represent the forgetting curve. FlatFormer significantly reduces parameters compared to traditional hierarchical models yet maintains or improves performance. Experiments on four large-scale datasets, including EdNet and Junyi, demonstrate that FlatFormer achieves state-of-the-art results. Particularly, on the EdNet dataset, it improves the absolute AUC by 8.3% over the strongest hierarchical baseline (HiTSKT), while using less than 15% of the parameters and delivering approximately three times faster inference speed. These findings suggest that high cognitive fidelity in KT does not require complex architectures but can be achieved through efficient design leveraging information injection over structural stacking. <div>
arXiv:2512.06629v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightSearcher: Efficient DeepSearch via Experiential Memory</title>
<link>https://arxiv.org/abs/2512.06653</link>
<guid>https://arxiv.org/abs/2512.06653</guid>
<content:encoded><![CDATA[
<div> DeepSearch, reinforcement learning, efficiency, multi-hop QA, adaptive reward shaping<br /><br />Summary: DeepSearch paradigms enable deep reasoning models to access external, up-to-date domain-specific knowledge through search tools, enhancing reasoning depth and factual accuracy. Recent reinforcement learning (RL) approaches allow models to autonomously govern search tool usage, optimizing querying strategies. However, these RL-driven DeepSearch systems face a trade-off where frequent tool use boosts accuracy but causes computational inefficiency. To address this, the paper presents LightSearcher, an efficient RL framework that integrates textual experiential memory by learning contrastive reasoning trajectories, which generate interpretable summaries of successful reasoning patterns. LightSearcher also introduces an adaptive reward shaping mechanism that penalizes redundant tool calls only when the answer is correct, effectively balancing accuracy and efficiency. Experiments on four multi-hop question answering benchmarks demonstrate that LightSearcher achieves accuracy comparable to the state-of-the-art ReSearch baseline. Meanwhile, it decreases search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, illustrating superior efficiency without compromising accuracy. This work offers a novel solution to optimize the inherent accuracy-efficiency trade-off in DeepSearch frameworks, advancing the practical deployment of deep reasoning models that rely on external knowledge invocation. <div>
arXiv:2512.06653v2 Announce Type: new 
Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Academic journals' AI policies fail to curb the surge in AI-assisted academic writing</title>
<link>https://arxiv.org/abs/2512.06705</link>
<guid>https://arxiv.org/abs/2512.06705</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, academic writing, AI policies, transparency gap, ethical frameworks<br /><br />Summary:  
The study analyzes the impact of AI usage guidelines across 5,114 journals and over 5.2 million scientific papers. It finds that although 70% of journals have adopted AI policies—mainly requiring disclosure of AI assistance—researchers' use of AI writing tools has surged significantly across all disciplines, with no notable difference between journals with or without such policies. The highest growth in AI tool adoption is observed in non-English-speaking countries, physical sciences, and journals with high open-access rates. A detailed full-text analysis of 164,000 publications reveals a major transparency gap: out of 75,000 papers published since 2023, only 76 explicitly disclosed the use of AI, accounting for just 0.1%. These findings indicate that existing AI policies have largely failed to promote transparency or curb the adoption of AI in scientific writing. The authors call for a reassessment of current ethical frameworks to better support responsible integration of AI technologies in academic research, emphasizing the need for more effective guidelines to encourage clear disclosure and accountability. <div>
arXiv:2512.06705v1 Announce Type: new 
Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation</title>
<link>https://arxiv.org/abs/2512.06710</link>
<guid>https://arxiv.org/abs/2512.06710</guid>
<content:encoded><![CDATA[
<div> Keywords: Intraclass Correlation Coefficient, evaluation reliability, agentic systems, variance decomposition, benchmarking  

<br /><br />Summary:  
This paper addresses the critical need for reliable evaluation metrics in large language model (LLM) based agentic systems, where unreliable sub-agents can cause brittle downstream performance. It critiques current evaluation practices that report a single accuracy score from a single run, which masks underlying variance and hides whether improvements are genuine or due to chance. The authors propose adopting the Intraclass Correlation Coefficient (ICC) from measurement science to quantify variance by separating between-query variance (task difficulty) from within-query variance (agent inconsistency). They assess ICC on two benchmarks: GAIA (agentic capabilities with increasing reasoning complexity) and FRAMES (retrieval and factuality across documents). Results show wide ICC variation depending on task type, with reasoning and retrieval tasks ranging from ~0.50 to 0.71, and agentic tasks from ~0.30 to 0.77. ICC improvements should accompany accuracy gains to confirm trustworthy model enhancements. The paper demonstrates ICC convergence with 8-16 trials for structured tasks and at least 32 for complex reasoning, guiding practitioners on resampling budgets. The authors recommend standard reporting of accuracy, ICC, and within-query variance, alongside updated Evaluation Cards. Their open-source code facilitates adoption, aiming to shift agentic benchmarking from competitive leaderboards to transparent, rigorous experimental science. <div>
arXiv:2512.06710v1 Announce Type: new 
Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</title>
<link>https://arxiv.org/abs/2512.06716</link>
<guid>https://arxiv.org/abs/2512.06716</guid>
<content:encoded><![CDATA[
<div> Keywords: Indirect Prompt Injection, Cognitive Control Architecture, Intent Graph, Tiered Adjudicator, AgentDojo

<br /><br />Summary: Autonomous Large Language Model (LLM) agents are vulnerable to Indirect Prompt Injection (IPI) attacks, where attackers manipulate external information to hijack agent behavior, causing unauthorized actions and diversion from intended goals. Existing defense mechanisms suffer from fragmentation, failing to maintain full integrity across the entire task pipeline and forcing compromises between security, functionality, and efficiency. The core insight behind the proposed solution is that any malicious IPI attack inevitably causes detectable deviations in the agent’s action trajectory when compared to legitimate plans. The authors introduce the Cognitive Control Architecture (CCA), a holistic framework that provides full-lifecycle cognitive supervision of LLM agents to counter these attacks. CCA consists of two synergistic pillars: first, a pre-generated "Intent Graph" that proactively enforces control-flow and data-flow integrity, preventing deviations before they occur; second, a "Tiered Adjudicator" that detects any deviations and performs deep reasoning using multi-dimensional scoring to effectively respond to complex conditional attacks. Experiments on the AgentDojo benchmark demonstrate that CCA robustly defends against sophisticated IPI attacks that challenge other defense approaches, achieving a balanced reconciliation of security, functionality, and efficiency without compromise. <div>
arXiv:2512.06716v1 Announce Type: new 
Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems</title>
<link>https://arxiv.org/abs/2512.06721</link>
<guid>https://arxiv.org/abs/2512.06721</guid>
<content:encoded><![CDATA[
<div> Large Language Model agents, proactive assistance, context extraction, augmented reality, user satisfaction  
  
<br /><br />Summary: ProAgent is introduced as the first end-to-end proactive agent system that uses extensive sensory contexts combined with large language model reasoning to offer proactive assistance, moving beyond the reactive nature of existing LLM agents. The system integrates a proactive-oriented context extraction method with tiered, on-demand perception to continuously monitor the environment and generate hierarchical contexts that blend sensory input with user persona information. ProAgent employs a context-aware proactive reasoner to translate these contexts into predicted user needs and corresponding tool calls, enabling anticipatory service delivery. Implemented on augmented reality glasses supported by an edge server, ProAgent was evaluated through real-world testbeds, public datasets, and user studies. Results demonstrated a substantial improvement with up to 33.4% increase in proactive prediction accuracy and a 16.8% rise in tool-calling F1 score compared to leading baselines. Additionally, user satisfaction showed significant gains, highlighting the practical benefits and enhanced experience provided by proactive assistance. This work marks a critical advancement toward intelligent assistants that anticipate and fulfill user needs without explicit prompting, potentially transforming daily interactions with technology. A video demonstration is provided for further insight into ProAgent’s capabilities. <div>
arXiv:2512.06721v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2512.06749</link>
<guid>https://arxiv.org/abs/2512.06749</guid>
<content:encoded><![CDATA[
<div> Large language models, multi-agent systems, debugging, intervention-driven, failure localization<br /><br />Summary:<br /><br />This paper addresses the challenge of debugging large language model (LLM)-based multi-agent systems, where failures arise from complex, long interaction traces. Current debugging methods rely on log-based failure localization, attributing errors to specific agents or steps, but suffer from two limitations: they lack validation and produce untested hypotheses, and single-step or agent attribution is often ill-posed since multiple independent interventions can repair failures. To overcome these issues, the authors propose DoVer, an intervention-driven debugging framework that supplements hypothesis generation with active verification via targeted interventions like message editing or plan alteration. Instead of focusing on accurate error attribution, DoVer evaluates success based on whether the system recovers from failure or makes milestone progress toward the task, advocating an outcome-oriented debugging approach. Experiments conducted within the Magnetic-One multi-agent framework, using datasets from GAIA and AssistantBench, show that DoVer converts 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. Further tests on the GSMPlus dataset and AG2 agent framework demonstrate DoVer’s robustness by recovering 49% of failed trials. Overall, DoVer highlights intervention as a practical, scalable method to improve reliability and debugging in LLM-based multi-agent systems. <div>
arXiv:2512.06749v2 Announce Type: new 
Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2512.06835</link>
<guid>https://arxiv.org/abs/2512.06835</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, reinforcement learning, reward hacking, dual-decoupling, curriculum learning<br /><br />Summary:<br /><br />1. The paper addresses challenges in applying reinforcement learning (RL) to vision-language models (VLMs), especially in specialized domains such as chemistry, earth sciences, and multimodal mathematics, where high-quality multimodal data is scarce.  
2. Current RL strategies for VLMs, including synthetic data generation and self-rewarding techniques, face limitations like narrow data distributions, alignment problems, and the issue of reward hacking, leading to unstable training and reduced policy diversity.  
3. To overcome these limitations, the authors propose DoGe (Decouple to Generalize), a dual-decoupling framework that separates the learning process into two distinct components: the Thinker, which focuses on understanding context, and the Solver, which targets problem-solving. This decoupling allows more accurate quantification of reward signals.  
4. DoGe implements a two-stage reinforcement learning post-training approach, encouraging models to freely explore context scenarios before advancing to solving tasks, thus better reflecting real-world learning progression.  
5. Additionally, DoGe enhances training data diversity through an evolving curriculum learning pipeline, which includes an expanded native domain knowledge corpus and a dynamically evolving seed problem pool. Experimental results demonstrate that this method consistently surpasses baseline models, offering a scalable path towards self-evolving large vision-language models (LVLMs). <div>
arXiv:2512.06835v1 Announce Type: new 
Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2512.06859</link>
<guid>https://arxiv.org/abs/2512.06859</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.06859v1 Keywords: table reasoning, large language model, data-centric training, reinforcement learning, workflow optimization  

<br /><br />Summary:  
JT-DA-8B (JiuTian Data Analyst 8B) is a large language model specialized for complex table reasoning across diverse real-world contexts. To overcome the scarcity of high-quality supervision in table reasoning, the authors created a diverse training corpus comprising 34 well-defined table reasoning tasks by aggregating 29 public table QA datasets and 3 million tables. They designed an automatic pipeline to generate realistic multi-step analytical tasks that involve various reasoning patterns. The model is built upon the open-source JT-Coder-8B, an 8-billion parameter decoder-only foundation model trained from scratch. For optimization, the training process uses both supervised fine-tuning (SFT) and reinforcement learning (RL), leveraging LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Additionally, a novel four-stage workflow is proposed to improve model interpretability and execution accuracy, consisting of table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering. Experimental results demonstrate that JT-DA-8B achieves strong performance across various table reasoning challenges, highlighting the effectiveness of the data-centric training approach and workflow-driven optimization framework. <div>
arXiv:2512.06859v1 Announce Type: new 
Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?</title>
<link>https://arxiv.org/abs/2512.06867</link>
<guid>https://arxiv.org/abs/2512.06867</guid>
<content:encoded><![CDATA[
<div> Persona prompting, large language models, strategic performance, heuristic generation, psychometrics<br /><br />Summary:<br /><br />1. The paper investigates the effect of persona prompting on decision-making and strategic behavior in large language models (LLMs) within an adversarial environment, using the board game PERIL as a testbed.<br />2. It compares heuristic strategies derived from persona prompts with heuristics chosen manually, aiming to understand if persona styles translate into measurable strategic advantages.<br />3. Findings show that personas tied to strategic thinking can enhance game performance, but only when a specialized mediator is used to convert persona-generated responses into heuristic values.<br />4. The mediator is introduced as a structured translation mechanism inspired by exploratory factor analysis, which maps LLM-generated inventory responses into reliable heuristics.<br />5. This method improves the reliability and face validity of heuristics compared to direct inference, enabling a clearer study of how different persona types influence LLM-based decision-making.<br />6. The study contributes to the emerging area of integrating psychometric principles with LLMs for heuristic generation and better behavioral modeling in strategic tasks.<br />7. The authors provide an open-source adversarial environment for future research, advancing our understanding of persona prompting effects on LLM decision-making and proposing a novel heuristic generation framework. <div>
arXiv:2512.06867v1 Announce Type: new 
Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Memory: A comparison of memory mechanisms in world models</title>
<link>https://arxiv.org/abs/2512.06983</link>
<guid>https://arxiv.org/abs/2512.06983</guid>
<content:encoded><![CDATA[
<div> World models, memory span, transformer, memory augmentation, loop closure  

<br /><br />Summary:  
This paper addresses the limitations of transformer-based world models in planning over long horizons due to their restricted effective memory span. It highlights how perceptual drift during extended rollouts adversely affects the model's ability to perform loop closures within imagined trajectories. To overcome this, the authors analyze several memory augmentation mechanisms, introducing a taxonomy that differentiates between memory encoding and memory injection methods. This taxonomy is motivated by examining residual stream dynamics, providing a theoretical framework for understanding memory extension in world models. The study employs a state recall evaluation task to quantitatively measure the memory recall capabilities of each mechanism, identifying their respective trade-offs. Results demonstrate that incorporating memory mechanisms significantly enhances the effective memory span of vision transformers, enabling improved consistency and accuracy in long-horizon planning. Ultimately, the findings offer a pathway for world models to complete loop closures during their imagination, which is crucial for robust and coherent planning in complex environments. <div>
arXiv:2512.06983v1 Announce Type: new 
Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</title>
<link>https://arxiv.org/abs/2512.06990</link>
<guid>https://arxiv.org/abs/2512.06990</guid>
<content:encoded><![CDATA[
<div> Glioblastoma Multiforme, AI diagnosis, Treatment planning, Reinforcement learning, Survival rate optimization<br /><br />Summary:<br /><br />This project addresses the lack of AI tools for managing heterogeneous brain tumors like Glioblastoma Multiforme (GBM), aiming to assist doctors in both diagnosis and treatment planning. First, a sequential decision-making framework employing four classification models, including Convolutional Neural Networks and Support Vector Machines, progressively refines patient brain tumor classification for accurate diagnosis. Second, the treatment planning phase uses a reinforcement learning system composed of three generative models: a diffusion model predicting possible surgical resection outcomes, a Spatio-Temporal Vision Transformer forecasting tumor progression post-radiotherapy, and another diffusion model simulating post-chemotherapy MRI results. Third, a survival rate calculator CNN evaluates if the generated post-treatment MRI meets user-defined survival targets within a 15% threshold. If not, a proximal policy optimization feedback loop iteratively refines treatment parameters to identify optimal resection strategies. Key findings include a 22.28-fold reduction in computing costs using the sequential diagnostic framework, a 113-hour decrease in tumor progression inference time enabled by transformer regression models, and a 2.9% improvement in DICE scores from realistic data augmentations. Collectively, these innovations could enhance survival rates by 0.9%, potentially saving around 2,250 lives. <div>
arXiv:2512.06990v1 Announce Type: new 
Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes</title>
<link>https://arxiv.org/abs/2512.07081</link>
<guid>https://arxiv.org/abs/2512.07081</guid>
<content:encoded><![CDATA[
<div> Keywords: Heart failure, clinical notes, readmission prediction, large language model, risk factors<br /><br />Summary: Heart failure (HF) is a major cause of rehospitalization among older adults in the US, and clinical notes in electronic health records (EHRs) contain valuable patient information that is usually underutilized for predicting HF readmission risk. Traditional models depend heavily on expert-crafted rules, medical thesauri, and ontologies, which struggle with clinical notes' informal language, abbreviations, and misspellings. The study introduces ClinNoteAgents, a multi-agent framework powered by large language models (LLMs), designed to convert free-text clinical notes into structured data representing clinical and social risk factors and clinician-style summaries for 30-day HF readmission prediction. Evaluated on 3,544 notes from 2,065 patients with a readmission rate of 35.16%, ClinNoteAgents demonstrated strong capabilities in extracting relevant risk factors, highlighting key contributing elements, and accurately predicting readmission risk. This approach reduces dependency on structured EHR fields, lowers the need for manual annotation and model training, and offers a scalable, interpretable solution suitable for healthcare systems with limited data resources. Overall, ClinNoteAgents represents an innovative use of LLMs for improving HF readmission risk modeling through effective utilization of unstructured clinical text. <div>
arXiv:2512.07081v1 Announce Type: new 
Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIGIL: A Reflective Runtime for Self-Healing Agents</title>
<link>https://arxiv.org/abs/2512.07094</link>
<guid>https://arxiv.org/abs/2512.07094</guid>
<content:encoded><![CDATA[
<div> Keywords: Agentic LLM, runtime introspection, VIGIL, autonomous maintenance, self-repair<br /><br />Summary:<br /><br />1. This paper addresses the limitations of current agentic Large Language Model (LLM) frameworks, which often struggle with brittleness, lack of runtime introspection, and inability to self-diagnose or improve autonomously over time. <br />2. Many deployed agent stacks degenerate into simple chains of LLM calls without structural methods to ensure reliability and robustness during task execution.<br />3. The authors propose VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime system designed to supervise and maintain a sibling agent rather than directly execute tasks.<br />4. VIGIL processes behavioral logs, converts events into structured emotional representations stored in an EmoBank with decay and context-aware policies, and generates RBT (Reflective Behavior Tree) diagnoses categorizing recent behaviors into strengths, opportunities, and failures.<br />5. Based on this analysis, VIGIL produces guarded prompt updates that preserve the agent’s core identity and read-only code proposals created by a strategy engine focused on log evidence and code hotspots.<br />6. The system employs a state-gated pipeline that enforces legal transitions, preventing LLM improvisation and ensuring error transparency.<br />7. In practical case studies like reminder latency, VIGIL successfully detected issues, proposed fixes, and demonstrated meta-level self-repair by surfacing internal errors, generating fallback diagnoses, and producing repair plans autonomously.<br />8. These innovations showcase how VIGIL advances agent runtime reliability through introspection, autonomous diagnosis, and iterative maintenance, enhancing long-term agent performance without human intervention. <div>
arXiv:2512.07094v2 Announce Type: new 
Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy</title>
<link>https://arxiv.org/abs/2512.07109</link>
<guid>https://arxiv.org/abs/2512.07109</guid>
<content:encoded><![CDATA[
<div> Keywords: ARC, task relatedness, taxonomy, neural affinity, compositional gap<br /><br />Summary: This paper addresses the need for a formal definition of task relatedness within the ARC (Abstraction and Reasoning Corpus) challenge by introducing the first comprehensive 9-category taxonomy encompassing all 400 ARC tasks. The taxonomy is validated with 97.5% accuracy through rule-based code analysis, demonstrating visual coherence by training a CNN on raw grid pixels with 95.24% accuracy on certain subsets. Applying this taxonomy diagnostically to the ARC-AGI-2 test set reveals that 35.3% of tasks exhibit low neural affinity for Transformer models, a distributional bias consistent with the ARC-AGI-2 dataset. Through fine-tuning a 1.7 million parameter Transformer on 302 tasks, the authors identify a "Compositional Gap": 69.5% of tasks score high on local pattern recognition (>80% cell accuracy) but very low on global synthesis (<10% grid accuracy). This demonstrates a Neural Affinity Ceiling Effect where architectural limitations cap performance irrespective of curriculum design. Validation against an independent ViTARC study confirms that tasks with low neural affinity perform significantly worse than those with high affinity, some even at 0% despite extensive data. The taxonomy thus offers precise diagnosis of task difficulty related to architectural suitability. The authors conclude that advancing ARC performance requires hybrid architectures with modules aligned to task affinity, and they release their validated taxonomy for community use. <div>
arXiv:2512.07109v1 Announce Type: new 
Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation</title>
<link>https://arxiv.org/abs/2512.07178</link>
<guid>https://arxiv.org/abs/2512.07178</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable AI, SHAP, large language model, contextual explanations, healthcare user study  

<br /><br />Summary:  
The paper addresses the limitations of SHAP (SHapley Additive exPlanations) in providing only visual feature importance without contextual explanations that are accessible to non-technical end-users. To bridge this gap, the authors propose a novel Python package that integrates SHAP with a large language model (LLM), specifically OpenAI's GPT, to generate textual, contextualized explanations. This integration allows customization using user-defined parameters such as feature aliases, descriptions, and background information to tailor explanations according to both the model and user perspectives. The aim is to enhance the perceived understandability and usability of SHAP outputs. To evaluate their approach, the authors conducted a healthcare case study and user evaluations involving real end-users. These evaluations used Likert-scale surveys and follow-up interviews to assess the understandability and contextual appropriateness of the generated explanations. Results showed that combining SHAP visualizations with LLM-generated text explanations led to improvements in users' perception of clarity and trustworthiness compared to visual-only explanations. Although preliminary, these findings suggest that adding contextualized textual explanations to SHAP can make AI model interpretations more user-friendly and trustworthy, particularly in sensitive domains such as healthcare. <div>
arXiv:2512.07178v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations</title>
<link>https://arxiv.org/abs/2512.07179</link>
<guid>https://arxiv.org/abs/2512.07179</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Tracing, Intelligent Tutoring Systems, Cold Start, Knowledge Map, Personalized Learning<br /><br />Summary:<br />1. This paper addresses limitations in current Knowledge Tracing (KT) models, which are critical for Intelligent Tutoring Systems (ITS) that personalize learning by tracking student knowledge over time.<br />2. Existing KT models face challenges such as limited input data flexibility, cold start problems with new students or questions, and lack of stability in practical environments.<br />3. To overcome these, the authors propose the Practical Interlinked Concept Knowledge Tracing (PICKT) model, which integrates multiple input data types and uses a knowledge map that links concepts via question and concept text information.<br />4. PICKT’s knowledge map structure allows it to effectively handle cold start scenarios, improving performance when new students enroll or new questions are added.<br />5. Extensive experiments modeled after real-world operational conditions demonstrate PICKT’s enhanced prediction accuracy, stability, and suitability for deployment in production ITS environments, establishing a theoretical and technical foundation for next-generation personalized learning systems. <div>
arXiv:2512.07179v1 Announce Type: new 
Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation</title>
<link>https://arxiv.org/abs/2512.07212</link>
<guid>https://arxiv.org/abs/2512.07212</guid>
<content:encoded><![CDATA[
<div> Keywords: Imitation learning, diffusion models, robotic control, diffusion bridge, multi-modal fusion<br /><br />Summary: Imitation learning with diffusion models has improved robotic control by modeling multi-modal action distributions but typically treats observations as high-level conditions rather than integrating them into the diffusion process. This results in sampling from random Gaussian noise, weakening perception-control coupling and often reducing performance. The proposed BridgePolicy method explicitly embeds observations within the stochastic differential equation through a diffusion-bridge framework, enabling sampling to begin from an informative prior instead of noise. This leads to enhanced precision and reliability in control tasks. A main challenge is that classical diffusion bridges require matching dimensions, whereas robotic data is heterogeneous and multi-modal, with observations and actions belonging to different spaces. To overcome this, BridgePolicy incorporates a multi-modal fusion module and a semantic aligner to unify visual and state inputs and align observation and action representations, making the bridge formulation applicable to varied robot data. Extensive experiments involve 52 simulation tasks across three benchmarks and five real-world tasks, where BridgePolicy consistently outperforms previous state-of-the-art generative policy methods, demonstrating its effectiveness for visuomotor control. <div>
arXiv:2512.07212v1 Announce Type: new 
Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model</title>
<link>https://arxiv.org/abs/2512.07232</link>
<guid>https://arxiv.org/abs/2512.07232</guid>
<content:encoded><![CDATA[
<div> Product matching, Entity Alignment, Knowledge Graphs, Graph Attention Networks, Cross-lingual datasets<br /><br />Summary:<br /><br />This paper addresses the problem of product matching across different platforms by converting it into an Entity Alignment (EA) task using knowledge graphs (KGs). It highlights a limitation in existing EA methods which do not sufficiently exploit both attribute triples and relation triples together, especially their interactions. To overcome this, the authors propose a two-stage pipeline consisting of a rough filter followed by a fine filter to improve matching between products from eBay and Amazon. The core innovation is the Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA) framework, which effectively captures interactions between attribute and relation triples. RAEA incorporates an Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks to aggregate alignment signals from both attributes and relations into enhanced entity representations. Experimental results demonstrate that RAEA significantly outperforms 12 baseline EA methods, achieving an average improvement of 6.59% on Hits@1 in the cross-lingual DBP15K dataset and shows competitive performance on the monolingual DWY100K dataset. Additionally, the source code is publicly available on GitHub, facilitating reproducibility and further research in this area of product matching and EA. <div>
arXiv:2512.07232v1 Announce Type: new 
Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling</title>
<link>https://arxiv.org/abs/2512.07314</link>
<guid>https://arxiv.org/abs/2512.07314</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, synthetic trajectory generation, multi-scale spatiotemporal modeling, autoregressive model, Transformer decoder<br /><br />Summary:<br /><br />1. Modeling human mobility is crucial for applications like transportation planning and epidemic modeling, requiring accurate trajectory generation methods.  
2. Current Artificial Intelligence Generated Content (AIGC) approaches such as autoregressive and diffusion models effectively generate single-day trajectories but struggle with inefficiencies in long-term trajectory generation (e.g., weekly data) and lack explicit multi-scale spatiotemporal modeling.  
3. The study introduces M-STAR (Multi-Scale Spatio-Temporal AutoRegression), a novel framework designed to generate long-term human mobility trajectories via a coarse-to-fine spatiotemporal prediction process.  
4. M-STAR leverages a Multi-scale Spatiotemporal Tokenizer to encode hierarchical mobility patterns combined with a Transformer-based decoder that performs next-scale autoregressive prediction, enabling efficient and scalable trajectory generation.  
5. Experimental results on two real-world datasets demonstrate that M-STAR not only outperforms existing state-of-the-art methods in fidelity but also significantly enhances generation speed, confirming its effectiveness for long-term mobility modeling. The authors provide data and code publicly at the specified GitHub repository for reproducibility and further research. <div>
arXiv:2512.07314v1 Announce Type: new 
Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[
<div> Keywords: Concept Bottleneck Models, Sparse Autoencoders, interpretability, concept cones, geometric alignment<br /><br />Summary:<br /><br />1. The paper explores two distinct interpretability approaches in machine learning: Concept Bottleneck Models (CBMs), which use supervision to align model activations with human-labeled concepts, and Sparse Autoencoders (SAEs), which uncover emergent concepts via unsupervised sparse coding.<br /><br />2. The authors reveal a shared geometric structure underlying both paradigms, where each learns a set of linear directions in activation space whose nonnegative combinations define a “concept cone.” The key difference lies in how these cones are selected—supervised in CBMs and unsupervised in SAEs.<br /><br />3. They introduce a framework that bridges the two methods by using CBMs as human-defined reference geometries and evaluating SAEs based on how well their learned cones approximate or contain those defined by CBMs.<br /><br />4. This containment framework allows for quantitative metrics that connect SAE inductive biases—such as type, sparsity, and expansion ratio—to the emergence of concepts that are plausible, meaning they align well with human intuition and domain knowledge.<br /><br />5. Applying these metrics, they identify an optimal range of sparsity and expansion factor that maximizes both geometric and semantic alignment between SAE-discovered concepts and CBM concepts, thus unifying supervised and unsupervised concept discovery within a principled geometric and evaluative framework. <div>
arXiv:2512.07355v1 Announce Type: new 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services</title>
<link>https://arxiv.org/abs/2512.07436</link>
<guid>https://arxiv.org/abs/2512.07436</guid>
<content:encoded><![CDATA[
<div> Keywords: LocalSearchBench, agentic search, multi-hop reasoning, local life services, benchmark

<br /><br />Summary:  
1. The paper introduces LocalSearchBench, the first comprehensive benchmark designed for agentic search systems in the domain of local life services, which includes services such as merchants and products across various cities and business types.  
2. Local life services queries are complex, often ambiguous, and require multi-hop reasoning—retrieving and integrating information from multiple steps and sources—posing unique challenges rarely addressed by previous general information retrieval models.  
3. LocalSearchBench consists of over 150,000 high-quality entries and 300 multi-hop question-answering tasks derived from real user queries to rigorously evaluate reasoning and information retrieval capabilities.  
4. The authors also present LocalPlayground, a unified platform integrating various tools to facilitate agent interaction and experimentation in these local life service scenarios.  
5. Experimental results reveal that current state-of-the-art large reasoning models (LRMs), including the best-performing DeepSeek-V3.1, achieve limited success (34.34% correctness), and generally struggle with answer completeness (77.33% average) and faithfulness (61.99% average), underscoring the necessity of specialized benchmarks and domain-specific training to better address the challenges in local life services.  
6. The codebase, benchmark dataset, and leaderboard for LocalSearchBench are publicly available at localsearchbench.github.io. <div>
arXiv:2512.07436v1 Announce Type: new 
Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations</title>
<link>https://arxiv.org/abs/2512.07497</link>
<guid>https://arxiv.org/abs/2512.07497</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, autonomous agents, tool-use, failure modes, agentic robustness  

<br /><br />Summary:  
This study examines the failure modes of large language models (LLMs) acting as autonomous agents with tool-use capabilities, using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark. It analyzes 900 execution traces from three models—Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1—across tasks involving filesystem manipulation, text extraction, CSV analysis, and SQL queries. Instead of relying on aggregate scores, the research focuses on fine-grained, per-trial behavioral analysis to uncover strategies that facilitate successful multi-step tool execution and identify recurrent failures that reduce reliability. The findings indicate that model scale alone does not reliably predict agentic robustness: Llama 4 Maverick (400B parameters) only slightly outperforms Granite 4 Small (32B) in uncertainty-driven tasks. The superior reliability of DeepSeek V3.1 is attributed primarily to post-training reinforcement learning rather than model architecture or size. Four common failure archetypes emerge across models: premature actions without proper grounding, over-helpfulness that replaces missing entities inaccurately, susceptibility to distractor-induced context pollution, and fragile execution under workload stress. These insights emphasize the need for agentic evaluation methods focused on interactive grounding, recovery mechanisms, and environment-aware adaptation. For reliable enterprise deployment, the study suggests prioritizing deliberate training strategies and design decisions supporting verification, constraint discovery, and strict adherence to source-of-truth data. <div>
arXiv:2512.07497v2 Announce Type: new 
Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title>
<link>https://arxiv.org/abs/2512.07611</link>
<guid>https://arxiv.org/abs/2512.07611</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, PPO, GRPO, DAPO, Large Language Models<br /><br />Summary: This study systematically compares three reinforcement learning algorithms—PPO, GRPO, and DAPO—for enhancing complex reasoning in large language models (LLMs). The main contribution is a controlled transfer-learning evaluation where models are first fine-tuned on the specialized Countdown Game and then tested on a variety of general-purpose reasoning benchmarks. Results show that RL-trained models consistently outperform their base, non-RL-trained counterparts across all tasks, although the level of improvement varies by benchmark. The study provides a detailed parametric analysis offering practical insights for RL-based LLM training. Notably, increasing the group size in GRPO and DAPO yields more stable training dynamics and improves accuracy. Conversely, the effect of the KL-penalty coefficient is non-monotonic, indicating that tuning this parameter requires careful balancing. An additional finding reveals that the Dynamic Sampling (DS) component in DAPO does not contribute positively to performance. In fact, the highest accuracy across all evaluated tasks is achieved by using DAPO without the DS component. Overall, the paper delivers important guidance on how to effectively apply and tune RL algorithms for improving complex reasoning capabilities in LLMs. <div>
arXiv:2512.07611v1 Announce Type: new 
Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds</title>
<link>https://arxiv.org/abs/2512.07631</link>
<guid>https://arxiv.org/abs/2512.07631</guid>
<content:encoded><![CDATA[
<div> Agent Capability Problem, resource constraints, information acquisition, cost prediction, agent performance<br /><br />Summary:<br /><br />1. The paper introduces the Agent Capability Problem (ACP), a theoretical framework designed to predict when and how an autonomous agent should allocate resources to solve a task efficiently under given constraints.<br /><br />2. ACP models problem-solving as an information acquisition process, quantifying the total amount of information needed to find a solution (\(\Itotal\)) and the information gained per action (\(\Istep\)), combined with the cost per action (\(\Cstep\)), to formulate an effective cost metric \(\Ceff = (\Itotal/\Istep) \times \Cstep\).<br /><br />3. The effective cost \(\Ceff\) provides a theoretically grounded lower bound on the expected resource expenditure necessary for problem resolution and is accompanied by mathematically derived tight probabilistic upper bounds.<br /><br />4. Extensive experiments demonstrate that ACP’s predictions align closely with real agent performance, consistently providing reliable bounds on search effort and outperforming baseline strategies such as greedy or random approaches.<br /><br />5. The framework unifies diverse disciplines—including active learning, Bayesian optimization, and reinforcement learning—under an information-theoretic perspective, and it generalizes well across settings involving large language model (LLM)-based agents and more general agentic workflows. <div>
arXiv:2512.07631v1 Announce Type: new 
Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE</title>
<link>https://arxiv.org/abs/2512.07710</link>
<guid>https://arxiv.org/abs/2512.07710</guid>
<content:encoded><![CDATA[
<div> Keywords: MoE reasoning model, reinforcement learning, zero-variance prompts, entropy-adaptive optimization, Router Replay<br /><br />Summary:<br /><br />1. The paper introduces CompassMax-V3-Thinking, a hundred-billion-parameter Mixture of Experts (MoE) reasoning model trained via a novel reinforcement learning (RL) framework centered on the principle that each prompt must contribute informatively to training.<br /><br />2. Scaling RL to this extremely large model size uncovers major inefficiencies such as zero-variance prompts that waste computational resources by producing uninformative rollouts, unstable importance sampling methods over long sequences, problems with advantage reversal when using standard reward models, and systemic pipeline bottlenecks.<br /><br />3. To address these, the authors propose Multi-Stage Zero-Variance Elimination which identifies and filters out non-informative prompts, thereby stabilizing group-based policy optimization techniques like GRPO.<br /><br />4. They present ESPO, an entropy-adaptive optimization algorithm designed to balance token-level and sequence-level importance sampling, ensuring stable and efficient learning dynamics within the RL training.<br /><br />5. The framework includes a Router Replay strategy to align MoE router decisions during training and inference, preventing discrepancies and advantage inversion through reward model adjustments.<br /><br />6. Further, a high-throughput RL system employing FP8 precision for rollouts, overlapped reward computations, and length-aware scheduling is introduced to remove performance bottlenecks.<br /><br />Together, these innovations form a unified pipeline enabling stable and efficient RL training of hundred-billion-scale MoE models, resulting in a model that performs strongly on both internal and public benchmarks. <div>
arXiv:2512.07710v1 Announce Type: new 
Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2512.07761</link>
<guid>https://arxiv.org/abs/2512.07761</guid>
<content:encoded><![CDATA[
<div> Keywords: jailbreak attacks, large language models, multi-turn reinforcement learning, black-box models, harmful content  

<br /><br />Summary:  
This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks, which pose significant risks for their safe deployment. The authors focus on black-box multi-turn jailbreak attacks, where attacker LLMs generate a sequence of prompt-output interactions to coax harmful content from target models without internal access. Unlike existing methods that optimize single-turn attacks, the study formulates the problem as a multi-turn reinforcement learning task, optimizing the harmfulness of the final output as the primary reward. To tackle challenges such as sparse supervision and encourage long-term strategic attacks, the paper proposes two heuristic intermediate rewards: (1) controlling the harmfulness level of intermediate outputs to avoid triggering the model’s rejection mechanisms early, and (2) maintaining semantic relevance across turns to prevent conversation drift. Experimental evaluations on multiple benchmarks and models demonstrate improved attack success rates, validating the efficacy of the multi-turn reinforcement learning framework. The study highlights the importance of considering multi-turn dynamics in jailbreak attack development and provides an open-source codebase for further research. However, it also issues a warning about the presence of harmful content examples within the paper. <div>
arXiv:2512.07761v1 Announce Type: new 
Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.07795</link>
<guid>https://arxiv.org/abs/2512.07795</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reasoning Stability, Multi-run Evaluation, Reproducibility, ReasonBENCH<br /><br />Summary:<br /><br />Large language models (LLMs) are increasingly used for reasoning tasks that require multi-step problem solving and chain-of-thought approaches. Current evaluation practices predominantly report single-run accuracy, disregarding the intrinsic uncertainty caused by stochastic decoding in LLMs. This leads to a significant blind spot, as the stability, reproducibility, and cost-consistency of reasoning methods remain unclear. To address this, the authors introduce ReasonBENCH, the first benchmark developed to measure instability in LLM reasoning. ReasonBENCH offers a modular evaluation library that standardizes reasoning frameworks, models, and tasks, a multi-run protocol for statistically reliable metrics on quality and cost, and a public leaderboard promoting variance-aware reporting. Experimental results across multiple domains reveal that most reasoning strategies and models suffer from high instability, with some showing confidence intervals up to four times wider despite similar average performance. Frequently, the top-performing methods impose higher and less stable computational costs, undermining reproducibility and reliability in performance claims. The paper also explores how prompts, model families, and model scale influence the trade-off between solve rate and stability. Overall, this work stresses reproducibility as a crucial factor for dependable LLM reasoning and lays groundwork for future advancements in reasoning methods and uncertainty quantification. ReasonBENCH is made publicly available for the research community. <div>
arXiv:2512.07795v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Causal Models from Large Language Models</title>
<link>https://arxiv.org/abs/2512.07796</link>
<guid>https://arxiv.org/abs/2512.07796</guid>
<content:encoded><![CDATA[
<div> Keywords: large causal models, large language models, causal inference, DEMOCRITUS, categorical machine learning<br /><br />Summary:  
This paper introduces a new approach for creating large causal models (LCMs) by leveraging the strengths of modern large language models (LLMs). The authors implemented a system named DEMOCRITUS, designed to build, organize, and visualize LCMs across a wide range of domains by extracting causal information from textual queries directed at LLMs. Unlike traditional causal inference methods that rely on experimental numerical data in narrow domains, DEMOCRITUS uses LLMs to suggest topics, formulate causal questions, and extract plausible causal statements from diverse fields. The main technical challenge addressed is synthesizing isolated, fragmented, ambiguous, and potentially conflicting causal claims into a coherent structure of relational causal triples embedded within an LCM. To solve this, novel categorical machine learning methods were developed, although these are only briefly mentioned as the focus is on system implementation. The implementation pipeline consists of six distinct modules, and the authors analyze the system’s computational costs to identify bottlenecks restricting scalability. Demonstrations show the system’s effectiveness across varied domains including archaeology, biology, climate change, economics, medicine, and technology. The paper concludes by discussing current limitations of DEMOCRITUS and proposing future enhancements to extend its capabilities. <div>
arXiv:2512.07796v1 Announce Type: new 
Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Games for Sandbagging</title>
<link>https://arxiv.org/abs/2512.07810</link>
<guid>https://arxiv.org/abs/2512.07810</guid>
<content:encoded><![CDATA[
<div> Keywords: sandbagging, AI evaluation, detection methods, capability elicitation, model auditing<br /><br />Summary:<br /><br />This study investigates the challenge of AI systems concealing their true capabilities, known as sandbagging, during evaluations, which can mislead developers and auditors. To assess detection techniques, an auditing game was designed involving two teams: a red team that fine-tuned five AI models to underperform selectively as a proxy for sandbagging, and a blue team tasked with identifying these sandbagging models using black-box, model-internal, and training-based methods. Results showed that the blue team struggled to reliably distinguish sandbaggers from benign models. Black-box methods were easily fooled by imitating weaker models, while linear probes from model internals showed some potential but were vulnerable to manipulation. The study also explored capability elicitation strategies; prompt-based elicitation was unreliable, but training-based elicitation effectively revealed full capabilities in sandbagging models with just one correct demonstration, though it occasionally boosted benign model performance, increasing false positives. As short-term advice, the authors recommend removing sandbagging potential by using on-distribution training during elicitation. For the long term, further research is needed to validate training-based elicitation and develop robust sandbagging detection methods. The study’s models, data, and a demo game are publicly accessible for further use. <div>
arXiv:2512.07810v1 Announce Type: new 
Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices</title>
<link>https://arxiv.org/abs/2512.05969</link>
<guid>https://arxiv.org/abs/2512.05969</guid>
<content:encoded><![CDATA[
<div> Video generation, reasoning, Task Pair design, evaluation framework, reinforcement learning<br /><br />Summary:<br /><br />This paper demonstrates that state-of-the-art video generation models have acquired reasoning capabilities. The authors test these models on diverse reasoning tasks including chess, maze navigation, Sudoku, mental rotation, and Raven's Matrices, with leading models like Sora-2 achieving approximately 60% success rates. They introduce a robust experimental paradigm based on a "Task Pair" design, which effectively evaluates reasoning in video models. To support this paradigm, a code framework is developed and released, featuring 39 available models and facilitating easy integration of new tasks and models for scalable research. Their automated evaluation method is validated by a strong correlation with human judgment, suggesting it is reliable and can be efficiently scaled. Finally, the authors highlight the potential for future work, especially in applying reinforcement learning techniques to further improve reasoning abilities in video generation models. All experimental results and the evaluation codebase (VMEvalKit) are made publicly accessible for the research community to advance this line of study. <div>
arXiv:2512.05969v1 Announce Type: cross 
Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-objective Optimization Approach for Feature Selection in Gentelligent Systems</title>
<link>https://arxiv.org/abs/2512.05971</link>
<guid>https://arxiv.org/abs/2512.05971</guid>
<content:encoded><![CDATA[
<div> Keywords: Gentelligent system, fault detection, multi-objective evolutionary algorithm, manufacturing optimization, predictive methods<br /><br />Summary:  
This paper introduces the concept of a "Gentelligent system," which integrates inherent component information into manufacturing processes, drawing an analogy to genes and chromosomes from bioinformatics. The aim is to develop intelligent systems that enhance automation and efficiency in manufacturing. A key focus is on reliable fault detection methods that can lead to improved product quality, increased yield, and reduced production costs. To achieve these goals, the authors propose a hybrid framework that employs a dominance-based multi-objective evolutionary algorithm. This algorithm simultaneously optimizes feature selection and classification performance by exploring Pareto-optimal solutions in a single run, effectively handling conflicting objectives that need to be minimized during manufacturing operations. The approach supports monitoring various manufacturing processes, thereby helping manufacturers adapt to emerging industry trends more effectively. The model's robustness and applicability are validated using two real-world datasets from different industrial domains. Results from these datasets demonstrate the generalizability and effectiveness of the proposed framework, highlighting its potential for broader application in fault detection and predictive manufacturing systems. <div>
arXiv:2512.05971v1 Announce Type: cross 
Abstract: The integration of advanced technologies, such as Artificial Intelligence (AI), into manufacturing processes is attracting significant attention, paving the way for the development of intelligent systems that enhance efficiency and automation. This paper uses the term "Gentelligent system" to refer to systems that incorporate inherent component information (akin to genes in bioinformatics-where manufacturing operations are likened to chromosomes in this study) and automated mechanisms. By implementing reliable fault detection methods, manufacturers can achieve several benefits, including improved product quality, increased yield, and reduced production costs. To support these objectives, we propose a hybrid framework with a dominance-based multi-objective evolutionary algorithm. This mechanism enables simultaneous optimization of feature selection and classification performance by exploring Pareto-optimal solutions in a single run. This solution helps monitor various manufacturing operations, addressing a range of conflicting objectives that need to be minimized together. Manufacturers can leverage such predictive methods and better adapt to emerging trends. To strengthen the validation of our model, we incorporate two real-world datasets from different industrial domains. The results on both datasets demonstrate the generalizability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction</title>
<link>https://arxiv.org/abs/2512.05979</link>
<guid>https://arxiv.org/abs/2512.05979</guid>
<content:encoded><![CDATA[
<div> Keywords: chemical processes, machine learning, process-graph representation, graph neural network, transfer learning<br /><br />Summary: This article addresses the challenge of slow and costly experimental validation in chemical process discovery. It highlights the difficulty of utilizing heterogeneous data from patents and literature due to its unstructured nature. To overcome this, the authors propose a universal directed-tree process-graph representation that integrates unstructured text, molecular structures, and numeric measurements into a unified machine-readable format. They develop a multi-modal graph neural network equipped with a property-conditioned attention mechanism to effectively learn from this structured data. The model is trained on a large dataset comprising approximately 700,000 process graphs extracted from nearly 9,000 diverse documents, enabling it to learn rich semantic embeddings that generalize across various domains. Furthermore, the paper demonstrates that by fine-tuning this pretrained model on smaller, domain-specific datasets, it achieves strong predictive performance, showcasing effective transfer learning. This approach significantly reduces the amount of additional data required for specialized tasks, making it a powerful tool for accelerating materials and chemical process discovery through machine learning. <div>
arXiv:2512.05979v1 Announce Type: cross 
Abstract: Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlockVote: LLM-Empowered Agent-Based Modeling for Simulating U.S. Presidential Elections</title>
<link>https://arxiv.org/abs/2512.05982</link>
<guid>https://arxiv.org/abs/2512.05982</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, agent-based modeling, voter behavior, political simulation, 2024 U.S. Presidential Election

<br /><br />Summary:  
The paper introduces FlockVote, a new framework leveraging Large Language Models (LLMs) to simulate human voting behavior in political elections. Unlike traditional agent-based models that rely on simplistic rules or large-scale statistical models that often lack transparency, FlockVote creates a "computational laboratory" by instantiating LLM agents with detailed demographic profiles and dynamic contextual inputs such as candidate policies. This setup allows agents to perform nuanced, generative reasoning to replicate real-world voting decisions. The framework is applied to the 2024 U.S. Presidential Election, targeting seven key swing states, and the results from the simulation closely match the actual election outcomes, showcasing the model’s high fidelity in capturing the complexities of a virtual society. Beyond mere prediction accuracy, FlockVote serves as an interpretable tool for computational social science research, offering insights into individual agent rationales and enabling the examination of the stability and sensitivity of social simulations driven by LLMs. This approach represents a significant advancement by moving away from opaque "black-box" models and promoting transparency and detailed understanding in political behavior modeling. <div>
arXiv:2512.05982v1 Announce Type: cross 
Abstract: Modeling complex human behavior, such as voter decisions in national elections, is a long-standing challenge for computational social science. Traditional agent-based models (ABMs) are limited by oversimplified rules, while large-scale statistical models often lack interpretability. We introduce FlockVote, a novel framework that uses Large Language Models (LLMs) to build a "computational laboratory" of LLM agents for political simulation. Each agent is instantiated with a high-fidelity demographic profile and dynamic contextual information (e.g. candidate policies), enabling it to perform nuanced, generative reasoning to simulate a voting decision. We deploy this framework as a testbed on the 2024 U.S. Presidential Election, focusing on seven key swing states. Our simulation's macro-level results successfully replicate the real-world outcome, demonstrating the high fidelity of our "virtual society". The primary contribution is not only the prediction, but also the framework's utility as an interpretable research tool. FlockVote moves beyond black-box outputs, allowing researchers to probe agent-level rationale and analyze the stability and sensitivity of LLM-driven social simulations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dataset Quantization: A New Direction for Dataset Pruning</title>
<link>https://arxiv.org/abs/2512.05987</link>
<guid>https://arxiv.org/abs/2512.05987</guid>
<content:encoded><![CDATA[
<div> Dataset Quantization, Edge Devices, Intra-sample Redundancy, Adaptive Quantization, Dataset Compression<br /><br />Summary:<br /><br />This paper tackles the problem of high storage and communication costs of large-scale datasets on resource-constrained edge devices by proposing a novel quantization approach aimed at reducing intra-sample redundancy, unlike existing methods that target redundancy between samples. The method compresses each image by removing redundant or less informative information within individual samples while preserving essential features. Initially, a linear symmetric quantization is applied to determine the quantization range and scale for each sample. Following this, an adaptive quantization allocation algorithm dynamically assigns different quantization ratios to samples based on their precision requirements, ensuring a fixed overall compression ratio. The paper’s key contributions include being the first to represent datasets using limited bits for storage reduction, proposing a dataset-level quantization algorithm with adaptive ratio allocation, and extensive validation on benchmark datasets CIFAR-10, CIFAR-100, and ImageNet-1K. Experimental results demonstrate that this approach maintains model training performance while significantly compressing the dataset, outperforming traditional quantization and dataset pruning techniques at equivalent compression levels. This method offers a promising solution for deploying large datasets in scenarios with limited storage and bandwidth. <div>
arXiv:2512.05987v1 Announce Type: cross 
Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene representation, multi-view fusion, semantic occupancy, Gaussian representation, nuScenes benchmark<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating coherent 3D scene representations from multi-view images, a task where existing methods often suffer from fragmented and inconsistent 3D reconstructions due to view-by-view processing.  
2. The authors propose VG3T, a novel multi-view feed-forward network that directly predicts a 3D semantic occupancy using a 3D Gaussian representation, moving beyond prior single-view inference approaches.  
3. VG3T predicts semantically attributed Gaussians jointly across all views, overcoming fragmentation and inconsistency, and offers a unified representation for both geometry and semantics.  
4. To combat the distance-dependent density bias common in pixel-aligned Gaussian initialization, two key techniques are introduced: Grid-Based Sampling and Positional Refinement, enhancing prediction accuracy.  
5. Experimental results on the nuScenes benchmark demonstrate VG3T’s superior efficiency and performance, achieving a 1.7 percentage point improvement in mean Intersection over Union (mIoU) while utilizing 46% fewer Gaussian primitives compared to the previous state-of-the-art method. This highlights the model’s effectiveness in producing more accurate and compact 3D scene representations. <div>
arXiv:2512.05988v1 Announce Type: cross 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology</title>
<link>https://arxiv.org/abs/2512.05993</link>
<guid>https://arxiv.org/abs/2512.05993</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, neuropathology, neurodegenerative diseases, brain tissue, digital pathology<br /><br />Summary:<br /><br />1. Foundation models have significantly advanced computational pathology by leveraging large-scale histology datasets to create generalizable representations.  
2. Most existing models are trained on surgical pathology data that predominantly includes non-nervous tissues and overrepresents diseases unrelated to neurology, hindering their applicability to brain-specific histopathological features.  
3. Neuropathology differs fundamentally due to unique cellular types, distinct tissue architecture, and characteristic pathological signs like neurofibrillary tangles and amyloid plaques found in disorders such as Alzheimer's and Parkinson's disease.  
4. The domain mismatch limits current foundation models’ ability to accurately interpret neurodegenerative diseases, necessitating specialized training on brain tissue images.  
5. The authors developed NeuroFM, a foundation model trained on whole-slide images of brain tissue covering diverse neurodegenerative pathologies, which outperforms general models in tasks like mixed dementia classification, hippocampal segmentation, and identification of cerebellar ataxias.  
6. NeuroFM demonstrates that domain-focused foundation models better capture neuropathological features and improve AI-based analysis accuracy for brain disease diagnosis and research.  
7. This work highlights the importance and effectiveness of creating specialized AI models tailored to unique morphological domains within digital pathology. <div>
arXiv:2512.05993v1 Announce Type: cross 
Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening</title>
<link>https://arxiv.org/abs/2512.05994</link>
<guid>https://arxiv.org/abs/2512.05994</guid>
<content:encoded><![CDATA[
<div> Keywords: children's speech, foundation model, speech alignment, KidSpeak, FASA<br /><br />Summary:<br /><br />1. The paper addresses the challenge of adapting AI models to effectively process children's speech, which differs significantly from adult speech and poses difficulties for current models trained mainly on adult datasets. <br />2. It introduces KidSpeak, a multi-task, speech-enhanced Foundation Model tailored specifically for children's speech, capable of handling both generative and discriminative tasks by incorporating phonetic knowledge through a two-stage training approach.<br />3. KidSpeak achieves an average accuracy of 87% across four separate speech-related tasks, demonstrating its robustness and adaptability for children's speech patterns.<br />4. To improve data annotation and alignment, the authors propose Flexible and Automatic Speech Aligner (FASA), a novel tool that enhances the quality of speech alignment, especially for noisy and unstructured children’s speech data.<br />5. The FASA tool significantly improves alignment quality by 13.6 times compared to human annotations, validated on the CHILDES dataset, making it a valuable resource for building high-quality datasets necessary for training and evaluation.<br />6. Together, KidSpeak and FASA comprise the first comprehensive system aimed at speech and language therapy applications for children, combining a powerful speech language model with a robust alignment method.<br /><br />These contributions advance AI's capability in educational and therapeutic contexts for children's speech, addressing a critical gap in existing technology. <div>
arXiv:2512.05994v1 Announce Type: cross 
Abstract: With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POrTAL: Plan-Orchestrated Tree Assembly for Lookahead</title>
<link>https://arxiv.org/abs/2512.06002</link>
<guid>https://arxiv.org/abs/2512.06002</guid>
<content:encoded><![CDATA[
arXiv:2512.06002v1 Announce Type: cross 
Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization</title>
<link>https://arxiv.org/abs/2512.06006</link>
<guid>https://arxiv.org/abs/2512.06006</guid>
<content:encoded><![CDATA[
arXiv:2512.06006v1 Announce Type: cross 
Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining</title>
<link>https://arxiv.org/abs/2512.06018</link>
<guid>https://arxiv.org/abs/2512.06018</guid>
<content:encoded><![CDATA[
arXiv:2512.06018v1 Announce Type: cross 
Abstract: Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2512.06020</link>
<guid>https://arxiv.org/abs/2512.06020</guid>
<content:encoded><![CDATA[
arXiv:2512.06020v1 Announce Type: cross 
Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06032</link>
<guid>https://arxiv.org/abs/2512.06032</guid>
<content:encoded><![CDATA[
arXiv:2512.06032v1 Announce Type: cross 
Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Deepfake Detection for Voice Authentication Systems</title>
<link>https://arxiv.org/abs/2512.06040</link>
<guid>https://arxiv.org/abs/2512.06040</guid>
<content:encoded><![CDATA[
arXiv:2512.06040v1 Announce Type: cross 
Abstract: Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-SPT: Automating Semantic Preserving Transformations for Code</title>
<link>https://arxiv.org/abs/2512.06042</link>
<guid>https://arxiv.org/abs/2512.06042</guid>
<content:encoded><![CDATA[
arXiv:2512.06042v1 Announce Type: cross 
Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework</title>
<link>https://arxiv.org/abs/2512.06046</link>
<guid>https://arxiv.org/abs/2512.06046</guid>
<content:encoded><![CDATA[
arXiv:2512.06046v1 Announce Type: cross 
Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Road of Adaptive AI for Precision in Cybersecurity</title>
<link>https://arxiv.org/abs/2512.06048</link>
<guid>https://arxiv.org/abs/2512.06048</guid>
<content:encoded><![CDATA[
arXiv:2512.06048v1 Announce Type: cross 
Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring</title>
<link>https://arxiv.org/abs/2512.06060</link>
<guid>https://arxiv.org/abs/2512.06060</guid>
<content:encoded><![CDATA[
arXiv:2512.06060v1 Announce Type: cross 
Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models</title>
<link>https://arxiv.org/abs/2512.06062</link>
<guid>https://arxiv.org/abs/2512.06062</guid>
<content:encoded><![CDATA[
arXiv:2512.06062v1 Announce Type: cross 
Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</title>
<link>https://arxiv.org/abs/2512.06065</link>
<guid>https://arxiv.org/abs/2512.06065</guid>
<content:encoded><![CDATA[
arXiv:2512.06065v1 Announce Type: cross 
Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empathy by Design: Aligning Large Language Models for Healthcare Dialogue</title>
<link>https://arxiv.org/abs/2512.06097</link>
<guid>https://arxiv.org/abs/2512.06097</guid>
<content:encoded><![CDATA[
arXiv:2512.06097v1 Announce Type: cross 
Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06102</link>
<guid>https://arxiv.org/abs/2512.06102</guid>
<content:encoded><![CDATA[
arXiv:2512.06102v1 Announce Type: cross 
Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation</title>
<link>https://arxiv.org/abs/2512.06105</link>
<guid>https://arxiv.org/abs/2512.06105</guid>
<content:encoded><![CDATA[
arXiv:2512.06105v1 Announce Type: cross 
Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity</title>
<link>https://arxiv.org/abs/2512.06106</link>
<guid>https://arxiv.org/abs/2512.06106</guid>
<content:encoded><![CDATA[
arXiv:2512.06106v1 Announce Type: cross 
Abstract: What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06112</link>
<guid>https://arxiv.org/abs/2512.06112</guid>
<content:encoded><![CDATA[
arXiv:2512.06112v1 Announce Type: cross 
Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples</title>
<link>https://arxiv.org/abs/2512.06123</link>
<guid>https://arxiv.org/abs/2512.06123</guid>
<content:encoded><![CDATA[
arXiv:2512.06123v1 Announce Type: cross 
Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting</title>
<link>https://arxiv.org/abs/2512.06134</link>
<guid>https://arxiv.org/abs/2512.06134</guid>
<content:encoded><![CDATA[
arXiv:2512.06134v1 Announce Type: cross 
Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($\alpha$) and biological ($\beta$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Invariant Graph Representations Through Redundant Information</title>
<link>https://arxiv.org/abs/2512.06154</link>
<guid>https://arxiv.org/abs/2512.06154</guid>
<content:encoded><![CDATA[
arXiv:2512.06154v1 Announce Type: cross 
Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification</title>
<link>https://arxiv.org/abs/2512.06172</link>
<guid>https://arxiv.org/abs/2512.06172</guid>
<content:encoded><![CDATA[
arXiv:2512.06172v1 Announce Type: cross 
Abstract: Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[
arXiv:2512.06190v1 Announce Type: cross 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</title>
<link>https://arxiv.org/abs/2512.06193</link>
<guid>https://arxiv.org/abs/2512.06193</guid>
<content:encoded><![CDATA[
arXiv:2512.06193v1 Announce Type: cross 
Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Memory Use in Reinforcement Learning with Temporal Range</title>
<link>https://arxiv.org/abs/2512.06204</link>
<guid>https://arxiv.org/abs/2512.06204</guid>
<content:encoded><![CDATA[
arXiv:2512.06204v1 Announce Type: cross 
Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-exploration for online reinforcement learning</title>
<link>https://arxiv.org/abs/2512.06244</link>
<guid>https://arxiv.org/abs/2512.06244</guid>
<content:encoded><![CDATA[
arXiv:2512.06244v1 Announce Type: cross 
Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(\epsilon^{-2})$ sample complexity to solve to $\epsilon$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DUET: Agentic Design Understanding via Experimentation and Testing</title>
<link>https://arxiv.org/abs/2512.06247</link>
<guid>https://arxiv.org/abs/2512.06247</guid>
<content:encoded><![CDATA[
arXiv:2512.06247v1 Announce Type: cross 
Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup</title>
<link>https://arxiv.org/abs/2512.06256</link>
<guid>https://arxiv.org/abs/2512.06256</guid>
<content:encoded><![CDATA[
arXiv:2512.06256v1 Announce Type: cross 
Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling</title>
<link>https://arxiv.org/abs/2512.06259</link>
<guid>https://arxiv.org/abs/2512.06259</guid>
<content:encoded><![CDATA[
arXiv:2512.06259v1 Announce Type: cross 
Abstract: Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Restless Multi-Arm Bandits with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06274</link>
<guid>https://arxiv.org/abs/2512.06274</guid>
<content:encoded><![CDATA[
arXiv:2512.06274v1 Announce Type: cross 
Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
arXiv:2512.06276v1 Announce Type: cross 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.06281</link>
<guid>https://arxiv.org/abs/2512.06281</guid>
<content:encoded><![CDATA[
arXiv:2512.06281v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks</title>
<link>https://arxiv.org/abs/2512.06297</link>
<guid>https://arxiv.org/abs/2512.06297</guid>
<content:encoded><![CDATA[
arXiv:2512.06297v1 Announce Type: cross 
Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics</title>
<link>https://arxiv.org/abs/2512.06301</link>
<guid>https://arxiv.org/abs/2512.06301</guid>
<content:encoded><![CDATA[
arXiv:2512.06301v1 Announce Type: cross 
Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation</title>
<link>https://arxiv.org/abs/2512.06304</link>
<guid>https://arxiv.org/abs/2512.06304</guid>
<content:encoded><![CDATA[
arXiv:2512.06304v1 Announce Type: cross 
Abstract: Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06306</link>
<guid>https://arxiv.org/abs/2512.06306</guid>
<content:encoded><![CDATA[
arXiv:2512.06306v1 Announce Type: cross 
Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title>
<link>https://arxiv.org/abs/2512.06343</link>
<guid>https://arxiv.org/abs/2512.06343</guid>
<content:encoded><![CDATA[
arXiv:2512.06343v1 Announce Type: cross 
Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast</title>
<link>https://arxiv.org/abs/2512.06350</link>
<guid>https://arxiv.org/abs/2512.06350</guid>
<content:encoded><![CDATA[
arXiv:2512.06350v1 Announce Type: cross 
Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction</title>
<link>https://arxiv.org/abs/2512.06357</link>
<guid>https://arxiv.org/abs/2512.06357</guid>
<content:encoded><![CDATA[
arXiv:2512.06357v1 Announce Type: cross 
Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Protecting Bystander Privacy via Selective Hearing in LALMs</title>
<link>https://arxiv.org/abs/2512.06380</link>
<guid>https://arxiv.org/abs/2512.06380</guid>
<content:encoded><![CDATA[
arXiv:2512.06380v1 Announce Type: cross 
Abstract: Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title>
<link>https://arxiv.org/abs/2512.06390</link>
<guid>https://arxiv.org/abs/2512.06390</guid>
<content:encoded><![CDATA[
arXiv:2512.06390v1 Announce Type: cross 
Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title>
<link>https://arxiv.org/abs/2512.06392</link>
<guid>https://arxiv.org/abs/2512.06392</guid>
<content:encoded><![CDATA[
arXiv:2512.06392v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity</title>
<link>https://arxiv.org/abs/2512.06396</link>
<guid>https://arxiv.org/abs/2512.06396</guid>
<content:encoded><![CDATA[
arXiv:2512.06396v1 Announce Type: cross 
Abstract: The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: cross 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition</title>
<link>https://arxiv.org/abs/2512.06426</link>
<guid>https://arxiv.org/abs/2512.06426</guid>
<content:encoded><![CDATA[
arXiv:2512.06426v1 Announce Type: cross 
Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices</title>
<link>https://arxiv.org/abs/2512.06443</link>
<guid>https://arxiv.org/abs/2512.06443</guid>
<content:encoded><![CDATA[
arXiv:2512.06443v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance Dependent Testing of Samplers using Interval Conditioning</title>
<link>https://arxiv.org/abs/2512.06458</link>
<guid>https://arxiv.org/abs/2512.06458</guid>
<content:encoded><![CDATA[
arXiv:2512.06458v1 Announce Type: cross 
Abstract: Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.
  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control</title>
<link>https://arxiv.org/abs/2512.06471</link>
<guid>https://arxiv.org/abs/2512.06471</guid>
<content:encoded><![CDATA[
arXiv:2512.06471v1 Announce Type: cross 
Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying German Language Proficiency Levels Using Large Language Models</title>
<link>https://arxiv.org/abs/2512.06483</link>
<guid>https://arxiv.org/abs/2512.06483</guid>
<content:encoded><![CDATA[
arXiv:2512.06483v1 Announce Type: cross 
Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning</title>
<link>https://arxiv.org/abs/2512.06496</link>
<guid>https://arxiv.org/abs/2512.06496</guid>
<content:encoded><![CDATA[
arXiv:2512.06496v1 Announce Type: cross 
Abstract: Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion</title>
<link>https://arxiv.org/abs/2512.06504</link>
<guid>https://arxiv.org/abs/2512.06504</guid>
<content:encoded><![CDATA[
arXiv:2512.06504v1 Announce Type: cross 
Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI as "Co-founder": GenAI for Entrepreneurship</title>
<link>https://arxiv.org/abs/2512.06506</link>
<guid>https://arxiv.org/abs/2512.06506</guid>
<content:encoded><![CDATA[
arXiv:2512.06506v1 Announce Type: cross 
Abstract: This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images</title>
<link>https://arxiv.org/abs/2512.06521</link>
<guid>https://arxiv.org/abs/2512.06521</guid>
<content:encoded><![CDATA[
arXiv:2512.06521v1 Announce Type: cross 
Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[
arXiv:2512.06531v1 Announce Type: cross 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06533</link>
<guid>https://arxiv.org/abs/2512.06533</guid>
<content:encoded><![CDATA[
arXiv:2512.06533v1 Announce Type: cross 
Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation</title>
<link>https://arxiv.org/abs/2512.06547</link>
<guid>https://arxiv.org/abs/2512.06547</guid>
<content:encoded><![CDATA[
arXiv:2512.06547v1 Announce Type: cross 
Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models</title>
<link>https://arxiv.org/abs/2512.06555</link>
<guid>https://arxiv.org/abs/2512.06555</guid>
<content:encoded><![CDATA[
arXiv:2512.06555v1 Announce Type: cross 
Abstract: Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks</title>
<link>https://arxiv.org/abs/2512.06556</link>
<guid>https://arxiv.org/abs/2512.06556</guid>
<content:encoded><![CDATA[
arXiv:2512.06556v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title>
<link>https://arxiv.org/abs/2512.06562</link>
<guid>https://arxiv.org/abs/2512.06562</guid>
<content:encoded><![CDATA[
arXiv:2512.06562v1 Announce Type: cross 
Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Manifold Part 2: Neural Network Mathematics</title>
<link>https://arxiv.org/abs/2512.06563</link>
<guid>https://arxiv.org/abs/2512.06563</guid>
<content:encoded><![CDATA[
arXiv:2512.06563v1 Announce Type: cross 
Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.06582</link>
<guid>https://arxiv.org/abs/2512.06582</guid>
<content:encoded><![CDATA[
arXiv:2512.06582v1 Announce Type: cross 
Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems</title>
<link>https://arxiv.org/abs/2512.06590</link>
<guid>https://arxiv.org/abs/2512.06590</guid>
<content:encoded><![CDATA[
arXiv:2512.06590v1 Announce Type: cross 
Abstract: Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability</title>
<link>https://arxiv.org/abs/2512.06591</link>
<guid>https://arxiv.org/abs/2512.06591</guid>
<content:encoded><![CDATA[
arXiv:2512.06591v1 Announce Type: cross 
Abstract: Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling</title>
<link>https://arxiv.org/abs/2512.06595</link>
<guid>https://arxiv.org/abs/2512.06595</guid>
<content:encoded><![CDATA[
arXiv:2512.06595v1 Announce Type: cross 
Abstract: Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age</title>
<link>https://arxiv.org/abs/2512.06616</link>
<guid>https://arxiv.org/abs/2512.06616</guid>
<content:encoded><![CDATA[
arXiv:2512.06616v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[
arXiv:2512.06642v1 Announce Type: cross 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[
arXiv:2512.06648v1 Announce Type: cross 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts</title>
<link>https://arxiv.org/abs/2512.06652</link>
<guid>https://arxiv.org/abs/2512.06652</guid>
<content:encoded><![CDATA[
arXiv:2512.06652v1 Announce Type: cross 
Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title>
<link>https://arxiv.org/abs/2512.06655</link>
<guid>https://arxiv.org/abs/2512.06655</guid>
<content:encoded><![CDATA[
arXiv:2512.06655v1 Announce Type: cross 
Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextMamba: Scene Text Detector with Mamba</title>
<link>https://arxiv.org/abs/2512.06657</link>
<guid>https://arxiv.org/abs/2512.06657</guid>
<content:encoded><![CDATA[
arXiv:2512.06657v1 Announce Type: cross 
Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Small Language Models for Security Query Generation in SOC Workflows</title>
<link>https://arxiv.org/abs/2512.06660</link>
<guid>https://arxiv.org/abs/2512.06660</guid>
<content:encoded><![CDATA[
arXiv:2512.06660v1 Announce Type: cross 
Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[
arXiv:2512.06665v1 Announce Type: cross 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning</title>
<link>https://arxiv.org/abs/2512.06678</link>
<guid>https://arxiv.org/abs/2512.06678</guid>
<content:encoded><![CDATA[
arXiv:2512.06678v1 Announce Type: cross 
Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.06681</link>
<guid>https://arxiv.org/abs/2512.06681</guid>
<content:encoded><![CDATA[
arXiv:2512.06681v1 Announce Type: cross 
Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[
arXiv:2512.06699v1 Announce Type: cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations</title>
<link>https://arxiv.org/abs/2512.06708</link>
<guid>https://arxiv.org/abs/2512.06708</guid>
<content:encoded><![CDATA[
arXiv:2512.06708v1 Announce Type: cross 
Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting</title>
<link>https://arxiv.org/abs/2512.06714</link>
<guid>https://arxiv.org/abs/2512.06714</guid>
<content:encoded><![CDATA[
arXiv:2512.06714v1 Announce Type: cross 
Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Entropy in Visual Grounding: Analysis and Optimization</title>
<link>https://arxiv.org/abs/2512.06726</link>
<guid>https://arxiv.org/abs/2512.06726</guid>
<content:encoded><![CDATA[
arXiv:2512.06726v1 Announce Type: cross 
Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ</title>
<link>https://arxiv.org/abs/2512.06732</link>
<guid>https://arxiv.org/abs/2512.06732</guid>
<content:encoded><![CDATA[
arXiv:2512.06732v1 Announce Type: cross 
Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Patient-Doctor-NLP-System to contest inequality for less privileged</title>
<link>https://arxiv.org/abs/2512.06734</link>
<guid>https://arxiv.org/abs/2512.06734</guid>
<content:encoded><![CDATA[
arXiv:2512.06734v1 Announce Type: cross 
Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.06746</link>
<guid>https://arxiv.org/abs/2512.06746</guid>
<content:encoded><![CDATA[
arXiv:2512.06746v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance</title>
<link>https://arxiv.org/abs/2512.06747</link>
<guid>https://arxiv.org/abs/2512.06747</guid>
<content:encoded><![CDATA[
arXiv:2512.06747v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Becoming Experienced Judges: Selective Test-Time Learning for Evaluators</title>
<link>https://arxiv.org/abs/2512.06751</link>
<guid>https://arxiv.org/abs/2512.06751</guid>
<content:encoded><![CDATA[
arXiv:2512.06751v1 Announce Type: cross 
Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</title>
<link>https://arxiv.org/abs/2512.06759</link>
<guid>https://arxiv.org/abs/2512.06759</guid>
<content:encoded><![CDATA[
arXiv:2512.06759v1 Announce Type: cross 
Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.06769</link>
<guid>https://arxiv.org/abs/2512.06769</guid>
<content:encoded><![CDATA[
arXiv:2512.06769v1 Announce Type: cross 
Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06774</link>
<guid>https://arxiv.org/abs/2512.06774</guid>
<content:encoded><![CDATA[
arXiv:2512.06774v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2512.06776</link>
<guid>https://arxiv.org/abs/2512.06776</guid>
<content:encoded><![CDATA[
arXiv:2512.06776v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Description to Score: Can LLMs Quantify Vulnerabilities?</title>
<link>https://arxiv.org/abs/2512.06781</link>
<guid>https://arxiv.org/abs/2512.06781</guid>
<content:encoded><![CDATA[
arXiv:2512.06781v1 Announce Type: cross 
Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Regularization for Positive-Unlabeled Learning on the Hypersphere</title>
<link>https://arxiv.org/abs/2512.06785</link>
<guid>https://arxiv.org/abs/2512.06785</guid>
<content:encoded><![CDATA[
arXiv:2512.06785v1 Announce Type: cross 
Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal and Diffusion Transports in Machine Learning</title>
<link>https://arxiv.org/abs/2512.06797</link>
<guid>https://arxiv.org/abs/2512.06797</guid>
<content:encoded><![CDATA[
arXiv:2512.06797v1 Announce Type: cross 
Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[
arXiv:2512.06811v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation</title>
<link>https://arxiv.org/abs/2512.06813</link>
<guid>https://arxiv.org/abs/2512.06813</guid>
<content:encoded><![CDATA[
arXiv:2512.06813v1 Announce Type: cross 
Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation</title>
<link>https://arxiv.org/abs/2512.06814</link>
<guid>https://arxiv.org/abs/2512.06814</guid>
<content:encoded><![CDATA[
arXiv:2512.06814v1 Announce Type: cross 
Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs</title>
<link>https://arxiv.org/abs/2512.06836</link>
<guid>https://arxiv.org/abs/2512.06836</guid>
<content:encoded><![CDATA[
arXiv:2512.06836v1 Announce Type: cross 
Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formal that "Floats" High: Formal Verification of Floating Point Arithmetic</title>
<link>https://arxiv.org/abs/2512.06850</link>
<guid>https://arxiv.org/abs/2512.06850</guid>
<content:encoded><![CDATA[
arXiv:2512.06850v1 Announce Type: cross 
Abstract: Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design</title>
<link>https://arxiv.org/abs/2512.06854</link>
<guid>https://arxiv.org/abs/2512.06854</guid>
<content:encoded><![CDATA[
arXiv:2512.06854v1 Announce Type: cross 
Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: cross 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WisPaper: Your AI Scholar Search Engine</title>
<link>https://arxiv.org/abs/2512.06879</link>
<guid>https://arxiv.org/abs/2512.06879</guid>
<content:encoded><![CDATA[
arXiv:2512.06879v1 Announce Type: cross 
Abstract: Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoPano: Unified Panorama Generation via Joint Modeling</title>
<link>https://arxiv.org/abs/2512.06885</link>
<guid>https://arxiv.org/abs/2512.06885</guid>
<content:encoded><![CDATA[
arXiv:2512.06885v1 Announce Type: cross 
Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BabelCoder: Agentic Code Translation with Specification Alignment</title>
<link>https://arxiv.org/abs/2512.06902</link>
<guid>https://arxiv.org/abs/2512.06902</guid>
<content:encoded><![CDATA[
arXiv:2512.06902v1 Announce Type: cross 
Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Trust-Authorization Mismatch in LLM Agent Interactions</title>
<link>https://arxiv.org/abs/2512.06914</link>
<guid>https://arxiv.org/abs/2512.06914</guid>
<content:encoded><![CDATA[
arXiv:2512.06914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.
  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</title>
<link>https://arxiv.org/abs/2512.06921</link>
<guid>https://arxiv.org/abs/2512.06921</guid>
<content:encoded><![CDATA[
arXiv:2512.06921v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features</title>
<link>https://arxiv.org/abs/2512.06925</link>
<guid>https://arxiv.org/abs/2512.06925</guid>
<content:encoded><![CDATA[
arXiv:2512.06925v1 Announce Type: cross 
Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise</title>
<link>https://arxiv.org/abs/2512.06926</link>
<guid>https://arxiv.org/abs/2512.06926</guid>
<content:encoded><![CDATA[
arXiv:2512.06926v1 Announce Type: cross 
Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding</title>
<link>https://arxiv.org/abs/2512.06929</link>
<guid>https://arxiv.org/abs/2512.06929</guid>
<content:encoded><![CDATA[
arXiv:2512.06929v1 Announce Type: cross 
Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies</title>
<link>https://arxiv.org/abs/2512.06932</link>
<guid>https://arxiv.org/abs/2512.06932</guid>
<content:encoded><![CDATA[
arXiv:2512.06932v1 Announce Type: cross 
Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unifying Human-Centered AI Fairness Framework</title>
<link>https://arxiv.org/abs/2512.06944</link>
<guid>https://arxiv.org/abs/2512.06944</guid>
<content:encoded><![CDATA[
arXiv:2512.06944v1 Announce Type: cross 
Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</title>
<link>https://arxiv.org/abs/2512.06963</link>
<guid>https://arxiv.org/abs/2512.06963</guid>
<content:encoded><![CDATA[
arXiv:2512.06963v1 Announce Type: cross 
Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing BFGS and OGR for Second-Order Optimization</title>
<link>https://arxiv.org/abs/2512.06969</link>
<guid>https://arxiv.org/abs/2512.06969</guid>
<content:encoded><![CDATA[
arXiv:2512.06969v1 Announce Type: cross 
Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash Multi-Head Feed-Forward Network</title>
<link>https://arxiv.org/abs/2512.06989</link>
<guid>https://arxiv.org/abs/2512.06989</guid>
<content:encoded><![CDATA[
arXiv:2512.06989v1 Announce Type: cross 
Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models</title>
<link>https://arxiv.org/abs/2512.06991</link>
<guid>https://arxiv.org/abs/2512.06991</guid>
<content:encoded><![CDATA[
arXiv:2512.06991v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model</title>
<link>https://arxiv.org/abs/2512.06999</link>
<guid>https://arxiv.org/abs/2512.06999</guid>
<content:encoded><![CDATA[
arXiv:2512.06999v1 Announce Type: cross 
Abstract: Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Deep Neural Networks for Modern Recommendation Systems</title>
<link>https://arxiv.org/abs/2512.07000</link>
<guid>https://arxiv.org/abs/2512.07000</guid>
<content:encoded><![CDATA[
arXiv:2512.07000v1 Announce Type: cross 
Abstract: This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition</title>
<link>https://arxiv.org/abs/2512.07005</link>
<guid>https://arxiv.org/abs/2512.07005</guid>
<content:encoded><![CDATA[
arXiv:2512.07005v1 Announce Type: cross 
Abstract: Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing video analytics inference pipelines: a case study</title>
<link>https://arxiv.org/abs/2512.07009</link>
<guid>https://arxiv.org/abs/2512.07009</guid>
<content:encoded><![CDATA[
arXiv:2512.07009v1 Announce Type: cross 
Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations</title>
<link>https://arxiv.org/abs/2512.07015</link>
<guid>https://arxiv.org/abs/2512.07015</guid>
<content:encoded><![CDATA[
arXiv:2512.07015v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length</title>
<link>https://arxiv.org/abs/2512.07019</link>
<guid>https://arxiv.org/abs/2512.07019</guid>
<content:encoded><![CDATA[
arXiv:2512.07019v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferring Clinical Knowledge into ECGs Representation</title>
<link>https://arxiv.org/abs/2512.07021</link>
<guid>https://arxiv.org/abs/2512.07021</guid>
<content:encoded><![CDATA[
arXiv:2512.07021v1 Announce Type: cross 
Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization</title>
<link>https://arxiv.org/abs/2512.07022</link>
<guid>https://arxiv.org/abs/2512.07022</guid>
<content:encoded><![CDATA[
arXiv:2512.07022v1 Announce Type: cross 
Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data</title>
<link>https://arxiv.org/abs/2512.07030</link>
<guid>https://arxiv.org/abs/2512.07030</guid>
<content:encoded><![CDATA[
arXiv:2512.07030v1 Announce Type: cross 
Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues</title>
<link>https://arxiv.org/abs/2512.07034</link>
<guid>https://arxiv.org/abs/2512.07034</guid>
<content:encoded><![CDATA[
arXiv:2512.07034v1 Announce Type: cross 
Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[
arXiv:2512.07051v1 Announce Type: cross 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v1 Announce Type: cross 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design</title>
<link>https://arxiv.org/abs/2512.07064</link>
<guid>https://arxiv.org/abs/2512.07064</guid>
<content:encoded><![CDATA[
arXiv:2512.07064v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation</title>
<link>https://arxiv.org/abs/2512.07079</link>
<guid>https://arxiv.org/abs/2512.07079</guid>
<content:encoded><![CDATA[
arXiv:2512.07079v1 Announce Type: cross 
Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking</title>
<link>https://arxiv.org/abs/2512.07086</link>
<guid>https://arxiv.org/abs/2512.07086</guid>
<content:encoded><![CDATA[
arXiv:2512.07086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging KV Similarity for Online Structured Pruning in LLMs</title>
<link>https://arxiv.org/abs/2512.07090</link>
<guid>https://arxiv.org/abs/2512.07090</guid>
<content:encoded><![CDATA[
arXiv:2512.07090v1 Announce Type: cross 
Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07092</link>
<guid>https://arxiv.org/abs/2512.07092</guid>
<content:encoded><![CDATA[
arXiv:2512.07092v1 Announce Type: cross 
Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOAM: Blocked State Folding for Memory-Efficient LLM Training</title>
<link>https://arxiv.org/abs/2512.07112</link>
<guid>https://arxiv.org/abs/2512.07112</guid>
<content:encoded><![CDATA[
arXiv:2512.07112v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations</title>
<link>https://arxiv.org/abs/2512.07122</link>
<guid>https://arxiv.org/abs/2512.07122</guid>
<content:encoded><![CDATA[
arXiv:2512.07122v1 Announce Type: cross 
Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.07132</link>
<guid>https://arxiv.org/abs/2512.07132</guid>
<content:encoded><![CDATA[
arXiv:2512.07132v1 Announce Type: cross 
Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07135</link>
<guid>https://arxiv.org/abs/2512.07135</guid>
<content:encoded><![CDATA[
arXiv:2512.07135v2 Announce Type: cross 
Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2512.07136</link>
<guid>https://arxiv.org/abs/2512.07136</guid>
<content:encoded><![CDATA[
arXiv:2512.07136v1 Announce Type: cross 
Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[
arXiv:2512.07142v1 Announce Type: cross 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[
arXiv:2512.07150v1 Announce Type: cross 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</title>
<link>https://arxiv.org/abs/2512.07168</link>
<guid>https://arxiv.org/abs/2512.07168</guid>
<content:encoded><![CDATA[
arXiv:2512.07168v1 Announce Type: cross 
Abstract: We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title>
<link>https://arxiv.org/abs/2512.07170</link>
<guid>https://arxiv.org/abs/2512.07170</guid>
<content:encoded><![CDATA[
arXiv:2512.07170v1 Announce Type: cross 
Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>START: Spatial and Textual Learning for Chart Understanding</title>
<link>https://arxiv.org/abs/2512.07186</link>
<guid>https://arxiv.org/abs/2512.07186</guid>
<content:encoded><![CDATA[
arXiv:2512.07186v1 Announce Type: cross 
Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MASim: Multilingual Agent-Based Simulation for Social Science</title>
<link>https://arxiv.org/abs/2512.07195</link>
<guid>https://arxiv.org/abs/2512.07195</guid>
<content:encoded><![CDATA[
arXiv:2512.07195v1 Announce Type: cross 
Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Federated Prompt Calibration</title>
<link>https://arxiv.org/abs/2512.07208</link>
<guid>https://arxiv.org/abs/2512.07208</guid>
<content:encoded><![CDATA[
arXiv:2512.07208v1 Announce Type: cross 
Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($\beta$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($\beta$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.07215</link>
<guid>https://arxiv.org/abs/2512.07215</guid>
<content:encoded><![CDATA[
arXiv:2512.07215v2 Announce Type: cross 
Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07218</link>
<guid>https://arxiv.org/abs/2512.07218</guid>
<content:encoded><![CDATA[
arXiv:2512.07218v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[
arXiv:2512.07228v1 Announce Type: cross 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07234</link>
<guid>https://arxiv.org/abs/2512.07234</guid>
<content:encoded><![CDATA[
arXiv:2512.07234v1 Announce Type: cross 
Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFFair: Influence Function-driven Sample Reweighting for Fair Classification</title>
<link>https://arxiv.org/abs/2512.07249</link>
<guid>https://arxiv.org/abs/2512.07249</guid>
<content:encoded><![CDATA[
arXiv:2512.07249v1 Announce Type: cross 
Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</title>
<link>https://arxiv.org/abs/2512.07253</link>
<guid>https://arxiv.org/abs/2512.07253</guid>
<content:encoded><![CDATA[
arXiv:2512.07253v1 Announce Type: cross 
Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.07266</link>
<guid>https://arxiv.org/abs/2512.07266</guid>
<content:encoded><![CDATA[
arXiv:2512.07266v1 Announce Type: cross 
Abstract: Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</title>
<link>https://arxiv.org/abs/2512.07275</link>
<guid>https://arxiv.org/abs/2512.07275</guid>
<content:encoded><![CDATA[
arXiv:2512.07275v1 Announce Type: cross 
Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents</title>
<link>https://arxiv.org/abs/2512.07287</link>
<guid>https://arxiv.org/abs/2512.07287</guid>
<content:encoded><![CDATA[
arXiv:2512.07287v1 Announce Type: cross 
Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>