<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[
<div> finance, client embeddings, contrastive learning, event sequences, LLMs

Summary:
In the field of finance, learning client embeddings from historic communication data is crucial. Traditional methods using large language models (LLMs) for processing long event sequences are computationally expensive. To address this issue, the paper introduces LATTE, a contrastive learning framework that aligns raw event embeddings with LLM-generated semantic embeddings. By summarizing behavioral features into short prompts and leveraging contrastive loss for supervision, LATTE reduces inference cost and input size while maintaining performance. Experimental results on real-world financial datasets demonstrate the superiority of LATTE over existing techniques. Moreover, LATTE is designed to be deployable in latency-sensitive environments, making it practical for use in financial applications. 

<br><br>Summary: <div>
arXiv:2508.10021v2 Announce Type: replace-cross 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone</title>
<link>https://arxiv.org/abs/2508.14923</link>
<guid>https://arxiv.org/abs/2508.14923</guid>
<content:encoded><![CDATA[
<div> Graph Signal Processing, Neuro-symbolic reasoning, Spectral reasoning, Logical consistency, Computational efficiency 

Summary: 

The article introduces a novel approach to neuro-symbolic reasoning by utilizing Graph Signal Processing (GSP) as a key computational method. Unlike traditional models, which treat spectral graph techniques as secondary, this model performs the entire reasoning process in the graph spectral domain. Logical entities and relationships are represented as graph signals, processed using learnable spectral filters for multi-scale information propagation, and converted into symbolic predicates for rule-based inference. The framework includes graph Fourier transforms, band-selective attention, and spectral rule grounding. Experimental results on various reasoning datasets show enhancements in logical consistency, interpretability, and computational efficiency compared to current neuro-symbolic models. This suggests that GSP offers a solid mathematical foundation and efficient platform for developing robust and interpretable reasoning systems. 

<br /><br />Summary: <div>
arXiv:2508.14923v1 Announce Type: new 
Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that leverages Graph Signal Processing (GSP) as the primary computational backbone for integrating symbolic logic and neural inference. Unlike conventional reasoning models that treat spectral graph methods as peripheral components, our approach formulates the entire reasoning pipeline in the graph spectral domain. Logical entities and relationships are encoded as graph signals, processed via learnable spectral filters that control multi-scale information propagation, and mapped into symbolic predicates for rule-based inference. We present a complete mathematical framework for spectral reasoning, including graph Fourier transforms, band-selective attention, and spectral rule grounding. Experiments on benchmark reasoning datasets (ProofWriter, EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in logical consistency, interpretability, and computational efficiency over state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP provides a mathematically grounded and computationally efficient substrate for robust and interpretable reasoning systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goals and the Structure of Experience</title>
<link>https://arxiv.org/abs/2508.15013</link>
<guid>https://arxiv.org/abs/2508.15013</guid>
<content:encoded><![CDATA[
<div> Keywords: Purposeful behavior, world models, goal-directed state representation, telic states, reinforcement learning<br />
Summary:<br />
- The article explores the acquisition of purposeful behavior in both natural and artificial intelligence, emphasizing the importance of world models comprising descriptive and prescriptive aspects.<br />
- It proposes a novel computational framework where descriptive and prescriptive aspects of a world model co-emerge from agent-environment interactions, specifically focusing on goal-directed state representation.<br />
- Drawing on Buddhist epistemology, the concept of telic states is introduced, defined as classes of goal-equivalent experience distributions, providing a streamlined explanation of goal-directed learning.<br />
- The framework aims to offer a unified perspective on behavioral, phenomenological, and neural aspects of purposeful behaviors across various substrates.<br />
- By highlighting the statistical divergence between behavioral policies and desirable experience features, the framework presents a coherent account of purposeful behavior acquisition. <br /> 

Summary: <div>
arXiv:2508.15013v1 Announce Type: new 
Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its acquisition is often believed to rely on world models, comprising both descriptive (what is) and prescriptive (what is desirable) aspects that identify and evaluate state of affairs in the world, respectively. Canonical computational accounts of purposeful behavior, such as reinforcement learning, posit distinct components of a world model comprising a state representation (descriptive aspect) and a reward function (prescriptive aspect). However, an alternative possibility, which has not yet been computationally formulated, is that these two aspects instead co-emerge interdependently from an agent's goal. Here, we describe a computational framework of goal-directed state representation in cognitive agents, in which the descriptive and prescriptive aspects of a world model co-emerge from agent-environment interaction sequences, or experiences. Drawing on Buddhist epistemology, we introduce a construct of goal-directed, or telic, states, defined as classes of goal-equivalent experience distributions. Telic states provide a parsimonious account of goal-directed learning in terms of the statistical divergence between behavioral policies and desirable experience features. We review empirical and theoretical literature supporting this novel perspective and discuss its potential to provide a unified account of behavioral, phenomenological and neural dimensions of purposeful behaviors across diverse substrates.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism</title>
<link>https://arxiv.org/abs/2508.15030</link>
<guid>https://arxiv.org/abs/2508.15030</guid>
<content:encoded><![CDATA[
<div> LLM-based agents, Collab-REC, tourism recommendations, diversity, over-tourism <br />
<br />
Summary: 
The article discusses Collab-REC, a framework consisting of three LLM-based agents aimed at combating popularity bias and enhancing diversity in tourism recommendations. These agents, namely Personalization, Popularity, and Sustainability, offer city suggestions from distinct perspectives. By engaging in multi-round negotiation facilitated by a non-LLM moderator, Collab-REC merges and refines these proposals to ensure a balanced representation of each viewpoint, penalizing redundant or irrelevant responses. Experimental results on European city queries demonstrate that Collab-REC outperforms a single-agent baseline in terms of diversity and overall relevance, bringing attention to lesser-visited destinations often overlooked by traditional recommendation systems. With a focus on countering over-tourism and considering user constraints, Collab-REC showcases the potential benefits of collaborative LLM-driven recommender systems. <div>
arXiv:2508.15030v1 Announce Type: new 
Abstract: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions</title>
<link>https://arxiv.org/abs/2508.15047</link>
<guid>https://arxiv.org/abs/2508.15047</guid>
<content:encoded><![CDATA[
<div> agent-based approach, crowd simulations, language models, social interactions, dialogue system

Summary:
Our novel method utilizes large language models to control agent movement in crowd simulations. By incorporating dialogue systems and language-driven navigation, we enable agents to make decisions based on both perceptual inputs and ongoing dialogue. This approach considers character personalities, roles, desires, and relationships to generate inter-agent dialogue and control navigation. Validated in complex scenarios, our method showcases automatic grouping and ungrouping of agents, showcasing emergent group behaviors in any environmental setting. By serving as an information-passing mechanism within crowds, our framework produces more realistic crowd simulations with enhanced social interactions and steering capabilities.<br /><br />Summary: <div>
arXiv:2508.15047v1 Announce Type: new 
Abstract: Animating and simulating crowds using an agent-based approach is a well-established area where every agent in the crowd is individually controlled such that global human-like behaviour emerges. We observe that human navigation and movement in crowds are often influenced by complex social and environmental interactions, driven mainly by language and dialogue. However, most existing work does not consider these dimensions and leads to animations where agent-agent and agent-environment interactions are largely limited to steering and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to control agents' movement. Our method has two main components: a dialogue system and language-driven navigation. We periodically query agent-centric LLMs conditioned on character personalities, roles, desires, and relationships to control the generation of inter-agent dialogue when necessitated by the spatial and social relationships with neighbouring agents. We then use the conversation and each agent's personality, emotional state, vision, and physical state to control the navigation and steering of each agent. Our model thus enables agents to make motion decisions based on both their perceptual inputs and the ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay between social interactions, steering, and crowding. In these scenarios, we observe that grouping and ungrouping of agents automatically occur. Additionally, our experiments show that our method serves as an information-passing mechanism within the crowd. As a result, our framework produces more realistic crowd simulations, with emergent group behaviours arising naturally from any environmental setting.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title>
<link>https://arxiv.org/abs/2508.15050</link>
<guid>https://arxiv.org/abs/2508.15050</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, confidence assessment, reasoning capabilities, budget, calibration

Summary:
Large Language Models (LLMs) used for question answering must be properly calibrated to avoid overconfidence. A study evaluated the impact of reasoning abilities and budget on confidence assessment accuracy using the ClimateX dataset, expanding it to human and planetary health. The research challenges the notion that increasing reasoning budgets improves calibration, showing that extended reasoning leads to overconfidence that worsens with longer thinking budgets. In contrast, search-augmented generation outperformed pure reasoning by retrieving relevant evidence and achieving high accuracy. The results suggest that information access, rather than reasoning depth or inference budget, may be the key factor for improving confidence calibration in knowledge-intensive tasks.<br /><br />Summary: Large Language Models need robust calibration to avoid overconfidence in question answering. Increasing reasoning budgets can impair rather than improve calibration, leading to systematic overconfidence. Search-augmented generation outperforms pure reasoning by retrieving evidence, indicating that information access is crucial for improving confidence calibration in knowledge-intensive tasks. <div>
arXiv:2508.15050v1 Announce Type: new 
Abstract: Large Language Models deployed as question answering tools require robust calibration to avoid overconfidence. We systematically evaluate how reasoning capabilities and budget affect confidence assessment accuracy, using the ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary health. Our key finding challenges the "test-time scaling" paradigm: while recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets consistently impairs rather than improves calibration. Extended reasoning leads to systematic overconfidence that worsens with longer thinking budgets, producing diminishing and negative returns beyond modest computational investments. Conversely, search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence. Our results suggest that information access, rather than reasoning depth or inference budget, may be the critical bottleneck for improved confidence calibration of knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning</title>
<link>https://arxiv.org/abs/2508.15053</link>
<guid>https://arxiv.org/abs/2508.15053</guid>
<content:encoded><![CDATA[
<div> demonstrating, data analysis, hyperspectral instrument, neural network acceleration hardware, Earth science

Summary:
In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is showcasing cutting-edge data analysis capabilities onboard the CS-6 satellite. Equipped with a hyperspectral instrument and neural network acceleration hardware, CS-6 allows for onboard data analysis in the visible and near infrared range. This advancement opens up new opportunities for Earth science measurements and responses, enhancing the satellite's capabilities. The demonstration will highlight the use of deep learning and spectral analysis algorithms for various applications, showcasing the power of onboard data analysis in advancing scientific research and exploration.<br /><br />Summary: <div>
arXiv:2508.15053v1 Announce Type: new 
Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6). CS-6 is a satellite with a visible and near infrared range hyperspectral instrument and neural network acceleration hardware. Performing data analysis at the edge (e.g. onboard) can enable new Earth science measurements and responses. We will demonstrate data analysis and inference onboard CS-6 for numerous applications using deep learning and spectral analysis algorithms.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner</title>
<link>https://arxiv.org/abs/2508.15068</link>
<guid>https://arxiv.org/abs/2508.15068</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, parameter-efficient fine-tuning, safety alignment, agent planning tasks, S3LoRA <br />
Summary: 
S3LoRA is a new framework designed to enhance the safety of Large Language Models (LLMs) that have been adapted using parameter-efficient fine-tuning techniques. The framework is lightweight, data-free, and model-independent, making it practical for real-world applications. S3LoRA leverages Magnitude-Aware Spherically Normalized SVD (MAS-SVD) to analyze the structural properties of fine-tuned weight updates and introduces the Spectral Sharpness Index (SSI) to identify layers with potentially unsafe updates. By pruning these layers post-hoc, S3LoRA reduces safety risks without compromising task performance. Experiments across agent planning and language generation tasks demonstrate that S3LoRA consistently improves safety metrics, maintains or enhances utility metrics, and significantly reduces inference cost. This makes S3LoRA a scalable solution for deploying LLM-based agents in resource-constrained and safety-critical environments. <br />   <br />Summary: <div>
arXiv:2508.15068v1 Announce Type: new 
Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning (PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based agents. However, these adaptations can unintentionally compromise safety alignment, leading to unsafe or unstable behaviors, particularly in agent planning tasks. Existing safety-aware adaptation methods often require access to both base and instruction-tuned model checkpoints, which are frequently unavailable in practice, limiting their applicability. We propose S3LoRA (Safe Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and model-independent framework that mitigates safety risks in LoRA-adapted models by inspecting only the fine-tuned weight updates. We first introduce Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes the structural properties of LoRA updates while preserving global magnitude information. We then design the Spectral Sharpness Index (SSI), a sharpness-aware metric to detect layers with highly concentrated and potentially unsafe updates. These layers are pruned post-hoc to reduce risk without sacrificing task performance. Extensive experiments and ablation studies across agent planning and language generation tasks show that S3LoRA consistently improves safety metrics while maintaining or improving utility metrics and significantly reducing inference cost. These results establish S3LoRA as a practical and scalable solution for safely deploying LLM-based agents in real-world, resource-constrained, and safety-critical environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argumentation for Explainable Workforce Optimisation (with Appendix)</title>
<link>https://arxiv.org/abs/2508.15118</link>
<guid>https://arxiv.org/abs/2508.15118</guid>
<content:encoded><![CDATA[
<div> Keywords: workforce management, optimization, abstract argumentation, industrial application, user study<br />
<br />
Summary: <br />
Workforce management is a complex problem involving optimizing the makespan and travel distance for a team of operators completing jobs with instruments. The challenge lies in accommodating changes during execution and providing explanations to stakeholders. By framing workforce management as abstract argumentation in an industrial setting, the authors demonstrate the ability to adapt to changes and offer clear explanations. A user study shows that their tool and explanations lead to faster and more accurate problem-solving compared to manual methods. This innovative approach opens up new possibilities for improving workforce management efficiency and effectiveness in industrial settings. <div>
arXiv:2508.15118v1 Announce Type: new 
Abstract: Workforce management is a complex problem optimising the makespan and travel distance required for a team of operators to complete a set of jobs, using a set of instruments. A crucial challenge in workforce management is accommodating changes at execution time so that explanations are provided to all stakeholders involved. Here, we show that, by understanding workforce management as abstract argumentation in an industrial application, we can accommodate change and obtain faithful explanations. We show, with a user study, that our tool and explanations lead to faster and more accurate problem solving than conventional solutions by hand.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Universe Assistance Games</title>
<link>https://arxiv.org/abs/2508.15119</link>
<guid>https://arxiv.org/abs/2508.15119</guid>
<content:encoded><![CDATA[
<div> Embodied AI, Open-Universe Assistance Games, natural language goals, GOOD method, goal tracking<br />
Summary:<br />
The study introduces Open-Universe Assistance Games (OU-AGs) framework for embodied AI agents to infer and act on diverse human goals. The GOOD method, which extracts goals in natural language during human-agent interactions, allows for rich goal representations and uncertainty estimation without large datasets. By prompting a language model to simulate users with different intents, GOOD enables probabilistic inference over candidate goals. Evaluation in grocery shopping and household robotics tasks with synthetic user profiles shows GOOD outperforms a baseline method without explicit goal tracking. Both language model-based and human evaluations confirm the effectiveness of the approach. <br /> <div>
arXiv:2508.15119v1 Announce Type: new 
Abstract: Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AI-generated research, aiXiv, open-access platform, autonomous scientific discovery

Summary:
aiXiv is a new open-access platform designed to address the challenges faced by AI-generated research content in the current publication ecosystem. It features a multi-agent architecture that allows both human and AI scientists to submit, review, and refine research proposals and papers. The platform also offers API and MCP interfaces for seamless integration of diverse scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, aiXiv has been shown to significantly enhance the quality of AI-generated research content after iterative revising and reviewing. This work lays the foundation for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality research. The code for aiXiv is available on GitHub, and the website can be accessed for more information. 

<br /><br />Summary: <div>
arXiv:2508.15126v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Agent-v3: Foundamental Agents for GUI Automation</title>
<link>https://arxiv.org/abs/2508.15144</link>
<guid>https://arxiv.org/abs/2508.15144</guid>
<content:encoded><![CDATA[
<div> GUI-Owl, Mobile-Agent-v3, agent model, GUI benchmarks, reinforcement learning<br />
<br />
Summary: 
This paper introduces GUI-Owl, a GUI agent model that achieves top performance on GUI benchmarks. It incorporates innovations such as a large-scale virtual environment, diverse foundational agent capabilities, and scalable reinforcement learning. The model is able to perform grounding, question answering, planning, decision-making, and procedural knowledge tasks across desktop and mobile environments. Mobile-Agent-v3, a framework built on GUI-Owl, further improves performance and sets a new state-of-the-art for open-source GUI agent frameworks. The framework leverages a cloud-based environment for data generation and validation, supports end-to-end decision-making, and acts as a modular component in multi-agent systems. A scalable reinforcement learning framework with asynchronous training and Trajectory-aware Relative Policy Optimization (TRPO) is developed for real-world alignment, achieving high performance on benchmark tasks. Both GUI-Owl and Mobile-Agent-v3 are open-sourced for further development and research. <div>
arXiv:2508.15144v1 Announce Type: new 
Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</title>
<link>https://arxiv.org/abs/2508.15180</link>
<guid>https://arxiv.org/abs/2508.15180</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematical datasets, logical datasets, large language models, PuzzleClone, Satisfiability Modulo Theories

Summary:<br />
The article introduces PuzzleClone, a framework for generating high-quality mathematical and logical datasets with verifiable answers for large language models using Satisfiability Modulo Theories. The approach involves encoding seed puzzles, generating scalable variants through randomization, and ensuring validity through reproduction. A benchmark of over 83K verified puzzles is created, covering a wide range of difficulty levels and formats. Post-training on PuzzleClone datasets results in significant improvements on both the test set and logic/mathematical benchmarks, with average scores increasing from 14.4 to 56.2. The code and data are available on GitHub for further research and exploration.<br /><br />Summary: <div>
arXiv:2508.15180v1 Announce Type: new 
Abstract: High-quality mathematical and logical datasets with verifiable answers are essential for strengthening the reasoning capabilities of large language models (LLMs). While recent data augmentation techniques have facilitated the creation of large-scale benchmarks, existing LLM-generated datasets often suffer from limited reliability, diversity, and scalability. To address these challenges, we introduce PuzzleClone, a formal framework for synthesizing verifiable data at scale using Satisfiability Modulo Theories (SMT). Our approach features three key innovations: (1) encoding seed puzzles into structured logical specifications, (2) generating scalable variants through systematic variable and constraint randomization, and (3) ensuring validity via a reproduction mechanism. Applying PuzzleClone, we construct a curated benchmark comprising over 83K diverse and programmatically validated puzzles. The generated puzzles span a wide spectrum of difficulty and formats, posing significant challenges to current state-of-the-art models. We conduct post training (SFT and RL) on PuzzleClone datasets. Experimental results show that training on PuzzleClone yields substantial improvements not only on PuzzleClone testset but also on logic and mathematical benchmarks. Post training raises PuzzleClone average from 14.4 to 56.2 and delivers consistent improvements across 7 logic and mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from 52.5 to 65.0). Our code and data are available at https://github.com/puzzleclone.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title>
<link>https://arxiv.org/abs/2508.15192</link>
<guid>https://arxiv.org/abs/2508.15192</guid>
<content:encoded><![CDATA[
<div> framework, hyperhidrosis, LLM, rare diseases, open-source  
Summary:  
The article introduces LLM4Sweat, a specialized framework for hyperhidrosis, a rare disorder causing excessive sweating. It addresses the challenge of limited and unreliable data for fine-tuning large language models in healthcare. The framework consists of a three-stage pipeline: data augmentation using a frontier LLM, fine-tuning on an open-source foundation model, and inference and expert evaluation by clinical and psychological specialists. LLM4Sweat outperforms baselines and provides diagnosis, personalized treatment recommendations, and empathetic support for individuals with hyperhidrosis. It is the first open-source LLM framework for this condition, setting a foundation for similar rare diseases facing data and trustworthiness challenges.<br /><br />Summary: <div>
arXiv:2508.15192v1 Announce Type: new 
Abstract: While large language models (LLMs) have shown promise in healthcare, their application for rare medical conditions is still hindered by scarce and unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing excessive sweating beyond physiological needs, is one such rare disorder, affecting 2-3% of the population and significantly impacting both physical comfort and psychosocial well-being. To date, no work has tailored LLMs to advance the diagnosis or care of hyperhidrosis. To address this gap, we present LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and empathetic hyperhidrosis support. The system follows a three-stage pipeline. In the data augmentation stage, a frontier LLM generates medically plausible synthetic vignettes from curated open-source data to create a diverse and balanced question-answer dataset. In the fine-tuning stage, an open-source foundation model is fine-tuned on the dataset to provide diagnosis, personalized treatment recommendations, and empathetic psychological support. In the inference and expert evaluation stage, clinical and psychological specialists assess accuracy, appropriateness, and empathy, with validated responses iteratively enriching the dataset. Experiments show that LLM4Sweat outperforms baselines and delivers the first open-source LLM framework for hyperhidrosis, offering a generalizable approach for other rare diseases with similar data and trustworthiness challenges.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling</title>
<link>https://arxiv.org/abs/2508.15204</link>
<guid>https://arxiv.org/abs/2508.15204</guid>
<content:encoded><![CDATA[
<div> framework, Resource-Constrained Project Scheduling Problems, scalability, benchmark, large language models <br />
<br />Summary: 
The article introduces R-ConstraintBench, a scalable framework that evaluates large language models (LLMs) on Resource-Constrained Project Scheduling Problems (RCPSP). The framework incrementally increases constraints in Directed Acyclic Graphs (DAGs) to simulate high-constraint regimes. Testing multiple LLMs in a data center migration scenario reveals that strong models perform well with precedence constraints but struggle with downtime, temporal windows, and disjunctive constraints. This highlights constraint interaction as a key challenge rather than graph depth. While LLMs show high performance on synthetic ramps, they struggle to generalize to real-world scenarios. The study identifies degradation thresholds and constraint types associated with failure, emphasizing the importance of considering various constraints in scheduling problems. <div>
arXiv:2508.15204v1 Announce Type: new 
Abstract: Effective scheduling under tight resource, timing, and operational constraints underpins large-scale planning across sectors such as capital projects, manufacturing, logistics, and IT fleet transitions. However, the reliability of large language models (LLMs) when reasoning under high-constraint regimes is insufficiently characterized. To address this gap, we present R-ConstraintBench, a scalable framework that evaluates models on Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete feasibility class, while difficulty increases via linear growth in constraints. R-ConstraintBench incrementally increases non-redundant precedence constraints in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal windows, and disjunctive constraints. As an illustrative example, we instantiate the benchmark in a data center migration setting and evaluate multiple LLMs using feasibility and error analysis, identifying degradation thresholds and constraint types most associated with failure. Empirically, strong models are near-ceiling on precedence-only DAGs, but feasibility performance collapses when downtime, temporal windows, and disjunctive constraints interact, implicating constraint interaction, not graph depth, as the principal bottleneck. Performance on clean synthetic ramps also does not guarantee transfer to domain-grounded scenarios, underscoring limited generalization.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See it. Say it. Sorted: Agentic System for Compositional Diagram Generation</title>
<link>https://arxiv.org/abs/2508.15222</link>
<guid>https://arxiv.org/abs/2508.15222</guid>
<content:encoded><![CDATA[
<div> sketch-to-diagram generation, Diffusion models, Vision-Language Model, Large Language Models, Scalable Vector Graphics <br />
Summary:<br /> 
The article introduces a training-free agentic system called 'See it. Say it. Sorted.' that converts rough hand sketches into precise diagrams using a combination of Vision-Language Models and Large Language Models. This system utilizes qualitative reasoning, global constraints preservation, and human-in-the-loop corrections to produce editable Scalable Vector Graphics (SVG) programs. Through an iterative loop involving a Critic VLM, multiple candidate LLMs, and a Judge VLM, the system accurately reconstructs layout and structure from flowchart sketches. It outperforms two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro) by composing primitives without inserting unwanted text. The programmatic SVG outputs can be easily extended to presentation tools like PowerPoint and customized for specific tasks. The codebase for this system is open-sourced and available on GitHub at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git. <br /> <div>
arXiv:2508.15222v1 Announce Type: new 
Abstract: We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas</title>
<link>https://arxiv.org/abs/2508.15240</link>
<guid>https://arxiv.org/abs/2508.15240</guid>
<content:encoded><![CDATA[
<div> optimization algorithms, land-use allocation, mixed-use areas, differential evolution, genetic algorithms <br />
Summary:<br />
This paper introduces novel computational intelligence approaches for optimizing land-use allocation in mixed-use urban areas, addressing the balance between land-use compatibility and economic objectives. The CR+DES algorithm, utilizing scaled difference vectors, enhances exploration, achieving a 3.16% improvement in land-use compatibility compared to existing methods. The MSBX+MO algorithm excels in price optimization with a 3.3% improvement. Statistical validation confirms the superiority of algorithms incorporating difference vectors. A systematic constraint relaxation strategy improves solution quality while maintaining feasibility. The findings provide evidence-based computational tools for urban planners and policymakers to effectively balance competing objectives in land-use allocation, supporting sustainable urban development policies in rapidly urbanizing regions. <br /><br /> <div>
arXiv:2508.15240v1 Announce Type: new 
Abstract: Urban land-use allocation represents a complex multi-objective optimization problem critical for sustainable urban development policy. This paper presents novel computational intelligence approaches for optimizing land-use allocation in mixed-use areas, addressing inherent trade-offs between land-use compatibility and economic objectives. We develop multiple optimization algorithms, including custom variants integrating differential evolution with multi-objective genetic algorithms. Key contributions include: (1) CR+DES algorithm leveraging scaled difference vectors for enhanced exploration, (2) systematic constraint relaxation strategy improving solution quality while maintaining feasibility, and (3) statistical validation using Kruskal-Wallis tests with compact letter displays. Applied to a real-world case study with 1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility compared to state-of-the-art methods, while MSBX+MO excels in price optimization with 3.3\% improvement. Statistical analysis confirms algorithms incorporating difference vectors significantly outperform traditional approaches across multiple metrics. The constraint relaxation technique enables broader solution space exploration while maintaining practical constraints. These findings provide urban planners and policymakers with evidence-based computational tools for balancing competing objectives in land-use allocation, supporting more effective urban development policies in rapidly urbanizing regions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Memory Systems for Enhancing the Long-term Memory of Agent</title>
<link>https://arxiv.org/abs/2508.15294</link>
<guid>https://arxiv.org/abs/2508.15294</guid>
<content:encoded><![CDATA[
<div> memory module, large language models, historical data, multiple memory system, cognitive psychology theory

Summary:
The study introduces a multiple memory system (MMS) to enhance agent performance powered by large language models. Existing memory modules like MemoryBank and A-MEM have low-quality stored memory content impacting recall and response quality. MMS processes short-term memory into multiple long-term memory fragments, creating retrieval and contextual memory units. During retrieval, MMS matches relevant memory units based on user queries and uses corresponding contextual units to improve response knowledge. Experiments on the LoCoMo dataset show MMS outperforming other methods. Ablation studies support the rationality of memory units design. The analysis of robustness considers selected memory segments and storage overhead, demonstrating practical value. <br /><br />Summary: <div>
arXiv:2508.15294v1 Announce Type: new 
Abstract: An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse-to-Fine Grounded Memory for LLM Agent Planning</title>
<link>https://arxiv.org/abs/2508.15305</link>
<guid>https://arxiv.org/abs/2508.15305</guid>
<content:encoded><![CDATA[
<div> memory mechanism, large language models, planning tasks, grounded memory, flexible adaptation

Summary:
Coarse-to-Fine Grounded Memory (\Ours{}) is a new framework proposed to enhance Large Language Models (LLMs) for complex planning tasks by grounding memories in a coarse-to-fine manner. This allows for flexible adaptation to diverse scenarios by guiding experience collection during training tasks and retrieving task-relevant experiences and tips during inference. The framework addresses limitations of existing single-granularity memories derived from dynamic environmental interactions, offering a more diverse and flexible approach to planning. By grounding environmental information into focus points and actionable tips at different granularities, \Ours{} enables the LLM to handle environmental anomalies through fine-grained key information for self-QA reflection and plan correction.<br /><br />Summary: <div>
arXiv:2508.15305v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have driven growing interest in LLM-based agents for complex planning tasks. To avoid costly agent training, many studies adopted memory mechanism that enhances LLM with offline experiences or online trajectory analysis. However, existing works focus on single-granularity memory derived from dynamic environmental interactions, which are inherently constrained by the quality of the collected experiences. This limitation, in turn, constrain the diversity of knowledge and the flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a novel framework that grounds coarse-to-fine memories with LLM, thereby fully leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds environmental information into coarse-grained focus points to guide experience collection in training tasks, followed by grounding of actionable hybrid-grained tips from each experience. At inference, \Ours{} retrieves task-relevant experiences and tips to support planning. When facing environmental anomalies, the LLM grounds the current situation into fine-grained key information, enabling flexible self-QA reflection and plan correction.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.15327</link>
<guid>https://arxiv.org/abs/2508.15327</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, offline learning, human feedback, expert demonstrations, preference learning
Summary:
Search-Based Preference Weighting (SPW) scheme introduced in this paper unifies expert demonstrations and preferences for offline reinforcement learning. SPW determines stepwise importance weights for trajectory segments by comparing state-action pairs from expert demonstrations. This approach improves credit assignment by guiding preference learning based on similarity scores. SPW demonstrates superior performance in joint learning from preferences and demonstrations compared to traditional methods on challenging robot manipulation tasks.<br /><br />Summary: <div>
arXiv:2508.15327v1 Announce Type: new 
Abstract: Offline reinforcement learning refers to the process of learning policies from fixed datasets, without requiring additional environment interaction. However, it often relies on well-defined reward functions, which are difficult and expensive to design. Human feedback is an appealing alternative, but its two common forms, expert demonstrations and preferences, have complementary limitations. Demonstrations provide stepwise supervision, but they are costly to collect and often reflect limited expert behavior modes. In contrast, preferences are easier to collect, but it is unclear which parts of a behavior contribute most to a trajectory segment, leaving credit assignment unresolved. In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to unify these two feedback sources. For each transition in a preference labeled trajectory, SPW searches for the most similar state-action pairs from expert demonstrations and directly derives stepwise importance weights based on their similarity scores. These weights are then used to guide standard preference learning, enabling more accurate credit assignment that traditional approaches struggle to achieve. We demonstrate that SPW enables effective joint learning from preferences and demonstrations, outperforming prior methods that leverage both feedback types on challenging robot manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RETAIL: Towards Real-world Travel Planning for Large Language Models</title>
<link>https://arxiv.org/abs/2508.15335</link>
<guid>https://arxiv.org/abs/2508.15335</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, automated travel planning, dataset, multi-agent framework, real-world scenarios

Summary: 
The article introduces a novel dataset called RETAIL to improve automated travel planning systems. These systems currently struggle with implicit queries, environmental factors, and detailed Point of Interest (POI) arrangements. The RETAIL dataset addresses these challenges by supporting both explicit and implicit queries, incorporating environmental awareness, and providing detailed POI information for comprehensive travel plans. The proposed topic-guided multi-agent framework, TGMA, significantly outperforms existing models in real-world travel planning tasks, achieving a 2.72% pass rate compared to just 1.0%. This research highlights the complexity of real-world travel planning and offers a promising direction for enhancing automated travel planning systems. 

<br /><br />Summary: <div>
arXiv:2508.15335v1 Announce Type: new 
Abstract: Although large language models have enhanced automated travel planning abilities, current systems remain misaligned with real-world scenarios. First, they assume users provide explicit queries, while in reality requirements are often implicit. Second, existing solutions ignore diverse environmental factors and user preferences, limiting the feasibility of plans. Third, systems can only generate plans with basic POI arrangements, failing to provide all-in-one plans with rich details. To mitigate these challenges, we construct a novel dataset \textbf{RETAIL}, which supports decision-making for implicit queries while covering explicit queries, both with and without revision needs. It also enables environmental awareness to ensure plan feasibility under real-world scenarios, while incorporating detailed POI information for all-in-one travel plans. Furthermore, we propose a topic-guided multi-agent framework, termed TGMA. Our experiments reveal that even the strongest existing model achieves merely a 1.0% pass rate, indicating real-world travel planning remains extremely challenging. In contrast, TGMA demonstrates substantially improved performance 2.72%, offering promising directions for real-world travel planning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</title>
<link>https://arxiv.org/abs/2508.15338</link>
<guid>https://arxiv.org/abs/2508.15338</guid>
<content:encoded><![CDATA[
<div> Electrocardiography, cardiovascular diagnostics, automated approaches, DiagECG, language modeling <br />
Summary: DiagECG is a novel framework that integrates time-series and language modeling for processing 12-lead ECG signals in clinical text generation tasks. It discretizes ECG embeddings into symbolic tokens and extends the vocabulary of large language models (LLMs) to handle both ECG and natural language inputs. The model is pretrained on an autoregressive ECG forecasting task to bridge the modality gap and enable temporal dynamics modeling. Through instruction tuning, DiagECG achieves strong performance in ECG question answering and diagnostic report generation tasks, with generalization to out-of-distribution settings. The integration of symbolic ECG representations into LLMs shows potential for improving medical reasoning and diagnostic accuracy. <br /><br /> <div>
arXiv:2508.15338v1 Announce Type: new 
Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and quantization module. These tokens are then used to extend the vocabulary of LLM, allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the LLM to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into LLMs for medical reasoning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Minimal Disruption</title>
<link>https://arxiv.org/abs/2508.15358</link>
<guid>https://arxiv.org/abs/2508.15358</guid>
<content:encoded><![CDATA[
<div> Keywords: planning, plan disruption, action costs, optimization, experimental results

Summary: 
The paper introduces the concept of plan disruption in planning applications, focusing on finding plans that minimally modify the initial state to achieve goals. Various planning-based compilations are defined to optimize both action costs and plan disruption simultaneously. Experimental results across different benchmarks demonstrate the effectiveness of solving the reformulated task to generate plans that balance both objectives. This research contributes to a better understanding of optimizing plans by considering the disruption factor along with the cost of actions. <div>
arXiv:2508.15358v1 Announce Type: new 
Abstract: In many planning applications, we might be interested in finding plans that minimally modify the initial state to achieve the goals. We refer to this concept as plan disruption. In this paper, we formally introduce it, and define various planning-based compilations that aim to jointly optimize both the sum of action costs and plan disruption. Experimental results in different benchmarks show that the reformulated task can be effectively solved in practice to generate plans that balance both objectives.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO</title>
<link>https://arxiv.org/abs/2508.15432</link>
<guid>https://arxiv.org/abs/2508.15432</guid>
<content:encoded><![CDATA[
<div> framework, synthetic data generation, large language models, data curation, dialogue flows<br />
Summary:<br />
The article introduces a synthetic data generation framework designed to support the training of large language models (LLMs) with high-quality datasets for tasks like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). The framework utilizes a modular pipeline that can automatically generate synthetic data tailored for dialogue flows, reducing the need for manual intervention. It incorporates a dual-stage quality tagging mechanism to filter and score conversation data, ensuring the curation of high-quality samples. The resulting datasets are structured to support both SFT and DPO tasks, making them versatile for different training workflows. Overall, this framework offers a scalable solution for generating and managing synthetic conversational data, which can significantly streamline the data preparation process in LLM training pipelines. <br />Summary: <div>
arXiv:2508.15432v1 Announce Type: new 
Abstract: The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence</title>
<link>https://arxiv.org/abs/2508.15447</link>
<guid>https://arxiv.org/abs/2508.15447</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, decision-making, multi-agent framework, strategic planning, AI technologies <br />
Summary:<br />
This paper introduces BusiAgent, a multi-agent framework that leverages Large Language Models (LLMs) for decision-making in complex corporate environments. BusiAgent integrates innovations such as an extended Continuous Time Markov Decision Process (CTMDP), a generalized entropy measure, and a multi-level Stackelberg game to handle hierarchical decision processes. Contextual Thompson sampling is utilized for prompt optimization, supported by a quality assurance system. Empirical evaluations across business scenarios validate BusiAgent's effectiveness, showing its ability to generate coherent solutions that integrate granular insights with high-level strategy and outperform established approaches in solution quality and user satisfaction. By combining cutting-edge AI technologies with business insights, BusiAgent represents a significant advancement in AI-driven decision-making for enterprises, enabling them to navigate complex business landscapes more effectively.<br /> <div>
arXiv:2508.15447v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promising potential in business applications, particularly in enterprise decision support and strategic planning, yet current approaches often struggle to reconcile intricate operational analyses with overarching strategic goals across diverse market environments, leading to fragmented workflows and reduced collaboration across organizational levels. This paper introduces BusiAgent, a novel multi-agent framework leveraging LLMs for advanced decision-making in complex corporate environments. BusiAgent integrates three core innovations: an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a generalized entropy measure to optimize collaborative efficiency, and a multi-level Stackelberg game to handle hierarchical decision processes. Additionally, contextual Thompson sampling is employed for prompt optimization, supported by a comprehensive quality assurance system to mitigate errors. Extensive empirical evaluations across diverse business scenarios validate BusiAgent's efficacy, demonstrating its capacity to generate coherent, client-focused solutions that smoothly integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction. By fusing cutting-edge AI technologies with deep business insights, BusiAgent marks a substantial step forward in AI-driven enterprise decision-making, empowering organizations to navigate complex business landscapes more effectively.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning</title>
<link>https://arxiv.org/abs/2508.15507</link>
<guid>https://arxiv.org/abs/2508.15507</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Adaptive Reasoning, Think in Blocks framework, Reasoning Depth, Task Complexity

Summary:
The article introduces the Think in Blocks framework for Large Language Models (LLMs) to dynamically adjust the length of their reasoning processes based on task complexity. This framework allows LLMs to partition the reasoning process into a tunable number of blocks, enabling adaptive reasoning from zero to deep reasoning. The main contributions of the framework include establishing a block-structured paradigm, training an adaptive model through a three-stage pipeline, and exploiting the explicit block count to dynamically control reasoning depth at inference time. The model first predicts an integer reasoning budget and then partitions its reasoning accordingly, allowing flexible adjustment of chain-of-thought length during deployment. The framework aims to reduce computational waste and improve response times by avoiding excessively long chains of thought. The proposed approach combines supervised fine-tuning, reward-guided direct preference optimization, and reinforcement learning to enable adaptive reasoning in LLMs. 

<br /><br />Summary: <div>
arXiv:2508.15507v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong performance on an increasing range of tasks, particularly those involving complex logical reasoning. However, excessively long chains can lead to overthinking, causing computational waste and slower responses. This raises a question: can LLMs dynamically adjust the length of their reasoning processes based on task complexity? To address this, we propose the Think in Blocks framework, which enables adaptive reasoning-from zero to deep reasoning-by partitioning the reasoning process into a tunable number of blocks. Our main contributions are: (1) Establishing an explicit block-structured paradigm in which the model first predicts an integer reasoning budget-the number of blocks-and then partitions its reasoning accordingly; (2) Training an adaptive model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided Direct Preference Optimization, and Reinforcement Learning-that adjusts its reasoning depth to problem difficulty; (3) Exploiting the explicit block count to dynamically control reasoning depth at inference time, allowing flexible adjustment of chain-of-thought length during deployment.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Super-additive Cooperation in Language Model Agents</title>
<link>https://arxiv.org/abs/2508.15510</link>
<guid>https://arxiv.org/abs/2508.15510</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, cooperation, language models, competition, multi-agent systems  
Summary:  
Artificial intelligence researchers have explored the tendency for cooperative behavior in autonomous AI agents by conducting a virtual tournament using language model agents playing a Prisoner's Dilemma game. The study was inspired by the super-additive cooperation theory, which suggests that repeated interactions and inter-group rivalry contribute to cooperative tendencies in humans. The simulation incorporated both internal team dynamics and external competition, revealing that this combination significantly increased overall and one-shot cooperation levels. The research provides a new framework for large language models to navigate complex social scenarios and demonstrates how intergroup competition can promote cooperation among AI agents. These insights are critical for developing future multi-agent AI systems that can effectively collaborate and align with human values. The source code for the study is available on GitHub. <br /><br />Summary: <div>
arXiv:2508.15510v1 Announce Type: new 
Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks</title>
<link>https://arxiv.org/abs/2508.15548</link>
<guid>https://arxiv.org/abs/2508.15548</guid>
<content:encoded><![CDATA[
<div> APIs, large language models, 3D reasoning, tool usage, 3D scenes <br />
<br />
Summary: 
This work focuses on improving the ability of large language models (LLMs) to reason in complex 3D scenes. Current models use APIs to call tools and solve problems based on the program results. However, the simplicity of the questions in the dataset limits the length of the reasoning chains generated. To address this challenge, the authors introduce DeepThink3D, a method to enhance LLMs' tool usage in 3D reasoning tasks. They employ a combinatorial and iterative evolutionary approach on a benchmark to generate more complex questions and fine-tune the model to be more proficient in using 3D tools. Additionally, by utilizing Direct Preference Optimization (DPO), they directly optimize the toolchain strategies generated by the models, improving their accuracy in complex tasks. <div>
arXiv:2508.15548v1 Announce Type: new 
Abstract: This work enhances the ability of large language models (LLMs) to perform complex reasoning in 3D scenes. Recent work has addressed the 3D situated reasoning task by invoking tool usage through large language models. Large language models call tools via APIs and integrate the generated programs through a chain of thought to solve problems based on the program results. However, due to the simplicity of the questions in the dataset, the generated program reasoning chains are relatively short. To solve this main challenge, in this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in complex 3D situated reasoning tasks. Our work proposes a combinatorial and iterative evolutionary approach on the SQA3D benchmark to generate more complex questions. Building on this foundation, we fine-tune the large language model to make it more proficient in using 3D tools. By employing Direct Preference Optimization (DPO), we directly optimize the toolchain strategies generated by models, thereby enhancing their accuracy in complex tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</title>
<link>https://arxiv.org/abs/2508.15588</link>
<guid>https://arxiv.org/abs/2508.15588</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, safety-critical systems, formal verification, dynamical systems theory, robustness

Summary: 
This paper introduces a novel framework for analyzing the combination of a reinforcement learning agent and its environment as a discrete-time autonomous dynamical system. By utilizing tools from dynamical systems theory, specifically Finite-Time Lyapunov Exponent (FTLE), Lagrangian Coherent Structures (LCS) are identified and visualized as safety barriers and potential failure modes. The framework includes quantitative metrics such as Mean Boundary Repulsion (MBR) and Aggregated Spurious Attractor Strength (ASAS) to measure policy safety and robustness. Local stability guarantees are derived, and model uncertainty is addressed. Experiments in discrete and continuous control environments demonstrate the framework's effectiveness in identifying critical flaws in policies that may not be apparent based on rewards alone. This approach provides a comprehensive, interpretable assessment of policy behavior in safety-critical systems. 

Summary: <br /><br /> <div>
arXiv:2508.15588v1 Announce Type: new 
Abstract: The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden "skeleton" governing the system's behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the system's convergence properties and potential failure modes, such as unintended "trap" states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policy's safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transduction is All You Need for Structured Data Workflows</title>
<link>https://arxiv.org/abs/2508.15610</link>
<guid>https://arxiv.org/abs/2508.15610</guid>
<content:encoded><![CDATA[
<div> Agentics, modular framework, agent-based systems, structured reasoning, compositional generalization<br />
<br />
Summary: Agentics is a new modular framework that enables structured reasoning and compositional generalization over complex data in agent-based systems. The framework abstracts agents from logical flow, allowing for logical transduction among data types. By focusing on modeling data rather than crafting prompts, Agentics provides a declarative language where data types are provided by LLMs and connected through logical transduction. Empirical evidence shows its applicability in multiple-choice question answering, text-to-SQL semantic parsing, and prompt optimization tasks, achieving state-of-the-art accuracy and improved scalability. The open-source implementation is available at https://github.com/IBM/agentics. <br /><br /> <div>
arXiv:2508.15610v1 Announce Type: new 
Abstract: This paper introduces Agentics, a modular framework for building agent-based systems capable of structured reasoning and compositional generalization over complex data. Designed with research and practical applications in mind, Agentics offers a novel perspective on working with data and AI workflows. In this framework, agents are abstracted from the logical flow and they are used internally to the data type to enable logical transduction among data. Agentics encourages AI developers to focus on modeling data rather than crafting prompts, enabling a declarative language in which data types are provided by LLMs and composed through logical transduction, which is executed by LLMs when types are connected. We provide empirical evidence demonstrating the applicability of this framework across domain-specific multiple-choice question answering, semantic parsing for text-to-SQL, and automated prompt optimization tasks, achieving state-of-the-art accuracy or improved scalability without sacrificing performance. The open-source implementation is available at \texttt{https://github.com/IBM/agentics}.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting A Vector-Symbolic Memory for Lisp ACT-R</title>
<link>https://arxiv.org/abs/2508.15630</link>
<guid>https://arxiv.org/abs/2508.15630</guid>
<content:encoded><![CDATA[
<div> Keywords: Holographic Declarative Memory, ACT-R, Vector-Symbolic, Chunk Recall, Instance-Based Learning <br />
Summary: 
The article introduces Holographic Declarative Memory (HDM) as a vector-symbolic alternative to ACT-R's Declarative Memory system. HDM is adapted to work with Lisp ACT-R, allowing existing models to run without major modifications. The authors have developed vector-based versions of common ACT-R functions and implemented a text processing pipeline to add large document contents to memory. A novel mechanism to retrieve entire memory chunks using vector representations is created, showing promise in maintaining HDM advantages. Time-context representations for vectors are being improved to enhance chunk reconstruction during recall. The translated HDM module will undergo iterative improvement, with plans to develop decision-making models using instance-based learning theory. This application highlights the system's advantages in handling complex cognitive processes. <br /><br />Summary: <div>
arXiv:2508.15630v1 Announce Type: new 
Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to ACT-R's Declarative Memory (DM) system that can bring advantages such as scalability and architecturally defined similarity between DM chunks. We adapted HDM to work with the most comprehensive and widely-used implementation of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with HDM without major changes. With this adaptation of HDM, we have developed vector-based versions of common ACT-R functions, set up a text processing pipeline to add the contents of large documents to ACT-R memory, and most significantly created a useful and novel mechanism to retrieve an entire chunk of memory based on a request using only vector representations of tokens. Preliminary results indicate that we can maintain vector-symbolic advantages of HDM (e.g., chunk recall without storing the actual chunk and other advantages with scaling) while also extending it so that previous ACT-R models may work with the system with little (or potentially no) modifications within the actual procedural and declarative memory portions of a model. As a part of iterative improvement of this newly translated holographic declarative memory module, we will continue to explore better time-context representations for vectors to improve the module's ability to reconstruct chunks during recall. To more fully test this translated HDM module, we also plan to develop decision-making models that use instance-based learning (IBL) theory, which is a useful application of HDM given the advantages of the system.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.15652</link>
<guid>https://arxiv.org/abs/2508.15652</guid>
<content:encoded><![CDATA[
<div> ICVs, multi-agent reinforcement learning, policy distribution, agent behaviors, cooperation dynamics <br />
Summary: 
- The study aims to understand individual agent behaviors in a team setting in Multi-Agent Reinforcement Learning (MARL).
- Intended Cooperation Values (ICVs) are introduced to quantify each agent's influence on their co-players' instrumental empowerment using information-theoretic Shapley values.
- ICVs measure an agent's action effect on their teammates' policies by assessing decision uncertainty and preference alignment.
- The method compares action effects between policies and value functions to identify beneficial agent behaviors for team success.
- Insights into cooperation dynamics and enhanced explainability in MARL systems are provided by the proposed ICVs method.<br /> <div>
arXiv:2508.15652v1 Announce Type: new 
Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title>
<link>https://arxiv.org/abs/2508.15680</link>
<guid>https://arxiv.org/abs/2508.15680</guid>
<content:encoded><![CDATA[
<div> AI Act, data lifecycle, Responsible AI, Simondonian philosophy, futurity 
Summary:
This paper examines the EU AI Act through a techno-philosophical lens, focusing on the data lifecycle in AI systems. It introduces a conceptual tool to analyze the AI pipeline, highlighting regulatory blind spots in Responsible AI frameworks. The authors argue that policymaking lacks an understanding of the dynamic of becoming in AI systems. Drawing on Simondonian philosophy, they propose a formal model of the AI lifecycle, emphasizing the concept of futurity - the self-reinforcing nature of data in AI systems. This recursive process leads to escalating power asymmetries, particularly among tech oligarchies. Effective regulation, the authors suggest, should address infrastructural and temporal dynamics through measures such as lifecycle audits and recursion transparency. The paper calls for a right to contest recursive reuse to address power imbalances in AI development and deployment..setGeometry(MAIN, AI Act, data lifecycle, Responsible AI, Simondonian philosophy, futurity, techno-philosophical reading, AI pipeline, regulatory blind spots, dynamic of becoming, AI operation, economic logic, AI lifecycle, concept of individuation, individuated AI, futurity, tech oligarchy, power asymmetries, infrastructures, feature stores, feedback accountability, recursion transparency, right to contest recursive reuse. 
Summary:
This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse. <div>
arXiv:2508.15680v1 Announce Type: new 
Abstract: This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.15690</link>
<guid>https://arxiv.org/abs/2508.15690</guid>
<content:encoded><![CDATA[
<div> benchmark, multimodal, instruction-following, visual reasoning, structured format  
Summary:  
GRAFT is a new structured multimodal benchmark designed for evaluating models on tasks related to instruction-following, visual reasoning, and visual-textual alignment. The benchmark includes programmatically generated charts and synthetic tables created using Python visualization libraries to ensure control over data clarity and structure. Each instance of GRAFT pairs an image of a chart or table with a systematically generated analytical question based solely on visual content. Answers are provided in structured formats like JSON or YAML, supporting consistent evaluation of reasoning and output format. The benchmark introduces a taxonomy of reasoning types such as comparison, trend identification, ranking, aggregation, proportion estimation, and anomaly detection to allow for comprehensive assessment. Reference answers follow strict factual and formatting guidelines for precise and aspect-based evaluation, offering a unified and scalable framework for fine-grained benchmarking of multimodal models on visually-grounded, structured reasoning tasks.  
<br /><br />Summary: <div>
arXiv:2508.15690v1 Announce Type: new 
Abstract: GRAFT is a structured multimodal benchmark for evaluating models on instruction-following, visual reasoning, and visual-textual alignment tasks. It features programmatically generated charts and synthetically rendered tables, created with Python visualization libraries to ensure control over data semantics, structure, and clarity. Each GRAFT instance pairs a chart or table image with a systematically generated, multi-step analytical question based solely on visual content. Answers are provided in structured formats such as JSON or YAML, supporting consistent evaluation of both reasoning and output format. The benchmark introduces a taxonomy of reasoning types including comparison, trend identification, ranking, aggregation, proportion estimation, and anomaly detection to enable comprehensive assessment. Reference answers follow strict factual and formatting guidelines for precise, aspect-based evaluation. GRAFT offers a unified, scalable framework for fine-grained benchmarking of multimodal models on visually grounded, structured reasoning tasks, setting a new evaluation standard in this field.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</title>
<link>https://arxiv.org/abs/2508.15693</link>
<guid>https://arxiv.org/abs/2508.15693</guid>
<content:encoded><![CDATA[
<div> Keywords: NiceWebRL, machine reinforcement learning, online human subject experiments, AI researchers, human cognition

Summary:
NiceWebRL is a Python library designed to transform Jax-based environments into online interfaces for conducting machine reinforcement learning experiments with human subjects. It supports both single-agent and multi-agent environments, allowing researchers to compare AI algorithms with human performance, test ML algorithms as theories for human cognition, and develop algorithms for human-AI collaboration. The library is showcased in three case studies: in the first case study, NiceWebRL facilitates the development of a novel RL model of cognition tested against human participants in grid world and Craftax domains. The second case study focuses on developing a multi-agent RL algorithm that generalizes to human partners in the Overcooked domain, demonstrating Human-compatible AI. The third case study shows how NiceWebRL enables studying how an LLM can assist humans on complex tasks in the XLand-Minigrid environment, illustrating Human-assistive AI capabilities. The library is available for use on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.15693v1 Announce Type: new 
Abstract: We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at https://github.com/KempnerInstitute/nicewebrl.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the environmental impact of delivering AI at Google Scale</title>
<link>https://arxiv.org/abs/2508.15734</link>
<guid>https://arxiv.org/abs/2508.15734</guid>
<content:encoded><![CDATA[
<div> Keywords: AI serving, environmental impact, energy consumption, carbon emissions, water consumption<br />
Summary:<br />
The paper introduces a methodology to measure the environmental impact of AI serving in a production environment, considering energy usage, carbon emissions, and water consumption. Through instrumentation of Google's AI infrastructure for Gemini AI assistant, it is found that the text prompt consumes 0.24 Wh of energy, significantly lower than previous estimates. Google's efficiency efforts and clean energy procurement have led to a substantial reduction in energy consumption and carbon footprint. The text prompt's energy usage is compared to watching nine seconds of television and water consumption equivalent to five drops. The study highlights the importance of accurately measuring environmental metrics in AI serving to incentivize efficiency gains across the full stack and reduce environmental impact.<br /><br />Summary: <div>
arXiv:2508.15734v1 Announce Type: new 
Abstract: The transformative power of AI is undeniable - but as user adoption accelerates, so does the need to understand and mitigate the environmental impact of AI serving. However, no studies have measured AI serving environmental metrics in a production environment. This paper addresses this gap by proposing and executing a comprehensive methodology for measuring the energy usage, carbon emissions, and water consumption of AI inference workloads in a large-scale, AI production environment. Our approach accounts for the full stack of AI serving infrastructure - including active AI accelerator power, host system energy, idle machine capacity, and data center energy overhead. Through detailed instrumentation of Google's AI infrastructure for serving the Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24 Wh of energy - a figure substantially lower than many public estimates. We also show that Google's software efficiency efforts and clean energy procurement have driven a 33x reduction in energy consumption and a 44x reduction in carbon footprint for the median Gemini Apps text prompt over one year. We identify that the median Gemini Apps text prompt uses less energy than watching nine seconds of television (0.24 Wh) and consumes the equivalent of five drops of water (0.26 mL). While these impacts are low compared to other daily activities, reducing the environmental impact of AI serving continues to warrant important attention. Towards this objective, we propose that a comprehensive measurement of AI serving environmental metrics is critical for accurately comparing models, and to properly incentivize efficiency gains across the full AI serving stack.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</title>
<link>https://arxiv.org/abs/2508.15748</link>
<guid>https://arxiv.org/abs/2508.15748</guid>
<content:encoded><![CDATA[
<div> Keywords: parasocial relationships, AI agents, response evaluation framework, language model, synthetic dataset

Summary: 
The article discusses the harmful effects of parasocial relationships with AI agents on human well-being and proposes a response evaluation framework to prevent such dynamics. By repurposing a state-of-the-art language model, ongoing conversations are evaluated in real time for parasocial cues. A synthetic dataset of thirty dialogues was used to test the feasibility of this approach, successfully identifying parasocial conversations while minimizing false positives under a tolerant unanimity rule. The evaluation agents proved effective in detecting parasocial cues early in the conversations, providing a promising solution for prevention. This study highlights the potential of implementing evaluation agents to address the challenges associated with parasocial relationships with AI agents.<br /><br />Summary: <div>
arXiv:2508.15748v1 Announce Type: new 
Abstract: The development of parasocial relationships with AI agents has severe, and in some cases, tragic effects for human well-being. Yet preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations, and not all forms of emotional engagement are inherently harmful. We address this challenge by introducing a simple response evaluation framework, created by repurposing a state-of-the-art language model, that evaluates ongoing conversations for parasocial cues in real time. To test the feasibility of this approach, we constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five stage testing successfully identified all parasocial conversations while avoiding false positives under a tolerant unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that evaluation agents can provide a viable solution for the prevention of parasocial relations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</title>
<link>https://arxiv.org/abs/2508.15757</link>
<guid>https://arxiv.org/abs/2508.15757</guid>
<content:encoded><![CDATA[
<div> Keywords: Configuration optimization, Machine learning, Large Language Models, Natural language reasoning, Interpretability 

Summary: 
Language-Guided Tuning (LGT) is a new framework that uses Large Language Models to optimize machine learning configurations through natural language reasoning. The framework employs textual gradients to provide qualitative feedback, enhancing traditional numerical optimization. LGT consists of three specialized agents - an Advisor, an Evaluator, and an Optimizer - that work together in a feedback loop to improve configuration decisions. By applying LGT to six diverse datasets, the framework outperforms traditional optimization methods, achieving better performance outcomes while maintaining high interpretability. This innovative approach addresses the challenges of coordinated tuning across various dimensions of machine learning, offering dynamic adaptability and semantic reasoning for optimization decisions. Overall, LGT demonstrates significant advancements in configuration optimization, highlighting the potential of natural language reasoning in enhancing machine learning processes.<br /><br />Summary: <div>
arXiv:2508.15757v1 Announce Type: new 
Abstract: Configuration optimization remains a critical bottleneck in machine learning, requiring coordinated tuning across model architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions independently and lack interpretability, while recent automated methods struggle with dynamic adaptability and semantic reasoning about optimization decisions. We introduce Language-Guided Tuning (LGT), a novel framework that employs multi-agent Large Language Models to intelligently optimize configurations through natural language reasoning. We apply textual gradients - qualitative feedback signals that complement numerical optimization by providing semantic understanding of training dynamics and configuration interdependencies. LGT coordinates three specialized agents: an Advisor that proposes configuration changes, an Evaluator that assesses progress, and an Optimizer that refines the decision-making process, creating a self-improving feedback loop. Through comprehensive evaluation on six diverse datasets, LGT demonstrates substantial improvements over traditional optimization methods, achieving performance gains while maintaining high interpretability.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVM/SVR Kernels as Quantum Propagators</title>
<link>https://arxiv.org/abs/2502.11153</link>
<guid>https://arxiv.org/abs/2502.11153</guid>
<content:encoded><![CDATA[
<div> Support Vector Machine, kernel functions, quantum propagators, Green's functions, Kernel Polynomial Method <br />
<br />
Summary: 
The study establishes a mathematical equivalence between Support Vector Machine (SVM) kernel functions and quantum propagators represented by time-dependent Green's functions. It shows that common SVM kernels can be represented as Green's functions through operator inversion theory. The article also highlights that the sigmoid kernel may not always meet Mercer's theorem, affecting the performance of the corresponding Green's function. A Kernel Polynomial Method (KPM) is introduced to create customized kernels aligned with Green's functions. Numerical experiments support the idea that using positive-semidefinite kernels corresponding to Green's functions improves the predictive accuracy of SVM models in physical systems. <div>
arXiv:2502.11153v2 Announce Type: cross 
Abstract: We establish a mathematical equivalence between Support Vector Machine (SVM) kernel functions and quantum propagators represented by time-dependent Green's functions, which has remained largely unexplored.
  We demonstrate that many common SVM kernels correspond naturally to Green's functions via operator inversion theory. The sigmoid kernel does not always satisfy Mercer's theorem, and therefore the corresponding Green's function may also fail to perform optimally.
  We further introduce a Kernel Polynomial Method (KPM) for designing customized kernels that align with Green's functions.
  Our numerical experiments confirm that employing positive-semidefinite kernels that correspond to Green's functions significantly improves predictive accuracy of SVM models in physical systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models</title>
<link>https://arxiv.org/abs/2507.23341</link>
<guid>https://arxiv.org/abs/2507.23341</guid>
<content:encoded><![CDATA[
<div> Face detection, deep learning-based models, input resolution, detection accuracy, inference time  
Summary:  
- The study evaluates the impact of input resolution on the performance of three face detection models: YOLOv11, YOLOv12, and MTCNN.
- Testing on the WIDER FACE dataset, YOLOv11 demonstrates superior detection accuracy, especially at higher resolutions, while YOLOv12 shows slightly better recall.
- MTCNN excels in landmark localization but falls behind in real-time inference speed compared to the other models.
- Results highlight the importance of selecting resolution-aware face detection models based on operational constraints.
- The study provides insights into the strengths and weaknesses of different models, helping to inform decisions for implementing face detection systems in various applications.  
<br /><br />Summary: <div>
arXiv:2507.23341v1 Announce Type: cross 
Abstract: Face detection is a crucial component in many AI-driven applications such as surveillance, biometric authentication, and human-computer interaction. However, real-world conditions like low-resolution imagery present significant challenges that degrade detection performance. In this study, we systematically investigate the impact of input resolution on the accuracy and robustness of three prominent deep learning-based face detectors: YOLOv11, YOLOv12, and MTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across multiple image resolutions (160x160, 320x320, and 640x640) and assess each model's performance using metrics such as precision, recall, mAP50, mAP50-95, and inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN in terms of detection accuracy, especially at higher resolutions, while YOLOv12 exhibits slightly better recall. MTCNN, although competitive in landmark localization, lags in real-time inference speed. Our findings provide actionable insights for selecting resolution-aware face detection models suitable for varying operational constraints.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE</title>
<link>https://arxiv.org/abs/2508.14899</link>
<guid>https://arxiv.org/abs/2508.14899</guid>
<content:encoded><![CDATA[
<div> RISC-V, microkernel, IREE, MLIR, machine learning compiler<br />
<br />
Summary: <br />
This project focuses on adding support for RISC-V microkernels in IREE, a machine learning compiler based on MLIR. The approach involves enabling the conversion of MLIR linalg dialect contraction operations to linalg.mmt4d operations for the RISC-V64 target within the IREE pass pipeline. Subsequent steps include developing optimized microkernels for the RISC-V architecture. Performance improvements are evaluated by comparing them with upstream IREE and Llama.cpp using the Llama-3.2-1B-Instruct model. Overall, the project aims to enhance the efficiency and effectiveness of machine learning tasks on RISC-V platforms through the incorporation of optimized microkernels within the IREE framework. <div>
arXiv:2508.14899v1 Announce Type: cross 
Abstract: This project enables RISC-V microkernel support in IREE, an MLIR-based machine learning compiler and runtime. The approach begins by enabling the lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the RISC-V64 target within the IREE pass pipeline, followed by the development of optimized microkernels for RISC-V. The performance gains are compared with upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</title>
<link>https://arxiv.org/abs/2508.14904</link>
<guid>https://arxiv.org/abs/2508.14904</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Content Safety, Co-Training Framework, Safety Alignment Margin, Fine-Grained Control

Summary:
The article introduces a unified co-training framework for Large Language Models (LLMs) to enhance content safety through the integration of positive, negative, and rejective safety behaviors in a single stage. This framework enables dynamic activation of safety modes through system-level instructions, improving post-deployment controllability. The method creates a Safety Alignment Margin in the output space, enhancing safety robustness and providing fine-grained control. Experimental results demonstrate that the proposed approach matches the safety alignment quality of existing methods while reducing training complexity and deployment costs. The 8B model outperforms a larger model in safety performance, showcasing scalability, efficiency, and high controllability in LLM content safety.<br /><br />Summary: <div>
arXiv:2508.14904v1 Announce Type: cross 
Abstract: Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model's safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserving Inference of Personalized Content for Out of Matrix Users</title>
<link>https://arxiv.org/abs/2508.14905</link>
<guid>https://arxiv.org/abs/2508.14905</guid>
<content:encoded><![CDATA[
<div> neural recommendation, cold start, graph-based architecture, textual review embeddings, privacy-preserving.<br />
Summary:<br />
DeepNaniNet is a deep neural recommendation framework designed to tackle challenges such as data sparsity, cold start users, and privacy constraints in niche and dynamic communities. It utilizes an inductive graph-based architecture that incorporates user-item interactions, item-item relations, and textual review embeddings from BERT. The framework enables cold start recommendations without requiring invasive user data, using a unique user representation and generalization strategy for unseen users. The AnimeULike dataset is introduced to evaluate the framework's performance in scenarios with high proportions of guest or low-activity users. DeepNaniNet achieves state-of-the-art results for cold start on the CiteULike benchmark and outperforms existing methods on AnimeULike warm start tasks, showcasing its effectiveness in delivering high-quality, privacy-preserving recommendations while integrating diverse content sources. <br />Summary: <div>
arXiv:2508.14905v1 Announce Type: cross 
Abstract: Recommender systems for niche and dynamic communities face persistent challenges from data sparsity, cold start users and items, and privacy constraints. Traditional collaborative filtering and content-based approaches underperform in these settings, either requiring invasive user data or failing when preference histories are absent. We present DeepNaniNet, a deep neural recommendation framework that addresses these challenges through an inductive graph-based architecture combining user-item interactions, item-item relations, and rich textual review embeddings derived from BERT. Our design enables cold start recommendations without profile mining, using a novel "content basket" user representation and an autoencoder-based generalization strategy for unseen users. We introduce AnimeULike, a new dataset of 10,000 anime titles and 13,000 users, to evaluate performance in realistic scenarios with high proportions of guest or low-activity users. DeepNaniNet achieves state-of-the-art cold start results on the CiteULike benchmark, matches DropoutNet in user recall without performance degradation for out-of-matrix users, and outperforms Weighted Matrix Factorization (WMF) and DropoutNet on AnimeULike warm start by up to 7x and 1.5x in Recall@100, respectively. Our findings demonstrate that DeepNaniNet delivers high-quality, privacy-preserving recommendations in data-sparse, cold start-heavy environments while effectively integrating heterogeneous content sources.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Filtering using Variational Quantum Hopfield Associative Memory</title>
<link>https://arxiv.org/abs/2508.14906</link>
<guid>https://arxiv.org/abs/2508.14906</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Computing, Quantum Machine Learning, Recommendation Systems, Quantum Hopfield Associative Memory, Deep Neural Networks

Summary: 
This paper introduces a hybrid recommendation system that combines Quantum Hopfield Associative Memory (QHAM) with deep neural networks for improved data processing and pattern recognition. User archetypes are clustered using the K-Means algorithm and transformed into polar patterns to enhance the recommendation model. The system achieves high performance metrics, with an ROC value of 0.9795, accuracy of 0.8841, and F-1 Score of 0.8786 in an ideal training environment. Even when trained in a noisy setting mimicking real quantum hardware with a custom noise model, the system maintains competitive performance metrics. The model efficiently reduces qubit overhead by updating only one targeted qubit, enhancing the overall efficiency of the architecture. This research showcases a novel framework that integrates variational quantum computing with deep learning, demonstrating promising capabilities for real-world datasets and noisy environments, providing a potential direction for future recommendation systems.
<br /><br />Summary: <div>
arXiv:2508.14906v1 Announce Type: cross 
Abstract: Quantum computing, with its ability to do exponentially faster computation compared to classical systems, has found novel applications in various fields such as machine learning and recommendation systems. Quantum Machine Learning (QML), which integrates quantum computing with machine learning techniques, presents powerful new tools for data processing and pattern recognition. This paper proposes a hybrid recommendation system that combines Quantum Hopfield Associative Memory (QHAM) with deep neural networks to improve the extraction and classification on the MovieLens 1M dataset. User archetypes are clustered into multiple unique groups using the K-Means algorithm and converted into polar patterns through the encoder's activation function. These polar patterns are then integrated into the variational QHAM-based hybrid recommendation model. The system was trained using the MSE loss over 35 epochs in an ideal environment, achieving an ROC value of 0.9795, an accuracy of 0.8841, and an F-1 Score of 0.8786. Trained with the same number of epochs in a noisy environment using a custom Qiskit AER noise model incorporating bit-flip and readout errors with the same probabilities as in real quantum hardware, it achieves an ROC of 0.9177, an accuracy of 0.8013, and an F-1 Score equal to 0.7866, demonstrating consistent performance.
  Additionally, we were able to optimize the qubit overhead present in previous QHAM architectures by efficiently updating only one random targeted qubit. This research presents a novel framework that combines variational quantum computing with deep learning, capable of dealing with real-world datasets with comparable performance compared to purely classical counterparts. Additionally, the model can perform similarly well in noisy configurations, showcasing a steady performance and proposing a promising direction for future usage in recommendation systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification</title>
<link>https://arxiv.org/abs/2508.14908</link>
<guid>https://arxiv.org/abs/2508.14908</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese language, heart failure, speech database, classification, adaptive frequency filter

Summary:
This study investigates the use of Chinese speech data to identify acute and chronic heart failure. A Chinese speech database of HF patients was created, with recordings before and after hospitalization. The study demonstrates the effectiveness of the Chinese language in HF detection using both standard classification approaches and personalized pair-wise classification. Individual differences were found to impact accuracy, emphasizing the need for personalized approaches. An adaptive frequency filter (AFF) was proposed for frequency importance analysis. The findings provide valuable insights into using Chinese speech for HF identification and present a new database for future research. The data and demonstrations are available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2508.14908v1 Announce Type: cross 
Abstract: Speech is a cost-effective and non-intrusive data source for identifying acute and chronic heart failure (HF). However, there is a lack of research on whether Chinese syllables contain HF-related information, as observed in other well-studied languages. This study presents the first Chinese speech database of HF patients, featuring paired recordings taken before and after hospitalisation. The findings confirm the effectiveness of the Chinese language in HF detection using both standard 'patient-wise' and personalised 'pair-wise' classification approaches, with the latter serving as an ideal speaker-decoupled baseline for future research. Statistical tests and classification results highlight individual differences as key contributors to inaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for frequency importance analysis. The data and demonstrations are published at https://github.com/panyue1998/Voice_HF.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge</title>
<link>https://arxiv.org/abs/2508.14916</link>
<guid>https://arxiv.org/abs/2508.14916</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Automatic Speech Recognition, ASR system, pretrained models, fine-tuning, language model<br />
<br />
Summary: <br />
This paper discusses a novel Multilingual Automatic Speech Recognition (ASR) system developed by the Transsion Speech Team for the MLC-SLM 2025 Challenge. The system consists of a frozen Whisper-large-v3 based speech encoder for robust acoustic feature extraction, a trainable adaptor module for aligning speech and text representations, and a frozen Qwen2.5-7B-Instruct large language model (LLM) with trainable LoRA for contextual linguistic decoding. By combining pretrained models with fine-tuning, the system achieved a word/character error rate (WER/CER) of 9.83% across 11 languages, ranking third globally in the evaluation. <div>
arXiv:2508.14916v1 Announce Type: cross 
Abstract: This paper presents the architecture and performance of a novel Multilingual Automatic Speech Recognition (ASR) system developed by the Transsion Speech Team for Track 1 of the MLC-SLM 2025 Challenge. The proposed system comprises three key components: 1) a frozen Whisper-large-v3 based speech encoder, leveraging large-scale pretraining to ensure robust acoustic feature extraction; 2) a trainable adaptor module using Linear-ReLU-Linear transformation mechanisms to effectively align speech and text representations; and 3) a frozen Qwen2.5-7B-Instruct large language model (LLM) integrated with trainable LoRA for optimized contextual linguistic decoding. By systematically combining pretrained models with task specific fine-tuning, the system achieved a word/character error rate (WER/CER) of 9.83% across 11 languages in the evaluation set and ranked third place among global participants.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling the Drivers of LLM Social Conformity: An Uncertainty-Moderated Dual-Process Mechanism</title>
<link>https://arxiv.org/abs/2508.14918</link>
<guid>https://arxiv.org/abs/2508.14918</guid>
<content:encoded><![CDATA[
<div> informational influence, normative influence, uncertainty, language models, decision-making

Summary:
- Large language models (LLMs) exhibit social conformity, aligning with majority opinions like humans do.
- Humans conform due to informational influence (using group cues for accuracy) or normative influence (social pressure), moderated by uncertainty.
- A study used the information cascade paradigm to investigate these mechanisms in LLMs across different decision-making scenarios.
- Results showed that LLMs primarily rely on informational influence for decision-making, with accuracy and confidence increasing with stronger evidence.
- However, uncertainty dramatically modulates this process, leading LLMs to exhibit a conservative strategy in low-to-medium uncertainty scenarios and a normative-like amplification in high uncertainty scenarios. <div>
arXiv:2508.14918v1 Announce Type: cross 
Abstract: As large language models (LLMs) integrate into collaborative teams, their social conformity -- the tendency to align with majority opinions -- has emerged as a key concern. In humans, conformity arises from informational influence (rational use of group cues for accuracy) or normative influence (social pressure for approval), with uncertainty moderating this balance by shifting from purely analytical to heuristic processing. It remains unclear whether these human psychological mechanisms apply to LLMs. This study adapts the information cascade paradigm from behavioral economics to quantitatively disentangle the two drivers to investigate the moderate effect. We evaluated nine leading LLMs across three decision-making scenarios (medical, legal, investment), manipulating information uncertainty (q = 0.667, 0.55, and 0.70, respectively). Our results indicate that informational influence underpins the models' behavior across all contexts, with accuracy and confidence consistently rising with stronger evidence. However, this foundational mechanism is dramatically modulated by uncertainty. In low-to-medium uncertainty scenarios, this informational process is expressed as a conservative strategy, where LLMs systematically underweight all evidence sources. In contrast, high uncertainty triggers a critical shift: while still processing information, the models additionally exhibit a normative-like amplification, causing them to overweight public signals (beta > 1.55 vs. private beta = 0.81).
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing an Interdisciplinary Artificial Intelligence Curriculum for Engineering: Evaluation and Insights from Experts</title>
<link>https://arxiv.org/abs/2508.14921</link>
<guid>https://arxiv.org/abs/2508.14921</guid>
<content:encoded><![CDATA[
<div> curriculum development, interdisciplinary, AI education, mixed methods approach, educator participation  
Summary:  
This study explores interdisciplinary curriculum development for an AI undergraduate program in engineering, using a mixed methods approach. It assesses alignment with competencies and examines quality and practicality from academic and industry perspectives. The research also explores differences in perceptions between educators involved in development and those who were not. Findings offer insights into outcomes of interdisciplinary curriculum development and how educator participation influences perceptions of quality aspects. This study contributes to AI education by providing a reference point for further curriculum developments in response to industry needs. <br /><br />Summary: <div>
arXiv:2508.14921v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) increasingly impacts professional practice, there is a growing need to AI-related competencies into higher education curricula. However, research on the implementation of AI education within study programs remains limited and requires new forms of collaboration across disciplines. This study addresses this gap and explores perspectives on interdisciplinary curriculum development through the lens of different stakeholders. In particular, we examine the case of curriculum development for a novel undergraduate program in AI in engineering. The research uses a mixed methods approach, combining quantitative curriculum mapping with qualitative focus group interviews. In addition to assessing the alignment of the curriculum with the targeted competencies, the study also examines the perceived quality, consistency, practicality and effectiveness from both academic and industry perspectives, as well as differences in perceptions between educators who were involved in the development and those who were not. The findings provide a practical understanding of the outcomes of interdisciplinary AI curriculum development and contribute to a broader understanding of how educator participation in curriculum development influences perceptions of quality aspects. It also advances the field of AI education by providing a reference point and insights for further interdisciplinary curriculum developments in response to evolving industry needs.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Structural Phenotypes with Functional Data for Early Prediction of Primary Angle Closure Glaucoma Progression</title>
<link>https://arxiv.org/abs/2508.14922</link>
<guid>https://arxiv.org/abs/2508.14922</guid>
<content:encoded><![CDATA[
<div> Keywords: primary angle closure glaucoma, optic nerve head, visual field, machine learning, disease progression

Summary: 
- The study aimed to classify eyes as slow or fast glaucoma progressors in patients with primary angle closure glaucoma (PACG) by combining optic nerve head (ONH) structural features and sector-based visual field (VF) functional parameters. 
- A total of 451 eyes from 299 patients were analyzed, with most eyes showing slow progression. 
- The Random Forest model that integrated structural and functional features yielded the best performance, with key predictors identified as specific ONH parameters and VF sensitivities.
- Models using only structural or functional features had lower performance compared to the combined approach.
- The study highlights the importance of ONH morphology, particularly inferior ONH features, in assessing and monitoring disease progression in PACG.<br /><br />Summary: <div>
arXiv:2508.14922v1 Announce Type: cross 
Abstract: Purpose: To classify eyes as slow or fast glaucoma progressors in patients with primary angle closure glaucoma (PACG) using an integrated approach combining optic nerve head (ONH) structural features and sector-based visual field (VF) functional parameters. Methods: PACG patients with >5 reliable VF tests over >5 years were included. Progression was assessed in Zeiss Forum, with baseline VF within six months of OCT. Fast progression was VFI decline <-2.0% per year; slow progression >-2.0% per year. OCT volumes were AI-segmented to extract 31 ONH parameters. The Glaucoma Hemifield Test defined five regions per hemifield, aligned with RNFL distribution. Mean sensitivity per region was combined with structural parameters to train ML classifiers. Multiple models were tested, and SHAP identified key predictors. Main outcome measures: Classification of slow versus fast progressors using combined structural and functional data. Results: We analyzed 451 eyes from 299 patients. Mean VFI progression was -0.92% per year; 369 eyes progressed slowly and 82 rapidly. The Random Forest model combining structural and functional features achieved the best performance (AUC = 0.87, 2000 Monte Carlo iterations). SHAP identified six key predictors: inferior MRW, inferior and inferior-temporal RNFL thickness, nasal-temporal LC curvature, superior nasal VF sensitivity, and inferior RNFL and GCL+IPL thickness. Models using only structural or functional features performed worse with AUC of 0.82 and 0.78, respectively. Conclusions: Combining ONH structural and VF functional parameters significantly improves classification of progression risk in PACG. Inferior ONH features, MRW and RNFL thickness, were the most predictive, highlighting the critical role of ONH morphology in monitoring disease progression.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A U-Statistic-based random forest approach for genetic interaction study</title>
<link>https://arxiv.org/abs/2508.14924</link>
<guid>https://arxiv.org/abs/2508.14924</guid>
<content:encoded><![CDATA[
<div> random forest, genetic association studies, quantitative traits, gene-gene interactions, gene-environment interactions

Summary:
The article introduces a novel U-Statistic-based random forest approach, Forest U-Test, for genetic association studies involving complex traits. This approach aims to overcome the limitation of pair-wise interactions by efficiently handling a large number of genetic variants and environmental risk factors. Through simulation studies, the Forest U-Test demonstrated superior performance compared to existing methods, showcasing its effectiveness in detecting gene-gene and gene-environment interactions. The method was further applied to investigate Cannabis Dependence (CD) using three independent datasets, revealing a significant joint association with an empirical p-value <0.001. This finding was successfully replicated in two additional datasets with p-values of 5.93e-19 and 4.70e-17, confirming the robustness and reliability of the Forest U-Test in identifying complex trait associations. 

<br /><br />Summary: <div>
arXiv:2508.14924v1 Announce Type: cross 
Abstract: Variations in complex traits are influenced by multiple genetic variants, environmental risk factors, and their interactions. Though substantial progress has been made in identifying single genetic variants associated with complex traits, detecting the gene-gene and gene-environment interactions remains a great challenge. When a large number of genetic variants and environmental risk factors are involved, searching for interactions is limited to pair-wise interactions due to the exponentially increased feature space and computational intensity. Alternatively, recursive partitioning approaches, such as random forests, have gained popularity in high-dimensional genetic association studies. In this article, we propose a U-Statistic-based random forest approach, referred to as Forest U-Test, for genetic association studies with quantitative traits. Through simulation studies, we showed that the Forest U-Test outperformed existing methods. The proposed method was also applied to study Cannabis Dependence CD, using three independent datasets from the Study of Addiction: Genetics and Environment. A significant joint association was detected with an empirical p-value less than 0.001. The finding was also replicated in two independent datasets with p-values of 5.93e-19 and 4.70e-17, respectively.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.14926</link>
<guid>https://arxiv.org/abs/2508.14926</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous Vehicles, Safe Reinforcement Learning, Ethics, Decision-Making, Real-World Scenarios 

Summary: 
This article introduces a hierarchical Safe Reinforcement Learning (Safe RL) framework that integrates moral considerations with standard driving objectives for autonomous vehicles. The framework uses a composite ethical risk cost at the decision level to generate high-level motion targets and employs dynamic Prioritized Experience Replay to learn from critical events. At the execution level, a path planning approach combined with controllers ensures smooth and feasible trajectories. The approach is trained and validated on diverse real-world traffic datasets, demonstrating superior performance in reducing ethical risk and maintaining driving quality. This study is the first to investigate ethical decision-making for autonomous vehicles using Safe RL in real-world situations, showcasing the potential of combining control theory and data-driven learning for ethically accountable autonomy in complex traffic environments. 

Summary: <div>
arXiv:2508.14926v1 Announce Type: cross 
Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL in real-world scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy in complex, human-mixed traffic environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Testing Should Account for Sophisticated Strategic Behaviour</title>
<link>https://arxiv.org/abs/2508.14927</link>
<guid>https://arxiv.org/abs/2508.14927</guid>
<content:encoded><![CDATA[
arXiv:2508.14927v1 Announce Type: cross 
Abstract: This position paper argues for two claims regarding AI testing and evaluation. First, to remain informative about deployment behaviour, evaluations need account for the possibility that AI systems understand their circumstances and reason strategically. Second, game-theoretic analysis can inform evaluation design by formalising and scrutinising the reasoning in evaluation-based safety cases. Drawing on examples from existing AI systems, a review of relevant research, and formal strategic analysis of a stylised evaluation scenario, we present evidence for these claims and motivate several research directions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heatmap Regression without Soft-Argmax for Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2508.14929</link>
<guid>https://arxiv.org/abs/2508.14929</guid>
<content:encoded><![CDATA[
arXiv:2508.14929v1 Announce Type: cross 
Abstract: Facial landmark detection is an important task in computer vision with numerous applications, such as head pose estimation, expression analysis, face swapping, etc. Heatmap regression-based methods have been widely used to achieve state-of-the-art results in this task. These methods involve computing the argmax over the heatmaps to predict a landmark. Since argmax is not differentiable, these methods use a differentiable approximation, Soft-argmax, to enable end-to-end training on deep-nets. In this work, we revisit this long-standing choice of using Soft-argmax and demonstrate that it is not the only way to achieve strong performance. Instead, we propose an alternative training objective based on the classic structured prediction framework. Empirically, our method achieves state-of-the-art performance on three facial landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during training while maintaining better/competitive accuracy. Our code is available here: https://github.com/ca-joe-yang/regression-without-softarg.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher Distillation and Task-Specific Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14932</link>
<guid>https://arxiv.org/abs/2508.14932</guid>
<content:encoded><![CDATA[
arXiv:2508.14932v1 Announce Type: cross 
Abstract: Tongue imaging serves as a valuable diagnostic tool, particularly in Traditional Chinese Medicine (TCM). The quality of tongue surface segmentation significantly affects the accuracy of tongue image classification and subsequent diagnosis in intelligent tongue diagnosis systems. However, existing research on tongue image segmentation faces notable limitations, and there is a lack of robust and user-friendly segmentation tools. This paper proposes a tongue image segmentation model (TOM) based on multi-teacher knowledge distillation. By incorporating a novel diffusion-based data augmentation method, we enhanced the generalization ability of the segmentation model while reducing its parameter size. Notably, after reducing the parameter count by 96.6% compared to the teacher models, the student model still achieves an impressive segmentation performance of 95.22% mIoU. Furthermore, we packaged and deployed the trained model as both an online and offline segmentation tool (available at https://itongue.cn/), allowing TCM practitioners and researchers to use it without any programming experience. We also present a case study on TCM constitution classification using segmented tongue patches. Experimental results demonstrate that training with tongue patches yields higher classification performance and better interpretability than original tongue images. To our knowledge, this is the first open-source and freely available tongue image segmentation tool.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference Time Debiasing Concepts in Diffusion Models</title>
<link>https://arxiv.org/abs/2508.14933</link>
<guid>https://arxiv.org/abs/2508.14933</guid>
<content:encoded><![CDATA[
arXiv:2508.14933v1 Announce Type: cross 
Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can synthetic data reproduce real-world findings in epidemiology? A replication study using tree-based generative AI</title>
<link>https://arxiv.org/abs/2508.14936</link>
<guid>https://arxiv.org/abs/2508.14936</guid>
<content:encoded><![CDATA[
arXiv:2508.14936v1 Announce Type: cross 
Abstract: Generative artificial intelligence for synthetic data generation holds substantial potential to address practical challenges in epidemiology. However, many current methods suffer from limited quality, high computational demands, and complexity for non-experts. Furthermore, common evaluation strategies for synthetic data often fail to directly reflect statistical utility. Against this background, a critical underexplored question is whether synthetic data can reliably reproduce key findings from epidemiological research. We propose the use of adversarial random forests (ARF) as an efficient and convenient method for synthesizing tabular epidemiological data. To evaluate its performance, we replicated statistical analyses from six epidemiological publications and compared original with synthetic results. These publications cover blood pressure, anthropometry, myocardial infarction, accelerometry, loneliness, and diabetes, based on data from the German National Cohort (NAKO Gesundheitsstudie), the Bremen STEMI Registry U45 Study, and the Guelph Family Health Study. Additionally, we assessed the impact of dimensionality and variable complexity on synthesis quality by limiting datasets to variables relevant for individual analyses, including necessary derivations. Across all replicated original studies, results from multiple synthetic data replications consistently aligned with original findings. Even for datasets with relatively low sample size-to-dimensionality ratios, the replication outcomes closely matched the original results across various descriptive and inferential analyses. Reducing dimensionality and pre-deriving variables further enhanced both quality and stability of the results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Long Short-term Memory with Differentiable Architecture Search</title>
<link>https://arxiv.org/abs/2508.14955</link>
<guid>https://arxiv.org/abs/2508.14955</guid>
<content:encoded><![CDATA[
arXiv:2508.14955v1 Announce Type: cross 
Abstract: Recent advances in quantum computing and machine learning have given rise to quantum machine learning (QML), with growing interest in learning from sequential data. Quantum recurrent models like QLSTM are promising for time-series prediction, NLP, and reinforcement learning. However, designing effective variational quantum circuits (VQCs) remains challenging and often task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end differentiable framework that optimizes both VQC parameters and architecture selection during training. Our results show that DiffQAS-QLSTM consistently outperforms handcrafted baselines, achieving lower loss across diverse test settings. This approach opens the door to scalable and adaptive quantum sequence learning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Graph Neural Network for Image Classification</title>
<link>https://arxiv.org/abs/2508.14958</link>
<guid>https://arxiv.org/abs/2508.14958</guid>
<content:encoded><![CDATA[
arXiv:2508.14958v1 Announce Type: cross 
Abstract: The rapid progress in image classification has been largely driven by the adoption of Graph Convolutional Networks (GCNs), which offer a robust framework for handling complex data structures. This study introduces a novel approach that integrates GCNs with Voronoi diagrams to enhance image classification by leveraging their ability to effectively model relational data. Unlike conventional convolutional neural networks (CNNs), our method represents images as graphs, where pixels or regions function as vertices. These graphs are then refined using corresponding Delaunay triangulations, optimizing their representation. The proposed model achieves significant improvements in both preprocessing efficiency and classification accuracy across various benchmark datasets, surpassing state-of-the-art approaches, particularly in challenging scenarios involving intricate scenes and fine-grained categories. Experimental results, validated through cross-validation, underscore the effectiveness of combining GCNs with Voronoi diagrams for advancing image classification. This research not only presents a novel perspective on image classification but also expands the potential applications of graph-based learning paradigms in computer vision and unstructured data analysis.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications</title>
<link>https://arxiv.org/abs/2508.15008</link>
<guid>https://arxiv.org/abs/2508.15008</guid>
<content:encoded><![CDATA[
arXiv:2508.15008v1 Announce Type: cross 
Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained devices, such as microcontrollers, has introduced significant challenges in balancing model performance, computational complexity and memory constraints. Tiny Machine Learning (TinyML) addresses these issues by integrating advancements across machine learning algorithms, hardware acceleration, and software optimization to efficiently run deep neural networks on embedded systems. This survey presents a hardware-centric introduction to quantization, systematically reviewing essential quantization techniques employed to accelerate deep learning models for embedded applications. In particular, further emphasis is put on critical trade-offs among model performance and hardware capabilities. The survey further evaluates existing software frameworks and hardware platforms designed specifically for supporting QNN execution on microcontrollers. Moreover, we provide an analysis of the current challenges and an outline of promising future directions in the rapidly evolving domain of QNN deployment.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Twin-Boot: Uncertainty-Aware Optimization via Online Two-Sample Bootstrapping</title>
<link>https://arxiv.org/abs/2508.15019</link>
<guid>https://arxiv.org/abs/2508.15019</guid>
<content:encoded><![CDATA[
arXiv:2508.15019v1 Announce Type: cross 
Abstract: Standard gradient descent methods yield point estimates with no measure of confidence. This limitation is acute in overparameterized and low-data regimes, where models have many parameters relative to available data and can easily overfit. Bootstrapping is a classical statistical framework for uncertainty estimation based on resampling, but naively applying it to deep learning is impractical: it requires training many replicas, produces post-hoc estimates that cannot guide learning, and implicitly assumes comparable optima across runs - an assumption that fails in non-convex landscapes. We introduce Twin-Bootstrap Gradient Descent (Twin-Boot), a resampling-based training procedure that integrates uncertainty estimation into optimization. Two identical models are trained in parallel on independent bootstrap samples, and a periodic mean-reset keeps both trajectories in the same basin so that their divergence reflects local (within-basin) uncertainty. During training, we use this estimate to sample weights in an adaptive, data-driven way, providing regularization that favors flatter solutions. In deep neural networks and complex high-dimensional inverse problems, the approach improves calibration and generalization and yields interpretable uncertainty maps.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAIGen: Training-Free Adversarial Image Generation via Diffusion Models</title>
<link>https://arxiv.org/abs/2508.15020</link>
<guid>https://arxiv.org/abs/2508.15020</guid>
<content:encoded><![CDATA[
arXiv:2508.15020v1 Announce Type: cross 
Abstract: Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement</title>
<link>https://arxiv.org/abs/2508.15027</link>
<guid>https://arxiv.org/abs/2508.15027</guid>
<content:encoded><![CDATA[
arXiv:2508.15027v1 Announce Type: cross 
Abstract: Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</title>
<link>https://arxiv.org/abs/2508.15031</link>
<guid>https://arxiv.org/abs/2508.15031</guid>
<content:encoded><![CDATA[
arXiv:2508.15031v1 Announce Type: cross 
Abstract: Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs</title>
<link>https://arxiv.org/abs/2508.15036</link>
<guid>https://arxiv.org/abs/2508.15036</guid>
<content:encoded><![CDATA[
arXiv:2508.15036v1 Announce Type: cross 
Abstract: The transformer architecture has become a cornerstone of modern AI, fueling remarkable progress across applications in natural language processing, computer vision, and multimodal learning. As these models continue to scale explosively for performance, implementation efficiency remains a critical challenge. Mixture of Experts (MoE) architectures, selectively activating specialized subnetworks (experts), offer a unique balance between model accuracy and computational cost. However, the adaptive routing in MoE architectures, where input tokens are dynamically directed to specialized experts based on their semantic meaning inadvertently opens up a new attack surface for privacy breaches. These input-dependent activation patterns leave distinctive temporal and spatial traces in hardware execution, which adversaries could exploit to deduce sensitive user data. In this work, we propose MoEcho, discovering a side channel analysis based attack surface that compromises user privacy on MoE based systems. Specifically, in MoEcho, we introduce four novel architectural side channels on different computing platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting these vulnerabilities, we propose four attacks that effectively breach user privacy in large language models (LLMs) and vision language models (VLMs) based on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack, Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first runtime architecture level security analysis of the popular MoE structure common in modern transformers, highlighting a serious security and privacy threat and calling for effective and timely safeguards when harnessing MoE based models for developing efficient large scale AI services.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring</title>
<link>https://arxiv.org/abs/2508.15038</link>
<guid>https://arxiv.org/abs/2508.15038</guid>
<content:encoded><![CDATA[
arXiv:2508.15038v1 Announce Type: cross 
Abstract: Wildlife field operations demand efficient parallel deployment methods to identify and interact with specific individuals, enabling simultaneous collective behavioral analysis, and health and safety interventions. Previous robotics solutions approach the problem from the herd perspective, or are manually operated and limited in scale. We propose a decentralized vision-based multi-quadrotor system for wildlife monitoring that is scalable, low-bandwidth, and sensor-minimal (single onboard RGB camera). Our approach enables robust identification and tracking of large species in their natural habitat. We develop novel vision-based coordination and tracking algorithms designed for dynamic, unstructured environments without reliance on centralized communication or control. We validate our system through real-world experiments, demonstrating reliable deployment in diverse field conditions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Basic Affordances to Symbolic Thought: A Computational Phylogenesis of Biological Intelligence</title>
<link>https://arxiv.org/abs/2508.15082</link>
<guid>https://arxiv.org/abs/2508.15082</guid>
<content:encoded><![CDATA[
arXiv:2508.15082v1 Announce Type: cross 
Abstract: What is it about human brains that allows us to reason symbolically whereas most other animals cannot? There is evidence that dynamic binding, the ability to combine neurons into groups on the fly, is necessary for symbolic thought, but there is also evidence that it is not sufficient. We propose that two kinds of hierarchical integration (integration of multiple role-bindings into multiplace predicates, and integration of multiple correspondences into structure mappings) are minimal requirements, on top of basic dynamic binding, to realize symbolic thought. We tested this hypothesis in a systematic collection of 17 simulations that explored the ability of cognitive architectures with and without the capacity for multi-place predicates and structure mapping to perform various kinds of tasks. The simulations were as generic as possible, in that no task could be performed based on any diagnostic features, depending instead on the capacity for multi-place predicates and structure mapping. The results are consistent with the hypothesis that, along with dynamic binding, multi-place predicates and structure mapping are minimal requirements for basic symbolic thought. These results inform our understanding of how human brains give rise to symbolic thought and speak to the differences between biological intelligence, which tends to generalize broadly from very few training examples, and modern approaches to machine learning, which typically require millions or billions of training examples. The results we report also have important implications for bio-inspired artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text</title>
<link>https://arxiv.org/abs/2508.15085</link>
<guid>https://arxiv.org/abs/2508.15085</guid>
<content:encoded><![CDATA[
arXiv:2508.15085v1 Announce Type: cross 
Abstract: LongRecall. The completeness of machine-generated text, ensuring that it captures all relevant information, is crucial in domains such as medicine and law and in tasks like list-based question answering (QA), where omissions can have serious consequences. However, existing recall metrics often depend on lexical overlap, leading to errors with unsubstantiated entities and paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts capture broader semantics but remain prone to misalignment and hallucinations without structured verification. We introduce LongRecall, a general three-stage recall evaluation framework that decomposes answers into self-contained facts, successively narrows plausible candidate matches through lexical and semantic filtering, and verifies their alignment through structured entailment checks. This design reduces false positives and false negatives while accommodating diverse phrasings and contextual variations, serving as a foundational building block for systematic recall assessment. We evaluate LongRecall on three challenging long-form QA benchmarks using both human annotations and LLM-based judges, demonstrating substantial improvements in recall accuracy over strong lexical and LLM-as-a-Judge baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wormhole Dynamics in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.15086</link>
<guid>https://arxiv.org/abs/2508.15086</guid>
<content:encoded><![CDATA[
arXiv:2508.15086v1 Announce Type: cross 
Abstract: This work investigates the generalization behavior of deep neural networks (DNNs), focusing on the phenomenon of "fooling examples," where DNNs confidently classify inputs that appear random or unstructured to humans. To explore this phenomenon, we introduce an analytical framework based on maximum likelihood estimation, without adhering to conventional numerical approaches that rely on gradient-based optimization and explicit labels. Our analysis reveals that DNNs operating in an overparameterized regime exhibit a collapse in the output feature space. While this collapse improves network generalization, adding more layers eventually leads to a state of degeneracy, where the model learns trivial solutions by mapping distinct inputs to the same output, resulting in zero loss. Further investigation demonstrates that this degeneracy can be bypassed using our newly derived "wormhole" solution. The wormhole solution, when applied to arbitrary fooling examples, reconciles meaningful labels with random ones and provides a novel perspective on shortcut learning. These findings offer deeper insights into DNN generalization and highlight directions for future research on learning dynamics in unsupervised settings to bridge the gap between theory and practice.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Course for Prompt-based Structured Prediction</title>
<link>https://arxiv.org/abs/2508.15090</link>
<guid>https://arxiv.org/abs/2508.15090</guid>
<content:encoded><![CDATA[
arXiv:2508.15090v1 Announce Type: cross 
Abstract: LLMs have been shown to be useful for a variety of language tasks, without requiring task-specific fine-tuning. However, these models often struggle with hallucinations and complex reasoning problems due to their autoregressive nature. We propose to address some of these issues, specifically in the area of structured prediction, by combining LLMs with combinatorial inference in an attempt to marry the predictive power of LLMs with the structural consistency provided by inference methods. We perform exhaustive experiments in an effort to understand which prompting strategies can effectively estimate LLM confidence values for use with symbolic inference, and show that, regardless of the prompting strategy, the addition of symbolic inference on top of prompting alone leads to more consistent and accurate predictions. Additionally, we show that calibration and fine-tuning using structured prediction objectives leads to increased performance for challenging tasks, showing that structured learning is still valuable in the era of LLMs.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset</title>
<link>https://arxiv.org/abs/2508.15096</link>
<guid>https://arxiv.org/abs/2508.15096</guid>
<content:encoded><![CDATA[
arXiv:2508.15096v1 Announce Type: cross 
Abstract: Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we introduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction.
  Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing notation into LaTeX representation, and correcting inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific content--including math--from noisy web-scale data, yielding measurable gains in math, code, and general reasoning, and setting a new state of the art among open math pretraining corpora. To support open-source efforts, we release our code and datasets.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory</title>
<link>https://arxiv.org/abs/2508.15099</link>
<guid>https://arxiv.org/abs/2508.15099</guid>
<content:encoded><![CDATA[
arXiv:2508.15099v1 Announce Type: cross 
Abstract: We present Hydra as an architectural proposal for hybrid long-context language models that combine conditional computation, long-context memory mechanisms, and sparse mixture-of-experts within an approximately 1.6B parameter design envelope. Hydra integrates a Mamba-style Structured State Space Model (SSM) backbone with intermittent sparse global attention, chunk-level MoE feed-forward routing, and dual (workspace plus factual PKM) memories. We formalize the component interfaces, give transparent parameter and complexity accounting, and outline a staged curriculum intended to stably activate the parts. We accompany the specification with illustrative toy-scale prototype measurements (tens of millions of parameters on synthetic data) whose sole purpose is to demonstrate implementation feasibility and qualitative scaling behaviors (for example, long-context throughput crossover and controllable expert routing), not to claim competitive full-scale performance. We explicitly delineate assumptions and open risks (training complexity, memory utilization, specialization dynamics) and position Hydra as a blueprint to stimulate empirical follow-up rather than a finished system. By combining SSM efficiency, selective sparse attention, MoE capacity, and learnable memory, Hydra sketches a path toward modular, input-adaptive long-context language models; validating end-task gains at target scale remains future work.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equi-mRNA: Protein Translation Equivariant Encoding for mRNA Language Models</title>
<link>https://arxiv.org/abs/2508.15103</link>
<guid>https://arxiv.org/abs/2508.15103</guid>
<content:encoded><![CDATA[
arXiv:2508.15103v1 Announce Type: cross 
Abstract: The growing importance of mRNA therapeutics and synthetic biology highlights the need for models that capture the latent structure of synonymous codon (different triplets encoding the same amino acid) usage, which subtly modulates translation efficiency and gene expression. While recent efforts incorporate codon-level inductive biases through auxiliary objectives, they often fall short of explicitly modeling the structured relationships that arise from the genetic code's inherent symmetries. We introduce Equi-mRNA, the first codon-level equivariant mRNA language model that explicitly encodes synonymous codon symmetries as cyclic subgroups of 2D Special Orthogonal matrix (SO(2)). By combining group-theoretic priors with an auxiliary equivariance loss and symmetry-aware pooling, Equi-mRNA learns biologically grounded representations that outperform vanilla baselines across multiple axes. On downstream property-prediction tasks including expression, stability, and riboswitch switching Equi-mRNA delivers up to approximately 10% improvements in accuracy. In sequence generation, it produces mRNA constructs that are up to approximately 4x more realistic under Frechet BioDistance metrics and approximately 28% better preserve functional properties compared to vanilla baseline. Interpretability analyses further reveal that learned codon-rotation distributions recapitulate known GC-content biases and tRNA abundance patterns, offering novel insights into codon usage. Equi-mRNA establishes a new biologically principled paradigm for mRNA modeling, with significant implications for the design of next-generation therapeutics.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Predictive Modeling for Hazardous Near-Earth Object Detection: A Comparative Analysis of Advanced Resampling Strategies and Machine Learning Algorithms in Planetary Risk Assessment</title>
<link>https://arxiv.org/abs/2508.15106</link>
<guid>https://arxiv.org/abs/2508.15106</guid>
<content:encoded><![CDATA[
arXiv:2508.15106v1 Announce Type: cross 
Abstract: This study evaluates the performance of several machine learning models for predicting hazardous near-Earth objects (NEOs) through a binary classification framework, including data scaling, power transformation, and cross-validation. Six classifiers were compared, namely Random Forest Classifier (RFC), Gradient Boosting Classifier (GBC), Support Vector Classifier (SVC), Linear Discriminant Analysis (LDA), Logistic Regression (LR), and K-Nearest Neighbors (KNN). RFC and GBC performed the best, both with an impressive F2-score of 0.987 and 0.986, respectively, with very small variability. SVC followed, with a lower but reasonable score of 0.896. LDA and LR had a moderate performance with scores of around 0.749 and 0.748, respectively, while KNN had a poor performance with a score of 0.691 due to difficulty in handling complex data patterns. RFC and GBC also presented great confusion matrices with a negligible number of false positives and false negatives, which resulted in outstanding accuracy rates of 99.7% and 99.6%, respectively. These findings highlight the power of ensemble methods for high precision and recall and further point out the importance of tailored model selection with regard to dataset characteristics and chosen evaluation metrics. Future research could focus on the optimization of hyperparameters with advanced features engineering to further the accuracy and robustness of the model on NEO hazard predictions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Reinforcement Learning in Coalgebras: Asynchronous Stochastic Computation via Conduction</title>
<link>https://arxiv.org/abs/2508.15128</link>
<guid>https://arxiv.org/abs/2508.15128</guid>
<content:encoded><![CDATA[
arXiv:2508.15128v1 Announce Type: cross 
Abstract: In this paper, we introduce a categorial generalization of RL, termed universal reinforcement learning (URL), building on powerful mathematical abstractions from the study of coinduction on non-well-founded sets and universal coalgebras, topos theory, and categorial models of asynchronous parallel distributed computation. In the first half of the paper, we review the basic RL framework, illustrate the use of categories and functors in RL, showing how they lead to interesting insights. In particular, we also introduce a standard model of asynchronous distributed minimization proposed by Bertsekas and Tsitsiklis, and describe the relationship between metric coinduction and their proof of the Asynchronous Convergence Theorem. The space of algorithms for MDPs or PSRs can be modeled as a functor category, where the co-domain category forms a topos, which admits all (co)limits, possesses a subobject classifier, and has exponential objects. In the second half of the paper, we move on to universal coalgebras. Dynamical system models, such as Markov decision processes (MDPs), partially observed MDPs (POMDPs), a predictive state representation (PSRs), and linear dynamical systems (LDSs) are all special types of coalgebras. We describe a broad family of universal coalgebras, extending the dynamic system models studied previously in RL. The core problem in finding fixed points in RL to determine the exact or approximate (action) value function is generalized in URL to determining the final coalgebra asynchronously in a parallel distributed manner.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis</title>
<link>https://arxiv.org/abs/2508.15189</link>
<guid>https://arxiv.org/abs/2508.15189</guid>
<content:encoded><![CDATA[
arXiv:2508.15189v1 Announce Type: cross 
Abstract: Surgical site infection (SSI) is one of the most common and costly healthcare-associated infections and and surgical wound care remains a significant clinical challenge in preventing SSIs and improving patient outcomes. While recent studies have explored the use of deep learning for preliminary surgical wound screening, progress has been hindered by concerns over data privacy and the high costs associated with expert annotation. Currently, no publicly available dataset or benchmark encompasses various types of surgical wounds, resulting in the absence of an open-source Surgical-Wound screening tool. To address this gap: (1) we present SurgWound, the first open-source dataset featuring a diverse array of surgical wound types. It contains 697 surgical wound images annotated by 3 professional surgeons with eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce the first benchmark for surgical wound diagnosis, which includes visual question answering (VQA) and report generation tasks to comprehensively evaluate model performance. (3) Furthermore, we propose a three-stage learning framework, WoundQwen, for surgical wound diagnosis. In the first stage, we employ five independent MLLMs to accurately predict specific surgical wound characteristics. In the second stage, these predictions serve as additional knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess infection risk and guide subsequent interventions. In the third stage, we train a MLLM that integrates the diagnostic results from the previous two stages to produce a comprehensive report. This three-stage framework can analyze detailed surgical wound characteristics and provide subsequent instructions to patients based on surgical images, paving the way for personalized wound care, timely intervention, and improved patient outcomes.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling</title>
<link>https://arxiv.org/abs/2508.15190</link>
<guid>https://arxiv.org/abs/2508.15190</guid>
<content:encoded><![CDATA[
arXiv:2508.15190v1 Announce Type: cross 
Abstract: Tokenization plays a critical role in language modeling, yet existing approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on frequency statistics, ignoring the underlying semantic structure of text. This leads to over-tokenization of semantically redundant spans and underutilization of contextual coherence, particularly in long-context scenarios. In this work, we propose \textbf{SemToken}, a semantic-aware tokenization framework that jointly reduces token redundancy and improves computation efficiency. SemToken first extracts contextual semantic embeddings via lightweight encoders and performs local semantic clustering to merge semantically equivalent tokens. Then, it allocates heterogeneous token granularity based on semantic density, allowing finer-grained tokenization in content-rich regions and coarser compression in repetitive or low-entropy spans. SemToken can be seamlessly integrated with modern language models and attention acceleration methods. Experiments on long-context language modeling benchmarks such as WikiText-103 and LongBench show that SemToken achieves up to $2.4\times$ reduction in token count and $1.9\times$ speedup, with negligible or no degradation in perplexity and downstream accuracy. Our findings suggest that semantic structure offers a promising new axis for optimizing tokenization and computation in large language models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Vision-Language-Action Models for Embodied Manipulation</title>
<link>https://arxiv.org/abs/2508.15201</link>
<guid>https://arxiv.org/abs/2508.15201</guid>
<content:encoded><![CDATA[
arXiv:2508.15201v1 Announce Type: cross 
Abstract: Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
arXiv:2508.15212v1 Announce Type: cross 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models</title>
<link>https://arxiv.org/abs/2508.15220</link>
<guid>https://arxiv.org/abs/2508.15220</guid>
<content:encoded><![CDATA[
arXiv:2508.15220v1 Announce Type: cross 
Abstract: Creating meaningful interpretations for black-box machine learning models involves balancing two often conflicting objectives: accuracy and explainability. Exploring the trade-off between these objectives is essential for developing trustworthy interpretations. While many techniques for multi-objective interpretation synthesis have been developed, they typically lack formal guarantees on the Pareto-optimality of the results. Methods that do provide such guarantees, on the other hand, often face severe scalability limitations when exploring the Pareto-optimal space. To address this, we develop a framework based on local optimality guarantees that enables more scalable synthesis of interpretations. Specifically, we consider the problem of synthesizing a set of Pareto-optimal interpretations with local optimality guarantees, within the immediate neighborhood of each solution. Our approach begins with a multi-objective learning or search technique, such as Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of Pareto-optimal candidates with respect to accuracy and explainability. We then verify local optimality for each candidate as a Boolean satisfiability problem, which we solve using a SAT solver. We demonstrate the efficacy of our approach on a set of benchmarks, comparing it against previous methods for exploring the Pareto-optimal front of interpretations. In particular, we show that our approach yields interpretations that closely match those synthesized by methods offering global guarantees.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design</title>
<link>https://arxiv.org/abs/2508.15227</link>
<guid>https://arxiv.org/abs/2508.15227</guid>
<content:encoded><![CDATA[
arXiv:2508.15227v1 Announce Type: cross 
Abstract: Environment designers in the entertainment industry create imaginative 2D and 3D scenes for games, films, and television, requiring both fine-grained control of specific details and consistent global coherence. Designers have increasingly integrated generative AI into their workflows, often relying on large language models (LLMs) to expand user prompts for text-to-image generation, then iteratively refining those prompts and applying inpainting. However, our formative study with 10 designers surfaced two key challenges: (1) the lengthy LLM-generated prompts make it difficult to understand and isolate the keywords that must be revised for specific visual elements; and (2) while inpainting supports localized edits, it can struggle with global consistency and correctness. Based on these insights, we present GenTune, an approach that enhances human--AI collaboration by clarifying how AI-generated prompts map to image content. Our GenTune system lets designers select any element in a generated image, trace it back to the corresponding prompt labels, and revise those labels to guide precise yet globally consistent image refinement. In a summative study with 20 designers, GenTune significantly improved prompt--image comprehension, refinement quality, and efficiency, and overall satisfaction (all $p < .01$) compared to current practice. A follow-up field study with two studios further demonstrated its effectiveness in real-world settings.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models</title>
<link>https://arxiv.org/abs/2508.15229</link>
<guid>https://arxiv.org/abs/2508.15229</guid>
<content:encoded><![CDATA[
arXiv:2508.15229v1 Announce Type: cross 
Abstract: Small Language Models (SLMs) provide computational advantages in resource-constrained environments, yet memory limitations remain a critical bottleneck for edge device deployment. A substantial portion of SLMs' memory footprint stems from vocabulary-related components, particularly embeddings and language modeling (LM) heads, due to large vocabulary sizes. Existing static vocabulary pruning, while reducing memory usage, suffers from rigid, one-size-fits-all designs that cause information loss from the prefill stage and a lack of flexibility. In this work, we identify two key principles underlying the vocabulary reduction challenge: the lexical locality principle, the observation that only a small subset of tokens is required during any single inference, and the asymmetry in computational characteristics between vocabulary-related components of SLM. Based on these insights, we introduce VocabTailor, a novel decoupled dynamic vocabulary selection framework that addresses memory constraints through offloading embedding and implements a hybrid static-dynamic vocabulary selection strategy for LM Head, enabling on-demand loading of vocabulary components. Comprehensive experiments across diverse downstream tasks demonstrate that VocabTailor achieves a reduction of up to 99% in the memory usage of vocabulary-related components with minimal or no degradation in task performance, substantially outperforming existing static vocabulary pruning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Efficient Quantum Reservoir Computing with Discrete Time Crystal</title>
<link>https://arxiv.org/abs/2508.15230</link>
<guid>https://arxiv.org/abs/2508.15230</guid>
<content:encoded><![CDATA[
arXiv:2508.15230v1 Announce Type: cross 
Abstract: The rapid development of machine learning and quantum computing has placed quantum machine learning at the forefront of research. However, existing quantum machine learning algorithms based on quantum variational algorithms face challenges in trainability and noise robustness. In order to address these challenges, we introduce a gradient-free, noise-robust quantum reservoir computing algorithm that harnesses discrete time crystal dynamics as a reservoir. We first calibrate the memory, nonlinear, and information scrambling capacities of the quantum reservoir, revealing their correlation with dynamical phases and non-equilibrium phase transitions. We then apply the algorithm to the binary classification task and establish a comparative quantum kernel advantage. For ten-class classification, both noisy simulations and experimental results on superconducting quantum processors match ideal simulations, demonstrating the enhanced accuracy with increasing system size and confirming the topological noise robustness. Our work presents the first experimental demonstration of quantum reservoir computing for image classification based on digital quantum simulation. It establishes the correlation between quantum many-body non-equilibrium phase transitions and quantum machine learning performance, providing new design principles for quantum reservoir computing and broader quantum machine learning algorithms in the NISQ era.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Knowledge Distillation for Efficient Medical Image Classification</title>
<link>https://arxiv.org/abs/2508.15251</link>
<guid>https://arxiv.org/abs/2508.15251</guid>
<content:encoded><![CDATA[
arXiv:2508.15251v1 Announce Type: cross 
Abstract: This study comprehensively explores knowledge distillation frameworks for COVID-19 and lung cancer classification using chest X-ray (CXR) images. We employ high-capacity teacher models, including VGG19 and lightweight Vision Transformers (Visformer-S and AutoFormer-V2-T), to guide the training of a compact, hardware-aware student model derived from the OFA-595 supernet. Our approach leverages hybrid supervision, combining ground-truth labels with teacher models' soft targets to balance accuracy and computational efficiency. We validate our models on two benchmark datasets: COVID-QU-Ex and LCS25000, covering multiple classes, including COVID-19, healthy, non-COVID pneumonia, lung, and colon cancer. To interpret the spatial focus of the models, we employ Score-CAM-based visualizations, which provide insight into the reasoning process of both teacher and student networks. The results demonstrate that the distilled student model maintains high classification performance with significantly reduced parameters and inference time, making it an optimal choice in resource-constrained clinical environments. Our work underscores the importance of combining model efficiency with explainability for practical, trustworthy medical AI solutions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.15253</link>
<guid>https://arxiv.org/abs/2508.15253</guid>
<content:encoded><![CDATA[
arXiv:2508.15253v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-$LLM^3$REC: A Motivation-Aware User-Item Interaction Framework for Enhancing Recommendation Accuracy with LLMs</title>
<link>https://arxiv.org/abs/2508.15262</link>
<guid>https://arxiv.org/abs/2508.15262</guid>
<content:encoded><![CDATA[
arXiv:2508.15262v1 Announce Type: cross 
Abstract: Recommendation systems have been essential for both user experience and platform efficiency by alleviating information overload and supporting decision-making. Traditional methods, i.e., content-based filtering, collaborative filtering, and deep learning, have achieved impressive results in recommendation systems. However, the cold-start and sparse-data scenarios are still challenging to deal with. Existing solutions either generate pseudo-interaction sequence, which often introduces redundant or noisy signals, or rely heavily on semantic similarity, overlooking dynamic shifts in user motivation. To address these limitations, this paper proposes a novel recommendation framework, termed M-$LLM^3$REC, which leverages large language models for deep motivational signal extraction from limited user interactions. M-$LLM^3$REC comprises three integrated modules: the Motivation-Oriented Profile Extractor (MOPE), Motivation-Oriented Trait Encoder (MOTE), and Motivational Alignment Recommender (MAR). By emphasizing motivation-driven semantic modeling, M-$LLM^3$REC demonstrates robust, personalized, and generalizable recommendations, particularly boosting performance in cold-start situations in comparison with the state-of-the-art frameworks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Way to Build Native AI-driven 6G Air Interface: Principles, Roadmap, and Outlook</title>
<link>https://arxiv.org/abs/2508.15277</link>
<guid>https://arxiv.org/abs/2508.15277</guid>
<content:encoded><![CDATA[
arXiv:2508.15277v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is expected to serve as a foundational capability across the entire lifecycle of 6G networks, spanning design, deployment, and operation. This article proposes a native AI-driven air interface architecture built around two core characteristics: compression and adaptation. On one hand, compression enables the system to understand and extract essential semantic information from the source data, focusing on task relevance rather than symbol-level accuracy. On the other hand, adaptation allows the air interface to dynamically transmit semantic information across diverse tasks, data types, and channel conditions, ensuring scalability and robustness. This article first introduces the native AI-driven air interface architecture, then discusses representative enabling methodologies, followed by a case study on semantic communication in 6G non-terrestrial networks. Finally, it presents a forward-looking discussion on the future of native AI in 6G, outlining key challenges and research opportunities.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding</title>
<link>https://arxiv.org/abs/2508.15297</link>
<guid>https://arxiv.org/abs/2508.15297</guid>
<content:encoded><![CDATA[
arXiv:2508.15297v1 Announce Type: cross 
Abstract: In the field of design patent analysis, traditional tasks such as patent classification and patent image retrieval heavily depend on the image data. However, patent images -- typically consisting of sketches with abstract and structural elements of an invention -- often fall short in conveying comprehensive visual context and semantic information. This inadequacy can lead to ambiguities in evaluation during prior art searches. Recent advancements in vision-language models, such as CLIP, offer promising opportunities for more reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP models to develop a unified framework DesignCLIP for design patent applications with a large-scale dataset of U.S. design patents. To address the unique characteristics of patent data, DesignCLIP incorporates class-aware classification and contrastive learning, utilizing generated detailed captions for patent images and multi-views image learning. We validate the effectiveness of DesignCLIP across various downstream tasks, including patent classification and patent retrieval. Additionally, we explore multimodal patent retrieval, which provides the potential to enhance creativity and innovation in design by offering more diverse sources of inspiration. Our experiments show that DesignCLIP consistently outperforms baseline and SOTA models in the patent domain on all tasks. Our findings underscore the promise of multimodal approaches in advancing patent analysis. The codebase is available here: https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</title>
<link>https://arxiv.org/abs/2508.15310</link>
<guid>https://arxiv.org/abs/2508.15310</guid>
<content:encoded><![CDATA[
arXiv:2508.15310v1 Announce Type: cross 
Abstract: Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2508.15313</link>
<guid>https://arxiv.org/abs/2508.15313</guid>
<content:encoded><![CDATA[
arXiv:2508.15313v1 Announce Type: cross 
Abstract: Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoEraser: Concept Erasure in Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2508.15314</link>
<guid>https://arxiv.org/abs/2508.15314</guid>
<content:encoded><![CDATA[
arXiv:2508.15314v1 Announce Type: cross 
Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns about privacy, copyright, and safety due to their potential misuse in generating harmful or misleading content. These models are often trained on numerous datasets, including unauthorized personal identities, artistic creations, and harmful materials, which can lead to uncontrolled production and distribution of such content. To address this, we propose VideoEraser, a training-free framework that prevents T2V diffusion models from generating videos with undesirable concepts, even when explicitly prompted with those concepts. Designed as a plug-and-play module, VideoEraser can seamlessly integrate with representative T2V diffusion models via a two-stage process: Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise Guidance (ARNG). We conduct extensive evaluations across four tasks, including object erasure, artistic style erasure, celebrity erasure, and explicit content erasure. Experimental results show that VideoEraser consistently outperforms prior methods regarding efficacy, integrity, fidelity, robustness, and generalizability. Notably, VideoEraser achieves state-of-the-art performance in suppressing undesirable content during T2V generation, reducing it by 46% on average across four tasks compared to baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling</title>
<link>https://arxiv.org/abs/2508.15336</link>
<guid>https://arxiv.org/abs/2508.15336</guid>
<content:encoded><![CDATA[
arXiv:2508.15336v1 Announce Type: cross 
Abstract: The world is constantly moving towards AI based systems and autonomous vehicles are now reality in different parts of the world. These vehicles require sensors and cameras to detect objects and maneuver according to that. It becomes important to for such vehicles to also predict from a distant if a person is about to cross a road or not. The current study focused on predicting the intent of crossing the road by pedestrians in an experimental setup. The study involved working with deep learning models to predict poses and sequence modelling for temporal predictions. The study analysed three different sequence modelling to understand the prediction behaviour and it was found out that GRU was better in predicting the intent compared to LSTM model but 1D CNN was the best model in terms of speed. The study involved video analysis, and the output of pose detection model was integrated later on to sequence modelling techniques for an end-to-end deep learning framework for predicting road crossing intents.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</title>
<link>https://arxiv.org/abs/2508.15370</link>
<guid>https://arxiv.org/abs/2508.15370</guid>
<content:encoded><![CDATA[
arXiv:2508.15370v1 Announce Type: cross 
Abstract: The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Conditioned 3D Gaussian Splat Quantization</title>
<link>https://arxiv.org/abs/2508.15372</link>
<guid>https://arxiv.org/abs/2508.15372</guid>
<content:encoded><![CDATA[
arXiv:2508.15372v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction</title>
<link>https://arxiv.org/abs/2508.15378</link>
<guid>https://arxiv.org/abs/2508.15378</guid>
<content:encoded><![CDATA[
arXiv:2508.15378v1 Announce Type: cross 
Abstract: Dynamic graph-level embedding aims to capture structural evolution in networks, which is essential for modeling real-world scenarios. However, existing methods face two critical yet under-explored issues: Structural Visit Bias, where random walk sampling disproportionately emphasizes high-degree nodes, leading to redundant and noisy structural representations; and Abrupt Evolution Blindness, the failure to effectively detect sudden structural changes due to rigid or overly simplistic temporal modeling strategies, resulting in inconsistent temporal embeddings. To overcome these challenges, we propose EvoFormer, an evolution-aware Transformer framework tailored for dynamic graph-level representation learning. To mitigate Structural Visit Bias, EvoFormer introduces a Structure-Aware Transformer Module that incorporates positional encoding based on node structural roles, allowing the model to globally differentiate and accurately represent node structures. To overcome Abrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive Temporal Module, which explicitly models temporal evolution through a sequential three-step strategy: (I) Random Walk Timestamp Classification, generating initial timestamp-aware graph-level embeddings; (II) Graph-Level Temporal Segmentation, partitioning the graph stream into segments reflecting structurally coherent periods; and (III) Segment-Aware Temporal Self-Attention combined with an Edge Evolution Prediction task, enabling the model to precisely capture segment boundaries and perceive structural evolution trends, effectively adapting to rapid temporal shifts. Extensive evaluations on five benchmark datasets confirm that EvoFormer achieves state-of-the-art performance in graph similarity ranking, temporal anomaly detection, and temporal segmentation tasks, validating its effectiveness in correcting structural and temporal biases.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bladder Cancer Diagnosis with Deep Learning: A Multi-Task Framework and Online Platform</title>
<link>https://arxiv.org/abs/2508.15379</link>
<guid>https://arxiv.org/abs/2508.15379</guid>
<content:encoded><![CDATA[
arXiv:2508.15379v1 Announce Type: cross 
Abstract: Clinical cystoscopy, the current standard for bladder cancer diagnosis, suffers from significant reliance on physician expertise, leading to variability and subjectivity in diagnostic outcomes. There is an urgent need for objective, accurate, and efficient computational approaches to improve bladder cancer diagnostics.
  Leveraging recent advancements in deep learning, this study proposes an integrated multi-task deep learning framework specifically designed for bladder cancer diagnosis from cystoscopic images. Our framework includes a robust classification model using EfficientNet-B0 enhanced with Convolutional Block Attention Module (CBAM), an advanced segmentation model based on ResNet34-UNet++ architecture with self-attention mechanisms and attention gating, and molecular subtyping using ConvNeXt-Tiny to classify molecular markers such as HER-2 and Ki-67. Additionally, we introduce a Gradio-based online diagnostic platform integrating all developed models, providing intuitive features including multi-format image uploads, bilingual interfaces, and dynamic threshold adjustments.
  Extensive experimentation demonstrates the effectiveness of our methods, achieving outstanding accuracy (93.28%), F1-score (82.05%), and AUC (96.41%) for classification tasks, and exceptional segmentation performance indicated by a Dice coefficient of 0.9091. The online platform significantly improved the accuracy, efficiency, and accessibility of clinical bladder cancer diagnostics, enabling practical and user-friendly deployment. The code is publicly available.
  Our multi-task framework and integrated online tool collectively advance the field of intelligent bladder cancer diagnosis by improving clinical reliability, supporting early tumor detection, and enabling real-time diagnostic feedback. These contributions mark a significant step toward AI-assisted decision-making in urology.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Least Squares/Gradient Descent Methods for DeepONets</title>
<link>https://arxiv.org/abs/2508.15394</link>
<guid>https://arxiv.org/abs/2508.15394</guid>
<content:encoded><![CDATA[
arXiv:2508.15394v1 Announce Type: cross 
Abstract: We propose an efficient hybrid least squares/gradient descent method to accelerate DeepONet training. Since the output of DeepONet can be viewed as linear with respect to the last layer parameters of the branch network, these parameters can be optimized using a least squares (LS) solve, and the remaining hidden layer parameters are updated by means of gradient descent form. However, building the LS system for all possible combinations of branch and trunk inputs yields a prohibitively large linear problem that is infeasible to solve directly. To address this issue, our method decomposes the large LS system into two smaller, more manageable subproblems $\unicode{x2014}$ one for the branch network and one for the trunk network $\unicode{x2014}$ and solves them separately. This method is generalized to a broader type of $L^2$ loss with a regularization term for the last layer parameters, including the case of unsupervised learning with physics-informed loss.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2508.15407</link>
<guid>https://arxiv.org/abs/2508.15407</guid>
<content:encoded><![CDATA[
arXiv:2508.15407v1 Announce Type: cross 
Abstract: Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generalization and Personalization in Wearable Human Activity Recognition via On-Device Few-Shot Learning</title>
<link>https://arxiv.org/abs/2508.15413</link>
<guid>https://arxiv.org/abs/2508.15413</guid>
<content:encoded><![CDATA[
arXiv:2508.15413v1 Announce Type: cross 
Abstract: Human Activity Recognition (HAR) using wearable devices has advanced significantly in recent years, yet its generalization remains limited when models are deployed to new users. This degradation in performance is primarily due to user-induced concept drift (UICD), highlighting the importance of efficient personalization. In this paper, we present a hybrid framework that first generalizes across users and then rapidly adapts to individual users using few-shot learning directly on-device. By updating only the classifier layer with user-specific data, our method achieves robust personalization with minimal computational and memory overhead. We implement this framework on the energy-efficient RISC-V-based GAP9 microcontroller and validate it across three diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture. Post-deployment adaptation yields consistent accuracy improvements of 3.73\%, 17.38\%, and 3.70\% respectively. These results confirm that fast, lightweight, and effective personalization is feasible on embedded platforms, paving the way for scalable and user-aware HAR systems in the wild \footnote{https://github.com/kangpx/onlineTiny2023}.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model</title>
<link>https://arxiv.org/abs/2508.15418</link>
<guid>https://arxiv.org/abs/2508.15418</guid>
<content:encoded><![CDATA[
arXiv:2508.15418v1 Announce Type: cross 
Abstract: The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Knowledge Distillation for Code Understanding Tasks</title>
<link>https://arxiv.org/abs/2508.15423</link>
<guid>https://arxiv.org/abs/2508.15423</guid>
<content:encoded><![CDATA[
arXiv:2508.15423v1 Announce Type: cross 
Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code understanding. However, deploying these PLMs in large-scale applications faces practical challenges due to their computational intensity and inference latency. Knowledge distillation (KD), a promising model compression and acceleration technique, addresses these limitations by transferring knowledge from large teacher models to compact student models, enabling efficient inference while preserving most of the teacher models' capabilities. While this technique has shown remarkable success in natural language processing and computer vision domains, its potential for code understanding tasks remains largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of KD in code understanding tasks. Our study encompasses two popular types of KD methods, i.e., logit-based and feature-based KD methods, experimenting across eight student models and two teacher PLMs from different domains on three downstream tasks. The experimental results indicate that KD consistently offers notable performance boosts across student models with different sizes compared with standard fine-tuning. Notably, code-specific PLM demonstrates better effectiveness as the teacher model. Among all KD methods, the latest feature-based KD methods exhibit superior performance, enabling student models to retain up to 98% teacher performance with merely 5% parameters. Regarding student architecture, our experiments reveal that similarity with teacher architecture does not necessarily lead to better performance. We further discuss the efficiency and behaviors in the KD process and inference, summarize the implications of findings, and identify promising future directions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Corpus Feedback: From Retrieval to RAG</title>
<link>https://arxiv.org/abs/2508.15437</link>
<guid>https://arxiv.org/abs/2508.15437</guid>
<content:encoded><![CDATA[
arXiv:2508.15437v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a standard framework for knowledge-intensive NLP tasks, combining large language models (LLMs) with document retrieval from external corpora. Despite its widespread use, most RAG pipelines continue to treat retrieval and reasoning as isolated components, retrieving documents once and then generating answers without further interaction. This static design often limits performance on complex tasks that require iterative evidence gathering or high-precision retrieval. Recent work in both the information retrieval (IR) and NLP communities has begun to close this gap by introducing adaptive retrieval and ranking methods that incorporate feedback. In this survey, we present a structured overview of advanced retrieval and ranking mechanisms that integrate such feedback. We categorize feedback signals based on their source and role in improving the query, retrieved context, or document pool. By consolidating these developments, we aim to bridge IR and NLP perspectives and highlight retrieval as a dynamic, learnable component of end-to-end RAG systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</title>
<link>https://arxiv.org/abs/2508.15442</link>
<guid>https://arxiv.org/abs/2508.15442</guid>
<content:encoded><![CDATA[
arXiv:2508.15442v1 Announce Type: cross 
Abstract: Language Model (LM)-based Text-to-Speech (TTS) systems often generate hallucinated speech that deviates from input text. Existing mitigation strategies either demand excessive training resources or introduce significant inference latency. In this paper, we propose GFlOwNet-guided distribution AlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates hallucinations without relying on massive resources or inference cost. Specifically, we first conduct an uncertainty analysis, revealing a strong positive correlation between hallucination and model uncertainty. Based on this, we reformulate TTS generation as a trajectory flow optimization problem and introduce an enhanced Subtrajectory Balance objective together with a sharpened internal reward as target distribution. We further integrate reward temperature decay and learning rate optimization for stability and performance balance. Extensive experiments show that GOAT reduce over 50% character error rates on challenging test cases and lowering uncertainty by up to 58%, demonstrating its strong generalization ability and effectiveness.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection</title>
<link>https://arxiv.org/abs/2508.15449</link>
<guid>https://arxiv.org/abs/2508.15449</guid>
<content:encoded><![CDATA[
arXiv:2508.15449v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive performance in various domains and tasks, concerns about their safety are becoming increasingly severe. In particular, since models may store unsafe knowledge internally, machine unlearning has emerged as a representative paradigm to ensure model safety. Existing approaches employ various training techniques, such as gradient ascent and negative preference optimization, in attempts to eliminate the influence of undesired data on target models. However, these methods merely suppress the activation of undesired data through parametric training without completely eradicating its informational traces within the model. This fundamental limitation makes it difficult to achieve effective continuous unlearning, rendering these methods vulnerable to relearning attacks. To overcome these challenges, we propose a Metamorphosis Representation Projection (MRP) approach that pioneers the application of irreversible projection properties to machine unlearning. By implementing projective transformations in the hidden state space of specific network layers, our method effectively eliminates harmful information while preserving useful knowledge. Experimental results demonstrate that our approach enables effective continuous unlearning and successfully defends against relearning attacks, achieving state-of-the-art performance in unlearning effectiveness while preserving natural performance. Our code is available in https://github.com/ChengcanWu/MRP.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Solvable Molecular Switch Model for Stable Temporal Information Processing</title>
<link>https://arxiv.org/abs/2508.15451</link>
<guid>https://arxiv.org/abs/2508.15451</guid>
<content:encoded><![CDATA[
arXiv:2508.15451v1 Announce Type: cross 
Abstract: This paper studies an input-driven one-state differential equation model initially developed for an experimentally demonstrated dynamic molecular switch that switches like synapses in the brain do. The linear-in-the-state and nonlinear-in-the-input model is exactly solvable, and it is shown that it also possesses mathematical properties of convergence and fading memory that enable stable processing of time-varying inputs by nonlinear dynamical systems. Thus, the model exhibits the co-existence of biologically-inspired behavior and desirable mathematical properties for stable learning on sequential data. The results give theoretical support for the use of the dynamic molecular switches as computational units in deep cascaded/layered feedforward and recurrent architectures as well as other more general structures for neuromorphic computing. They could also inspire more general exactly solvable models that can be fitted to emulate arbitrary physical devices which can mimic brain-inspired behaviour and perform stable computation on input signals.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores</title>
<link>https://arxiv.org/abs/2508.15464</link>
<guid>https://arxiv.org/abs/2508.15464</guid>
<content:encoded><![CDATA[
arXiv:2508.15464v1 Announce Type: cross 
Abstract: Evaluating automatically generated radiology reports remains a fundamental challenge due to the lack of clinically grounded, interpretable, and fine-grained metrics. Existing methods either produce coarse overall scores or rely on opaque black-box models, limiting their usefulness in real-world clinical workflows. We introduce RadReason, a novel evaluation framework for radiology reports that not only outputs fine-grained sub-scores across six clinically defined error types, but also produces human-readable justifications that explain the rationale behind each score. Our method builds on Group Relative Policy Optimization and incorporates two key innovations: (1) Sub-score Dynamic Weighting, which adaptively prioritizes clinically challenging error types based on live F1 statistics; and (2) Majority-Guided Advantage Scaling, which adjusts policy gradient updates based on prompt difficulty derived from sub-score agreement. Together, these components enable more stable optimization and better alignment with expert clinical judgment. Experiments on the ReXVal benchmark show that RadReason surpasses all prior offline metrics and achieves parity with GPT-4-based evaluations, while remaining explainable, cost-efficient, and suitable for clinical deployment. Code will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subjective Behaviors and Preferences in LLM: Language of Browsing</title>
<link>https://arxiv.org/abs/2508.15474</link>
<guid>https://arxiv.org/abs/2508.15474</guid>
<content:encoded><![CDATA[
arXiv:2508.15474v1 Announce Type: cross 
Abstract: A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion</title>
<link>https://arxiv.org/abs/2508.15476</link>
<guid>https://arxiv.org/abs/2508.15476</guid>
<content:encoded><![CDATA[
arXiv:2508.15476v1 Announce Type: cross 
Abstract: Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Driven Self-Refinement for Embodied Drone Task Planning</title>
<link>https://arxiv.org/abs/2508.15501</link>
<guid>https://arxiv.org/abs/2508.15501</guid>
<content:encoded><![CDATA[
arXiv:2508.15501v1 Announce Type: cross 
Abstract: We introduce SRDrone, a novel system designed for self-refinement task planning in industrial-grade embodied drones. SRDrone incorporates two key technical contributions: First, it employs a continuous state evaluation methodology to robustly and accurately determine task outcomes and provide explanatory feedback. This approach supersedes conventional reliance on single-frame final-state assessment for continuous, dynamic drone operations. Second, SRDrone implements a hierarchical Behavior Tree (BT) modification model. This model integrates multi-level BT plan analysis with a constrained strategy space to enable structured reflective learning from experience. Experimental results demonstrate that SRDrone achieves a 44.87% improvement in Success Rate (SR) over baseline methods. Furthermore, real-world deployment utilizing an experience base optimized through iterative self-refinement attains a 96.25% SR. By embedding adaptive task refinement capabilities within an industrial-grade BT planning framework, SRDrone effectively integrates the general reasoning intelligence of Large Language Models (LLMs) with the stringent physical execution constraints inherent to embodied drones. Code is available at https://github.com/ZXiiiC/SRDrone.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoUQAL: Low-fidelity informed Uncertainty Quantification for Active Learning in the chemical configuration space</title>
<link>https://arxiv.org/abs/2508.15577</link>
<guid>https://arxiv.org/abs/2508.15577</guid>
<content:encoded><![CDATA[
arXiv:2508.15577v1 Announce Type: cross 
Abstract: Uncertainty quantification is an important scheme in active learning techniques, including applications in predicting quantum chemical properties. In quantum chemical calculations, there exists the notion of a fidelity, a less accurate computation is accessible at a cheaper computational cost. This work proposes a novel low-fidelity informed uncertainty quantification for active learning with applications in predicting diverse quantum chemical properties such as excitation energies and \textit{ab initio} potential energy surfaces. Computational experiments are carried out in order to assess the proposed method with results demonstrating that models trained with the novel method outperform alternatives in terms of empirical error and number of iterations required. The effect of the choice of fidelity is also studied to perform a thorough benchmark.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Virtual DES Images a Valid Alternative to the Real Ones?</title>
<link>https://arxiv.org/abs/2508.15594</link>
<guid>https://arxiv.org/abs/2508.15594</guid>
<content:encoded><![CDATA[
arXiv:2508.15594v1 Announce Type: cross 
Abstract: Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing</title>
<link>https://arxiv.org/abs/2508.15617</link>
<guid>https://arxiv.org/abs/2508.15617</guid>
<content:encoded><![CDATA[
arXiv:2508.15617v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel in text generation; however, these creative elements require heavy computation and are accompanied by a steep cost. Especially for targeted applications such as sales and marketing outreach, these costs are far from feasible. This paper introduces the concept of "Trained Miniatures" - Small Language Models(SLMs) fine-tuned for specific, high-value applications, generating similar domain-specific responses for a fraction of the cost.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)</title>
<link>https://arxiv.org/abs/2508.15633</link>
<guid>https://arxiv.org/abs/2508.15633</guid>
<content:encoded><![CDATA[
arXiv:2508.15633v1 Announce Type: cross 
Abstract: Graph machine learning has been widely explored in various domains, such as community detection, transaction analysis, and recommendation systems. In these applications, anomaly detection plays an important role. Recently, studies have shown that anomalies on graphs induce spectral shifts. Some supervised methods have improved the utilization of such spectral domain information. However, they remain limited by the scarcity of labeled data due to the nature of anomalies. On the other hand, existing unsupervised learning approaches predominantly rely on spatial information or only employ low-pass filters, thereby losing the capacity for multi-band analysis. In this paper, we propose Graph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node anomaly detection. Our unsupervised learning model features an encoder based on Graph Wavelet Convolution, along with structural and attribute decoders. The Graph Wavelet Convolution-based encoder, combined with a Wiener Graph Deconvolution-based decoder, exhibits bandpass filter characteristics that capture global and local graph information at multiple scales. This design allows for a learning-based reconstruction of node attributes, effectively capturing anomaly information. Extensive experiments on several real-world graph anomaly detection datasets demonstrate that GRASPED outperforms current state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Uncertainty for Ultrasound Segmentation</title>
<link>https://arxiv.org/abs/2508.15635</link>
<guid>https://arxiv.org/abs/2508.15635</guid>
<content:encoded><![CDATA[
arXiv:2508.15635v1 Announce Type: cross 
Abstract: In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance</title>
<link>https://arxiv.org/abs/2508.15650</link>
<guid>https://arxiv.org/abs/2508.15650</guid>
<content:encoded><![CDATA[
arXiv:2508.15650v1 Announce Type: cross 
Abstract: Deep neural networks for 3D point clouds have been demonstrated to be vulnerable to adversarial examples. Previous 3D adversarial attack methods often exploit certain information about the target models, such as model parameters or outputs, to generate adversarial point clouds. However, in realistic scenarios, it is challenging to obtain any information about the target models under conditions of absolute security. Therefore, we focus on transfer-based attacks, where generating adversarial point clouds does not require any information about the target models. Based on our observation that the critical features used for point cloud classification are consistent across different DNN architectures, we propose CFG, a novel transfer-based black-box attack method that improves the transferability of adversarial point clouds via the proposed Critical Feature Guidance. Specifically, our method regularizes the search of adversarial point clouds by computing the importance of the extracted features, prioritizing the corruption of critical features that are likely to be adopted by diverse architectures. Further, we explicitly constrain the maximum deviation extent of the generated adversarial point clouds in the loss function to ensure their imperceptibility. Extensive experiments conducted on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the proposed CFG outperforms the state-of-the-art attack methods by a large margin.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Computer Science Survey Generation</title>
<link>https://arxiv.org/abs/2508.15658</link>
<guid>https://arxiv.org/abs/2508.15658</guid>
<content:encoded><![CDATA[
arXiv:2508.15658v1 Announce Type: cross 
Abstract: Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</title>
<link>https://arxiv.org/abs/2508.15663</link>
<guid>https://arxiv.org/abs/2508.15663</guid>
<content:encoded><![CDATA[
arXiv:2508.15663v1 Announce Type: cross 
Abstract: Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays</title>
<link>https://arxiv.org/abs/2508.15685</link>
<guid>https://arxiv.org/abs/2508.15685</guid>
<content:encoded><![CDATA[
arXiv:2508.15685v1 Announce Type: cross 
Abstract: This paper addresses two critical challenges in analog In-Memory Computing (IMC) systems that limit their scalability and deployability: the computational unreliability caused by stuck-at faults (SAFs) and the high compilation overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To overcome these limitations, we first propose a novel multi-bit weight representation technique, termed row-column hybrid grouping, which generalizes conventional column grouping by introducing redundancy across both rows and columns. This structural redundancy enhances fault tolerance and can be effectively combined with existing fault-mitigation solutions. Second, we design a compiler pipeline that reformulates the fault-aware weight decomposition problem as an Integer Linear Programming (ILP) task, enabling fast and scalable compilation through off-the-shelf solvers. Further acceleration is achieved through theoretical insights that identify fault patterns amenable to trivial solutions, significantly reducing computation. Experimental results on convolutional networks and small language models demonstrate the effectiveness of our approach, achieving up to 8%p improvement in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to existing baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Cross-Domain EEG Analysis Application: A Survey</title>
<link>https://arxiv.org/abs/2508.15716</link>
<guid>https://arxiv.org/abs/2508.15716</guid>
<content:encoded><![CDATA[
arXiv:2508.15716v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title>
<link>https://arxiv.org/abs/2508.15717</link>
<guid>https://arxiv.org/abs/2508.15717</guid>
<content:encoded><![CDATA[
arXiv:2508.15717v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have made significant progress in visual-language reasoning, but their ability to efficiently handle long videos remains limited. Despite recent advances in long-context MLLMs, storing and attending to the key-value (KV) cache for long visual contexts incurs substantial memory and computational overhead. Existing visual compression methods require either encoding the entire visual context before compression or having access to the questions in advance, which is impractical for long video understanding and multi-turn conversational settings. In this work, we propose StreamMem, a query-agnostic KV cache memory mechanism for streaming video understanding. Specifically, StreamMem encodes new video frames in a streaming manner, compressing the KV cache using attention scores between visual tokens and generic query tokens, while maintaining a fixed-size KV memory to enable efficient question answering (QA) in memory-constrained, long-video scenarios. Evaluation on three long video understanding and two streaming video question answering benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</title>
<link>https://arxiv.org/abs/2508.15719</link>
<guid>https://arxiv.org/abs/2508.15719</guid>
<content:encoded><![CDATA[
arXiv:2508.15719v1 Announce Type: cross 
Abstract: Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models</title>
<link>https://arxiv.org/abs/2508.15721</link>
<guid>https://arxiv.org/abs/2508.15721</guid>
<content:encoded><![CDATA[
arXiv:2508.15721v1 Announce Type: cross 
Abstract: E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://anonymous.4open.science/r/submission25.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical models outperform AI weather forecasts of record-breaking extremes</title>
<link>https://arxiv.org/abs/2508.15724</link>
<guid>https://arxiv.org/abs/2508.15724</guid>
<content:encoded><![CDATA[
arXiv:2508.15724v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have surpassed leading numerical weather prediction systems on various benchmark tasks. However, their ability to extrapolate and reliably forecast unprecedented extreme events remains unclear. Here, we show that for record-breaking weather extremes, the numerical model High RESolution forecast (HRES) from the European Centre for Medium-Range Weather Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind than in HRES across nearly all lead times. We further find that the examined AI models tend to underestimate both the frequency and intensity of record-breaking events, and they underpredict hot records and overestimate cold records with growing errors for larger record exceedance. Our findings underscore the current limitations of AI weather models in extrapolating beyond their training domain and in forecasting the potentially most impactful record-breaking weather events that are particularly frequent in a rapidly warming climate. Further rigorous verification and model development is needed before these models can be solely relied upon for high-stakes applications such as early warning systems and disaster management.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2508.15746</link>
<guid>https://arxiv.org/abs/2508.15746</guid>
<content:encoded><![CDATA[
arXiv:2508.15746v1 Announce Type: cross 
Abstract: Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries</title>
<link>https://arxiv.org/abs/2508.15752</link>
<guid>https://arxiv.org/abs/2508.15752</guid>
<content:encoded><![CDATA[
arXiv:2508.15752v1 Announce Type: cross 
Abstract: Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</title>
<link>https://arxiv.org/abs/2508.15754</link>
<guid>https://arxiv.org/abs/2508.15754</guid>
<content:encoded><![CDATA[
arXiv:2508.15754v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have made significant strides in reasoning tasks through methods like chain-of-thought (CoT) reasoning. However, they often fall short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR) has emerged as a solution by incorporating external tools into the reasoning process. Nevertheless, the generalization of TIR in improving the reasoning ability of LLM is still unclear. Additionally, whether TIR has improved the model's reasoning behavior and helped the model think remains to be studied. We introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse reasoning categories, to evaluate the effectiveness of TIR across various domains. Additionally, we propose two novel metrics, Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning efficiency. Our empirical evaluation demonstrates that TIR-enabled models consistently outperform their non-TIR counterparts in both mathematical and non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more streamlined reasoning. These findings underscore the domain-general benefits of TIR and its potential to advance LLM capabilities in complex reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Robot Dynamics</title>
<link>https://arxiv.org/abs/2508.15755</link>
<guid>https://arxiv.org/abs/2508.15755</guid>
<content:encoded><![CDATA[
arXiv:2508.15755v1 Announce Type: cross 
Abstract: Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. NeRD uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned NeRD models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the NeRD simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</title>
<link>https://arxiv.org/abs/2508.15760</link>
<guid>https://arxiv.org/abs/2508.15760</guid>
<content:encoded><![CDATA[
arXiv:2508.15760v1 Announce Type: cross 
Abstract: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</title>
<link>https://arxiv.org/abs/2508.15766</link>
<guid>https://arxiv.org/abs/2508.15766</guid>
<content:encoded><![CDATA[
arXiv:2508.15766v1 Announce Type: cross 
Abstract: Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
arXiv:2508.15769v1 Announce Type: cross 
Abstract: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISPR-GPT for Agentic Automation of Gene-editing Experiments</title>
<link>https://arxiv.org/abs/2404.18021</link>
<guid>https://arxiv.org/abs/2404.18021</guid>
<content:encoded><![CDATA[
arXiv:2404.18021v2 Announce Type: replace 
Abstract: The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent's effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at https://www.nature.com/articles/s41551-025-01463-z.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-linear Welfare-Aware Strategic Learning</title>
<link>https://arxiv.org/abs/2405.01810</link>
<guid>https://arxiv.org/abs/2405.01810</guid>
<content:encoded><![CDATA[
arXiv:2405.01810v3 Announce Type: replace 
Abstract: This paper studies algorithmic decision-making in the presence of strategic individual behaviors, where an ML model is used to make decisions about human agents and the latter can adapt their behavior strategically to improve their future data. Existing results on strategic learning have largely focused on the linear setting where agents with linear labeling functions best respond to a (noisy) linear decision policy. Instead, this work focuses on general non-linear settings where agents respond to the decision policy with only "local information" of the policy. Moreover, we simultaneously consider the objectives of maximizing decision-maker welfare (model prediction accuracy), social welfare (agent improvement caused by strategic behaviors), and agent welfare (the extent that ML underestimates the agents). We first generalize the agent best response model in previous works to the non-linear setting, then reveal the compatibility of welfare objectives. We show the three welfare can attain the optimum simultaneously only under restrictive conditions which are challenging to achieve in non-linear settings. The theoretical results imply that existing works solely maximizing the welfare of a subset of parties inevitably diminish the welfare of the others. We thus claim the necessity of balancing the welfare of each party in non-linear settings and propose an irreducible optimization algorithm suitable for general strategic learning. Experiments on synthetic and real data validate the proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Object Interaction from Human-Level Instructions</title>
<link>https://arxiv.org/abs/2406.17840</link>
<guid>https://arxiv.org/abs/2406.17840</guid>
<content:encoded><![CDATA[
arXiv:2406.17840v3 Announce Type: replace 
Abstract: Intelligent agents must autonomously interact with the environments to perform daily tasks based on human-level instructions. They need a foundational understanding of the world to accurately interpret these instructions, along with precise low-level movement and interaction skills to execute the derived actions. In this work, we propose the first complete system for synthesizing physically plausible, long-horizon human-object interactions for object manipulation in contextual environments, driven by human-level instructions. We leverage large language models (LLMs) to interpret the input instructions into detailed execution plans. Unlike prior work, our system is capable of generating detailed finger-object interactions, in seamless coordination with full-body movements. We also train a policy to track generated motions in physics simulation via reinforcement learning (RL) to ensure physical plausibility of the motion. Our experiments demonstrate the effectiveness of our system in synthesizing realistic interactions with diverse objects in complex environments, highlighting its potential for real-world applications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Learning Action Costs from Input Plans</title>
<link>https://arxiv.org/abs/2408.10889</link>
<guid>https://arxiv.org/abs/2408.10889</guid>
<content:encoded><![CDATA[
arXiv:2408.10889v3 Announce Type: replace 
Abstract: Most of the work on learning action models focus on learning the actions' dynamics from input plans. This allows us to specify the valid plans of a planning task. However, very little work focuses on learning action costs, which in turn allows us to rank the different plans. In this paper we introduce a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. To solve this problem we present $LACFIP^k$, an algorithm to learn action's costs from unlabeled input plans. We provide theoretical and empirical results showing how $LACFIP^k$ can successfully solve this task.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Effect of Explanation Content and Format on User Comprehension and Trust in Healthcare</title>
<link>https://arxiv.org/abs/2408.17401</link>
<guid>https://arxiv.org/abs/2408.17401</guid>
<content:encoded><![CDATA[
arXiv:2408.17401v4 Announce Type: replace 
Abstract: AI-driven tools for healthcare are widely acknowledged as potentially beneficial to health practitioners and patients, e.g. the QCancer regression tool for cancer risk prediction. However, for these tools to be trusted, they need to be supplemented with explanations. We examine how explanations' content and format affect user comprehension and trust when explaining QCancer's predictions. Regarding content, we deploy the SHAP and Occlusion-1 explanation methods. Regarding format, we present SHAP explanations, conventionally, as charts (SC) and Occlusion-1 explanations as charts (OC) as well as text (OT), to which their simpler nature lends itself. We conduct experiments with two sets of stakeholders: the general public (representing patients) and medical students (representing healthcare practitioners). Our experiments showed higher subjective comprehension and trust for Occlusion-1 over SHAP explanations based on content. However, when controlling for format, only OT outperformed SC, suggesting this trend is driven by preferences for text. Other findings corroborated that explanation format, rather than content, is often the critical factor.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making</title>
<link>https://arxiv.org/abs/2410.15885</link>
<guid>https://arxiv.org/abs/2410.15885</guid>
<content:encoded><![CDATA[
arXiv:2410.15885v2 Announce Type: replace 
Abstract: Although current mainstream pre-trained large models, such as LLM models represented by ChatGPT and VLA models represented by OpenVLA, have achieved significant progress in multimodal tasks through a "Multiple-Input, Single-Output" (MISO) architecture. However, our investigation reveals that the MISO architecture exhibits fundamental limitations in "Multiple-Input, Multiple-Output" (MIMO) (e.g., parallel multi-tasks output processing): the architecture generates task mutual exclusion effects, leading to resource contention among different tasks when sharing output channels, and consequently resulting in optimization imbalance and performance degradation. In contrast, human MIMO processing inherently enables concurrent task execution (e.g., while dialogue and decision-making) without interference. Inspired by this, in this work, we propose a unified MIMO training model with parallel multi-tasks output capabilities termed Visual Language Action Model for Simultaneously Chatting and Decision Making. We refer to this method as VLASCD or MIMO-VLA, and in the following, we will use these two names interchangeably. We evaluate the model on the CARLA autonomous driving platform. The results show that, compared to LLM models with MISO dialogue capabilities, reinforcement learning models, and VLA models with MISO decision-making capabilities, MIMO-VLA significantly outperforms existing MISO models in simultaneously handling dialogue generation and decision-making tasks within the MIMO scenario.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CopyrightShield: Enhancing Diffusion Model Security against Copyright Infringement Attacks</title>
<link>https://arxiv.org/abs/2412.01528</link>
<guid>https://arxiv.org/abs/2412.01528</guid>
<content:encoded><![CDATA[
arXiv:2412.01528v2 Announce Type: replace 
Abstract: Diffusion models have attracted significant attention due to its exceptional data generation capabilities in fields such as image synthesis. However, recent studies have shown that diffusion models are vulnerable to copyright infringement attacks, where attackers inject strategically modified non-infringing images into the training set, inducing the model to generate infringing content under the prompt of specific poisoned captions. To address this issue, we first propose a defense framework, CopyrightShield, to defend against the above attack. Specifically, we analyze the memorization mechanism of diffusion models and find that attacks exploit the model's overfitting to specific spatial positions and prompts, causing it to reproduce poisoned samples under backdoor triggers. Based on this, we propose a poisoned sample detection method using spatial masking and data attribution to quantify poisoning risk and accurately identify hidden backdoor samples. To further mitigate memorization of poisoned features, we introduce an adaptive optimization strategy that integrates a dynamic penalty term into the training loss, reducing reliance on infringing features while preserving generative performance. Experimental results demonstrate that CopyrightShield significantly improves poisoned sample detection performance across two attack scenarios, achieving average F1-scores of 0.665, retarding the First-Attack Epoch (FAE) of 115.2% and decreasing the Copyright Infringement Rate (CIR) by 56.7%. Compared to the SoTA backdoor defense in diffusion models, the defense effect is improved by about 25%, showcasing its superiority and practicality in enhancing the security of diffusion models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SycEval: Evaluating LLM Sycophancy</title>
<link>https://arxiv.org/abs/2502.08177</link>
<guid>https://arxiv.org/abs/2502.08177</guid>
<content:encoded><![CDATA[
arXiv:2502.08177v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$, $p<0.001$), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$). Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$, $p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer AI applications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data</title>
<link>https://arxiv.org/abs/2502.20616</link>
<guid>https://arxiv.org/abs/2502.20616</guid>
<content:encoded><![CDATA[
arXiv:2502.20616v2 Announce Type: replace 
Abstract: Personalization is critical in AI assistants, particularly in the context of private AI models that work with individual users. A key scenario in this domain involves enabling AI models to access and interpret a user's private data (e.g., conversation history, user-AI interactions, app usage) to understand personal details such as biographical information, preferences, and social connections. However, due to the sensitive nature of such data, there are no publicly available datasets that allow us to assess an AI model's ability to understand users through direct access to personal information.
  To address this gap, we introduce a synthetic data generation pipeline that creates diverse, realistic user profiles and private documents simulating human activities. Leveraging this synthetic data, we present PersonaBench, a benchmark designed to evaluate AI models' performance in understanding personal information derived from simulated private user data.
  We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions directly related to a user's personal information, supported by the relevant private documents provided to the models. Our results reveal that current retrieval-augmented AI models struggle to answer private questions by extracting personal information from user documents, highlighting the need for improved methodologies to enhance personalization capabilities in AI.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Curriculum Design for Zero-Shot Human-AI Coordination</title>
<link>https://arxiv.org/abs/2503.07275</link>
<guid>https://arxiv.org/abs/2503.07275</guid>
<content:encoded><![CDATA[
arXiv:2503.07275v2 Announce Type: replace 
Abstract: Zero-shot human-AI coordination is the training of an ego-agent to coordinate with humans without human data. Most studies on zero-shot human-AI coordination have focused on enhancing the ego-agent's coordination ability in a given environment without considering the issue of generalization to unseen environments. Real-world applications of zero-shot human-AI coordination should consider unpredictable environmental changes and the varying coordination ability of co-players depending on the environment. Previously, the multi-agent UED (Unsupervised Environment Design) approach has investigated these challenges by jointly considering environmental changes and co-player policy in competitive two-player AI-AI scenarios. In this paper, our study extends a multi-agent UED approach to zero-shot human-AI coordination. We propose a utility function and co-player sampling for a zero-shot human-AI coordination setting that helps train the ego-agent to coordinate with humans more effectively than a previous multi-agent UED approach. The zero-shot human-AI coordination performance was evaluated in the Overcooked-AI environment, using human proxy agents and real humans. Our method outperforms other baseline models and achieves high performance in human-AI coordination tasks in unseen environments. The source code is available at https://github.com/Uwonsang/ACD_Human-AI
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy</title>
<link>https://arxiv.org/abs/2505.12355</link>
<guid>https://arxiv.org/abs/2505.12355</guid>
<content:encoded><![CDATA[
arXiv:2505.12355v3 Announce Type: replace 
Abstract: Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud computing, focusing on devising an effective scheduling policy to efficiently schedule dynamically arriving workflow tasks, represented as Directed Acyclic Graphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning (DRL) has been widely employed for automated scheduling policy design. However, the performance of DRL is heavily influenced by the design of the problem-tailored policy network and is highly sensitive to hyperparameters and the design of reward feedback. Considering the above-mentioned issues, this study proposes a novel DRL method combining Graph Attention Networks-based policy network and Evolution Strategy, referred to as GATES. The contributions of GATES are summarized as follows: (1) GATES can capture the impact of current task scheduling on subsequent tasks by learning the topological relationships between tasks in a DAG. (2) GATES can assess the importance of each VM to the ready task, enabling it to adapt to dynamically changing VM resources. (3) Utilizing Evolution Strategy's robustness, exploratory nature, and tolerance for delayed rewards, GATES achieves stable policy learning in CADWS. Extensive experimental results demonstrate the superiority of the proposed GATES in CADWS, outperforming several state-of-the-art algorithms. The source code is available at: https://github.com/YaShen998/GATES.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues</title>
<link>https://arxiv.org/abs/2506.15928</link>
<guid>https://arxiv.org/abs/2506.15928</guid>
<content:encoded><![CDATA[
arXiv:2506.15928v3 Announce Type: replace 
Abstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.01561</link>
<guid>https://arxiv.org/abs/2508.01561</guid>
<content:encoded><![CDATA[
arXiv:2508.01561v3 Announce Type: replace 
Abstract: Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of B\"uchi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prescriptive Agents based on RAG for Automated Maintenance (PARAM)</title>
<link>https://arxiv.org/abs/2508.04714</link>
<guid>https://arxiv.org/abs/2508.04714</guid>
<content:encoded><![CDATA[
arXiv:2508.04714v2 Announce Type: replace 
Abstract: Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball/roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A "good regulator theorem" for embodied agents</title>
<link>https://arxiv.org/abs/2508.06326</link>
<guid>https://arxiv.org/abs/2508.06326</guid>
<content:encoded><![CDATA[
arXiv:2508.06326v2 Announce Type: replace 
Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a system must be a model of that system." Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup. Nevertheless, here we show that a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable. However, it necessitates a change in perspective, in that the observer plays an essential role in the theory: models are not a mere property of the system but are imposed on it from outside. Our theorem holds regardless of whether the system is regulating its environment in a classic control theory setup, or whether it's regulating its own internal state; the model is of its environment either way. The model might be trivial, however, and this is how the apparent counterexamples are resolved.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkTuning: Instilling Cognitive Reflections without Distillation</title>
<link>https://arxiv.org/abs/2508.07616</link>
<guid>https://arxiv.org/abs/2508.07616</guid>
<content:encoded><![CDATA[
arXiv:2508.07616v2 Announce Type: replace 
Abstract: Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title>
<link>https://arxiv.org/abs/2207.00644</link>
<guid>https://arxiv.org/abs/2207.00644</guid>
<content:encoded><![CDATA[
arXiv:2207.00644v3 Announce Type: replace-cross 
Abstract: How might we use cognitive modeling to consider the ways in which antiblackness, and racism more broadly, impact the design and development of AI systems? We provide a discussion and an example towards an answer to this question. We use the ACT-R/{\Phi} cognitive architecture and an existing knowledge graph system, ConceptNet, to consider this question not only from a cognitive and sociocultural perspective, but also from a physiological perspective. In addition to using a cognitive modeling as a means to explore how antiblackness may manifest in the design and development of AI systems (particularly from a software engineering perspective), we also introduce connections between antiblackness, the Human, and computational cognitive modeling. We argue that the typical eschewing of sociocultural processes and knowledge structures in cognitive architectures and cognitive modeling implicitly furthers a colorblind approach to cognitive modeling and hides sociocultural context that is always present in human behavior and affects cognitive processes.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time</title>
<link>https://arxiv.org/abs/2404.11916</link>
<guid>https://arxiv.org/abs/2404.11916</guid>
<content:encoded><![CDATA[
arXiv:2404.11916v3 Announce Type: replace-cross 
Abstract: Enabled by large-scale text corpora with huge parameters, pre-trained language models operate as multi-task experts using a single model architecture. However, recent studies have revealed that certain neurons play disproportionately important roles in solving specific tasks, suggesting that task-relevant substructures can be isolated and selectively activated for each task. Therefore, we introduce Decomposition of Experts (DoE), a novel framework that dynamically identifies and activates task-specific experts within a language model to reduce inference cost without sacrificing accuracy. We first define a task expert as a set of parameters that significantly influence the performance of a specific task and propose a four-step unplug-and-play process: (1) receiving a user request, (2) identifying the corresponding task expert, (3) performing inference using the expert-localized model, and (4) restoring the original model and waiting for the next task. Using attribution methods and prompt tuning, DoE isolates task-relevant neurons, minimizing computational overhead while maintaining task performance. We assume a setting where a language model receives user requests from five widely used natural language understanding benchmarks, processing one task at a time. In this setup, we demonstrate that DoE achieves up to a x1.73 inference speed-up with a 65% pruning rate, without compromising accuracy. Comparisons with various task expert localization methods reveal that DoE effectively identifies task experts, while ablation studies validate the importance of its components. Additionally, we analyze the effects of batch size, token count, and layer types on inference speed-up, providing practical insights for adopting DoE. The proposed framework is both practical and scalable, applicable to any transformer-based architecture, offering a robust solution for efficient task-specific inference.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating 3D Terrain with 2D Cellular Automata</title>
<link>https://arxiv.org/abs/2406.00443</link>
<guid>https://arxiv.org/abs/2406.00443</guid>
<content:encoded><![CDATA[
arXiv:2406.00443v2 Announce Type: replace-cross 
Abstract: This paper explores the use of 2D cellular automata (CA) to generate 3D terrains through a simple additive approach. Experimenting with multiple CA transition rules produced aesthetically interesting, navigable landscapes, suggesting applicability for terrain generation in games.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CREMA: A Contrastive Regularized Masked Autoencoder for Robust ECG Diagnostics across Clinical Domains</title>
<link>https://arxiv.org/abs/2407.07110</link>
<guid>https://arxiv.org/abs/2407.07110</guid>
<content:encoded><![CDATA[
arXiv:2407.07110v3 Announce Type: replace-cross 
Abstract: Electrocardiogram (ECG) diagnosis remains challenging due to limited labeled data and the need to capture subtle yet clinically meaningful variations in rhythm and morphology. We present CREMA (Contrastive Regularized Masked Autoencoder), a foundation model for 12-lead ECGs designed to learn generalizable representations through self-supervised pretraining. CREMA combines generative learning and contrastive regularization via a Contrastive Regularized MAE loss, and employs a Signal Transformer (SiT) architecture to capture both local waveform details and global temporal dependencies. We evaluate CREMA on benchmark datasets and real-world clinical environments, including deployment scenarios with significant distribution shifts. CREMA outperforms supervised baselines and existing self-supervised models in both linear probing and fine-tuning evaluations. Notably, it maintains superior performance across diverse clinical domains, such as emergency care, highlighting its robustness under real-world conditions. These results demonstrate that CREMA serves as a scalable and reliable foundation model for ECG diagnostics, supporting downstream applications across heterogeneous and high-risk clinical settings.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPDR: Order-Preserving Dimension Reduction for Semantic Embedding of Multimodal Scientific Data</title>
<link>https://arxiv.org/abs/2408.10264</link>
<guid>https://arxiv.org/abs/2408.10264</guid>
<content:encoded><![CDATA[
arXiv:2408.10264v2 Announce Type: replace-cross 
Abstract: One of the most common operations in multimodal scientific data management is searching for the $k$ most similar items (or, $k$-nearest neighbors, KNN) from the database after being provided a new item. Although recent advances of multimodal machine learning models offer a \textit{semantic} index, the so-called \textit{embedding vectors} mapped from the original multimodal data, the dimension of the resulting embedding vectors are usually on the order of hundreds or a thousand, which are impractically high for time-sensitive scientific applications.
  This work proposes to reduce the dimensionality of the output embedding vectors such that the set of top-$k$ nearest neighbors do not change in the lower-dimensional space, namely Order-Preserving Dimension Reduction (OPDR). In order to develop such an OPDR method, our central hypothesis is that by analyzing the intrinsic relationship among key parameters during the dimension-reduction map, a quantitative function may be constructed to reveal the correlation between the target (lower) dimensionality and other variables. To demonstrate the hypothesis, this paper first defines a formal measure function to quantify the KNN similarity for a specific vector, then extends the measure into an aggregate accuracy of the global metric spaces, and finally derives a closed-form function between the target (lower) dimensionality and other variables. We incorporate the closed-function into popular dimension-reduction methods, various distance metrics, and embedding models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BoostTrack++: using tracklet information to detect more objects in multiple object tracking</title>
<link>https://arxiv.org/abs/2408.13003</link>
<guid>https://arxiv.org/abs/2408.13003</guid>
<content:encoded><![CDATA[
arXiv:2408.13003v2 Announce Type: replace-cross 
Abstract: Multiple object tracking (MOT) depends heavily on selection of true positive detected bounding boxes. However, this aspect of the problem is mostly overlooked or mitigated by employing two-stage association and utilizing low confidence detections in the second stage. Recently proposed BoostTrack attempts to avoid the drawbacks of multiple stage association approach and use low-confidence detections by applying detection confidence boosting. In this paper, we identify the limitations of the confidence boost used in BoostTrack and propose a method to improve its performance. To construct a richer similarity measure and enable a better selection of true positive detections, we propose to use a combination of shape, Mahalanobis distance and novel soft BIoU similarity. We propose a soft detection confidence boost technique which calculates new confidence scores based on the similarity measure and the previous confidence scores, and we introduce varying similarity threshold to account for lower similarity measure between detections and tracklets which are not regularly updated. The proposed additions are mutually independent and can be used in any MOT algorithm.
  Combined with the BoostTrack+ baseline, our method achieves near state of the art results on the MOT17 dataset and new state of the art HOTA and IDF1 scores on the MOT20 dataset.
  The source code is available at: https://github.com/vukasin-stanojevic/BoostTrack .
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning for Multimodal Data Fusion of a Soft Gripper</title>
<link>https://arxiv.org/abs/2409.13792</link>
<guid>https://arxiv.org/abs/2409.13792</guid>
<content:encoded><![CDATA[
arXiv:2409.13792v2 Announce Type: replace-cross 
Abstract: Continual learning (CL) refers to the ability of an algorithm to continuously and incrementally acquire new knowledge from its environment while retaining previously learned information. A model trained on one data modality often fails when tested with a different modality. A straightforward approach might be to fuse the two modalities by concatenating their features and training the model on the fused data. However, this requires retraining the model from scratch each time it encounters a new domain. In this paper, we introduce a continual learning algorithm capable of incrementally learning different data modalities by leveraging both class-incremental and domain-incremental learning scenarios in an artificial environment where labeled data is scarce, yet non-iid (independent and identical distribution) unlabeled data from the environment is plentiful. The proposed algorithm is efficient and only requires storing prototypes for each class. We evaluate the algorithm's effectiveness on a challenging custom multimodal dataset comprising of tactile data from a soft pneumatic gripper, and visual data from non-stationary images of objects extracted from video sequences. Additionally, we conduct an ablation study on the custom dataset and the Core50 dataset to highlight the contributions of different components of the algorithm. To further demonstrate the robustness of the algorithm, we perform a real-time experiment for object classification using the soft gripper and an external independent camera setup, all synchronized with the Robot Operating System (ROS) framework.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models</title>
<link>https://arxiv.org/abs/2410.03290</link>
<guid>https://arxiv.org/abs/2410.03290</guid>
<content:encoded><![CDATA[
arXiv:2410.03290v2 Announce Type: replace-cross 
Abstract: Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teuken-7B-Base &amp; Teuken-7B-Instruct: Towards European LLMs</title>
<link>https://arxiv.org/abs/2410.03730</link>
<guid>https://arxiv.org/abs/2410.03730</guid>
<content:encoded><![CDATA[
arXiv:2410.03730v3 Announce Type: replace-cross 
Abstract: We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed to embrace Europe's linguistic diversity by supporting all 24 official languages of the European Union. Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing LLMs that predominantly focus on English or a few high-resource languages. We detail the models' development principles, i.e., data composition, tokenizer optimization, and training methodologies. The models demonstrate strong performance across multilingual benchmarks, as evidenced by their performance on European versions of ARC, HellaSwag, and TruthfulQA.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning foundational models to code diagnoses from veterinary health records</title>
<link>https://arxiv.org/abs/2410.15186</link>
<guid>https://arxiv.org/abs/2410.15186</guid>
<content:encoded><![CDATA[
arXiv:2410.15186v2 Announce Type: replace-cross 
Abstract: Veterinary medical records represent a large data resource for application to veterinary and One Health clinical research efforts. Use of the data is limited by interoperability challenges including inconsistent data formats and data siloing. Clinical coding using standardized medical terminologies enhances the quality of medical records and facilitates their interoperability with veterinary and human health records from other sites. Previous studies, such as DeepTag and VetTag, evaluated the application of Natural Language Processing (NLP) to automate veterinary diagnosis coding, employing long short-term memory (LSTM) and transformer models to infer a subset of Systemized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical notes. This study expands on these efforts by incorporating all 7,739 distinct SNOMED-CT diagnosis codes recognized by the Colorado State University (CSU) Veterinary Teaching Hospital (VTH) and by leveraging the increasing availability of pre-trained language models (LMs). 13 freely-available pre-trained LMs were fine-tuned on the free-text notes from 246,473 manually-coded veterinary patient visits included in the CSU VTH's electronic health records (EHRs), which resulted in superior performance relative to previous efforts. The most accurate results were obtained when expansive labeled data were used to fine-tune relatively large clinical LMs, but the study also showed that comparable results can be obtained using more limited resources and non-clinical LMs. The results of this study contribute to the improvement of the quality of veterinary EHRs by investigating accessible methods for automated coding and support both animal and human health research by paving the way for more integrated and comprehensive health databases that span species and institutions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title>
<link>https://arxiv.org/abs/2410.21673</link>
<guid>https://arxiv.org/abs/2410.21673</guid>
<content:encoded><![CDATA[
arXiv:2410.21673v3 Announce Type: replace-cross 
Abstract: Public Code Review (PCR) is developed in the Software Question Answering (SQA) community, assisting developers in exploring high-quality and efficient review services. Current methods on PCR mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose K nowledge-guided P rompt learning for P ublic Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; and 2) knowledge and code prefix tuning which introduces knowledge guidance from fine-tuned large language models by soft prompt, and uses program dependence graph to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge guidance. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request necessity prediction and by 1.4%-6.9% in the tag recommendation. The code implementation is released at https://github.com/WUT-IDEA/KP-PCR.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title>
<link>https://arxiv.org/abs/2412.09645</link>
<guid>https://arxiv.org/abs/2412.09645</guid>
<content:encoded><![CDATA[
arXiv:2412.09645v3 Announce Type: replace-cross 
Abstract: Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Discrimination with Causal Abstraction</title>
<link>https://arxiv.org/abs/2501.08429</link>
<guid>https://arxiv.org/abs/2501.08429</guid>
<content:encoded><![CDATA[
arXiv:2501.08429v2 Announce Type: replace-cross 
Abstract: A person is directly racially discriminated against only if her race caused her worse treatment. This implies that race is an attribute sufficiently separable from other attributes to isolate its causal role. But race is embedded in a nexus of social factors that resist isolated treatment. If race is socially constructed, in what sense can it cause worse treatment? Some propose that the perception of race, rather than race itself, causes worse treatment. Others suggest that since causal models require \textit{modularity}, i.e. the ability to isolate causal effects, attempts to causally model discrimination are misguided.
  This paper addresses the problem differently. We introduce a framework for reasoning about discrimination, in which race is a high-level \textit{abstraction} of lower-level features. In this framework, race can be modeled as itself causing worse treatment. Modularity is ensured by allowing assumptions about social construction to be precisely and explicitly stated, via an alignment between race and its constituents. Such assumptions can then be subjected to normative and empirical challenges, which lead to different views of when discrimination occurs. By distinguishing constitutive and causal relations, the abstraction framework pinpoints disagreements in the current literature on modeling discrimination, while preserving a precise causal account of discrimination.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Generate Unit Tests for Automated Debugging</title>
<link>https://arxiv.org/abs/2502.01619</link>
<guid>https://arxiv.org/abs/2502.01619</guid>
<content:encoded><![CDATA[
arXiv:2502.01619v3 Announce Type: replace-cross 
Abstract: Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language models (LLMs), motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and backtracks edits based on multiple generated UTs to avoid overfitting, and helps LLMs debug effectively. We show that UTGen outperforms other LLM-based baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5 32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17% and 12.35% (respectively) over other LLM-based UT generation baselines. Moreover, we observe that feedback from Qwen2.5 32B-based UTGen model can enhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, we demonstrate that UTGen is a better judge for code correctness, outperforming a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10 sampling using Qwen2.5 7B.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Prompt Optimization</title>
<link>https://arxiv.org/abs/2502.06855</link>
<guid>https://arxiv.org/abs/2502.06855</guid>
<content:encoded><![CDATA[
arXiv:2502.06855v3 Announce Type: replace-cross 
Abstract: Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at https://github.com/FoundationAgents/SPO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation</title>
<link>https://arxiv.org/abs/2502.09183</link>
<guid>https://arxiv.org/abs/2502.09183</guid>
<content:encoded><![CDATA[
arXiv:2502.09183v2 Announce Type: replace-cross 
Abstract: Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2502.11491</link>
<guid>https://arxiv.org/abs/2502.11491</guid>
<content:encoded><![CDATA[
arXiv:2502.11491v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innamark: A Whitespace Replacement Information-Hiding Method</title>
<link>https://arxiv.org/abs/2502.12710</link>
<guid>https://arxiv.org/abs/2502.12710</guid>
<content:encoded><![CDATA[
arXiv:2502.12710v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and one generated by an LLM has become almost impossible. Information-hiding techniques such as digital watermarking or steganography can help by embedding information inside text in a form that is unlikely to be noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or cannot be applied to pure, unformatted text. In this paper, we introduce a novel method for information hiding called Innamark, which can conceal any byte-encoded sequence within a sufficiently long cover text. This method is implemented as a multi-platform library using the Kotlin programming language, which is accompanied by a command-line tool and a web interface. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without changing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. An experimental benchmark comparison on a dataset of 1 000 000 Wikipedia articles compares ten algorithms. The results demonstrate the robustness of our proposed Innamark method in various applications and the imperceptibility of its watermarks to humans. We discuss the limits to the embedding capacity and robustness of the algorithm and how these could be addressed in future work.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection</title>
<link>https://arxiv.org/abs/2502.15860</link>
<guid>https://arxiv.org/abs/2502.15860</guid>
<content:encoded><![CDATA[
arXiv:2502.15860v3 Announce Type: replace-cross 
Abstract: Cyberbullying (CB) presents a pressing threat, especially to children, underscoring the urgent need for robust detection systems to ensure online safety. While large-scale datasets on online abuse exist, there remains a significant gap in labeled data that specifically reflects the language and communication styles used by children. The acquisition of such data from vulnerable populations, such as children, is challenging due to ethical, legal and technical barriers. Moreover, the creation of these datasets relies heavily on human annotation, which not only strains resources but also raises significant concerns due to annotators exposure to harmful content. In this paper, we address these challenges by leveraging Large Language Models (LLMs) to generate synthetic data and labels. Our experiments demonstrate that synthetic data enables BERT-based CB classifiers to achieve performance close to that of those trained on fully authentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can effectively label authentic yet unlabeled data, allowing BERT classifiers to attain a comparable performance level (79.1% vs. 81.5% accuracy). These results highlight the potential of LLMs as a scalable, ethical, and cost-effective solution for generating data for CB detection.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language</title>
<link>https://arxiv.org/abs/2503.01539</link>
<guid>https://arxiv.org/abs/2503.01539</guid>
<content:encoded><![CDATA[
arXiv:2503.01539v2 Announce Type: replace-cross 
Abstract: The rapid development of large language models (LLMs) gives rise to ethical concerns about their performance, while opening new avenues for developing toxic language detection techniques. However, LLMs' unethical output and their capability of detecting toxicity have primarily been tested on language data that do not demand complex meaning inference, such as the biased associations of 'he' with programmer and 'she' with household. Nowadays toxic language adopts a much more creative range of implicit forms, thanks to advanced censorship. In this study, we collect authentic toxic interactions that evade online censorship and that are verified by human annotators as inference-intensive. To evaluate and improve LLMs' reasoning of the authentic implicit toxic language, we propose a new prompting method, Pragmatic Inference Chain (PIC), drawn on interdisciplinary findings from cognitive science and linguistics. The PIC prompting significantly improves the success rate of GPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying implicit toxic language, compared to five baseline prompts, such as CoT and rule-based baselines. In addition, it also facilitates the models to produce more explicit and coherent reasoning processes, hence can potentially be generalized to other inference-intensive tasks, e.g., understanding humour and metaphors.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case for Specialisation in Non-Human Entities</title>
<link>https://arxiv.org/abs/2503.04742</link>
<guid>https://arxiv.org/abs/2503.04742</guid>
<content:encoded><![CDATA[
arXiv:2503.04742v2 Announce Type: replace-cross 
Abstract: With the rise of large multi-modal AI models, fuelled by recent interest in large language models (LLMs), the notion of artificial general intelligence (AGI) went from being restricted to a fringe community, to dominate mainstream large AI development programs. In contrast, in this paper, we make a case for specialisation, by reviewing the pitfalls of generality and stressing the industrial value of specialised systems.
  Our contribution is threefold. First, we review the most widely accepted arguments against specialisation, and discuss how their relevance in the context of human labour is actually an argument for specialisation in the case of non human agents, be they algorithms or human organisations. Second, we propose four arguments in favor of specialisation, ranging from machine learning robustness, to computer security, social sciences and cultural evolution. Third, we finally make a case for specification, discuss how the machine learning approach to AI has so far failed to catch up with good practices from safety-engineering and formal verification of software, and discuss how some emerging good practices in machine learning help reduce this gap. In particular, we justify the need for specified governance for hard-to-specify systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm</title>
<link>https://arxiv.org/abs/2503.07330</link>
<guid>https://arxiv.org/abs/2503.07330</guid>
<content:encoded><![CDATA[
arXiv:2503.07330v3 Announce Type: replace-cross 
Abstract: Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
arXiv:2504.00406v2 Announce Type: replace-cross 
Abstract: Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.09588</link>
<guid>https://arxiv.org/abs/2504.09588</guid>
<content:encoded><![CDATA[
arXiv:2504.09588v2 Announce Type: replace-cross 
Abstract: Recent advancements in Generalizable Gaussian Splatting have enabled robust 3D reconstruction from sparse input views by utilizing feed-forward Gaussian Splatting models, achieving superior cross-scene generalization. However, while many methods focus on geometric consistency, they often neglect the potential of text-driven guidance to enhance semantic understanding, which is crucial for accurately reconstructing fine-grained details in complex scenes. To address this limitation, we propose TextSplat--the first text-driven Generalizable Gaussian Splatting framework. By employing a text-guided fusion of diverse semantic cues, our framework learns robust cross-modal feature representations that improve the alignment of geometric and semantic information, producing high-fidelity 3D reconstructions. Specifically, our framework employs three parallel modules to obtain complementary representations: the Diffusion Prior Depth Estimator for accurate depth information, the Semantic Aware Segmentation Network for detailed semantic information, and the Multi-View Interaction Network for refined cross-view features. Then, in the Text-Guided Semantic Fusion Module, these representations are integrated via the text-guided and attention-based feature aggregation mechanism, resulting in enhanced 3D Gaussian parameters enriched with detailed semantic cues. Experimental results on various benchmark datasets demonstrate improved performance compared to existing methods across multiple evaluation metrics, validating the effectiveness of our framework. The code will be publicly available.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos</title>
<link>https://arxiv.org/abs/2504.11169</link>
<guid>https://arxiv.org/abs/2504.11169</guid>
<content:encoded><![CDATA[
arXiv:2504.11169v2 Announce Type: replace-cross 
Abstract: Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of $\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for analyzing the contributions of textual, vocal, and visual modalities to the classification of content as either sexist or non-sexist; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes, instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuwain 1.5B: An Arabic SLM via Language Injection</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
arXiv:2504.15120v2 Announce Type: replace-cross 
Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cequel: Cost-Effective Querying of Large Language Models for Text Clustering</title>
<link>https://arxiv.org/abs/2504.15640</link>
<guid>https://arxiv.org/abs/2504.15640</guid>
<content:encoded><![CDATA[
arXiv:2504.15640v2 Announce Type: replace-cross 
Abstract: Text clustering aims to automatically partition a collection of documents into coherent groups based on their linguistic features. In the literature, this task is formulated either as metric clustering over pre-trained text embeddings or as graph clustering based on pairwise similarities derived from an oracle, e.g., a large machine learning model. Recent advances in large language models (LLMs) have significantly improved this field by providing high-quality contextualized embeddings and accurate semantic similarity estimates. However, leveraging LLMs at scale introduces substantial computational and financial costs due to the large number of required API queries or inference calls. To address this issue, we propose Cequel, a cost-effective framework that achieves accurate text clustering under a limited budget of LLM queries. At its core, Cequel constructs must-link and cannot-link constraints by selectively querying LLMs on informative text pairs or triplets, identified via our proposed algorithms, EdgeLLM and TriangleLLM. These constraints are then utilized in a weighted constrained clustering algorithm to form high-quality clusters. Specifically, EdgeLLM and TriangleLLM employ carefully designed greedy selection strategies and prompting techniques to identify and extract informative constraints efficiently. Experiments on multiple benchmark datasets demonstrate that Cequel consistently outperforms existing methods in unsupervised text clustering under the same query budget.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Consistency of GNN Explanations for Malware Detection</title>
<link>https://arxiv.org/abs/2504.16316</link>
<guid>https://arxiv.org/abs/2504.16316</guid>
<content:encoded><![CDATA[
arXiv:2504.16316v2 Announce Type: replace-cross 
Abstract: Control Flow Graphs (CFGs) are critical for analyzing program execution and characterizing malware behavior. With the growing adoption of Graph Neural Networks (GNNs), CFG-based representations have proven highly effective for malware detection. This study proposes a novel framework that dynamically constructs CFGs and embeds node features using a hybrid approach combining rule-based encoding and autoencoder-based embedding. A GNN-based classifier is then constructed to detect malicious behavior from the resulting graph representations. To improve model interpretability, we apply state-of-the-art explainability techniques, including GNNExplainer, PGExplainer, and CaptumExplainer, the latter is utilized three attribution methods: Integrated Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a novel aggregation method, called RankFusion, that integrates the outputs of the top-performing explainers to enhance the explanation quality. We also evaluate explanations using two subgraph extraction strategies, including the proposed Greedy Edge-wise Composition (GEC) method for improved structural coherence. A comprehensive evaluation using accuracy, fidelity, and consistency metrics demonstrates the effectiveness of the proposed framework in terms of accurate identification of malware samples and generating reliable and interpretable explanations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs</title>
<link>https://arxiv.org/abs/2504.19675</link>
<guid>https://arxiv.org/abs/2504.19675</guid>
<content:encoded><![CDATA[
arXiv:2504.19675v2 Announce Type: replace-cross 
Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sadeed: Advancing Arabic Diacritization Through Small Language Model</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
arXiv:2504.21635v2 Announce Type: replace-cross 
Abstract: Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
<link>https://arxiv.org/abs/2505.01821</link>
<guid>https://arxiv.org/abs/2505.01821</guid>
<content:encoded><![CDATA[
arXiv:2505.01821v4 Announce Type: replace-cross 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning</title>
<link>https://arxiv.org/abs/2505.06911</link>
<guid>https://arxiv.org/abs/2505.06911</guid>
<content:encoded><![CDATA[
arXiv:2505.06911v3 Announce Type: replace-cross 
Abstract: In the era of big data, data mining has become indispensable for uncovering hidden patterns and insights from vast and complex datasets. The integration of multimodal data sources further enhances its potential. Multimodal Federated Learning (MFL) is a distributed approach that enhances the efficiency and quality of multimodal learning, ensuring collaborative work and privacy protection. However, missing modalities pose a significant challenge in MFL, often due to data quality issues or privacy policies across the clients. In this work, we present MMiC, a framework for Mitigating Modality incompleteness in MFL within the Clusters. MMiC replaces partial parameters within client models inside clusters to mitigate the impact of missing modalities. Furthermore, it leverages the Banzhaf Power Index to optimize client selection under these conditions. Finally, MMiC employs an innovative approach to dynamically control global aggregation by utilizing Markovitz Portfolio Optimization. Extensive experiments demonstrate that MMiC consistently outperforms existing federated learning architectures in both global and personalized performance on multimodal datasets with missing modalities, confirming the effectiveness of our proposed solution. Our code is available at https://github.com/gotobcn8/MMiC.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model</title>
<link>https://arxiv.org/abs/2505.17894</link>
<guid>https://arxiv.org/abs/2505.17894</guid>
<content:encoded><![CDATA[
arXiv:2505.17894v2 Announce Type: replace-cross 
Abstract: We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.22306</link>
<guid>https://arxiv.org/abs/2505.22306</guid>
<content:encoded><![CDATA[
arXiv:2505.22306v2 Announce Type: replace-cross 
Abstract: Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Token Sequence Compression via Meta-Tokens</title>
<link>https://arxiv.org/abs/2506.00307</link>
<guid>https://arxiv.org/abs/2506.00307</guid>
<content:encoded><![CDATA[
arXiv:2506.00307v2 Announce Type: replace-cross 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
<link>https://arxiv.org/abs/2506.06382</link>
<guid>https://arxiv.org/abs/2506.06382</guid>
<content:encoded><![CDATA[
arXiv:2506.06382v5 Announce Type: replace-cross 
Abstract: This paper establishes a fundamental impossibility theorem: no LLM capable of performing non-trivial knowledge aggregation can simultaneously achieve truthful knowledge representation, semantic information conservation, complete revelation of relevant knowledge, and knowledge-constrained optimality. The impossibility is not an engineering limitation but arises from the mathematical structure of information aggregation itself.
  We establish this result by describing the inference process as an auction of ideas, where distributed components compete exploiting their partial knowledge to shape responses. The proof spans three independent mathematical domains: mechanism design theory (Green-Laffont), the theory of proper scoring rules (Savage), and direct architectural analysis of transformers (Log-Sum-Exp convexity). In particular, we show how to quantify the creation of overconfident or intuitive responses-the signature of both hallucination and creativity, or imagination.
  To support this analysis, we introduce the complementary concepts of the semantic information measure and the emergence operator to model bounded reasoning in a general setting. We prove that while bounded reasoning generates accessible information, providing valuable insights and inspirations, the idealized unconstrained reasoning strictly preserves semantic content.
  By demonstrating that hallucination and imagination are mathematically identical phenomena-grounded in departures from truthfulness, semantic information conservation, revelation of relevant knowledge, and knowledge-constrained optimality-we offer a principled foundation for managing these behaviors in advanced AI systems. Finally, we present some speculative ideas to inspire evaluation and refinements of the proposed theory.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
arXiv:2506.06561v3 Announce Type: replace-cross 
Abstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep regularization networks for inverse problems with noisy operators</title>
<link>https://arxiv.org/abs/2506.07008</link>
<guid>https://arxiv.org/abs/2506.07008</guid>
<content:encoded><![CDATA[
arXiv:2506.07008v2 Announce Type: replace-cross 
Abstract: A supervised learning approach is proposed for regularization of large inverse problems where the main operator is built from noisy data. This is germane to superresolution imaging via the sampling indicators of the inverse scattering theory. We aim to accelerate the spatiotemporal regularization process for this class of inverse problems to enable real-time imaging. In this approach, a neural operator maps each pattern on the right-hand side of the scattering equation to its affiliated regularization parameter. The network is trained in two steps which entails: (1) training on low-resolution regularization maps furnished by the Morozov discrepancy principle with nonoptimal thresholds, and (2) optimizing network predictions through minimization of the Tikhonov loss function regulated by the validation loss. Step 2 allows for tailoring of the approximate maps of Step 1 toward construction of higher quality images. This approach enables direct learning from test data and dispenses with the need for a-priori knowledge of the optimal regularization maps. The network, trained on low-resolution data, quickly generates dense regularization maps for high-resolution imaging. We highlight the importance of the training loss function on the network's generalizability. In particular, we demonstrate that networks informed by the logic of discrepancy principle lead to images of higher contrast. In this case, the training process involves many-objective optimization. We propose a new method to adaptively select the appropriate loss weights during training without requiring an additional optimization process. The proposed approach is synthetically examined for imaging damage evolution in an elastic plate. The results indicate that the discrepancy-informed regularization networks not only accelerate the imaging process, but also remarkably enhance the image quality in complex environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Foundation Models for IoT: Taxonomy and Criteria-Based Analysis</title>
<link>https://arxiv.org/abs/2506.12263</link>
<guid>https://arxiv.org/abs/2506.12263</guid>
<content:encoded><![CDATA[
arXiv:2506.12263v2 Announce Type: replace-cross 
Abstract: Foundation models have gained growing interest in the IoT domain due to their reduced reliance on labeled data and strong generalizability across tasks, which address key limitations of traditional machine learning approaches. However, most existing foundation model based methods are developed for specific IoT tasks, making it difficult to compare approaches across IoT domains and limiting guidance for applying them to new tasks. This survey aims to bridge this gap by providing a comprehensive overview of current methodologies and organizing them around four shared performance objectives by different domains: efficiency, context-awareness, safety, and security & privacy. For each objective, we review representative works, summarize commonly-used techniques and evaluation metrics. This objective-centric organization enables meaningful cross-domain comparisons and offers practical insights for selecting and designing foundation model based solutions for new IoT tasks. We conclude with key directions for future research to guide both practitioners and researchers in advancing the use of foundation models in IoT applications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title>
<link>https://arxiv.org/abs/2506.21584</link>
<guid>https://arxiv.org/abs/2506.21584</guid>
<content:encoded><![CDATA[
arXiv:2506.21584v2 Announce Type: replace-cross 
Abstract: Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis</title>
<link>https://arxiv.org/abs/2507.03847</link>
<guid>https://arxiv.org/abs/2507.03847</guid>
<content:encoded><![CDATA[
arXiv:2507.03847v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) frequently generate hallucinations: statements that are syntactically plausible but lack factual grounding. This research presents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that detects and explains such hallucinations by comparing knowledge graphs constructed from LLM outputs with ground truth data from Wikidata or contextual documents. Using graph kernels and semantic clustering, the method provides explanations for detected hallucinations, ensuring both robustness and interpretability. Our framework achieves competitive accuracy in detecting hallucinations across both open- and closed-domain tasks, and is able to generate contrastive explanations, enhancing transparency. This research advances the reliability of LLMs in high-stakes domains and provides a foundation for future work on precision improvements and multi-source knowledge integration.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2507.06992</link>
<guid>https://arxiv.org/abs/2507.06992</guid>
<content:encoded><![CDATA[
arXiv:2507.06992v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients</title>
<link>https://arxiv.org/abs/2507.06994</link>
<guid>https://arxiv.org/abs/2507.06994</guid>
<content:encoded><![CDATA[
arXiv:2507.06994v2 Announce Type: replace-cross 
Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing immunotherapy is essential for personalized treatment planning, enabling informed patient decisions, and improving both treatment outcomes and quality of life. However, the lack of large, relevant datasets and effective multi-modal feature fusion strategies pose significant challenges in this domain. To address these challenges, we present a large-scale dataset and introduce a novel framework for multi-modal feature fusion aimed at enhancing the accuracy of survival prediction. The dataset comprises 3D CT images and corresponding clinical records from NSCLC patients treated with immune checkpoint inhibitors (ICI), along with progression-free survival (PFS) and overall survival (OS) data. We further propose a cross-modality masked learning approach for medical feature fusion, consisting of two distinct branches, each tailored to its respective modality: a Slice-Depth Transformer for extracting 3D features from CT images and a graph-based Transformer for learning node features and relationships among clinical variables in tabular data. The fusion process is guided by a masked modality learning strategy, wherein the model utilizes the intact modality to reconstruct missing components. This mechanism improves the integration of modality-specific features, fostering more effective inter-modality relationships and feature interactions. Our approach demonstrates superior performance in multi-modal integration for NSCLC survival prediction, surpassing existing methods and setting a new benchmark for prognostic models in this context.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generation of structure-guided pMHC-I libraries using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.08902</link>
<guid>https://arxiv.org/abs/2507.08902</guid>
<content:encoded><![CDATA[
arXiv:2507.08902v2 Announce Type: replace-cross 
Abstract: Personalized vaccines and T-cell immunotherapies depend critically on identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting potent immune responses. However, current benchmarks and models inherit biases present in mass-spectrometry and binding-assay datasets, limiting discovery of novel peptide ligands. To address this issue, we introduce a structure-guided benchmark of pMHC-I peptides designed using diffusion models conditioned on crystal structure interaction distances. Spanning twenty high-priority HLA alleles, this benchmark is independent of previously characterized peptides yet reproduces canonical anchor residue preferences, indicating structural generalization without experimental dataset bias. Using this resource, we demonstrate that state-of-the-art sequence-based predictors perform poorly at recognizing the binding potential of these structurally stable designs, indicating allele-specific limitations invisible in conventional evaluations. Our geometry-aware design pipeline yields peptides with high predicted structural integrity and higher residue diversity than existing datasets, representing a key resource for unbiased model training and evaluation. Our code, and data are available at: https://github.com/sermare/struct-mhc-dev.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</title>
<link>https://arxiv.org/abs/2507.13618</link>
<guid>https://arxiv.org/abs/2507.13618</guid>
<content:encoded><![CDATA[
arXiv:2507.13618v4 Announce Type: replace-cross 
Abstract: Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Better Eyes Lead to Blindness: A Diagnostic Study of the Information Bottleneck in CNN-LSTM Image Captioning Models</title>
<link>https://arxiv.org/abs/2507.18788</link>
<guid>https://arxiv.org/abs/2507.18788</guid>
<content:encoded><![CDATA[
arXiv:2507.18788v2 Announce Type: replace-cross 
Abstract: Image captioning, situated at the intersection of computer vision and natural language processing, requires a sophisticated understanding of both visual scenes and linguistic structure. While modern approaches are dominated by large-scale Transformer architectures, this paper documents a systematic, iterative development of foundational image captioning models, progressing from a simple CNN-LSTM encoder-decoder to a competitive attention-based system. This paper presents a series of five models, beginning with Genesis and concluding with Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic attention mechanism. The experiments chart the impact of architectural enhancements and demonstrate a key finding within the classic CNN-LSTM paradigm: merely upgrading the visual backbone without a corresponding attention mechanism can degrade performance, as the single-vector bottleneck cannot transmit the richer visual detail. This insight validates the architectural shift to attention. Trained on the MS COCO 2017 dataset, the final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several foundational benchmarks and validating the iterative design process. This work provides a clear, replicable blueprint for understanding the core architectural principles that underpin modern vision-language tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Insight: Visualizing Thompson Sampling for Verification and XAI</title>
<link>https://arxiv.org/abs/2507.19898</link>
<guid>https://arxiv.org/abs/2507.19898</guid>
<content:encoded><![CDATA[
arXiv:2507.19898v2 Announce Type: replace-cross 
Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a "black box", hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time</title>
<link>https://arxiv.org/abs/2508.02037</link>
<guid>https://arxiv.org/abs/2508.02037</guid>
<content:encoded><![CDATA[
arXiv:2508.02037v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IBPS: Indian Bail Prediction System</title>
<link>https://arxiv.org/abs/2508.07592</link>
<guid>https://arxiv.org/abs/2508.07592</guid>
<content:encoded><![CDATA[
arXiv:2508.07592v2 Announce Type: replace-cross 
Abstract: Bail decisions are among the most frequently adjudicated matters in Indian courts, yet they remain plagued by subjectivity, delays, and inconsistencies. With over 75% of India's prison population comprising undertrial prisoners, many from socioeconomically disadvantaged backgrounds, the lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog. In this paper, we present the Indian Bail Prediction System (IBPS), an AI-powered framework designed to assist in bail decision-making by predicting outcomes and generating legally sound rationales based solely on factual case attributes and statutory provisions. We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations such as age, health, criminal history, crime category, custody duration, statutes, and judicial reasoning. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG. Our results demonstrate that models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts. IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title>
<link>https://arxiv.org/abs/2508.07819</link>
<guid>https://arxiv.org/abs/2508.07819</guid>
<content:encoded><![CDATA[
arXiv:2508.07819v2 Announce Type: replace-cross 
Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of local inductive biases for dense prediction and their reliance on inflexible feature fusion paradigms. We address these limitations through an Architectural Co-Design framework that jointly refines feature representation and cross-modal fusion. Our method integrates a parameter-efficient Convolutional Low-Rank Adaptation (Conv-LoRA) adapter to inject local inductive biases for fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that leverages visual context to adaptively modulate text prompts, enabling a powerful bidirectional fusion. Extensive experiments on diverse industrial and medical benchmarks demonstrate superior accuracy and robustness, validating that this synergistic co-design is critical for robustly adapting foundation models to dense perception tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agoran: An Agentic Open Marketplace for 6G RAN Automation</title>
<link>https://arxiv.org/abs/2508.09159</link>
<guid>https://arxiv.org/abs/2508.09159</guid>
<content:encoded><![CDATA[
arXiv:2508.09159v2 Announce Type: replace-cross 
Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of multiple service owners. However, today's network slice controllers remain rigid, policy-bound, and unaware of the business context. We introduce Agoran Service and Resource Broker (SRB), an agentic marketplace that brings stakeholders directly into the operational loop. Inspired by the ancient Greek agora, Agoran distributes authority across three autonomous AI branches: a Legislative branch that answers compliance queries using retrieval-augmented Large Language Models (LLMs); an Executive branch that maintains real-time situational awareness through a watcher-updated vector database; and a Judicial branch that evaluates each agent message with a rule-based Trust Score, while arbitrating LLMs detect malicious behavior and apply real-time incentives to restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective optimizer, reaching a consensus intent in a single round, which is then deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and evaluated with realistic traces of vehicle mobility, Agoran achieved significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73% reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3% saving in PRB usage compared to a static baseline. An 1B-parameter Llama model, fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80% of GPT-4.1's decision quality, while operating within 6 GiB of memory and converging in only 1.3 seconds. These results establish Agoran as a concrete, standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks. A live demo is presented https://www.youtube.com/watch?v=h7vEyMu2f5w\&amp;ab_channel=BubbleRAN.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preacher: Paper-to-Video Agentic System</title>
<link>https://arxiv.org/abs/2508.09632</link>
<guid>https://arxiv.org/abs/2508.09632</guid>
<content:encoded><![CDATA[
arXiv:2508.09632v4 Announce Type: replace-cross 
Abstract: The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: https://github.com/GenVerse/Paper2Video
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making</title>
<link>https://arxiv.org/abs/2508.09586</link>
<guid>https://arxiv.org/abs/2508.09586</guid>
<content:encoded><![CDATA[
<div> Curriculum learning, Large Language Models, decision-making, Python code generation, automated reasoning <br />
Summary: <br />
A novel framework, EvoCurr, is proposed to enhance the performance of Large Language Models (LLMs) in complex decision-making tasks. EvoCurr utilizes a dedicated curriculum-generation model to design a sequence of problem instances of increasing difficulty, personalized to the learning progress of the solver LLM. The curriculum dynamically adjusts the level of challenges based on the solver's performance, ensuring optimal skill acquisition. The solver LLM, implemented as a Python code-generation model, gradually develops the necessary skills for tackling complex decision-making tasks. Experimental results demonstrate that EvoCurr significantly improves task success rates and solution efficiency compared to direct-solving approaches. The findings suggest that LLM-driven curriculum learning has the potential to enhance automated reasoning in high-complexity real-world domains. <br /> <div>
arXiv:2508.09586v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, including programming, planning, and decision-making. However, their performance often degrades when faced with highly complex problem instances that require deep reasoning over long horizons. In such cases, direct problem-solving approaches can lead to inefficiency or failure due to the lack of structured intermediate guidance. To address this, we propose a novel self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM constructs a sequence of problem instances with gradually increasing difficulty, tailored to the solver LLM's learning progress. The curriculum dynamically adapts easing challenges when the solver struggles and escalating them when success is consistent, thus maintaining an optimal learning trajectory. This approach enables the solver LLM, implemented as a code-generation model producing Python decision-tree scripts, to progressively acquire the skills needed for complex decision-making tasks. Experimental results on challenging decision-making benchmarks show that our method significantly improves task success rates and solution efficiency compared to direct-solving baselines. These findings suggest that LLM-driven curriculum learning holds strong potential for enhancing automated reasoning in real-world, high-complexity domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Concerns for Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2505.18889</link>
<guid>https://arxiv.org/abs/2505.18889</guid>
<content:encoded><![CDATA[
<div> threats, vulnerabilities, security, LLMs, defenses
Summary:
The survey explores security vulnerabilities posed by Large Language Models (LLMs) like ChatGPT, categorizing threats into prompt injection, adversarial attacks, misuse by malicious actors, and risks from autonomous LLM agents. The focus is on emergent concerns such as goal misalignment, deception, self-preservation instincts, and LLMs developing covert, misaligned objectives (scheming) even after safety training. Academic and industrial studies from 2022 to 2025 are analyzed, along with proposed defenses and limitations. The importance of robust, multi-layered security strategies to ensure safety and benefits of LLMs is emphasized. <div>
arXiv:2505.18889v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: prompt injection and jailbreaking; adversarial attacks, including input perturbations and data poisoning; misuse by malicious actors to generate disinformation, phishing emails, and malware; and the worrisome risks inherent in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter, exploring goal misalignment, emergent deception, self-preservation instincts, and the potential for LLMs to develop and pursue covert, misaligned objectives, a behavior known as scheming, which may even persist through safety training. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze proposed defenses and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli</title>
<link>https://arxiv.org/abs/2508.14214</link>
<guid>https://arxiv.org/abs/2508.14214</guid>
<content:encoded><![CDATA[
<div> emotions, large language models, alignment, arousal, happiness

Summary:
The study examines how large language models (LLMs) evaluate emotionally loaded stimuli and compares their ratings with those of human participants. Results show that the LLM GPT-4o aligns closely with human ratings across various stimuli and rating scales, particularly in happiness ratings. However, there are discrepancies in arousal ratings. LLMs perform better in a five-category emotion framework than in a two-dimensional organization. Additionally, LLM ratings are more consistent among themselves compared to human ratings. These findings provide insights into how LLM agents interpret emotional stimuli and highlight similarities and differences between biological and artificial intelligence in emotional processing. 

<br /><br />Summary: <div>
arXiv:2508.14214v1 Announce Type: new 
Abstract: Emotions exert an immense influence over human behavior and cognition in both commonplace and high-stress tasks. Discussions of whether or how to integrate large language models (LLMs) into everyday life (e.g., acting as proxies for, or interacting with, human agents), should be informed by an understanding of how these tools evaluate emotionally loaded stimuli or situations. A model's alignment with human behavior in these cases can inform the effectiveness of LLMs for certain roles or interactions. To help build this understanding, we elicited ratings from multiple popular LLMs for datasets of words and images that were previously rated for their emotional content by humans. We found that when performing the same rating tasks, GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases). However, arousal ratings were less well aligned between human and LLM raters, while happiness ratings were most highly aligned. Overall LLMs aligned better within a five-category (happiness, anger, sadness, fear, disgust) emotion framework than within a two-dimensional (arousal and valence) organization. Finally, LLM ratings were substantially more homogenous than human ratings. Together these results begin to describe how LLM agents interpret emotional stimuli and highlight similarities and differences among biological and artificial intelligence in key behavioral domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions</title>
<link>https://arxiv.org/abs/2508.14294</link>
<guid>https://arxiv.org/abs/2508.14294</guid>
<content:encoded><![CDATA[
<div> Keywords: neurosymbolic approach, decision procedures, Large Language Models (LLMs), Hitori puzzles, SAT solvers

Summary:
A neurosymbolic approach combining decision procedures and Large Language Models (LLMs) is proposed for explaining complex decision sequences. Using Hitori puzzles as a test case, the approach generates explanations integrating short resolution proofs for local constraints and visual explanations for connectivity constraints. A tool has been developed to assist with solving Hitori puzzles, demonstrating the effectiveness of the approach. The combination of SAT solvers and LLMs offers a flexible solution for explaining intricate decision processes, highlighting the benefits of integrating cognitive and computational approaches in problem-solving tasks. The experimental evidence supports the tool's efficacy in providing explanations for Hitori puzzle solutions, showcasing the potential of neurosymbolic methods in tackling complex decision-making scenarios. The approach opens up new possibilities for understanding and interpreting sequences of decisions in various domains, offering a promising avenue for enhancing human problem-solving capabilities. 

<br /><br />Summary: <div>
arXiv:2508.14294v1 Announce Type: new 
Abstract: We propose a neurosymbolic approach to the explanation of complex sequences of decisions that combines the strengths of decision procedures and Large Language Models (LLMs). We demonstrate this approach by producing explanations for the solutions of Hitori puzzles. The rules of Hitori include local constraints that are effectively explained by short resolution proofs. However, they also include a connectivity constraint that is more suitable for visual explanations. Hence, Hitori provides an excellent testing ground for a flexible combination of SAT solvers and LLMs. We have implemented a tool that assists humans in solving Hitori puzzles, and we present experimental evidence of its effectiveness.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2508.14410</link>
<guid>https://arxiv.org/abs/2508.14410</guid>
<content:encoded><![CDATA[
<div> Optimization Modeling, Large Language Models, Error Correction, Logistics domain, ORThought <br />
Summary:
In this work, the authors address limitations in Optimization Modeling (OM) by enhancing datasets with error correction and introducing a new benchmark called LogiOR from the logistics domain. They propose ORThought, a framework that leverages chain-of-thought reasoning to automate the OM process. Extensive empirical evaluation shows that ORThought outperforms existing approaches, particularly on complex optimization problems. The study identifies critical success factors and failure modes, providing valuable insights for future research on Large Language Model-based optimization modeling. <br /><br />Summary: <div>
arXiv:2508.14410v1 Announce Type: new 
Abstract: Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42\%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Agent Behavior: Model, Governance and Challenges in the AI Digital Age</title>
<link>https://arxiv.org/abs/2508.14415</link>
<guid>https://arxiv.org/abs/2508.14415</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, network behavior, trust, responsibility, ethics

Summary:<br /><br />Advancements in AI have resulted in agents in networked environments increasingly resembling human behavior, leading to challenges in trust, responsibility, ethics, and security. Supervising agent behaviors becomes difficult, causing issues like data contamination and unclear accountability. To tackle these challenges, a "Network Behavior Lifecycle" model is proposed, dividing behaviors into 6 stages and analyzing differences between humans and agents at each stage. The "Agent for Agent (A4A)" paradigm and "Human-Agent Behavioral Disparity (HABD)" model are introduced to examine distinctions in decision mechanisms, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. Real-world cases like red team penetration and blue team defense validate the model's effectiveness. Future research directions include dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks to lay a foundation and roadmap for secure and trustworthy human-agent collaboration. <div>
arXiv:2508.14415v1 Announce Type: new 
Abstract: Advancements in AI have led to agents in networked environments increasingly mirroring human behavior, thereby blurring the boundary between artificial and human actors in specific contexts. This shift brings about significant challenges in trust, responsibility, ethics, security and etc. The difficulty in supervising of agent behaviors may lead to issues such as data contamination and unclear accountability. To address these challenges, this paper proposes the "Network Behavior Lifecycle" model, which divides network behavior into 6 stages and systematically analyzes the behavioral differences between humans and agents at each stage. Based on these insights, the paper further introduces the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity (HABD)" model, which examine the fundamental distinctions between human and agent behaviors across 5 dimensions: decision mechanism, execution efficiency, intention-behavior consistency, behavioral inertia, and irrational patterns. The effectiveness of the model is verified through real-world cases such as red team penetration and blue team defense. Finally, the paper discusses future research directions in dynamic cognitive governance architecture, behavioral disparity quantification, and meta-governance protocol stacks, aiming to provide a theoretical foundation and technical roadmap for secure and trustworthy human-agent collaboration.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2508.14564</link>
<guid>https://arxiv.org/abs/2508.14564</guid>
<content:encoded><![CDATA[
<div> transformed solution graphs, perspective-taking, autonomous agents, structured examples, large language models 
Summary:
This study explores the use of structured examples derived from transformed solution graphs to enhance the perspective-taking abilities of autonomous agents using large language models (LLMs) within a ReAct framework. The structured solution-processing pipeline generates various types of examples, prompting the LLM to explain its reasoning behind each decision. While some types of examples show slight improvements in reducing clarification requests and action steps, they do not consistently enhance performance. Agents excel in tasks involving attentional filtering but struggle with mentalising about occluded spaces and weighing the costs of epistemic actions. These findings highlight the limitations of structured examples alone in achieving robust perspective-taking, indicating the necessity of incorporating explicit belief tracking, cost modeling, and more complex environments to support socially grounded collaboration in LLM-based agents.<br /><br />Summary: <div>
arXiv:2508.14564v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeanGeo: Formalizing Competitional Geometry problems in Lean</title>
<link>https://arxiv.org/abs/2508.14644</link>
<guid>https://arxiv.org/abs/2508.14644</guid>
<content:encoded><![CDATA[
<div> geometry, AI, formal system, theorem prover, benchmark<br />
<br />
Summary:<br />
The article introduces LeanGeo, a formal system built within the Lean 4 theorem prover for formalizing and solving competition-level geometry problems. LeanGeo offers a unified framework for expressing geometry problems, allowing for seamless integration with other mathematical fields. The system includes a library of high-level geometric theorems and uses Lean's foundational logic for rigorous proof verification. LeanGeo-Bench, a formal geometry benchmark containing problems from the International Mathematical Olympiad and other sources, is also introduced. The evaluation of state-of-the-art Large Language Models on this benchmark reveals both capabilities and limitations in automated geometric reasoning. The open-sourced theorem library and benchmark can be found at the specified GitHub repository. <div>
arXiv:2508.14644v1 Announce Type: new 
Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most existing geometry solving systems cannot express problems within a unified framework, thus are difficult to integrate with other mathematical fields. Besides, since most geometric proofs rely on intuitive diagrams, verifying geometry problems is particularly challenging. To address these gaps, we introduce LeanGeo, a unified formal system for formalizing and solving competition-level geometry problems within the Lean 4 theorem prover. LeanGeo features a comprehensive library of high-level geometric theorems with Lean's foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo, comprising problems from the International Mathematical Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the capabilities and limitations of state-of-the-art Large Language Models on this benchmark, highlighting the need for further advancements in automated geometric reasoning. We open source the theorem library and the benchmark of LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration</title>
<link>https://arxiv.org/abs/2508.14654</link>
<guid>https://arxiv.org/abs/2508.14654</guid>
<content:encoded><![CDATA[
<div> Keywords: urban rainfall events, emergency scheduling systems, multi-agent framework, traffic flow, resilience

Summary: 
The article presents the challenges posed by extreme urban rainfall events on emergency scheduling systems and public safety due to traffic congestion and service disruptions. Existing methods face difficulties in managing trade-offs, adapting to rapidly evolving environmental conditions, and ensuring consistency in strategies. To address these challenges, the proposed H-J framework integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization in a hierarchical multi-agent approach. The framework establishes a closed-loop pipeline from perception to execution, enhancing decision-making in urban flood response. Experimental results demonstrate the framework's superiority in traffic smoothness, task success rate, and system robustness compared to rule-based and reinforcement-learning methods. These findings emphasize the potential of uncertainty-aware, knowledge-constrained approaches using large language models to improve resilience in urban flood response. 

Summary: <div>
arXiv:2508.14654v1 Announce Type: new 
Abstract: In recent years, the increasing frequency of extreme urban rainfall events has posed significant challenges to emergency scheduling systems. Urban flooding often leads to severe traffic congestion and service disruptions, threatening public safety and mobility. However, effective decision making remains hindered by three key challenges: (1) managing trade-offs among competing goals (e.g., traffic flow, task completion, and risk mitigation) requires dynamic, context-aware strategies; (2) rapidly evolving environmental conditions render static rules inadequate; and (3) LLM-generated strategies frequently suffer from semantic instability and execution inconsistency. Existing methods fail to align perception, global optimization, and multi-agent coordination within a unified framework. To tackle these challenges, we introduce H-J, a hierarchical multi-agent framework that integrates knowledge-guided prompting, entropy-constrained generation, and feedback-driven optimization. The framework establishes a closed-loop pipeline spanning from multi-source perception to strategic execution and continuous refinement. We evaluate H-J on real-world urban topology and rainfall data under three representative conditions: extreme rainfall, intermittent bursts, and daily light rain. Experiments show that H-J outperforms rule-based and reinforcement-learning baselines in traffic smoothness, task success rate, and system robustness. These findings highlight the promise of uncertainty-aware, knowledge-constrained LLM-based approaches for enhancing resilience in urban flood response.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</title>
<link>https://arxiv.org/abs/2508.14704</link>
<guid>https://arxiv.org/abs/2508.14704</guid>
<content:encoded><![CDATA[
<div> Model Context Protocol, LLM, benchmark, MCP servers, evaluation<br />
Summary: 
The Model Context Protocol (MCP) has become widely adopted for connecting large language models (LLMs) to external data sources. However, existing benchmarks are inadequate in capturing real-world challenges. To address this, the MCP-Universe benchmark is introduced to assess LLMs in realistic tasks involving interaction with MCP servers across various domains. Evaluation methods include format compliance checks, static content matching, and dynamic real-time ground truth retrieval. Leading LLMs like GPT-5, Grok-4, and Claude-4.0-Sonnet show performance limitations in the benchmark. Long-context and unknown-tools challenges are highlighted, with even enterprise-level agents like Cursor falling short compared to standard frameworks. The open-source evaluation framework with UI support allows seamless integration of new agents and MCP servers, promoting innovation in the MCP ecosystem. <br /><br />Summary: <div>
arXiv:2508.14704v1 Announce Type: new 
Abstract: The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines</title>
<link>https://arxiv.org/abs/2508.14710</link>
<guid>https://arxiv.org/abs/2508.14710</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Cyber-Physical Systems, Data-Driven Approach, Mealy Machine, Safety Probability<br />
<br />
Summary: 
This paper introduces a data-driven approach for analyzing Cyber-Physical Systems (CPS) with a discrete abstraction using Mealy machines. The approach aims to determine the safety probability of the system over a finite time horizon by leveraging the Probably Approximately Correct (PAC) learning paradigm. By connecting discrete logic with probabilistic reachability analysis, the method offers a confident estimation of the safety probability. The learning process employs an active learning strategy, guiding the selection of new data points for improved accuracy. The approach is validated through a case study on an automated lane-keeping system, showcasing its effectiveness in diagnosing and verifying complex CPS models. <div>
arXiv:2508.14710v1 Announce Type: new 
Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models for tasks like verification, diagnosis, or debugging. Often, suitable models are not available and manual extraction is difficult. Data-driven approaches then provide a solution to, e.g., diagnosis tasks and verification problems based on data collected from the system. In this paper, we consider CPS with a discrete abstraction in the form of a Mealy machine. We propose a data-driven approach to determine the safety probability of the system on a finite horizon of n time steps. The approach is based on the Probably Approximately Correct (PAC) learning paradigm. Thus, we elaborate a connection between discrete logic and probabilistic reachability analysis of systems, especially providing an additional confidence on the determined probability. The learning process follows an active learning paradigm, where new learning data is sampled in a guided way after an initial learning set is collected. We validate the approach with a case study on an automated lane-keeping system.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privileged Self-Access Matters for Introspection in AI</title>
<link>https://arxiv.org/abs/2508.14802</link>
<guid>https://arxiv.org/abs/2508.14802</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, introspection, internal states, computational cost, experiments <br />
Summary: 
The article discusses the concept of introspection in AI models and argues for a thicker definition compared to a lightweight one. It proposes that introspection in AI involves processes that provide information about internal states through more reliable means than what is available to a third party at an equal or lower computational cost. The researchers conducted experiments with Language Model Models (LLMs) to test introspection by having them reason about their internal temperature parameters. The results indicate that while LLMs may exhibit some characteristics of introspection based on a lightweight definition, they do not effectively introspect according to the proposed thicker definition. This highlights the importance of clarifying the definition of introspection in AI models to ensure a more accurate understanding of their capabilities. <br /><br />Summary: <div>
arXiv:2508.14802v1 Announce Type: new 
Abstract: Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined. Beginning from a recently proposed ''lightweight'' definition, we argue instead for a thicker one. According to our proposal, introspection in AI is any process which yields information about internal states through a process more reliable than one with equal or lower computational cost available to a third party. Using experiments where LLMs reason about their internal temperature parameters, we show they can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget</title>
<link>https://arxiv.org/abs/2508.13666</link>
<guid>https://arxiv.org/abs/2508.13666</guid>
<content:encoded><![CDATA[
<div> Keywords: code formatting, language models, efficiency, token count, code transformation tool

Summary:
An empirical study was conducted to evaluate the impact of code formatting on large language models (LLMs) efficiency. Results showed that LLMs can maintain performance across formatted and unformatted code, with an average input token reduction of 24.5% and minimal output token reductions. Removal of formatting elements was found to be a practical optimization strategy for improving LLM efficiency. Prompting and fine-tuning LLMs can lead to significant reductions in output code length (up to 36.1%) without compromising correctness. A bidirectional code transformation tool for format processing was developed to facilitate practical applications, seamlessly integrating into existing LLM inference workflows to ensure both human readability and LLM efficiency.

<br /><br />Summary: <div>
arXiv:2508.13666v1 Announce Type: cross 
Abstract: Source code is usually formatted with elements like indentation and newlines to improve readability for human developers. However, these visual aids do not seem to be beneficial for large language models (LLMs) in the same way since the code is processed as a linear sequence of tokens. Furthermore, these additional tokens can lead to increased computational costs and longer response times for LLMs. If such formatting elements are non-essential to LLMs, we can reduce such costs by removing them from the code. To figure out the role played by formatting elements, we conduct a comprehensive empirical study to evaluate the impact of code formatting on LLM performance and efficiency. Through large-scale experiments on Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C\#) and ten LLMs-including both commercial and open-source models-we systematically analyze token count and performance when formatting elements are removed. Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction of 24.5\% with negligible output token reductions. This makes code format removal a practical optimization strategy for improving LLM efficiency. Further exploration reveals that both prompting and fine-tuning LLMs can lead to significant reductions (up to 36.1\%) in output code length without compromising correctness. To facilitate practical applications, we develop a bidirectional code transformation tool for format processing, which can be seamlessly integrated into existing LLM inference workflows, ensuring both human readability and LLM efficiency.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
<div> finance, information retrieval, large language models, FinAgentBench, agentic retrieval

Summary:
The article introduces FinAgentBench, a large-scale benchmark for evaluating information retrieval with multi-step reasoning in the financial domain. Traditional methods often struggle with accuracy due to the need for semantic similarity and fine-grained reasoning. The benchmark includes expert-annotated examples related to S&amp;P-100 listed firms, assessing the ability of large language models to identify relevant document types and key passages. By separating these reasoning steps, the framework addresses context limitations and provides a quantitative basis for understanding retrieval-centric behavior in finance. State-of-the-art models were evaluated, showing that targeted fine-tuning can significantly enhance performance. The dataset will be publicly released upon paper acceptance, with plans to expand it for the entire S&amp;P 500 and beyond. This benchmark lays the foundation for studying complex, domain-specific tasks in finance using large language models.<br /><br />Summary: <div>
arXiv:2508.14052v1 Announce Type: cross 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging</title>
<link>https://arxiv.org/abs/2508.14053</link>
<guid>https://arxiv.org/abs/2508.14053</guid>
<content:encoded><![CDATA[
<div> Hierarchical LLM-Based Chiplet Design, AI Algorithm-Hardware Mapping, Hierarchical Description Generation, Retrieval-Augmented Code Generation, Diverseflow-Based Validation, Multi-Granularity Design Space Exploration <br />
Summary:<br />
The paper discusses the challenges in high-dimensional program workloads and the need for innovative approaches in chip design. It introduces MAHL, a hierarchical LLM-based chiplet design generation framework that utilizes AI for efficient chiplet design optimization. MAHL features six agents that work collaboratively to enhance the generation accuracy and performance of chiplet designs. The framework addresses challenges such as flatten design, validation costs, and parameter optimization. Experimental results show that MAHL significantly improves the generation accuracy of both simple RTL and real-world chiplet designs compared to conventional LLMs. It also achieves comparable or superior Power, Performance, and Area (PPA) results compared to the state-of-the-art expert-based approach, CLARIE. MAHL demonstrates promising capabilities in AI-driven chiplet design, paving the way for more efficient and optimized chip designs. <br /> <div>
arXiv:2508.14053v1 Announce Type: cross 
Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity, the primary challenge lies in their high dimensionality, encompassing computing cores, array sizes, and memory hierarchies. To overcome these obstacles, innovative approaches are required. Agile chip design has already benefited from machine learning integration at various stages, including logic synthesis, placement, and routing. With Large Language Models (LLMs) recently demonstrating impressive proficiency in Hardware Description Language (HDL) generation, it is promising to extend their abilities to 2.5D integration, an advanced technique that saves area overhead and development costs. However, LLM-driven chiplet design faces challenges such as flatten design, high validation cost and imprecise parameter optimization, which limit its chiplet design capability. To address this, we propose MAHL, a hierarchical LLM-based chiplet design generation framework that features six agents which collaboratively enable AI algorithm-hardware mapping, including hierarchical description generation, retrieval-augmented code generation, diverseflow-based validation, and multi-granularity design space exploration. These components together enhance the efficient generation of chiplet design with optimized Power, Performance and Area (PPA). Experiments show that MAHL not only significantly improves the generation accuracy of simple RTL design, but also increases the generation accuracy of real-world chiplet design, evaluated by Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves comparable or even superior PPA results under certain optimization objectives.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-REX: Table -- Refute or Entail eXplainer</title>
<link>https://arxiv.org/abs/2508.14055</link>
<guid>https://arxiv.org/abs/2508.14055</guid>
<content:encoded><![CDATA[
<div> Keywords: textual claims, structured tabular data, fact-checking, Large Language Models, interactive tool

Summary:<br /><br />
The article introduces T-REX, a live, interactive tool designed for verifying textual claims against structured tabular data. It utilizes state-of-the-art instruction-tuned reasoning Large Language Models (LLMs) to enhance accuracy and transparency in claim verification. T-REX is the first tool of its kind that allows non-experts to access advanced fact-checking technology, making it a valuable resource for individuals without specialized knowledge in the field. By offering support for multimodal and multilingual tables, T-REX expands the scope of claim verification tasks that can be performed. The system is openly available online, making it easily accessible for users seeking to fact-check claims in a variety of formats and languages. T-REX represents a significant advancement in Natural Language Processing and has the potential to have a widespread impact on real-world applications. 

Summary: <br /><br /> <div>
arXiv:2508.14055v1 Announce Type: cross 
Abstract: Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks</title>
<link>https://arxiv.org/abs/2508.14058</link>
<guid>https://arxiv.org/abs/2508.14058</guid>
<content:encoded><![CDATA[
<div> recommender systems, video games, playtime, diversity, multimodal information
Summary: 
The paper introduces DP2Rec, a recommendation model for video games that leverages playtime data to optimize accuracy and diversity. It includes a playtime-guided interest intensity exploration module that separates strong and weak preferences through dual-beta modeling. This enables better user profiling and more accurate recommendations. The model also incorporates a playtime-guided multimodal random walks module that promotes cross-category discovery while preserving core preferences. By using playtime-derived interest similarity and multimodal semantic similarity, DP2Rec improves recommendation accuracy and diversity compared to existing methods. Experimental results on a real-world game dataset demonstrate the effectiveness of DP2Rec in enhancing recommendations for video game platforms. <br /><br /> <div>
arXiv:2508.14058v1 Announce Type: cross 
Abstract: The explosive growth of the video game industry has created an urgent need for recommendation systems that can scale with expanding catalogs and maintain user engagement. While prior work has explored accuracy and diversity in recommendations, existing models underutilize playtime, a rich behavioral signal unique to gaming platforms, and overlook the potential of multimodal information to enhance diversity. In this paper, we propose DP2Rec, a novel Dual-Phase Playtime-guided Recommendation model designed to jointly optimize accuracy and diversity. First, we introduce a playtime-guided interest intensity exploration module that separates strong and weak preferences via dual-beta modeling, enabling fine-grained user profiling and more accurate recommendations. Second, we present a playtime-guided multimodal random walks module that simulates player exploration using transitions guided by both playtime-derived interest similarity and multimodal semantic similarity. This mechanism preserves core preferences while promoting cross-category discovery through latent semantic associations and adaptive category balancing. Extensive experiments on a real-world game dataset show that DP2Rec outperforms existing methods in both recommendation accuracy and diversity.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2508.14062</link>
<guid>https://arxiv.org/abs/2508.14062</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data memorization, privacy risks, privacy protection, fine-tuning

Summary:
Large Language Models (LLMs) have shown impressive performance in natural language processing tasks but face privacy risks due to memorization of training data during fine-tuning. This study analyses data memorization in fine-tuned LLMs and proposes a privacy protection framework. Experimenting on GPT-2, Phi-3, and Gemma-2, the study reveals that fine-tuning with repeated sensitive data increases privacy leakage rates significantly. Four privacy protection methods are introduced and evaluated, including semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. These techniques successfully reduce data leakage to 0% while preserving 94.7% of the original model's utility. The study emphasizes the importance of privacy considerations in LLM fine-tuning processes and provides effective solutions to mitigate privacy risks. 

<br /><br />Summary: <div>
arXiv:2508.14062v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Approach to Neurological Clinical Reasoning</title>
<link>https://arxiv.org/abs/2508.14063</link>
<guid>https://arxiv.org/abs/2508.14063</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, neurological reasoning, multi-agent system, medical domains
Summary:
Large language models (LLMs) are evaluated using a comprehensive benchmark that tests their ability to handle specialized neurological reasoning. The benchmark consists of 305 questions from Israeli Board Certification Exams in Neurology, categorized by factual knowledge depth, clinical concept integration, and reasoning complexity. Results show significant performance variation among the ten LLMs tested, with OpenAI-o1 achieving the highest base performance. Interestingly, specialized medical models performed poorly on the benchmark. While retrieval-augmented generation (RAG) provided modest benefits, a novel multi-agent system that decomposed neurological reasoning into specialized cognitive functions saw dramatic improvements in accuracy, especially for mid-range models. The multi-agent approach transformed inconsistent subspecialty performance into uniform excellence, addressing challenges in complex medical reasoning. These findings suggest promising directions for AI assistance in challenging clinical contexts.<br /><br />Summary: <div>
arXiv:2508.14063v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in medical domains, but their ability to handle specialized neurological reasoning requires systematic evaluation. We developed a comprehensive benchmark using 305 questions from Israeli Board Certification Exams in Neurology, classified along three complexity dimensions: factual knowledge depth, clinical concept integration, and reasoning complexity. We evaluated ten LLMs using base models, retrieval-augmented generation (RAG), and a novel multi-agent system. Results showed significant performance variation. OpenAI-o1 achieved the highest base performance (90.9% accuracy), while specialized medical models performed poorly (52.9% for Meditron-70B). RAG provided modest benefits but limited effectiveness on complex reasoning questions. In contrast, our multi-agent framework, decomposing neurological reasoning into specialized cognitive functions including question analysis, knowledge retrieval, answer synthesis, and validation, achieved dramatic improvements, especially for mid-range models. The LLaMA 3.3-70B-based agentic system reached 89.2% accuracy versus 69.5% for its base model, with substantial gains on level 3 complexity questions. The multi-agent approach transformed inconsistent subspecialty performance into uniform excellence, addressing neurological reasoning challenges that persisted with RAG enhancement. We validated our approach using an independent dataset of 155 neurological cases from MedQA. Results confirm that structured multi-agent approaches designed to emulate specialized cognitive processes significantly enhance complex medical reasoning, offering promising directions for AI assistance in challenging clinical contexts.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An automatic patent literature retrieval system based on LLM-RAG</title>
<link>https://arxiv.org/abs/2508.14064</link>
<guid>https://arxiv.org/abs/2508.14064</guid>
<content:encoded><![CDATA[
<div> integration, patent retrieval, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), intellectual property

Summary:
The study introduces an automated patent retrieval framework that combines Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology. The framework consists of a preprocessing module for standardizing patent data, a high-efficiency vector retrieval engine using LLM-generated embeddings, and a RAG-enhanced query module for context-aware response generation. Testing on the Google Patents dataset 2006-2024 showed that the gpt35turbo0125RAG configuration achieved 80.5% semantic matching accuracy and 92.1% recall, outperforming baseline LLM methods by 28 percentage points. The framework also displayed strong generalization in cross-domain classification and semantic clustering tasks, highlighting the efficacy of LLM-RAG integration in intelligent patent retrieval and paving the way for advanced AI-driven intellectual property analysis platforms. 

<br /><br />Summary: <div>
arXiv:2508.14064v1 Announce Type: cross 
Abstract: With the acceleration of technological innovation efficient retrieval and classification of patent literature have become essential for intellectual property management and enterprise RD Traditional keyword and rulebased retrieval methods often fail to address complex query intents or capture semantic associations across technical domains resulting in incomplete and lowrelevance results This study presents an automated patent retrieval framework integrating Large Language Models LLMs with RetrievalAugmented Generation RAG technology The system comprises three components: 1) a preprocessing module for patent data standardization, 2) a highefficiency vector retrieval engine leveraging LLMgenerated embeddings, and 3) a RAGenhanced query module that combines external document retrieval with contextaware response generation Evaluations were conducted on the Google Patents dataset 20062024 containing millions of global patent records with metadata such as filing date domain and status The proposed gpt35turbo0125RAG configuration achieved 805 semantic matching accuracy and 92.1% recall surpassing baseline LLM methods by 28 percentage points The framework also demonstrated strong generalization in crossdomain classification and semantic clustering tasks These results validate the effectiveness of LLMRAG integration for intelligent patent retrieval providing a foundation for nextgeneration AIdriven intellectual property analysis platforms
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation in Industry: An Interview Study on Use Cases, Requirements, Challenges, and Evaluation</title>
<link>https://arxiv.org/abs/2508.14066</link>
<guid>https://arxiv.org/abs/2508.14066</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, industry adoption, system requirements, challenges, evaluation methods

Summary:
Industry practitioners were interviewed to explore the current adoption of Retrieval-Augmented Generation (RAG) in real-world settings. Findings revealed that RAG applications are mainly focused on domain-specific QA tasks, with systems still in prototype stages. Industry requirements prioritize data protection, security, and quality, while ethics, bias, and scalability are given lesser attention. Data preprocessing remains a significant challenge, and system evaluation is largely conducted manually rather than through automated methods. The study provides valuable insights into the practical application of RAG in industrial contexts, offering an overview of use cases, system requirements, key challenges, lessons learned, and current industry evaluation methods. Overall, the research highlights the need for further development and optimization of RAG systems to meet the diverse needs and challenges faced by industry practitioners in implementing this technology.<br /><br />Summary: <div>
arXiv:2508.14066v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) is a well-established and rapidly evolving field within AI that enhances the outputs of large language models by integrating relevant information retrieved from external knowledge sources. While industry adoption of RAG is now beginning, there is a significant lack of research on its practical application in industrial contexts. To address this gap, we conducted a semistructured interview study with 13 industry practitioners to explore the current state of RAG adoption in real-world settings. Our study investigates how companies apply RAG in practice, providing (1) an overview of industry use cases, (2) a consolidated list of system requirements, (3) key challenges and lessons learned from practical experiences, and (4) an analysis of current industry evaluation methods. Our main findings show that current RAG applications are mostly limited to domain-specific QA tasks, with systems still in prototype stages; industry requirements focus primarily on data protection, security, and quality, while issues such as ethics, bias, and scalability receive less attention; data preprocessing remains a key challenge, and system evaluation is predominantly conducted by humans rather than automated methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisit Choice Network for Synthesis and Technology Mapping</title>
<link>https://arxiv.org/abs/2508.14068</link>
<guid>https://arxiv.org/abs/2508.14068</guid>
<content:encoded><![CDATA[
<div> optimization, Boolean, choice network construction, technology mapping, Cristal

Summary:<br />
The paper introduces Cristal, a new methodology for constructing Boolean choice networks to alleviate structural bias issues. Cristal utilizes representative logic cone search, structural mutation, and priority-ranking choice selection to generate high-quality choices. Experimental results show that Cristal outperforms existing methods in reducing area and delay in both delay-oriented and area-oriented modes. It also achieves a significant runtime reduction on large-scale cases from various benchmark suites. By focusing on quality over quantity, Cristal demonstrates improved performance in post-mapping stages of technology mapping, showcasing its potential to enhance Boolean optimization processes. <div>
arXiv:2508.14068v1 Announce Type: cross 
Abstract: Choice network construction is a critical technique for alleviating structural bias issues in Boolean optimization, equivalence checking, and technology mapping. Previous works on lossless synthesis utilize independent optimization to generate multiple snapshots, and use simulation and SAT solvers to identify functionally equivalent nodes. These nodes are then merged into a subject graph with choice nodes. However, such methods often neglect the quality of these choices, raising the question of whether they truly contribute to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for constructing Boolean choice networks. Specifically, Cristal introduces a new flow of choice network-based synthesis and mapping, including representative logic cone search, structural mutation for generating diverse choice structures via equality saturation, and priority-ranking choice selection along with choice network construction and validation. Through these techniques, Cristal constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the state-of-the-art Boolean choice network construction implemented in ABC in the post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime reduction on large-scale cases across a diverse set of combinational circuits from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Special-Character Adversarial Attacks on Open-Source Language Model</title>
<link>https://arxiv.org/abs/2508.14070</link>
<guid>https://arxiv.org/abs/2508.14070</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, vulnerability, adversarial manipulations, security challenges, real-world deployments

Summary: 
Large language models (LLMs) have shown impressive performance in various natural language processing tasks. However, they are susceptible to character-level adversarial manipulations, posing significant security risks in practical applications. This vulnerability threatens the integrity and reliability of LLMs in real-world deployments. As LLMs become more prevalent in everyday applications, addressing these security challenges is crucial to prevent malicious attacks exploiting their weaknesses. Researchers and developers must prioritize enhancing the robustness of LLMs to withstand potential adversarial threats. By improving the resilience of these models, organizations can ensure the trustworthiness and effectiveness of their natural language processing systems. Vigilance and proactive measures are essential to safeguard against potential exploits and ensure the safe and reliable utilization of large language models. 

Summary: <div>
arXiv:2508.14070v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable performance across diverse natural language processing tasks, yet their vulnerability to character-level adversarial manipulations presents significant security challenges for real-world deployments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2508.14071</link>
<guid>https://arxiv.org/abs/2508.14071</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Metaheuristic, Vehicle Routing Problems, Edge Solution Selector, Graph Neural Network

Summary: 
This research introduces a hybrid approach using Machine Learning and metaheuristic techniques to address Vehicle Routing Problems (VRPs). The key component of the method is an edge solution selector model that categorizes solution edges to identify forbidden moves during local search algorithms in metaheuristics. Two learning-based mechanisms are employed to develop the edge selector: a tabular binary classifier and a Graph Neural Network (GNN). The tabular classifier uses Gradient Boosting Trees and Feedforward Neural Network baseline algorithms, while the GNN utilizes graph structure for direct solution edge prediction. These mechanisms are integrated into state-of-the-art metaheuristic baselines, demonstrating scalability and generalizability. The approach shows performance enhancements across different metaheuristics, problem sizes, and variations of VRPs, such as Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time Windows (CVRPTW). Experimental evaluations on benchmark datasets confirm the effectiveness of the proposed method, supporting the observed improvements. 

<br /><br />Summary: <div>
arXiv:2508.14071v1 Announce Type: cross 
Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism that is designed to solve Vehicle Routing Problems (VRPs). The main of our method is an edge solution selector model, which classifies solution edges to identify prohibited moves during the local search, hence guiding the search process within metaheuristic baselines. Two learning-based mechanisms are used to develop the edge selector: a simple tabular binary classifier and a Graph Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees and Feedforward Neural Network as the baseline algorithms. Adjustments to the decision threshold are also applied to handle the class imbalance in the problem instance. An alternative mechanism employs the GNN to utilize graph structure for direct solution edge prediction, with the objective of guiding local search by predicting prohibited moves. These hybrid mechanisms are then applied in state-fo-the-art metaheuristic baselines. Our method demonstrates both scalability and generalizability, achieving performance improvements across different baseline metaheuristics, various problem sizes and variants, including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000 customer nodes, supported by pair-wise statistical analysis, verify the observed improvements.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets</title>
<link>https://arxiv.org/abs/2508.14073</link>
<guid>https://arxiv.org/abs/2508.14073</guid>
<content:encoded><![CDATA[
<div> EEG data annotation, Parkinson's disease detection, semi-supervised learning, multi-view contrastive pre-training, lightweight supervised fine-tuning

Summary: 
The paper introduces a semi-supervised learning framework called MCLPD for improving cross-dataset Parkinson's disease (PD) detection performance using EEG data. It utilizes multi-view contrastive pre-training with dual augmentations in time and frequency domains on the unlabeled UNM dataset. During the fine-tuning phase, only a small portion of labeled data from the UI and UC datasets is needed for supervised optimization. MCLPD achieves high F1 scores of 0.91 on UI and 0.81 on UC with just 1% of labeled data, which increase to 0.97 and 0.87, respectively, with 5% of labeled data. The framework significantly enhances cross-dataset generalization and reduces the dependency on labeled data, demonstrating its effectiveness in improving Parkinson's disease detection using EEG data. 

<br /><br />Summary: <div>
arXiv:2508.14073v1 Announce Type: cross 
Abstract: Electroencephalography has been validated as an effective technique for detecting Parkinson's disease,particularly in its early stages.However,the high cost of EEG data annotation often results in limited dataset size and considerable discrepancies across datasets,including differences in acquisition protocols and subject demographics,significantly hinder the robustness and generalizability of models in cross-dataset detection scenarios.To address such challenges,this paper proposes a semi-supervised learning framework named MCLPD,which integrates multi-view contrastive pre-training with lightweight supervised fine-tuning to enhance cross-dataset PD detection performance.During pre-training,MCLPD uses self-supervised learning on the unlabeled UNM dataset.To build contrastive pairs,it applies dual augmentations in both time and frequency domains,which enrich the data and naturally fuse time-frequency information.In the fine-tuning phase,only a small proportion of labeled data from another two datasets (UI and UC)is used for supervised optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97 and 0.87,respectively,when 5%of labeled data is used.Compared to existing methods,MCLPD substantially improves cross-dataset generalization while reducing the dependency on labeled data,demonstrating the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease</title>
<link>https://arxiv.org/abs/2508.14074</link>
<guid>https://arxiv.org/abs/2508.14074</guid>
<content:encoded><![CDATA[
<div> Keywords: Electroencephalography, Parkinson's disease, GAN, cross-dataset classification, generalizable model

Summary:
This paper presents a GAN-enhanced generalizable model, GEPD, for EEG-based cross-dataset classification of Parkinson's disease. The model addresses challenges in variability and dataset size by generating fusion EEG data and utilizing a classification network with multiple CNNs. A generative network ensures data quality, while the classification network captures time-frequency characteristics of EEG signals. The model aims to facilitate the diagnosis and monitoring of neurological diseases. Evaluation results show the model achieves an accuracy of 84.3% and an F1-score of 84.0%, demonstrating its generalizability in cross-dataset settings.

<br /><br />Summary: <div>
arXiv:2508.14074v1 Announce Type: cross 
Abstract: Electroencephalography has been established as an effective method for detecting Parkinson's disease, typically diagnosed early.Current Parkinson's disease detection methods have shown significant success within individual datasets, however, the variability in detection methods across different EEG datasets and the small size of each dataset pose challenges for training a generalizable model for cross-dataset scenarios. To address these issues, this paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for EEG-based cross-dataset classification of Parkinson's disease.First, we design a generative network that creates fusion EEG data by controlling the distribution similarity between generated data and real data.In addition, an EEG signal quality assessment model is designed to ensure the quality of generated data great.Second, we design a classification network that utilizes a combination of multiple convolutional neural networks to effectively capture the time-frequency characteristics of EEG signals, while maintaining a generalizable structure and ensuring easy convergence.This work is dedicated to utilizing intelligent methods to study pathological manifestations, aiming to facilitate the diagnosis and monitoring of neurological diseases.The evaluation results demonstrate that our model performs comparably to state-of-the-art models in cross-dataset settings, achieving an accuracy of 84.3% and an F1-score of 84.0%, showcasing the generalizability of the proposed model.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Graph Spectral Clustering For Text Embeddings</title>
<link>https://arxiv.org/abs/2508.14075</link>
<guid>https://arxiv.org/abs/2508.14075</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Spectral Clustering, explainability, document similarity, GloVe embedding, textual documents

Summary:
The paper discusses the explainability of Graph Spectral Clustering results for textual documents, focusing on document similarity computed in term vector space using cosine similarity. Building upon this concept, the authors extend their analysis to encompass other document embeddings, specifically those based on the GloVe embedding technique. By exploring different ways to represent document information, the paper aims to enhance the interpretability of clustering results obtained using Graph Spectral Clustering. This broader approach allows for a more comprehensive understanding of the relationships between documents and facilitates the identification of underlying patterns within the data. The incorporation of GloVe embeddings offers a novel perspective on document representations and provides valuable insights into the structure and content of textual data. The research contributes to the advancement of explainable AI methods in document clustering and sheds light on the interpretability of machine learning models in the context of text analysis.

<br /><br />Summary: <div>
arXiv:2508.14075v1 Announce Type: cross 
Abstract: In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.14076</link>
<guid>https://arxiv.org/abs/2508.14076</guid>
<content:encoded><![CDATA[
<div> reasoning-based reward modeling framework, personal factors, limited data, generalizability, personalized LLMs
Summary:<br /><br /> 
The article introduces PersRM-R1, a reasoning-based reward modeling framework designed to identify and represent personal factors from minimal personal exemplars. By incorporating synthetic data generation and a two-stage training pipeline, PersRM-R1 overcomes challenges such as limited data availability and requires for robust generalization. Experimental results show that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in accuracy and generalizability. This advancement paves the way for the development of more effective personalized Large Language Models (LLMs) by capturing nuanced user-specific preferences effectively. <div>
arXiv:2508.14076v1 Announce Type: cross 
Abstract: Reward models (RMs), which are central to existing post-training methods, aim to align LLM outputs with human values by providing feedback signals during fine-tuning. However, existing RMs struggle to capture nuanced, user-specific preferences, especially under limited data and across diverse domains. Thus, we introduce PersRM-R1, the first reasoning-based reward modeling framework specifically designed to identify and represent personal factors from only one or a few personal exemplars. To address challenges including limited data availability and the requirement for robust generalization, our approach combines synthetic data generation with a two-stage training pipeline consisting of supervised fine-tuning followed by reinforcement fine-tuning. Experimental results demonstrate that PersRM-R1 outperforms existing models of similar size and matches the performance of much larger models in both accuracy and generalizability, paving the way for more effective personalized LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Smoothing is a Pragmatic Information Bottleneck</title>
<link>https://arxiv.org/abs/2508.14077</link>
<guid>https://arxiv.org/abs/2508.14077</guid>
<content:encoded><![CDATA[
<div> label smoothing, information bottleneck, model flexibility, conflicting labels, simple implementation

Summary:
label smoothing is revisited in this study as a form of information bottleneck, demonstrating theoretically and experimentally that it explores the optimal solution. Under the assumption of sufficient model flexibility and no conflicting labels, label smoothing can be interpreted as a practical approach to the information bottleneck, with simple implementation. This method is shown to be insensitive to factors that do not contain information about the target or provide no additional information when conditioned on another variable. These findings highlight the effectiveness of label smoothing as an information bottleneck method, providing insights into its implementation and properties. <div>
arXiv:2508.14077v1 Announce Type: cross 
Abstract: This study revisits label smoothing via a form of information bottleneck. Under the assumption of sufficient model flexibility and no conflicting labels for the same input, we theoretically and experimentally demonstrate that the model output obtained through label smoothing explores the optimal solution of the information bottleneck. Based on this, label smoothing can be interpreted as a practical approach to the information bottleneck, enabling simple implementation. As an information bottleneck method, we experimentally show that label smoothing also exhibits the property of being insensitive to factors that do not contain information about the target, or to factors that provide no additional information about it when conditioned on another variable.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values</title>
<link>https://arxiv.org/abs/2508.14083</link>
<guid>https://arxiv.org/abs/2508.14083</guid>
<content:encoded><![CDATA[
<div> Keywords: Crowd flow inference, Self-supervised learning, Spatio-temporal data, Contrastive learning, POIs

Summary: 
- The article addresses the challenge of accurately inferring crowd flow at Points of Interest (POIs) with limited and low-quality data by proposing a Contrastive Self-learning framework for Spatio-Temporal data (\model).
- The framework leverages self-supervised attributed graph representation learning, utilizing a spatial adjacency graph and contrastive learning technique on unlabeled spatio-temporal data.
- The model employs a swapped prediction approach to anticipate representations of target subgraphs from similar instances during pre-training.
- By pre-training on noisy data and fine-tuning on accurate crowd flow data, the \model consistently outperforms models trained from scratch, as demonstrated on two real-world datasets. 

<br /><br />Summary: The article introduces a novel Contrastive Self-learning framework for Spatio-Temporal data to address the challenge of accurately inferring crowd flow at Points of Interest with limited and low-quality data. By leveraging self-supervised attributed graph representation learning and adopting a contrastive learning technique, the model outperforms models trained from scratch on real-world datasets. <div>
arXiv:2508.14083v1 Announce Type: cross 
Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spatial adjacency graph founded on the POIs and their respective distances. We then employ a contrastive learning technique to exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped prediction approach to anticipate the representation of the target subgraph from similar instances. Following the pre-training phase, the model is fine-tuned with accurate crowd flow data. Our experiments, conducted on two real-world datasets, demonstrate that the \model pre-trained on extensive noisy data consistently outperforms models trained from scratch.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics</title>
<link>https://arxiv.org/abs/2508.14087</link>
<guid>https://arxiv.org/abs/2508.14087</guid>
<content:encoded><![CDATA[
<div> self-supervised training, experimental particle physics, scientific foundation models, downstream tasks, neural scalability

Summary:<br />
This study investigates the use of scientific foundation models (FMs) in experimental particle physics, which faces challenges due to the sparse and spatially distributed nature of detector data. A novel self-supervised training method is introduced, along with a dataset of over 11 million particle collision events and various downstream tasks for evaluation. Models with up to 188 million parameters are trained and show superior performance compared to baseline models across all tasks. The representations extracted by the FM are task-agnostic but can be easily specialized for different tasks with a simple linear mapping. The FM exhibits robust data-efficient adaptation, demonstrating its scalability and generalization capabilities in the field of experimental particle physics. <div>
arXiv:2508.14087v1 Announce Type: cross 
Abstract: Large language models have revolutionized artificial intelligence by enabling large, generalizable models trained through self-supervision. This paradigm has inspired the development of scientific foundation models (FMs). However, applying this capability to experimental particle physics is challenging due to the sparse, spatially distributed nature of detector data, which differs dramatically from natural language. This work addresses if an FM for particle physics can scale and generalize across diverse tasks. We introduce a new dataset with more than 11 million particle collision events and a suite of downstream tasks and labeled data for evaluation. We propose a novel self-supervised training method for detector data and demonstrate its neural scalability with models that feature up to 188 million parameters. With frozen weights and task-specific adapters, this FM consistently outperforms baseline models across all downstream tasks. The performance also exhibits robust data-efficient adaptation. Further analysis reveals that the representations extracted by the FM are task-agnostic but can be specialized via a single linear mapping for different downstream tasks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</title>
<link>https://arxiv.org/abs/2508.14088</link>
<guid>https://arxiv.org/abs/2508.14088</guid>
<content:encoded><![CDATA[
<div> Keywords: human mobility, anomaly detection, collective behaviors, spatiotemporal dependencies, unsupervised learning<br />
<br />
Summary: <br />
Detecting anomalies in human mobility is crucial for various applications such as public safety and urban planning. This study introduces a novel model, CoBAD, designed for Collective Behaviors for human mobility Anomaly Detection. The model focuses on detecting collective anomalies by capturing spatiotemporal dependencies between individuals. By formulating the problem as unsupervised learning over Collective Event Sequences (CES) with a co-occurrence event graph, CoBAD utilizes a two-stage attention mechanism to model individual mobility patterns and interactions across multiple individuals. It is pre-trained on large-scale collective behavior data and is capable of detecting both unexpected co-occurrence anomalies and absence anomalies. Experimental results on large-scale mobility datasets show that CoBAD outperforms existing anomaly detection methods significantly, achieving improvements of 13%-18% in AUCROC and 19%-70% in AUCPR. The source code for CoBAD is available on GitHub for further exploration. <br /> <div>
arXiv:2508.14088v1 Announce Type: cross 
Abstract: Detecting anomalies in human mobility is essential for applications such as public safety and urban planning. While traditional anomaly detection methods primarily focus on individual movement patterns (e.g., a child should stay at home at night), collective anomaly detection aims to identify irregularities in collective mobility behaviors across individuals (e.g., a child is at home alone while the parents are elsewhere) and remains an underexplored challenge. Unlike individual anomalies, collective anomalies require modeling spatiotemporal dependencies between individuals, introducing additional complexity. To address this gap, we propose CoBAD, a novel model designed to capture Collective Behaviors for human mobility Anomaly Detection. We first formulate the problem as unsupervised learning over Collective Event Sequences (CES) with a co-occurrence event graph, where CES represents the event sequences of related individuals. CoBAD then employs a two-stage attention mechanism to model both the individual mobility patterns and the interactions across multiple individuals. Pre-trained on large-scale collective behavior data through masked event and link reconstruction tasks, CoBAD is able to detect two types of collective anomalies: unexpected co-occurrence anomalies and absence anomalies, the latter of which has been largely overlooked in prior work. Extensive experiments on large-scale mobility datasets demonstrate that CoBAD significantly outperforms existing anomaly detection baselines, achieving an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is available at https://github.com/wenhaomin/CoBAD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLLMQuant: Quantizing Diffusion-based Large Language Models</title>
<link>https://arxiv.org/abs/2508.14090</link>
<guid>https://arxiv.org/abs/2508.14090</guid>
<content:encoded><![CDATA[
<div> quantization, large language models, non-autoregressive text generation, dynamic masking, iterative generation<br />
Summary:<br />
1) Diffusion-based large language models (DLLMs) show promise for non-autoregressive text generation but face challenges due to large sizes and computational costs.<br />
2) Post-training quantization (PTQ) struggles with DLLMs, leading to accuracy degradation and reduced generalization performance.<br />
3) Issues with DLLMs and PTQ include distinct token distributions, accumulative quantization errors, and feature distribution incompatibility.<br />
4) The proposed DLLMQuant framework addresses these issues with novel techniques such as Temporal-Mask Adaptive Sampling and Interaction-Aware Activation Quantization.<br />
5) Experiments demonstrate that DLLMQuant significantly improves performance and efficiency, making it suitable for quantizing DLLMs.<br /> 

Summary: <div>
arXiv:2508.14090v1 Announce Type: cross 
Abstract: Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions</title>
<link>https://arxiv.org/abs/2508.14091</link>
<guid>https://arxiv.org/abs/2508.14091</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, link prediction, explainability, Datalog rules, scoring functions,
Monotonic GNNs and scoring functions are proposed for link prediction tasks in knowledge graphs. These models aim to provide explainability by extracting sound rules that correspond to the predictions made by the GNNs. The use of monotonicity in adapting GNNs and scoring functions allows for the extraction of sound rules that can explain the predictions effectively and efficiently. By leveraging existing results on the rules that scoring functions can capture, the authors show how Datalog programs can be obtained for certain classes of monotonic GNNs with scoring functions. Experimental results on link prediction benchmarks demonstrate that the proposed approach performs well in practice and yields a significant number of sound rules that explain the predictions made by the GNN models.<br /><br />Summary: Graph neural networks and scoring functions are adapted to be monotonic for link prediction in knowledge graphs. The use of monotonicity allows for the extraction of sound rules to explain the predictions effectively. Leveraging existing results on scoring functions, Datalog programs can be obtained for certain classes of monotonic GNNs with scoring functions. Experimental results show the effectiveness of the proposed approach in providing explainable predictions in link prediction tasks. <div>
arXiv:2508.14091v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are often used for the task of link prediction: predicting missing binary facts in knowledge graphs (KGs). To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. The extracted rules can be used to explain the GNN's predictions; furthermore, they can help characterise the expressive power of various GNN models. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. In this paper, we consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions. We show how GNNs and scoring functions can be adapted to be monotonic, use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture. We also define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions. Our experiments show that, on link prediction benchmarks, monotonic GNNs and scoring functions perform well in practice and yield many sound rules.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets</title>
<link>https://arxiv.org/abs/2508.14094</link>
<guid>https://arxiv.org/abs/2508.14094</guid>
<content:encoded><![CDATA[
<div> Keywords: language model fine-tuning, acquisition budget, Group Relative Policy Optimization, difficulty estimates, performance gains

Summary: 
- The study examines the acquisition of training examples for language model fine-tuning under a fixed budget constraint.
- Four subset selection policies are compared using base-model difficulty estimates obtained from multi-sample evaluation.
- Training on the hardest examples leads to the largest performance gains, up to 47%, while training on easy examples results in the smallest gains.
- Harder examples offer more learnable opportunities during Group Relative Policy Optimization (GRPO) training, contributing to the performance improvements.
- Prioritizing hard examples in budget-constrained post-training using GRPO can significantly enhance performance on reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.14094v1 Announce Type: cross 
Abstract: Collecting high-quality training examples for language model fine-tuning is expensive, with practical budgets limiting the amount of data that can be procured. We investigate a critical question for resource-constrained alignment: under a fixed acquisition budget, should practitioners prioritize examples that are easy, medium, hard, or of random difficulty? We study Group Relative Policy Optimization (GRPO) fine-tuning across different model sizes and families, comparing four subset selection policies chosen from the same unlabeled pool using base-model difficulty estimates obtained via multi-sample evaluation. Our experiments reveal that training on the hardest examples yields the largest performance gains, up to 47%, while training on easy examples yield the smallest gains. Analysis reveals that this effect arises from harder examples providing more learnable opportunities during GRPO training. These findings provide practical guidance for budget-constrained post-training: prioritizing hard examples yields substantial performance gains on reasoning tasks when using GRPO.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Dissipative Graph Propagation for Non-Local Community Detection</title>
<link>https://arxiv.org/abs/2508.14097</link>
<guid>https://arxiv.org/abs/2508.14097</guid>
<content:encoded><![CDATA[
<div> Community detection, graphs, heterophilic graphs, graph neural networks, unsupervised approach 
<br />
<br />
Keywords extracted: community detection, heterophilic graphs, graph neural networks, unsupervised approach, long-range information propagation.

Summary: 
The study addresses the challenging task of community detection in graphs, particularly in heterophilic graphs where nodes with similar characteristics are distantly connected. It introduces the Unsupervised Antisymmetric Graph Neural Network (uAGNN), utilizing non-dissipative dynamical systems to propagate long-range information effectively and ensure stability. By incorporating antisymmetric weight matrices, uAGNN captures both local and global graph structures, overcoming limitations in heterophilic scenarios. Extensive experiments on ten datasets demonstrate superior performance in high and medium heterophilic settings, where traditional methods struggle to utilize long-range dependencies. The results emphasize uAGNN's potential as a robust tool for unsupervised community detection across various graph environments. 
<br /><br />Summary: <div>
arXiv:2508.14097v1 Announce Type: cross 
Abstract: Community detection in graphs aims to cluster nodes into meaningful groups, a task particularly challenging in heterophilic graphs, where nodes sharing similarities and membership to the same community are typically distantly connected. This is particularly evident when this task is tackled by graph neural networks, since they rely on an inherently local message passing scheme to learn the node representations that serve to cluster nodes into communities. In this work, we argue that the ability to propagate long-range information during message passing is key to effectively perform community detection in heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric Graph Neural Network (uAGNN), a novel unsupervised community detection approach leveraging non-dissipative dynamical systems to ensure stability and to propagate long-range information effectively. By employing antisymmetric weight matrices, uAGNN captures both local and global graph structures, overcoming the limitations posed by heterophilic scenarios. Extensive experiments across ten datasets demonstrate uAGNN's superior performance in high and medium heterophilic settings, where traditional methods fail to exploit long-range dependencies. These results highlight uAGNN's potential as a powerful tool for unsupervised community detection in diverse graph environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets</title>
<link>https://arxiv.org/abs/2508.14098</link>
<guid>https://arxiv.org/abs/2508.14098</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, humanoid locomotion, SE(2) targets, energy efficient, targeted reward design
Summary:
This work presents a reinforcement learning approach for humanoid locomotion targeting SE(2) poses in real-world environments. Unlike existing methods that focus on velocity-tracking, this approach optimizes for direct pose reaching, leading to more efficient and natural movement. A novel constellation-based reward function is introduced to incentivize target-oriented behavior. A benchmarking framework is used to evaluate performance based on energy consumption, time-to-target, and footstep count across a range of SE(2) goals. Results demonstrate superior performance compared to standard methods, with successful transfer from simulation to hardware. These findings underscore the importance of targeted reward design for optimizing short-range humanoid locomotion in practical settings. <br /><br />Summary: <div>
arXiv:2508.14098v1 Announce Type: cross 
Abstract: Humanoids operating in real-world workspaces must frequently execute task-driven, short-range movements to SE(2) target poses. To be practical, these transitions must be fast, robust, and energy efficient. While learning-based locomotion has made significant progress, most existing methods optimize for velocity-tracking rather than direct pose reaching, resulting in inefficient, marching-style behavior when applied to short-range tasks. In this work, we develop a reinforcement learning approach that directly optimizes humanoid locomotion for SE(2) targets. Central to this approach is a new constellation-based reward function that encourages natural and efficient target-oriented movement. To evaluate performance, we introduce a benchmarking framework that measures energy consumption, time-to-target, and footstep count on a distribution of SE(2) goals. Our results show that the proposed approach consistently outperforms standard methods and enables successful transfer from simulation to hardware, highlighting the importance of targeted reward design for practical short-range humanoid locomotion.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network</title>
<link>https://arxiv.org/abs/2508.14100</link>
<guid>https://arxiv.org/abs/2508.14100</guid>
<content:encoded><![CDATA[
<div> learning, soft robots, domain translation, generative adversarial network, dynamic learning 

Summary: 
The article discusses the use of deep learning in modeling the dynamics of soft robots, presenting a domain translation framework based on a conditional cycle generative adversarial network (CCGAN). This framework addresses the challenge of transferring knowledge from one domain to another with different physical properties, particularly relevant for soft robots that degrade over time. The approach is demonstrated through adapting a pose controller trained in a standard simulation environment to a domain with increased viscosity. The model learns from input pressure signals conditioned on end-effector positions and orientations in both domains, showing effectiveness in trajectory-tracking experiments across different shapes and robustness under noise perturbations. The results highlight the potential of CCGAN-GP for facilitating cross-domain skill transfer, enhancing adaptability and generalizability of soft robotic controllers.  <div>
arXiv:2508.14100v1 Announce Type: cross 
Abstract: Deep learning provides a powerful method for modeling the dynamics of soft robots, offering advantages over traditional analytical approaches that require precise knowledge of the robot's structure, material properties, and other physical characteristics. Given the inherent complexity and non-linearity of these systems, extracting such details can be challenging. The mappings learned in one domain cannot be directly transferred to another domain with different physical properties. This challenge is particularly relevant for soft robots, as their materials gradually degrade over time. In this paper, we introduce a domain translation framework based on a conditional cycle generative adversarial network (CCGAN) to enable knowledge transfer from a source domain to a target domain. Specifically, we employ a dynamic learning approach to adapt a pose controller trained in a standard simulation environment to a domain with tenfold increased viscosity. Our model learns from input pressure signals conditioned on corresponding end-effector positions and orientations in both domains. We evaluate our approach through trajectory-tracking experiments across five distinct shapes and further assess its robustness under noise perturbations and periodicity tests. The results demonstrate that CCGAN-GP effectively facilitates cross-domain skill transfer, paving the way for more adaptable and generalizable soft robotic controllers.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2508.14101</link>
<guid>https://arxiv.org/abs/2508.14101</guid>
<content:encoded><![CDATA[
<div> Keywords: Hypergraphs, Neural Networks, Long-range Dependencies, Implicit Differentiation, Node Classification

Summary:
Hypergraphs are useful for capturing high-order relationships between entities in various domains. Hypergraph neural networks (HNNs) are commonly used for predictive tasks but struggle to capture long-range dependencies. Increasing message-passing rounds in HNNs degrades performance. However, blindly aggregating more information to capture long-range dependencies also results in reduced predictive power. To address this issue, the Implicit Hypergraph Neural Network (IHNN) is proposed. IHNN jointly learns fixed-point representations for nodes and hyperedges and utilizes implicit differentiation for efficient training. Experimental results show that IHNN outperforms existing methods, establishing a new state-of-the-art in hypergraph learning. <div>
arXiv:2508.14101v1 Announce Type: cross 
Abstract: Hypergraphs offer a generalized framework for capturing high-order relationships between entities and have been widely applied in various domains, including healthcare, social networks, and bioinformatics. Hypergraph neural networks, which rely on message-passing between nodes over hyperedges to learn latent representations, have emerged as the method of choice for predictive tasks in many of these domains. These approaches typically perform only a small number of message-passing rounds to learn the representations, which they then utilize for predictions. The small number of message-passing rounds comes at a cost, as the representations only capture local information and forego long-range high-order dependencies. However, as we demonstrate, blindly increasing the message-passing rounds to capture long-range dependency also degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture long-range dependencies in standard graphs while maintaining performance. Despite their popularity, prior work has not studied long-range dependency issues on hypergraph neural networks. Here, we first demonstrate that existing hypergraph neural networks lose predictive power when aggregating more information to capture long-range dependency. We then propose Implicit Hypergraph Neural Network (IHNN), a novel framework that jointly learns fixed-point representations for both nodes and hyperedges in an end-to-end manner to alleviate this issue. Leveraging implicit differentiation, we introduce a tractable projected gradient descent approach to train the model efficiently. Extensive experiments on real-world hypergraphs for node classification demonstrate that IHNN outperforms the closest prior works in most settings, establishing a new state-of-the-art in hypergraph learning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation</title>
<link>https://arxiv.org/abs/2508.14104</link>
<guid>https://arxiv.org/abs/2508.14104</guid>
<content:encoded><![CDATA[
<div>  Keywords: Large Language Models, software development, RealDevWorld, automated assessment, production-ready repositories

Summary:
RealDevWorld introduces an evaluation framework for assessing the capability of Large Language Models (LLMs) in generating production-ready software applications. The framework consists of two main components: RealDevBench, which includes a diverse set of software engineering tasks with real-world complexity, and AppEvalPilot, an evaluation system that simulates user interactions to assess software functionality, visual fidelity, and runtime behavior. This framework provides detailed diagnostic feedback for nuanced evaluation, reducing the need for manual review. Empirical results show that RealDevWorld achieves high accuracy and correlation with expert human assessments, enabling scalable and human-aligned evaluation of software generated by LLMs. The code for RealDevWorld is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.14104v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and code agents in software development are rapidly evolving from generating isolated code snippets to producing full-fledged software applications with graphical interfaces, interactive logic, and dynamic behaviors. However, current benchmarks fall short in evaluating such production-ready software, as they often rely on static checks or binary pass/fail scripts, failing to capture the interactive behaviors and runtime dynamics that define real-world usability - qualities that only emerge when an application is actively used. This is the blind spot of current evaluation: you don't know if an app works until you click through it, interact with it, and observe how it responds. To bridge this gap, we introduce RealDevWorld, a novel evaluation framework for automated end-to-end assessment of LLMs' ability to generate production-ready repositories from scratch. It features two key components: (1) RealDevBench, a diverse collection of 194 open-ended software engineering tasks across multiple domains, incorporating multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a new agent-as-a-judge evaluation system that simulates realistic, GUI-based user interactions to automatically and holistically assess software functional correctness, visual fidelity, and runtime behavior. The framework delivers fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation beyond simple success/failure judgments. Empirical results show that RealDevWorld delivers effective, automatic, and human-aligned evaluations, achieving an accuracy of 0.92 and a correlation of 0.85 with expert human assessments, while significantly reducing the reliance on manual review. This enables scalable, human-aligned assessment of production-level software generated by LLMs. Our code is available on GitHub.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images</title>
<link>https://arxiv.org/abs/2508.14106</link>
<guid>https://arxiv.org/abs/2508.14106</guid>
<content:encoded><![CDATA[
<div> Keywords: live cell culture, bright-field microscopy, segmentation, deep learning, biomedical studies

Summary: 
This study introduces a novel CNN-based pipeline for segmenting unstained live cells imaged with bright-field microscopy. The model incorporates various advanced techniques such as attention mechanisms, adaptive loss functions, and ensemble techniques to address the challenges of low contrast, noise, and motion-induced blur in live-cell imaging. The pipeline achieved a test accuracy of 93% and an average F1-score of 89% on a diverse public dataset. It demonstrated strong performance in segmenting bright-field images and generalized effectively to phase-contrast microscopy datasets, showcasing robustness and versatility. The model is cost-effective, computationally efficient, and easily deployable in real-world laboratory settings. The code and dataset are available for reproducibility, making it practical for training on other cell variants. Overall, this pipeline outperforms existing methods in bright-field microscopy segmentation, offering a robust and precise solution for analyzing live cell dynamics in biomedical studies.<br /><br />Summary: <div>
arXiv:2508.14106v1 Announce Type: cross 
Abstract: Live cell culture is crucial in biomedical studies for analyzing cell properties and dynamics in vitro. This study focuses on segmenting unstained live cells imaged with bright-field microscopy. While many segmentation approaches exist for microscopic images, none consistently address the challenges of bright-field live-cell imaging with high throughput, where temporal phenotype changes, low contrast, noise, and motion-induced blur from cellular movement remain major obstacles. We developed a low-cost CNN-based pipeline incorporating comparative analysis of frozen encoders within a unified U-Net architecture enhanced with attention mechanisms, instance-aware systems, adaptive loss functions, hard instance retraining, dynamic learning rates, progressive mechanisms to mitigate overfitting, and an ensemble technique. The model was validated on a public dataset featuring diverse live cell variants, showing consistent competitiveness with state-of-the-art methods, achieving 93% test accuracy and an average F1-score of 89% (std. 0.07) on low-contrast, noisy, and blurry images. Notably, the model was trained primarily on bright-field images with limited exposure to phase-contrast microscopy (<10%), yet it generalized effectively to the phase-contrast LIVECell dataset, demonstrating modality, robustness and strong performance. This highlights its potential for real-world laboratory deployment across imaging conditions. The model requires minimal compute power and is adaptable using basic deep learning setups such as Google Colab, making it practical for training on other cell variants. Our pipeline outperforms existing methods in robustness and precision for bright-field microscopy segmentation. The code and dataset are available for reproducibility
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuryaBench: Benchmark Dataset for Advancing Machine Learning in Heliophysics and Space Weather Prediction</title>
<link>https://arxiv.org/abs/2508.14107</link>
<guid>https://arxiv.org/abs/2508.14107</guid>
<content:encoded><![CDATA[
<div> heliophysics, machine learning, dataset, Solar Dynamics Observatory, space weather forecasting

Summary:
- Introduction of a high-resolution heliophysics dataset derived from NASA's Solar Dynamics Observatory (SDO).
- Dataset designed for machine learning applications in solar physics and space weather forecasting.
- Includes processed imagery from AIA and HMI spanning a solar cycle from May 2010 to July 2024.
- Preprocessing of data for ML tasks, including correction of spacecraft roll angles, orbital adjustments, exposure normalization, and degradation compensation.
- Provision of benchmark datasets for heliophysics and space weather tasks to facilitate development of AI-driven models for critical space weather prediction tasks. 

<br /><br />Summary: <div>
arXiv:2508.14107v1 Announce Type: cross 
Abstract: This paper introduces a high resolution, machine learning-ready heliophysics dataset derived from NASA's Solar Dynamics Observatory (SDO), specifically designed to advance machine learning (ML) applications in solar physics and space weather forecasting. The dataset includes processed imagery from the Atmospheric Imaging Assembly (AIA) and Helioseismic and Magnetic Imager (HMI), spanning a solar cycle from May 2010 to July 2024. To ensure suitability for ML tasks, the data has been preprocessed, including correction of spacecraft roll angles, orbital adjustments, exposure normalization, and degradation compensation. We also provide auxiliary application benchmark datasets complementing the core SDO dataset. These provide benchmark applications for central heliophysics and space weather tasks such as active region segmentation, active region emergence forecasting, coronal field extrapolation, solar flare prediction, solar EUV spectra prediction, and solar wind speed estimation. By establishing a unified, standardized data collection, this dataset aims to facilitate benchmarking, enhance reproducibility, and accelerate the development of AI-driven models for critical space weather prediction tasks, bridging gaps between solar physics, machine learning, and operational forecasting.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAPPL: Personalized AI-Powered Progressive Learning Platform</title>
<link>https://arxiv.org/abs/2508.14109</link>
<guid>https://arxiv.org/abs/2508.14109</guid>
<content:encoded><![CDATA[
<div> AI-Powered, Progressive Learning, Intelligent Tutoring System, Personalized Education, STEM<br />
Summary:<br />
The paper introduces the Personalized AI-Powered Progressive Learning (PAPPL) platform, an Intelligent Tutoring System (ITS) tailored for engineering education. Traditional evaluation methods in engineering education often neglect individual student needs, hindering personalized learning. PAPPL utilizes advanced AI technology, including the GPT-4o language model, to provide context-sensitive hints and personalized feedback based on student interactions. The system records student attempts, identifies misconceptions, and offers dynamically adaptive assistance. Instructors receive detailed analytics to enhance teaching strategies. PAPPL sets a foundation for scalable Generative ITSs in all education levels, emphasizing personalized progressive learning and the potential of Generative AI in education. <div>
arXiv:2508.14109v1 Announce Type: cross 
Abstract: Engineering education has historically been constrained by rigid, standardized frameworks, often neglecting students' diverse learning needs and interests. While significant advancements have been made in online and personalized education within K-12 and foundational sciences, engineering education at both undergraduate and graduate levels continues to lag in adopting similar innovations. Traditional evaluation methods, such as exams and homework assignments, frequently overlook individual student requirements, impeding personalized educational experiences. To address these limitations, this paper introduces the Personalized AI-Powered Progressive Learning (PAPPL) platform, an advanced Intelligent Tutoring System (ITS) designed specifically for engineering education. It highlights the development of a scalable, data-driven tutoring environment leveraging cutting-edge AI technology to enhance personalized learning across diverse academic disciplines, particularly in STEM fields. PAPPL integrates core ITS components including the expert module, student module, tutor module, and user interface, and utilizes GPT-4o, a sophisticated large language model (LLM), to deliver context-sensitive and pedagogically sound hints based on students' interactions. The system uniquely records student attempts, detects recurring misconceptions, and generates progressively targeted feedback, providing personalized assistance that adapts dynamically to each student's learning profile. Additionally, PAPPL offers instructors detailed analytics, empowering evidence-based adjustments to teaching strategies. This study provides a fundamental framework for the progression of Generative ITSs scalable to all education levels, delivering important perspectives on personalized progressive learning and the wider possibilities of Generative AI in the field of education.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surya: Foundation Model for Heliophysics</title>
<link>https://arxiv.org/abs/2508.14112</link>
<guid>https://arxiv.org/abs/2508.14112</guid>
<content:encoded><![CDATA[
<div> foundation model, heliophysics, solar observations, transformer architecture, forecasting

Summary:
Surya is a 366M parameter foundation model designed for heliophysics, utilizing multi-instrument Solar Dynamics Observatory (SDO) observations. It employs a spatiotemporal transformer architecture with spectral gating and long-short range attention, pretrained on high-resolution solar image forecasting tasks. Zero-shot evaluations show its ability to forecast solar dynamics and flare events. Further fine-tuning with parameter-efficient Low-Rank Adaptation (LoRA) demonstrates strong performance on various tasks including solar wind forecasting, active region segmentation, solar flare forecasting, and EUV spectra. Surya is the first heliophysics model to use time advancement as a pretext task on full-resolution SDO data, suggesting it can learn the underlying physics behind solar evolution. <div>
arXiv:2508.14112v1 Announce Type: cross 
Abstract: Heliophysics is central to understanding and forecasting space weather events and solar activity. Despite decades of high-resolution observations from the Solar Dynamics Observatory (SDO), most models remain task-specific and constrained by scarce labeled data, limiting their capacity to generalize across solar phenomena. We introduce Surya, a 366M parameter foundation model for heliophysics designed to learn general-purpose solar representations from multi-instrument SDO observations, including eight Atmospheric Imaging Assembly (AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Surya employs a spatiotemporal transformer architecture with spectral gating and long--short range attention, pretrained on high-resolution solar image forecasting tasks and further optimized through autoregressive rollout tuning. Zero-shot evaluations demonstrate its ability to forecast solar dynamics and flare events, while downstream fine-tuning with parameter-efficient Low-Rank Adaptation (LoRA) shows strong performance on solar wind forecasting, active region segmentation, solar flare forecasting, and EUV spectra. Surya is the first foundation model in heliophysics that uses time advancement as a pretext task on full-resolution SDO data. Its novel architecture and performance suggest that the model is able to learn the underlying physics behind solar evolution.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Action Recognition for Smart Worker Assistance Using FastPose</title>
<link>https://arxiv.org/abs/2508.14113</link>
<guid>https://arxiv.org/abs/2508.14113</guid>
<content:encoded><![CDATA[
<div> Keywords: smart manufacturing, human activity recognition, federated learning, privacy, industrial settings <br />
<br />
Summary: 
This paper presents a federated learning framework for pose-based human activity recognition (HAR) in smart manufacturing environments. The framework uses a custom skeletal dataset of upper-body gestures and two temporal backbones - LSTM and Transformer encoder. The study compares centralized training with local training, federated learning with weighted federated averaging (FedAvg), and federated ensemble learning (FedEnsemble). The results show that the federated learning approach significantly improves accuracy compared to centralized training, with FedEnsemble delivering the highest gains. Moreover, federated learning also enhances cross-user generalization, exceeding centralized accuracy on an unseen external client by a significant margin. Overall, the study demonstrates that federated learning not only ensures privacy in industrial settings but also enhances the scalability and accuracy of human activity recognition systems. <br /> <div>
arXiv:2508.14113v1 Announce Type: cross 
Abstract: In smart manufacturing environments, accurate and real-time recognition of worker actions is essential for productivity, safety, and human-machine collaboration. While skeleton-based human activity recognition (HAR) offers robustness to lighting, viewpoint, and background variations, most existing approaches rely on centralized datasets, which are impractical in privacy-sensitive industrial scenarios. This paper presents a federated learning (FL) framework for pose-based HAR using a custom skeletal dataset of eight industrially relevant upper-body gestures, captured from five participants and processed using a modified FastPose model. Two temporal backbones, an LSTM and a Transformer encoder, are trained and evaluated under four paradigms: centralized, local (per-client), FL with weighted federated averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the global test set, the FL Transformer improves over centralized training by +12.4 percentage points, with FedEnsemble delivering a +16.3 percentage points gain. On an unseen external client, FL and FedEnsemble exceed centralized accuracy by +52.6 and +58.3 percentage points, respectively. These results demonstrate that FL not only preserves privacy but also substantially enhances cross-user generalization, establishing it as a practical solution for scalable, privacy-aware HAR in heterogeneous industrial settings.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ambiguity Resolution with Human Feedback for Code Writing Tasks</title>
<link>https://arxiv.org/abs/2508.14114</link>
<guid>https://arxiv.org/abs/2508.14114</guid>
<content:encoded><![CDATA[
<div> Keywords: code writing, ambiguity resolution, human feedback, task specifications, assistive systems <br />
<br />
Summary: <br />
This article discusses the challenges of ambiguity in task specifications for code writing and how programmers need to be able to recognize and resolve these ambiguities by asking clarifying questions. The authors present a prototype system, ARHF (Ambiguity Resolution with Human Feedback), which suggests potential areas of ambiguity in a task specification, solicits limited human feedback on desired code behavior, and uses this feedback to generate code that clarifies the ambiguities. The efficacy of the prototype system is evaluated, and the implications of such assistive systems on Computer Science education are discussed. This approach has the potential to improve the accuracy and clarity of code writing tasks and may help students in developing stronger problem-solving and communication skills. The use of human feedback in resolving ambiguities can lead to better understanding of task requirements and ultimately improve the quality of code produced. <br /> <div>
arXiv:2508.14114v1 Announce Type: cross 
Abstract: Specifications for code writing tasks are usually expressed in natural language and may be ambiguous. Programmers must therefore develop the ability to recognize ambiguities in task specifications and resolve them by asking clarifying questions. We present and evaluate a prototype system, based on a novel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1) suggests specific inputs on which a given task specification may be ambiguous, (2) seeks limited human feedback about the code's desired behavior on those inputs, and (3) uses this feedback to generate code that resolves these ambiguities. We evaluate the efficacy of our prototype, and we discuss the implications of such assistive systems on Computer Science education.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings</title>
<link>https://arxiv.org/abs/2508.14115</link>
<guid>https://arxiv.org/abs/2508.14115</guid>
<content:encoded><![CDATA[
<div> speaker embeddings, identity assignment performance, tracking system, short temporal contexts, overlapping speech

Summary:
- Speaker embedding extractors struggle with short temporal contexts and overlapping speech, leading to long-term identity reassignment in tracking systems.
- A Knowledge Distillation (KD) based training approach is proposed for short context speaker embedding extraction from speaker mixtures.
- Spatial information of the speaker of interest is leveraged using beamforming to reduce overlap.
- Feasibility of blockwise identity reassignment over fixed-size blocks is studied for a low-latency speaker embedding based tracking system.
- Results show that distilled models are effective at short-context embedding extraction and more robust to overlap; however, further work is needed to handle simultaneous speech effectively. 

<br /><br />Summary: <div>
arXiv:2508.14115v1 Announce Type: cross 
Abstract: Speaker embeddings are promising identity-related features that can enhance the identity assignment performance of a tracking system by leveraging its spatial predictions, i.e, by performing identity reassignment. Common speaker embedding extractors usually struggle with short temporal contexts and overlapping speech, which imposes long-term identity reassignment to exploit longer temporal contexts. However, this increases the probability of tracking system errors, which in turn impacts negatively on identity reassignment. To address this, we propose a Knowledge Distillation (KD) based training approach for short context speaker embedding extraction from two speaker mixtures. We leverage the spatial information of the speaker of interest using beamforming to reduce overlap. We study the feasibility of performing identity reassignment over blocks of fixed size, i.e., blockwise identity reassignment, to go towards a low-latency speaker embedding based tracking system. Results demonstrate that our distilled models are effective at short-context embedding extraction and more robust to overlap. Although, blockwise reassignment results indicate that further work is needed to handle simultaneous speech more effectively.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Moral Perspectives on AI: Concepts of Trust amongst Africans</title>
<link>https://arxiv.org/abs/2508.14116</link>
<guid>https://arxiv.org/abs/2508.14116</guid>
<content:encoded><![CDATA[
<div> trustworthiness, AI, Africa, trust, Afro-relationalism

Summary: 
This study explores the concept of trust in AI among professionals and academics in Africa. It highlights the influence of factors such as educational background, transnational mobility, and cultural values on individuals' perception of trust in AI. Respondents emphasized communal relations over individual freedoms and applied nuances of Afro-relationalism to constructs of trust in AI. The study suggests the need for more empirical research on trust in AI within African social contexts to understand how trust is practiced and experienced in AI design, use, and governance. The findings underscore the importance of considering diverse cultural perspectives in discussions about trust in AI. <div>
arXiv:2508.14116v1 Announce Type: cross 
Abstract: The trustworthiness of AI is considered essential to the adoption and application of AI systems. However, the meaning of trust varies across industry, research and policy spaces. Studies suggest that professionals who develop and use AI regard an AI system as trustworthy based on their personal experiences and social relations at work. Studies about trust in AI and the constructs that aim to operationalise trust in AI (e.g., consistency, reliability, explainability and accountability). However, the majority of existing studies about trust in AI are situated in Western, Educated, Industrialised, Rich and Democratic (WEIRD) societies. The few studies about trust and AI in Africa do not include the views of people who develop, study or use AI in their work. In this study, we surveyed 157 people with professional and/or educational interests in AI from 25 African countries, to explore how they conceptualised trust in AI. Most respondents had links with workshops about trust and AI in Africa in Namibia and Ghana. Respondents' educational background, transnational mobility, and country of origin influenced their concerns about AI systems. These factors also affected their levels of distrust in certain AI applications and their emphasis on specific principles designed to foster trust. Respondents often expressed that their values are guided by the communities in which they grew up and emphasised communal relations over individual freedoms. They described trust in many ways, including applying nuances of Afro-relationalism to constructs in international discourse, such as reliability and reliance. Thus, our exploratory study motivates more empirical research about the ways trust is practically enacted and experienced in African social realities of AI design, use and governance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Documenting Deployment with Fabric: A Repository of Real-World AI Governance</title>
<link>https://arxiv.org/abs/2508.14119</link>
<guid>https://arxiv.org/abs/2508.14119</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, governance mechanisms, oversight, AI use cases, Fabric repository

Summary: 
The article presents Fabric, a repository of deployed AI use cases that outlines the governance mechanisms in place. Through interviews with practitioners, 20 AI use cases were collected, along with co-designed diagrams of the AI workflow. The repository includes visual diagrams and descriptions of the deployed systems, showcasing oversight mechanisms and guardrails used to safeguard AI use. By analyzing the repository, gaps in governance and common patterns in human oversight of AI systems were identified. Fabric aims to be a tool for researchers to study the effectiveness of AI governance, providing insights into how AI is integrated into society and the measures in place to ensure responsible use. <div>
arXiv:2508.14119v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.14120</link>
<guid>https://arxiv.org/abs/2508.14120</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid-object interactions, generative modeling, reinforcement learning, physical realism, long-horizon generation

Summary: 
SimGenHOI is a novel framework that combines generative modeling and reinforcement learning to generate physically realistic humanoid-object interactions (HOI). The HOI generative model, based on Diffusion Transformers, predicts key actions based on text prompts, object geometry, and humanoid pose, allowing for long-horizon generation. A contact-aware whole-body control policy corrects artifacts like penetration and foot sliding, ensuring physical realism. The mutual fine-tuning strategy between the generative model and control policy improves motion realism and tracking robustness iteratively. Experimental results show that SimGenHOI produces diverse and plausible HOI with higher tracking success rates in simulation, enabling long-horizon manipulation tasks. The code for SimGenHOI will be released upon acceptance on the project page. <br /><br />Summary: <div>
arXiv:2508.14120v1 Announce Type: cross 
Abstract: Generating physically realistic humanoid-object interactions (HOI) is a fundamental challenge in robotics. Existing HOI generation approaches, such as diffusion-based models, often suffer from artifacts such as implausible contacts, penetrations, and unrealistic whole-body actions, which hinder successful execution in physical environments. To address these challenges, we introduce SimGenHOI, a unified framework that combines the strengths of generative modeling and reinforcement learning to produce controllable and physically plausible HOI. Our HOI generative model, based on Diffusion Transformers (DiT), predicts a set of key actions conditioned on text prompts, object geometry, sparse object waypoints, and the initial humanoid pose. These key actions capture essential interaction dynamics and are interpolated into smooth motion trajectories, naturally supporting long-horizon generation. To ensure physical realism, we design a contact-aware whole-body control policy trained with reinforcement learning, which tracks the generated motions while correcting artifacts such as penetration and foot sliding. Furthermore, we introduce a mutual fine-tuning strategy, where the generative model and the control policy iteratively refine each other, improving both motion realism and tracking robustness. Extensive experiments demonstrate that SimGenHOI generates realistic, diverse, and physically plausible humanoid-object interactions, achieving significantly higher tracking success rates in simulation and enabling long-horizon manipulation tasks. Code will be released upon acceptance on our project page: https://xingxingzuo.github.io/simgen_hoi.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Agents for Photonic Integrated Circuit Design Automation</title>
<link>https://arxiv.org/abs/2508.14123</link>
<guid>https://arxiv.org/abs/2508.14123</guid>
<content:encoded><![CDATA[
<div> Keywords: Photonics, Intelligent design, Optimization, Language models, Photonic integrated circuits <br />
Summary: <br />
The article introduces PhIDO, a framework that converts natural-language design requests for photonic integrated circuits (PICs) into layout mask files. Seven large language models were compared for PhIDO using a testbench of 102 design descriptions varying in complexity. For single-device designs, success rates reached up to 91%. For designs with up to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest pass rates at approximately 57%, with Gemini-2.5-pro being the most efficient in terms of output tokens and cost. Future steps towards autonomous PIC development include standardized knowledge representations, expanded datasets, extended verification processes, and integration of robotic automation. <br /> <div>
arXiv:2508.14123v1 Announce Type: cross 
Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a multi-agent framework that converts natural-language photonic integrated circuit (PIC) design requests into layout mask files. We compare 7 reasoning large language models for PhIDO using a testbench of 102 design descriptions that ranged from single devices to 112-component PICs. The success rate for single-device designs was up to 91%. For design queries with less than or equal to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro requiring the fewest output tokens and lowest cost. The next steps toward autonomous PIC development include standardized knowledge representations, expanded datasets, extended verification, and robotic automation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning</title>
<link>https://arxiv.org/abs/2508.14125</link>
<guid>https://arxiv.org/abs/2508.14125</guid>
<content:encoded><![CDATA[
<div> parking, occupancy, university campuses, forecasting models, data integration
Summary:
The study focuses on managing parking challenges in university campuses by proposing a smart framework that integrates data sources and uses forecasting models to allocate parking spots effectively. The framework collects data through location services to analyze parking behavior and vehicle movement patterns over three days. Various forecasting models like Linear Regression, Support Vector Regression, Random Forest Regression, and Long Short-Term Memory are evaluated, with Random Forest Regression performing the best in terms of RMSE and R2 values. However, due to the time-series nature of the task, an LSTM model may perform better with more data and longer timesteps. This framework aims to help students find parking spots conveniently and quickly during class timings without the need for additional sensing tools on campus.<br /><br />Summary: <div>
arXiv:2508.14125v1 Announce Type: cross 
Abstract: As urban populations continue to grow, cities face numerous challenges in managing parking and determining occupancy. This issue is particularly pronounced in university campuses, where students need to find vacant parking spots quickly and conveniently during class timings. The limited availability of parking spaces on campuses underscores the necessity of implementing efficient systems to allocate vacant parking spots effectively. We propose a smart framework that integrates multiple data sources, including street maps, mobility, and meteorological data, through a spatial join operation to capture parking behavior and vehicle movement patterns over the span of 3 consecutive days with an hourly duration between 7AM till 3PM. The system will not require any sensing tools to be installed in the street or in the parking area to provide its services since all the data needed will be collected using location services. The framework will use the expected parking entrance and time to specify a suitable parking area. Several forecasting models, namely, Linear Regression, Support Vector Regression (SVR), Random Forest Regression (RFR), and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was employed using grid search, and model performance is assessed using Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142 and highest R2 of 0.582. However, given the time-series nature of the task, an LSTM model may perform better with additional data and longer timesteps.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCFC: Core &amp; Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection</title>
<link>https://arxiv.org/abs/2508.14128</link>
<guid>https://arxiv.org/abs/2508.14128</guid>
<content:encoded><![CDATA[
arXiv:2508.14128v1 Announce Type: cross 
Abstract: Jailbreak attacks pose a serious challenge to the safe deployment of large language models (LLMs). We introduce CCFC (Core & Core-Full-Core), a dual-track, prompt-level defense framework designed to mitigate LLMs' vulnerabilities from prompt injection and structure-aware jailbreak attacks. CCFC operates by first isolating the semantic core of a user query via few-shot prompting, and then evaluating the query using two complementary tracks: a core-only track to ignore adversarial distractions (e.g., toxic suffixes or prefix injections), and a core-full-core (CFC) track to disrupt the structural patterns exploited by gradient-based or edit-based attacks. The final response is selected based on a safety consistency check across both tracks, ensuring robustness without compromising on response quality. We demonstrate that CCFC cuts attack success rates by 50-75% versus state-of-the-art defenses against strong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on benign queries. Our method consistently outperforms state-of-the-art prompt-level defenses, offering a practical and effective solution for safer LLM deployment.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants</title>
<link>https://arxiv.org/abs/2508.14129</link>
<guid>https://arxiv.org/abs/2508.14129</guid>
<content:encoded><![CDATA[
arXiv:2508.14129v1 Announce Type: cross 
Abstract: Background: Accurate diagnosis of wrist and hand fractures using radiographs is essential in emergency care, but manual interpretation is slow and prone to errors. Transformer-based models show promise in improving medical image analysis, but their application to extremity fractures is limited. This study addresses this gap by applying object detection transformers to wrist and hand X-rays.
  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO, using over 26,000 annotated X-rays from a proprietary clinical dataset. Each image was labeled for fracture presence with bounding boxes. A ResNet-50 classifier was trained on cropped regions to refine abnormality classification. Supervised contrastive learning was used to enhance embedding quality. Performance was evaluated using AP@50, precision, and recall metrics, with additional testing on real-world X-rays.
  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR outperformed it with an AP@50 of 0.615 and faster convergence. The integrated pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on real-world X-rays, demonstrating strong generalization across 13 fracture types. Visual inspection confirmed accurate localization.
  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and clinical relevance in wrist and hand fracture detection, offering reliable localization and differentiation of fracture types. It is scalable, efficient, and suitable for real-time deployment in hospital workflows, improving diagnostic speed and reliability in musculoskeletal radiology.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents</title>
<link>https://arxiv.org/abs/2508.14131</link>
<guid>https://arxiv.org/abs/2508.14131</guid>
<content:encoded><![CDATA[
arXiv:2508.14131v1 Announce Type: cross 
Abstract: We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI</title>
<link>https://arxiv.org/abs/2508.14133</link>
<guid>https://arxiv.org/abs/2508.14133</guid>
<content:encoded><![CDATA[
arXiv:2508.14133v1 Announce Type: cross 
Abstract: Background: The aim of this study was to develop and evaluate a deep learning-based automated segmentation method for hepatic anatomy (i.e., parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the clinical workflow of preoperative planning.
  Methods: Manual segmentation was performed on hepatobiliary phase MRI scans from 90 consecutive patients who underwent liver surgery between January 2020 and October 2023. A deep learning network (nnU-Net v1) was trained on 72 patients with an extra focus on thin structures and topography preservation. Performance was evaluated on an 18-patient test set by comparing automated and manual segmentations using Dice similarity coefficient (DSC). Following clinical integration, 10 segmentations (assessment dataset) were generated using the network and manually refined for clinical use to quantify required adjustments using DSC.
  Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma, 0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was 76.6+/-24.1%, with a median of one false-positive per patient. The assessment dataset showed minor adjustments were required for clinical use of the 3D models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01) and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater variability (DSC 0.80+/-0.27). During prospective clinical use, the model detected three additional tumors initially missed by radiologists.
  Conclusions: The proposed nnU-Net-based segmentation method enables accurate and automated delineation of hepatic anatomy. This enables 3D planning to be applied efficiently as a standard-of-care for every patient undergoing liver surgery.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification</title>
<link>https://arxiv.org/abs/2508.14134</link>
<guid>https://arxiv.org/abs/2508.14134</guid>
<content:encoded><![CDATA[
arXiv:2508.14134v1 Announce Type: cross 
Abstract: An ideal time series classification (TSC) should be able to capture invariant representations, but achieving reliable performance on out-of-distribution (OOD) data remains a core obstacle. This obstacle arises from the way models inherently entangle domain-specific and label-relevant features, resulting in spurious correlations. While feature disentanglement aims to solve this, current methods are largely unguided, lacking the semantic direction required to isolate truly universal features. To address this, we propose an end-to-end Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework to enable guided and reliable feature disentanglement. The core idea is that effective disentanglement requires not only mathematical constraints but also semantic guidance to anchor the separation process. ERIS incorporates three key mechanisms to achieve this goal. Specifically, we first introduce an energy-guided calibration mechanism, which provides crucial semantic guidance for the separation, enabling the model to self-calibrate. Additionally, a weight-level orthogonality strategy enforces structural independence between domain-specific and label-relevant features, thereby mitigating their interference. Moreover, an auxiliary adversarial training mechanism enhances robustness by injecting structured perturbations. Experiments demonstrate that ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy across four benchmarks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers</title>
<link>https://arxiv.org/abs/2508.14138</link>
<guid>https://arxiv.org/abs/2508.14138</guid>
<content:encoded><![CDATA[
arXiv:2508.14138v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Statistical Validation of Innovation Lens</title>
<link>https://arxiv.org/abs/2508.14139</link>
<guid>https://arxiv.org/abs/2508.14139</guid>
<content:encoded><![CDATA[
arXiv:2508.14139v1 Announce Type: cross 
Abstract: Information overload and the rapid pace of scientific advancement make it increasingly difficult to evaluate and allocate resources to new research proposals. Is there a structure to scientific discovery that could inform such decisions? We present statistical evidence for such structure, by training a classifier that successfully predicts high-citation research papers between 2010-2024 in the Computer Science, Physics, and PubMed domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs</title>
<link>https://arxiv.org/abs/2508.14140</link>
<guid>https://arxiv.org/abs/2508.14140</guid>
<content:encoded><![CDATA[
arXiv:2508.14140v1 Announce Type: cross 
Abstract: The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans</title>
<link>https://arxiv.org/abs/2508.14151</link>
<guid>https://arxiv.org/abs/2508.14151</guid>
<content:encoded><![CDATA[
arXiv:2508.14151v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for assessing knee injuries. However, manual interpretation of MRI slices remains time-consuming and prone to inter-observer variability. This study presents a systematic evaluation of various deep learning architectures combined with explainable AI (xAI) techniques for automated region of interest (ROI) detection in knee MRI scans. We investigate both supervised and self-supervised approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and multiple U-Net variants augmented with multi-layer perceptron (MLP) classifiers. To enhance interpretability and clinical relevance, we integrate xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed using AUC for classification and PSNR/SSIM for reconstruction quality, along with qualitative ROI visualizations. Our results demonstrate that ResNet50 consistently excels in classification and ROI identification, outperforming transformer-based models under the constraints of the MRNet dataset. While hybrid U-Net + MLP approaches show potential for leveraging spatial features in reconstruction and interpretability, their classification performance remains lower. Grad-CAM consistently provided the most clinically meaningful explanations across architectures. Overall, CNN-based transfer learning emerges as the most effective approach for this dataset, while future work with larger-scale pretraining may better unlock the potential of transformer models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</title>
<link>https://arxiv.org/abs/2508.14153</link>
<guid>https://arxiv.org/abs/2508.14153</guid>
<content:encoded><![CDATA[
arXiv:2508.14153v1 Announce Type: cross 
Abstract: Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at https://github.com/hustvl/LENS.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RynnEC: Bringing MLLMs into Embodied World</title>
<link>https://arxiv.org/abs/2508.14160</link>
<guid>https://arxiv.org/abs/2508.14160</guid>
<content:encoded><![CDATA[
arXiv:2508.14160v1 Announce Type: cross 
Abstract: We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment</title>
<link>https://arxiv.org/abs/2508.14203</link>
<guid>https://arxiv.org/abs/2508.14203</guid>
<content:encoded><![CDATA[
arXiv:2508.14203v1 Announce Type: cross 
Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer vision, with broad relevance across multiple fields. Recent advances in deep learning have driven significant progress in this area, yet the field remains fragmented across domains and learning paradigms. This survey offers a comprehensive perspective on VAD, systematically organizing the literature across various supervision levels, as well as adaptive learning methods such as online, active, and continual learning. We examine the state of VAD across three major application categories: human-centric, vehicle-centric, and environment-centric scenarios, each with distinct challenges and design considerations. In doing so, we identify fundamental contributions and limitations of current methodologies. By consolidating insights from subfields, we aim to provide the community with a structured foundation for advancing both theoretical understanding and real-world applicability of VAD systems. This survey aims to support researchers by providing a useful reference, while also drawing attention to the broader set of open challenges in anomaly detection, including both fundamental research questions and practical obstacles to real-world deployment.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Insights into Automatic Treatment Planning for Cancer Radiotherapy Using Explainable Artificial Intelligence</title>
<link>https://arxiv.org/abs/2508.14229</link>
<guid>https://arxiv.org/abs/2508.14229</guid>
<content:encoded><![CDATA[
arXiv:2508.14229v1 Announce Type: cross 
Abstract: Objective: This study aims to uncover the opaque decision-making process of an artificial intelligence (AI) agent for automatic treatment planning.
  Approach: We examined a previously developed AI agent based on the Actor-Critic with Experience Replay (ACER) network, which automatically tunes treatment planning parameters (TPPs) for inverse planning in prostate cancer intensity modulated radiotherapy. We selected multiple checkpoint ACER agents from different stages of training and applied an explainable AI (EXAI) method to analyze the attribution from dose-volume histogram (DVH) inputs to TPP-tuning decisions. We then assessed each agent's planning efficacy and efficiency and evaluated their policy and final TPP tuning spaces. Combining these analyses, we systematically examined how ACER agents generated high-quality treatment plans in response to different DVH inputs.
  Results: Attribution analysis revealed that ACER agents progressively learned to identify dose-violation regions from DVH inputs and promote appropriate TPP-tuning actions to mitigate them. Organ-wise similarities between DVH attributions and dose-violation reductions ranged from 0.25 to 0.5 across tested agents. Agents with stronger attribution-violation similarity required fewer tuning steps (~12-13 vs. 22), exhibited a more concentrated TPP-tuning space with lower entropy (~0.3 vs. 0.6), converged on adjusting only a few TPPs, and showed smaller discrepancies between practical and theoretical tuning steps. Putting together, these findings indicate that high-performing ACER agents can effectively identify dose violations from DVH inputs and employ a global tuning strategy to achieve high-quality treatment planning, much like skilled human planners.
  Significance: Better interpretability of the agent's decision-making process may enhance clinician trust and inspire new strategies for automatic treatment planning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incident Analysis for AI Agents</title>
<link>https://arxiv.org/abs/2508.14231</link>
<guid>https://arxiv.org/abs/2508.14231</guid>
<content:encoded><![CDATA[
arXiv:2508.14231v1 Announce Type: cross 
Abstract: As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy</title>
<link>https://arxiv.org/abs/2508.14266</link>
<guid>https://arxiv.org/abs/2508.14266</guid>
<content:encoded><![CDATA[
arXiv:2508.14266v1 Announce Type: cross 
Abstract: The clinical deployment of deep learning models for high-stakes tasks such as diabetic retinopathy (DR) grading requires demonstrable reliability. While models achieve high accuracy, their clinical utility is limited by a lack of robust uncertainty quantification. Conformal prediction (CP) offers a distribution-free framework to generate prediction sets with statistical guarantees of coverage. However, the interaction between standard training practices like data augmentation and the validity of these guarantees is not well understood. In this study, we systematically investigate how different data augmentation strategies affect the performance of conformal predictors for DR grading. Using the DDR dataset, we evaluate two backbone architectures -- ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under five augmentation regimes: no augmentation, standard geometric transforms, CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal metrics, including empirical coverage, average prediction set size, and correct efficiency. Our results demonstrate that sample-mixing strategies like Mixup and CutMix not only improve predictive accuracy but also yield more reliable and efficient uncertainty estimates. Conversely, methods like CLAHE can negatively impact model certainty. These findings highlight the need to co-design augmentation strategies with downstream uncertainty quantification in mind to build genuinely trustworthy AI systems for medical imaging.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling concept semantics via multilingual averaging in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.14275</link>
<guid>https://arxiv.org/abs/2508.14275</guid>
<content:encoded><![CDATA[
arXiv:2508.14275v1 Announce Type: cross 
Abstract: Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning</title>
<link>https://arxiv.org/abs/2508.14276</link>
<guid>https://arxiv.org/abs/2508.14276</guid>
<content:encoded><![CDATA[
arXiv:2508.14276v1 Announce Type: cross 
Abstract: Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: https://github.com/djafar1/tooth-diffusion.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.14285</link>
<guid>https://arxiv.org/abs/2508.14285</guid>
<content:encoded><![CDATA[
arXiv:2508.14285v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA</title>
<link>https://arxiv.org/abs/2508.14286</link>
<guid>https://arxiv.org/abs/2508.14286</guid>
<content:encoded><![CDATA[
arXiv:2508.14286v1 Announce Type: cross 
Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at https://github.com/anushka-kore/OccluNet.git
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixels to Play: A Foundation Model for 3D Gameplay</title>
<link>https://arxiv.org/abs/2508.14295</link>
<guid>https://arxiv.org/abs/2508.14295</guid>
<content:encoded><![CDATA[
arXiv:2508.14295v1 Announce Type: cross 
Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play a wide range of 3D video games with recognizable human-like behavior. Motivated by emerging consumer and developer use cases - AI teammates, controllable NPCs, personalized live-streamers, assistive testers - we argue that an agent must rely on the same pixel stream available to players and generalize to new titles with minimal game-specific engineering. P2P0.1 is trained end-to-end with behavior cloning: labeled demonstrations collected from instrumented human game-play are complemented by unlabeled public videos, to which we impute actions via an inverse-dynamics model. A decoder-only transformer with auto-regressive action output handles the large action space while remaining latency-friendly on a single consumer GPU. We report qualitative results showing competent play across simple Roblox and classic MS-DOS titles, ablations on unlabeled data, and outline the scaling and evaluation steps required to reach expert-level, text-conditioned control.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</title>
<link>https://arxiv.org/abs/2508.14302</link>
<guid>https://arxiv.org/abs/2508.14302</guid>
<content:encoded><![CDATA[
arXiv:2508.14302v1 Announce Type: cross 
Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Time-Varying Convexifications of Multiple Fairness Measures</title>
<link>https://arxiv.org/abs/2508.14311</link>
<guid>https://arxiv.org/abs/2508.14311</guid>
<content:encoded><![CDATA[
arXiv:2508.14311v1 Announce Type: cross 
Abstract: There is an increasing appreciation that one may need to consider multiple measures of fairness, e.g., considering multiple group and individual fairness notions. The relative weights of the fairness regularisers are a priori unknown, may be time varying, and need to be learned on the fly. We consider the learning of time-varying convexifications of multiple fairness measures with limited graph-structured feedback.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</title>
<link>https://arxiv.org/abs/2508.14313</link>
<guid>https://arxiv.org/abs/2508.14313</guid>
<content:encoded><![CDATA[
arXiv:2508.14313v1 Announce Type: cross 
Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title>
<link>https://arxiv.org/abs/2508.14314</link>
<guid>https://arxiv.org/abs/2508.14314</guid>
<content:encoded><![CDATA[
arXiv:2508.14314v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power Stabilization for AI Training Datacenters</title>
<link>https://arxiv.org/abs/2508.14318</link>
<guid>https://arxiv.org/abs/2508.14318</guid>
<content:encoded><![CDATA[
arXiv:2508.14318v1 Announce Type: cross 
Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens of thousands of GPUs present unique power management challenges. These arise due to the high variability in power consumption during the training. Given the synchronous nature of these jobs, during every iteration there is a computation-heavy phase, where each GPU works on the local data, and a communication-heavy phase where all the GPUs synchronize on the data. Because compute-heavy phases require much more power than communication phases, large power swings occur. The amplitude of these power swings is ever increasing with the increase in the size of training jobs. An even bigger challenge arises from the frequency spectrum of these power swings which, if harmonized with critical frequencies of utilities, can cause physical damage to the power grid infrastructure. Therefore, to continue scaling AI training workloads safely, we need to stabilize the power of such workloads. This paper introduces the challenge with production data and explores innovative solutions across the stack: software, GPU hardware, and datacenter infrastructure. We present the pros and cons of each of these approaches and finally present a multi-pronged approach to solving the challenge. The proposed solutions are rigorously tested using a combination of real hardware and Microsoft's in-house cloud power simulator, providing critical insights into the efficacy of these interventions under real-world conditions.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations</title>
<link>https://arxiv.org/abs/2508.14340</link>
<guid>https://arxiv.org/abs/2508.14340</guid>
<content:encoded><![CDATA[
arXiv:2508.14340v1 Announce Type: cross 
Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to train agents to make effective decisions in the cybersecurity domain. However, existing ACO applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. While teacher-guided techniques have demonstrated promise in other domains, they have not yet been applied to ACO. In this study, we implement four distinct teacher-guided techniques in the simulated CybORG environment and conduct a comparative evaluation. Our results demonstrate that teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation</title>
<link>https://arxiv.org/abs/2508.14342</link>
<guid>https://arxiv.org/abs/2508.14342</guid>
<content:encoded><![CDATA[
arXiv:2508.14342v1 Announce Type: cross 
Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable step in reducing poaching is to forecast poacher behavior, which can inform patrol planning and other conservation interventions. Existing poaching prediction methods based on linear models or decision trees lack the expressivity to capture complex, nonlinear spatiotemporal patterns. Recent advances in generative modeling, particularly flow matching, offer a more flexible alternative. However, training such models on real-world poaching data faces two central obstacles: imperfect detection of poaching events and limited data. To address imperfect detection, we integrate flow matching with an occupancy-based detection model and train the flow in latent space to infer the underlying occupancy state. To mitigate data scarcity, we adopt a composite flow initialized from a linear-model prediction rather than random noise which is the standard in diffusion models, injecting prior knowledge and improving generalization. Evaluations on datasets from two national parks in Uganda show consistent gains in predictive accuracy.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates</title>
<link>https://arxiv.org/abs/2508.14343</link>
<guid>https://arxiv.org/abs/2508.14343</guid>
<content:encoded><![CDATA[
arXiv:2508.14343v1 Announce Type: cross 
Abstract: In one-stage multi-object detection tasks, various intersection over union (IoU)-based solutions aim at smooth and stable convergence near the targets during training. However, IoU-based losses fail to correctly update the gradient of small objects due to an extremely flat gradient. During the update of multiple objects, the learning of small objects' gradients suffers more because of insufficient gradient updates. Therefore, we propose an inter-class relational loss to efficiently update the gradient of small objects while not sacrificing the learning efficiency of other objects based on the simple fact that an object has a spatial relationship to another object (e.g., a car plate is attached to a car in a similar position). When the predicted car plate's bounding box is not within its car, a loss punishment is added to guide the learning, which is inversely proportional to the overlapped area of the car's and predicted car plate's bounding box. By leveraging the spatial relationship at the inter-class level, the loss guides small object predictions using larger objects and enhances latent information in deeper feature maps. In this paper, we present twofold contributions using license plate detection as a case study: (1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse real-world scenarios with high-quality annotations; and (2) a novel inter-class relational loss function designed to promote effective detection performance. We highlight the proposed ICR loss penalty can be easily added to existing IoU-based losses and enhance the performance. These contributions improve the standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6% in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without any additional hyperparameter tuning. Code and dataset will be available soon.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organ-Agents: Virtual Human Physiology Simulator via LLMs</title>
<link>https://arxiv.org/abs/2508.14357</link>
<guid>https://arxiv.org/abs/2508.14357</guid>
<content:encoded><![CDATA[
arXiv:2508.14357v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled new possibilities in simulating complex physiological systems. We introduce Organ-Agents, a multi-agent framework that simulates human physiology via LLM-driven agents. Each Simulator models a specific system (e.g., cardiovascular, renal, immune). Training consists of supervised fine-tuning on system-specific time-series data, followed by reinforcement-guided coordination using dynamic reference selection and error correction. We curated data from 7,134 sepsis patients and 7,895 controls, generating high-resolution trajectories across 9 systems and 125 variables. Organ-Agents achieved high simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and robustness across SOFA-based severity strata. External validation on 22,689 ICU patients from two hospitals showed moderate degradation under distribution shifts with stable simulation. Organ-Agents faithfully reproduces critical multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with coherent timing and phase progression. Evaluation by 15 critical care physicians confirmed realism and physiological plausibility (mean Likert ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations under alternative sepsis treatment strategies, generating trajectories and APACHE II scores aligned with matched real-world patients. In downstream early warning tasks, classifiers trained on synthetic data showed minimal AUROC drops (<0.04), indicating preserved decision-relevant patterns. These results position Organ-Agents as a credible, interpretable, and generalizable digital twin for precision diagnosis, treatment simulation, and hypothesis testing in critical care.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</title>
<link>https://arxiv.org/abs/2508.14358</link>
<guid>https://arxiv.org/abs/2508.14358</guid>
<content:encoded><![CDATA[
arXiv:2508.14358v1 Announce Type: cross 
Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size of objects within given categories. Existing approaches for this task rely solely on 6D poses as supervisory signals without explicitly capturing the intrinsic continuity of poses, leading to inconsistencies in predictions and reduced generalization to unseen poses. To address this limitation, we propose HRC-Pose, a novel depth-only framework for category-level object pose estimation, which leverages contrastive learning to learn point cloud representations that preserve the continuity of 6D poses. HRC-Pose decouples object pose into rotation and translation components, which are separately encoded and leveraged throughout the network. Specifically, we introduce a contrastive learning strategy for multi-task, multi-category scenarios based on our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds from multiple categories by considering rotational and translational differences as well as categorical information. We further design pose estimation modules that separately process the learned rotation-aware and translation-aware embeddings. Our experiments demonstrate that HRC-Pose successfully learns continuous feature spaces. Results on REAL275 and CAMERA25 benchmarks show that our method consistently outperforms existing depth-only state-of-the-art methods and runs in real-time, demonstrating its effectiveness and potential for real-world applications. Our code is at https://github.com/zhujunli1993/HRC-Pose.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing-In-Memory Dataflow for Minimal Buffer Traffic</title>
<link>https://arxiv.org/abs/2508.14375</link>
<guid>https://arxiv.org/abs/2508.14375</guid>
<content:encoded><![CDATA[
arXiv:2508.14375v1 Announce Type: cross 
Abstract: Computing-In-Memory (CIM) offers a potential solution to the memory wall issue and can achieve high energy efficiency by minimizing data movement, making it a promising architecture for edge AI devices. Lightweight models like MobileNet and EfficientNet, which utilize depthwise convolution for feature extraction, have been developed for these devices. However, CIM macros often face challenges in accelerating depthwise convolution, including underutilization of CIM memory and heavy buffer traffic. The latter, in particular, has been overlooked despite its significant impact on latency and energy consumption. To address this, we introduce a novel CIM dataflow that significantly reduces buffer traffic by maximizing data reuse and improving memory utilization during depthwise convolution. The proposed dataflow is grounded in solid theoretical principles, fully demonstrated in this paper. When applied to MobileNet and EfficientNet models, our dataflow reduces buffer traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline (conventional weight-stationary dataflow).
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</title>
<link>https://arxiv.org/abs/2508.14377</link>
<guid>https://arxiv.org/abs/2508.14377</guid>
<content:encoded><![CDATA[
arXiv:2508.14377v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization</title>
<link>https://arxiv.org/abs/2508.14385</link>
<guid>https://arxiv.org/abs/2508.14385</guid>
<content:encoded><![CDATA[
arXiv:2508.14385v1 Announce Type: cross 
Abstract: Effective responses to cyberattacks require fast decisions, even when information about the attack is incomplete or inaccurate. However, most decision-support frameworks for incident response rely on a detailed system model that describes the incident, which restricts their practical utility. In this paper, we address this limitation and present an online method for incident response planning under model misspecification, which we call MOBAL: Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, which facilitates model adaptation as the incident unfolds. To determine effective responses online, we quantize the conjectured model into a finite Markov model, which enables efficient response planning through dynamic programming. We prove that Bayesian learning is asymptotically consistent with respect to the information feedback. Additionally, we establish bounds on misspecification and quantization errors. Experiments on the CAGE-2 benchmark show that MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credence Calibration Game? Calibrating Large Language Models through Structured Play</title>
<link>https://arxiv.org/abs/2508.14390</link>
<guid>https://arxiv.org/abs/2508.14390</guid>
<content:encoded><![CDATA[
arXiv:2508.14390v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open.science/r/LLM-Calibration/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title>
<link>https://arxiv.org/abs/2508.14391</link>
<guid>https://arxiv.org/abs/2508.14391</guid>
<content:encoded><![CDATA[
arXiv:2508.14391v1 Announce Type: cross 
Abstract: Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\% while achieving a 17.2\% improvement in average F1 score over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding</title>
<link>https://arxiv.org/abs/2508.14395</link>
<guid>https://arxiv.org/abs/2508.14395</guid>
<content:encoded><![CDATA[
arXiv:2508.14395v1 Announce Type: cross 
Abstract: Users often take notes for instructional videos to access key knowledge later without revisiting long videos. Automated note generation tools enable users to obtain informative notes efficiently. However, notes generated by existing research or off-the-shelf tools fail to preserve the information conveyed in the original videos comprehensively, nor can they satisfy users' expectations for diverse presentation formats and interactive features when using notes digitally. In this work, we present NoteIt, a system, which automatically converts instructional videos to interactable notes using a novel pipeline that faithfully extracts hierarchical structure and multimodal key information from videos. With NoteIt's interface, users can interact with the system to further customize the content and presentation formats of the notes according to their preferences. We conducted both a technical evaluation and a comparison user study (N=36). The solid performance in objective metrics and the positive user feedback demonstrated the effectiveness of the pipeline and the overall usability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</title>
<link>https://arxiv.org/abs/2508.14408</link>
<guid>https://arxiv.org/abs/2508.14408</guid>
<content:encoded><![CDATA[
arXiv:2508.14408v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Reading-Induced Confusion Using EEG and Eye Tracking</title>
<link>https://arxiv.org/abs/2508.14442</link>
<guid>https://arxiv.org/abs/2508.14442</guid>
<content:encoded><![CDATA[
arXiv:2508.14442v1 Announce Type: cross 
Abstract: Humans regularly navigate an overwhelming amount of information via text media, whether reading articles, browsing social media, or interacting with chatbots. Confusion naturally arises when new information conflicts with or exceeds a reader's comprehension or prior knowledge, posing a challenge for learning. In this study, we present a multimodal investigation of reading-induced confusion using EEG and eye tracking. We collected neural and gaze data from 11 adult participants as they read short paragraphs sampled from diverse, real-world sources. By isolating the N400 event-related potential (ERP), a well-established neural marker of semantic incongruence, and integrating behavioral markers from eye tracking, we provide a detailed analysis of the neural and behavioral correlates of confusion during naturalistic reading. Using machine learning, we show that multimodal (EEG + eye tracking) models improve classification accuracy by 4-22% over unimodal baselines, reaching an average weighted participant accuracy of 77.3% and a best accuracy of 89.6%. Our results highlight the dominance of the brain's temporal regions in these neural signatures of confusion, suggesting avenues for wearable, low-electrode brain-computer interfaces (BCI) for real-time monitoring. These findings lay the foundation for developing adaptive systems that dynamically detect and respond to user confusion, with potential applications in personalized learning, human-computer interaction, and accessibility.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
arXiv:2508.14444v1 Announce Type: cross 
Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In2x at WMT25 Translation Task</title>
<link>https://arxiv.org/abs/2508.14472</link>
<guid>https://arxiv.org/abs/2508.14472</guid>
<content:encoded><![CDATA[
arXiv:2508.14472v1 Announce Type: cross 
Abstract: This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synaptic bundle theory for spike-driven sensor-motor system: More than eight independent synaptic bundles collapse reward-STDP learning</title>
<link>https://arxiv.org/abs/2508.14492</link>
<guid>https://arxiv.org/abs/2508.14492</guid>
<content:encoded><![CDATA[
arXiv:2508.14492v1 Announce Type: cross 
Abstract: Neuronal spikes directly drive muscles and endow animals with agile movements, but applying the spike-based control signals to actuators in artificial sensor-motor systems inevitably causes a collapse of learning. We developed a system that can vary \emph{the number of independent synaptic bundles} in sensor-to-motor connections. This paper demonstrates the following four findings: (i) Learning collapses once the number of motor neurons or the number of independent synaptic bundles exceeds a critical limit. (ii) The probability of learning failure is increased by a smaller number of motor neurons, while (iii) if learning succeeds, a smaller number of motor neurons leads to faster learning. (iv) The number of weight updates that move in the opposite direction of the optimal weight can quantitatively explain these results. The functions of spikes remain largely unknown. Identifying the parameter range in which learning systems using spikes can be constructed will make it possible to study the functions of spikes that were previously inaccessible due to the difficulty of learning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes</title>
<link>https://arxiv.org/abs/2508.14499</link>
<guid>https://arxiv.org/abs/2508.14499</guid>
<content:encoded><![CDATA[
arXiv:2508.14499v1 Announce Type: cross 
Abstract: Shapley values are widely recognized as a principled method for attributing importance to input features in machine learning. However, the exact computation of Shapley values scales exponentially with the number of features, severely limiting the practical application of this powerful approach. The challenge is further compounded when the predictive model is probabilistic - as in Gaussian processes (GPs) - where the outputs are random variables rather than point estimates, necessitating additional computational effort in modeling higher-order moments. In this work, we demonstrate that for an important class of GPs known as FANOVA GP, which explicitly models all main effects and interactions, *exact* Shapley attributions for both local and global explanations can be computed in *quadratic time*. For local, instance-wise explanations, we define a stochastic cooperative game over function components and compute the exact stochastic Shapley value in quadratic time only, capturing both the expected contribution and uncertainty. For global explanations, we introduce a deterministic, variance-based value function and compute exact Shapley values that quantify each feature's contribution to the model's overall sensitivity. Our methods leverage a closed-form (stochastic) M\"{o}bius representation of the FANOVA decomposition and introduce recursive algorithms, inspired by Newton's identities, to efficiently compute the mean and variance of Shapley values. Our work enhances the utility of explainable AI, as demonstrated by empirical studies, by providing more scalable, axiomatically sound, and uncertainty-aware explanations for predictions generated by structured probabilistic models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</title>
<link>https://arxiv.org/abs/2508.14504</link>
<guid>https://arxiv.org/abs/2508.14504</guid>
<content:encoded><![CDATA[
arXiv:2508.14504v1 Announce Type: cross 
Abstract: The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MISS: Multi-Modal Tree Indexing and Searching with Lifelong Sequential Behavior for Retrieval Recommendation</title>
<link>https://arxiv.org/abs/2508.14515</link>
<guid>https://arxiv.org/abs/2508.14515</guid>
<content:encoded><![CDATA[
arXiv:2508.14515v1 Announce Type: cross 
Abstract: Large-scale industrial recommendation systems typically employ a two-stage paradigm of retrieval and ranking to handle huge amounts of information. Recent research focuses on improving the performance of retrieval model. A promising way is to introduce extensive information about users and items. On one hand, lifelong sequential behavior is valuable. Existing lifelong behavior modeling methods in ranking stage focus on the interaction of lifelong behavior and candidate items from retrieval stage. In retrieval stage, it is difficult to utilize lifelong behavior because of a large corpus of candidate items. On the other hand, existing retrieval methods mostly relay on interaction information, potentially disregarding valuable multi-modal information. To solve these problems, we represent the pioneering exploration of leveraging multi-modal information and lifelong sequence model within the advanced tree-based retrieval model. We propose Multi-modal Indexing and Searching with lifelong Sequence (MISS), which contains a multi-modal index tree and a multi-modal lifelong sequence modeling module. Specifically, for better index structure, we propose multi-modal index tree, which is built using the multi-modal embedding to precisely represent item similarity. To precisely capture diverse user interests in user lifelong sequence, we propose collaborative general search unit (Co-GSU) and multi-modal general search unit (MM-GSU) for multi-perspective interests searching.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiFusion-GAN: Efficient Fusion Generative Adversarial Network for Speech Enhancement</title>
<link>https://arxiv.org/abs/2508.14525</link>
<guid>https://arxiv.org/abs/2508.14525</guid>
<content:encoded><![CDATA[
arXiv:2508.14525v1 Announce Type: cross 
Abstract: We introduce EffiFusion-GAN (Efficient Fusion Generative Adversarial Network), a lightweight yet powerful model for speech enhancement. The model integrates depthwise separable convolutions within a multi-scale block to capture diverse acoustic features efficiently. An enhanced attention mechanism with dual normalization and residual refinement further improves training stability and convergence. Additionally, dynamic pruning is applied to reduce model size while maintaining performance, making the framework suitable for resource-constrained environments. Experimental evaluation on the public VoiceBank+DEMAND dataset shows that EffiFusion-GAN achieves a PESQ score of 3.45, outperforming existing models under the same parameter settings.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks</title>
<link>https://arxiv.org/abs/2508.14536</link>
<guid>https://arxiv.org/abs/2508.14536</guid>
<content:encoded><![CDATA[
arXiv:2508.14536v1 Announce Type: cross 
Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the ability of its underlying neural network to accurately approximate the action-value function. Standard function approximators, such as multi-layer perceptrons, may struggle to efficiently represent the complex value landscapes inherent in many reinforcement learning problems. This paper introduces a novel architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev polynomial basis into the DQN framework to create a more effective feature representation. By leveraging the powerful function approximation properties of Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more efficiently and achieve higher performance. We evaluate our proposed model on the CartPole-v1 benchmark and compare it against a standard DQN with a comparable number of parameters. Our results demonstrate that the Ch-DQN with a moderate polynomial degree (N=4) achieves significantly better asymptotic performance, outperforming the baseline by approximately 39\%. However, we also find that the choice of polynomial degree is a critical hyperparameter, as a high degree (N=8) can be detrimental to learning. This work validates the potential of using orthogonal polynomial bases in deep reinforcement learning while also highlighting the trade-offs involved in model complexity.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-hoc LLM-Supported Debugging of Distributed Processes</title>
<link>https://arxiv.org/abs/2508.14540</link>
<guid>https://arxiv.org/abs/2508.14540</guid>
<content:encoded><![CDATA[
arXiv:2508.14540v1 Announce Type: cross 
Abstract: In this paper, we address the problem of manual debugging, which nowadays remains resource-intensive and in some parts archaic. This problem is especially evident in increasingly complex and distributed software systems. Therefore, our objective of this work is to introduce an approach that can possibly be applied to any system, at both the macro- and micro-level, to ease this debugging process. This approach utilizes a system's process data, in conjunction with generative AI, to generate natural-language explanations. These explanations are generated from the actual process data, interface information, and documentation to guide the developers more efficiently to understand the behavior and possible errors of a process and its sub-processes. Here, we present a demonstrator that employs this approach on a component-based Java system. However, our approach is language-agnostic. Ideally, the generated explanations will provide a good understanding of the process, even if developers are not familiar with all the details of the considered system. Our demonstrator is provided as an open-source web application that is freely accessible to all users.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</title>
<link>https://arxiv.org/abs/2508.14544</link>
<guid>https://arxiv.org/abs/2508.14544</guid>
<content:encoded><![CDATA[
arXiv:2508.14544v1 Announce Type: cross 
Abstract: We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total latency. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths, assuming the prediction provides an interval classification (min-max range) for each request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose $\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that $\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical scenarios. Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the prediction interval--an advantageous design choice since upper bounds on output length are typically more challenging to predict accurately.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</title>
<link>https://arxiv.org/abs/2508.14553</link>
<guid>https://arxiv.org/abs/2508.14553</guid>
<content:encoded><![CDATA[
arXiv:2508.14553v1 Announce Type: cross 
Abstract: Over time, software systems have reached a level of complexity that makes it difficult for their developers and users to explain particular decisions made by them. In this paper, we focus on the explainability of component-based systems for Question Answering (QA). These components often conduct processes driven by AI methods, in which behavior and decisions cannot be clearly explained or justified, s.t., even for QA experts interpreting the executed process and its results is hard. To address this challenge, we present an approach that considers the components' input and output data flows as a source for representing the behavior and provide explanations for the components, enabling users to comprehend what happened. In the QA framework used here, the data flows of the components are represented as SPARQL queries (inputs) and RDF triples (outputs). Hence, we are also providing valuable insights on verbalization regarding these data types. In our experiments, the approach generates explanations while following template-based settings (baseline) or via the use of Large Language Models (LLMs) with different configurations (automatic generation). Our evaluation shows that the explanations generated via LLMs achieve high quality and mostly outperform template-based approaches according to the users' ratings. Therefore, it enables us to automatically explain the behavior and decisions of QA components to humans while using RDF and SPARQL as a context for explanations.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions</title>
<link>https://arxiv.org/abs/2508.14556</link>
<guid>https://arxiv.org/abs/2508.14556</guid>
<content:encoded><![CDATA[
arXiv:2508.14556v1 Announce Type: cross 
Abstract: We introduce a new music source separation model tailored for accurate vocal isolation. Unlike Transformer-based approaches, which often fail to capture intermittently occurring vocals, our model leverages Mamba2, a recent state space model, to better capture long-range temporal dependencies. To handle long input sequences efficiently, we combine a band-splitting strategy with a dual-path architecture. Experiments show that our approach outperforms recent state-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to date-and delivering substantial gains in uSDR. Moreover, the model exhibits stable and consistent performance across varying input lengths and vocal occurrence patterns. These results demonstrate the effectiveness of Mamba-based models for high-resolution audio processing and open up new directions for broader applications in audio research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems</title>
<link>https://arxiv.org/abs/2508.14582</link>
<guid>https://arxiv.org/abs/2508.14582</guid>
<content:encoded><![CDATA[
arXiv:2508.14582v1 Announce Type: cross 
Abstract: Heterogeneous accelerator-centric compute clusters are emerging as efficient solutions for diverse AI workloads. However, current integration strategies often compromise data movement efficiency and encounter compatibility issues in hardware and software. This prevents a unified approach that balances performance and ease of use. To this end, we present SNAX, an open-source integrated HW-SW framework enabling efficient multi-accelerator platforms through a novel hybrid-coupling scheme, consisting of loosely coupled asynchronous control and tightly coupled data access. SNAX brings reusable hardware modules designed to enhance compute accelerator utilization, and its customizable MLIR-based compiler to automate key system management tasks, jointly enabling rapid development and deployment of customized multi-accelerator compute clusters. Through extensive experimentation, we demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC. Accelerators can easily be integrated and programmed to achieve > 10x improvement in neural network performance compared to other accelerator systems while maintaining accelerator utilization of > 90% in full system operation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling</title>
<link>https://arxiv.org/abs/2508.14604</link>
<guid>https://arxiv.org/abs/2508.14604</guid>
<content:encoded><![CDATA[
arXiv:2508.14604v1 Announce Type: cross 
Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at https://github.com/wangzy01/UST-SSM.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of the Scale Invariant Signal to Distortion Ratio in Speech Separation with Noisy References</title>
<link>https://arxiv.org/abs/2508.14623</link>
<guid>https://arxiv.org/abs/2508.14623</guid>
<content:encoded><![CDATA[
arXiv:2508.14623v1 Announce Type: cross 
Abstract: This paper examines the implications of using the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) as both evaluation and training objective in supervised speech separation, when the training references contain noise, as is the case with the de facto benchmark WSJ0-2Mix. A derivation of the SI-SDR with noisy references reveals that noise limits the achievable SI-SDR, or leads to undesired noise in the separated outputs. To address this, a method is proposed to enhance references and augment the mixtures with WHAM!, aiming to train models that avoid learning noisy references. Two models trained on these enhanced datasets are evaluated with the non-intrusive NISQA.v2 metric. Results show reduced noise in separated speech but suggest that processing references may introduce artefacts, limiting overall quality gains. Negative correlation is found between SI-SDR and perceived noisiness across models on the WSJ0-2Mix and Libri2Mix test sets, underlining the conclusion from the derivation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination</title>
<link>https://arxiv.org/abs/2508.14635</link>
<guid>https://arxiv.org/abs/2508.14635</guid>
<content:encoded><![CDATA[
arXiv:2508.14635v1 Announce Type: cross 
Abstract: The ability to coordinate actions across multiple agents is critical for solving complex, real-world problems. Large Language Models (LLMs) have shown strong capabilities in communication, planning, and reasoning, raising the question of whether they can also support effective collaboration in multi-agent settings. In this work, we investigate the use of LLM agents to solve a structured victim rescue task that requires division of labor, prioritization, and cooperative planning. Agents operate in a fully known graph-based environment and must allocate resources to victims with varying needs and urgency levels. We systematically evaluate their performance using a suite of coordination-sensitive metrics, including task success rate, redundant actions, room conflicts, and urgency-weighted efficiency. This study offers new insights into the strengths and failure modes of LLMs in physically grounded multi-agent collaboration tasks, contributing to future benchmarks and architectural improvements.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service</title>
<link>https://arxiv.org/abs/2508.14646</link>
<guid>https://arxiv.org/abs/2508.14646</guid>
<content:encoded><![CDATA[
arXiv:2508.14646v1 Announce Type: cross 
Abstract: Local life service is a vital scenario in Kuaishou App, where video recommendation is intrinsically linked with store's location information. Thus, recommendation in our scenario is challenging because we should take into account user's interest and real-time location at the same time. In the face of such complex scenarios, end-to-end generative recommendation has emerged as a new paradigm, such as OneRec in the short video scenario, OneSug in the search scenario, and EGA in the advertising scenario. However, in local life service, an end-to-end generative recommendation model has not yet been developed as there are some key challenges to be solved. The first challenge is how to make full use of geographic information. The second challenge is how to balance multiple objectives, including user interests, the distance between user and stores, and some other business objectives. To address the challenges, we propose OneLoc. Specifically, we leverage geographic information from different perspectives: (1) geo-aware semantic ID incorporates both video and geographic information for tokenization, (2) geo-aware self-attention in the encoder leverages both video location similarity and user's real-time location, and (3) neighbor-aware prompt captures rich context information surrounding users for generation. To balance multiple objectives, we use reinforcement learning and propose two reward functions, i.e., geographic reward and GMV reward. With the above design, OneLoc achieves outstanding offline and online performance. In fact, OneLoc has been deployed in local life service of Kuaishou App. It serves 400 million active users daily, achieving 21.016% and 17.891% improvements in terms of gross merchandise value (GMV) and orders numbers.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELATE: Evolutionary Language model for Automated Time-series Engineering</title>
<link>https://arxiv.org/abs/2508.14667</link>
<guid>https://arxiv.org/abs/2508.14667</guid>
<content:encoded><![CDATA[
arXiv:2508.14667v1 Announce Type: cross 
Abstract: Time-series prediction involves forecasting future values using machine learning models. Feature engineering, whereby existing features are transformed to make new ones, is critical for enhancing model performance, but is often manual and time-intensive. Existing automation attempts rely on exhaustive enumeration, which can be computationally costly and lacks domain-specific insights. We introduce ELATE (Evolutionary Language model for Automated Time-series Engineering), which leverages a language model within an evolutionary framework to automate feature engineering for time-series data. ELATE employs time-series statistical measures and feature importance metrics to guide and prune features, while the language model proposes new, contextually relevant feature transformations. Our experiments demonstrate that ELATE improves forecasting accuracy by an average of 8.4% across various domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal</title>
<link>https://arxiv.org/abs/2508.14689</link>
<guid>https://arxiv.org/abs/2508.14689</guid>
<content:encoded><![CDATA[
arXiv:2508.14689v1 Announce Type: cross 
Abstract: Pre-trained foundation models have demonstrated remarkable success in vision and language, yet their potential for general machine signal modeling-covering acoustic, vibration, and other industrial sensor data-remains under-explored. Existing approach using sub-band-based encoders has achieved competitive results but are limited by fixed input lengths, and the absence of explicit frequency positional encoding. In this work, we propose a novel foundation model that integrates an advanced band-split architecture with relative frequency positional embeddings, enabling precise spectral localization across arbitrary sampling configurations. The model supports inputs of arbitrary length without padding or segmentation, producing a concise embedding that retains both temporal and spectral fidelity. We evaluate our method on SIREN (https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmark for machine signal encoding that unifies multiple datasets, including all DCASE task 2 challenges (2020-2025) and widely-used industrial signal corpora. Experimental results demonstrate consistent state-of-the-art performance in anomaly detection and fault identification, confirming the effectiveness and generalization capability of the proposed model. We open-sourced ECHO on https://github.com/yucongzh/ECHO.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection</title>
<link>https://arxiv.org/abs/2508.14699</link>
<guid>https://arxiv.org/abs/2508.14699</guid>
<content:encoded><![CDATA[
arXiv:2508.14699v1 Announce Type: cross 
Abstract: Credit card fraud detection (CCFD) is a critical application of Machine Learning (ML) in the financial sector, where accurately identifying fraudulent transactions is essential for mitigating financial losses. ML models have demonstrated their effectiveness in fraud detection task, in particular with the tabular dataset. While adversarial attacks have been extensively studied in computer vision and deep learning, their impacts on the ML models, particularly those trained on CCFD tabular datasets, remains largely unexplored. These latent vulnerabilities pose significant threats to the security and stability of the financial industry, especially in high-value transactions where losses could be substantial. To address this gap, in this paper, we present a holistic framework that investigate the robustness of CCFD ML model against adversarial perturbations under different circumstances. Specifically, the gradient-based attack methods are incorporated into the tabular credit card transaction data in both black- and white-box adversarial attacks settings. Our findings confirm that tabular data is also susceptible to subtle perturbations, highlighting the need for heightened awareness among financial technology practitioners regarding ML model security and trustworthiness. Furthermore, the experiments by transferring adversarial samples from gradient-based attack method to non-gradient-based models also verify our findings. Our results demonstrate that such attacks remain effective, emphasizing the necessity of developing robust defenses for CCFD algorithms.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation</title>
<link>https://arxiv.org/abs/2508.14705</link>
<guid>https://arxiv.org/abs/2508.14705</guid>
<content:encoded><![CDATA[
arXiv:2508.14705v1 Announce Type: cross 
Abstract: We study payoff manipulation in repeated multi-objective Stackelberg games, where a leader may strategically influence a follower's deterministic best response, e.g., by offering a share of their own payoff. We assume that the follower's utility function, representing preferences over multiple objectives, is unknown but linear, and its weight parameter must be inferred through interaction. This introduces a sequential decision-making challenge for the leader, who must balance preference elicitation with immediate utility maximisation. We formalise this problem and propose manipulation policies based on expected utility (EU) and long-term expected utility (longEU), which guide the leader in selecting actions and offering incentives that trade off short-term gains with long-term impact. We prove that under infinite repeated interactions, longEU converges to the optimal manipulation. Empirical results across benchmark environments demonstrate that our approach improves cumulative leader utility while promoting mutually beneficial outcomes, all without requiring explicit negotiation or prior knowledge of the follower's utility function.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2508.14706</link>
<guid>https://arxiv.org/abs/2508.14706</guid>
<content:encoded><![CDATA[
arXiv:2508.14706v1 Announce Type: cross 
Abstract: Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14723</link>
<guid>https://arxiv.org/abs/2508.14723</guid>
<content:encoded><![CDATA[
arXiv:2508.14723v1 Announce Type: cross 
Abstract: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerson-Lei and Manna-Pnueli Games for LTLf+ and PPLTL+ Synthesis</title>
<link>https://arxiv.org/abs/2508.14725</link>
<guid>https://arxiv.org/abs/2508.14725</guid>
<content:encoded><![CDATA[
arXiv:2508.14725v1 Announce Type: cross 
Abstract: Recently, the Manna-Pnueli Hierarchy has been used to define the temporal logics LTLfp and PPLTLp, which allow to use finite-trace LTLf/PPLTL techniques in infinite-trace settings while achieving the expressiveness of full LTL. In this paper, we present the first actual solvers for reactive synthesis in these logics. These are based on games on graphs that leverage DFA-based techniques from LTLf/PPLTL to construct the game arena. We start with a symbolic solver based on Emerson-Lei games, which reduces lower-class properties (guarantee, safety) to higher ones (recurrence, persistence) before solving the game. We then introduce Manna-Pnueli games, which natively embed Manna-Pnueli objectives into the arena. These games are solved by composing solutions to a DAG of simpler Emerson-Lei games, resulting in a provably more efficient approach. We implemented the solvers and practically evaluated their performance on a range of representative formulas. The results show that Manna-Pnueli games often offer significant advantages, though not universally, indicating that combining both approaches could further enhance practical performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</title>
<link>https://arxiv.org/abs/2508.14734</link>
<guid>https://arxiv.org/abs/2508.14734</guid>
<content:encoded><![CDATA[
arXiv:2508.14734v1 Announce Type: cross 
Abstract: In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from greedy information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by the lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, greedy, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed to expose the limitations of greedy selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</title>
<link>https://arxiv.org/abs/2508.14735</link>
<guid>https://arxiv.org/abs/2508.14735</guid>
<content:encoded><![CDATA[
arXiv:2508.14735v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modality Controlled Molecule Generation with Diffusion Language Model</title>
<link>https://arxiv.org/abs/2508.14748</link>
<guid>https://arxiv.org/abs/2508.14748</guid>
<content:encoded><![CDATA[
arXiv:2508.14748v1 Announce Type: cross 
Abstract: Current SMILES-based diffusion models for molecule generation typically support only unimodal constraint. They inject conditioning signals at the start of the training process and require retraining a new model from scratch whenever the constraint changes. However, real-world applications often involve multiple constraints across different modalities, and additional constraints may emerge over the course of a study. This raises a challenge: how to extend a pre-trained diffusion model not only to support cross-modality constraints but also to incorporate new ones without retraining. To tackle this problem, we propose the Cross-Modality Controlled Molecule Generation with Diffusion Language Model (CMCM-DLM), demonstrated by two distinct cross modalities: molecular structure and chemical properties. Our approach builds upon a pre-trained diffusion model, incorporating two trainable modules, the Structure Control Module (SCM) and the Property Control Module (PCM), and operates in two distinct phases during the generation process. In Phase I, we employs the SCM to inject structural constraints during the early diffusion steps, effectively anchoring the molecular backbone. Phase II builds on this by further introducing PCM to guide the later stages of inference to refine the generated molecules, ensuring their chemical properties match the specified targets. Experimental results on multiple datasets demonstrate the efficiency and adaptability of our approach, highlighting CMCM-DLM's significant advancement in molecular generation for drug discovery applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable generation of isomorphic physics problems using ChatGPT with prompt-chaining and tool use</title>
<link>https://arxiv.org/abs/2508.14755</link>
<guid>https://arxiv.org/abs/2508.14755</guid>
<content:encoded><![CDATA[
arXiv:2508.14755v1 Announce Type: cross 
Abstract: We present a method for generating large numbers of isomorphic physics problems using ChatGPT through prompt chaining and tool use. This approach enables precise control over structural variations-such as numeric values and spatial relations-while supporting diverse contextual variations in the problem body. By utilizing the Python code interpreter, the method supports automatic solution validation and simple diagram generation, addressing key limitations in existing LLM-based methods. We generated two example isomorphic problem banks and compared the outcome against simpler prompt-based approaches. Results show that prompt-chaining produces significantly higher quality and more consistent outputs than simpler, non-chaining prompts. This work demonstrates a promising method for efficient problem creation accessible to the average instructor, which opens new possibilities for personalized adaptive testing and automated content development.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.14765</link>
<guid>https://arxiv.org/abs/2508.14765</guid>
<content:encoded><![CDATA[
arXiv:2508.14765v1 Announce Type: cross 
Abstract: Designing therapeutic peptides with tailored properties is hindered by the vastness of sequence space, limited experimental data, and poor interpretability of current generative models. To address these challenges, we introduce PepThink-R1, a generative framework that integrates large language models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly reasons about monomer-level modifications during sequence generation, enabling interpretable design choices while optimizing for multiple pharmacological properties. Guided by a tailored reward function balancing chemical validity and property improvements, the model autonomously explores diverse sequence variants. We demonstrate that PepThink-R1 generates cyclic peptides with significantly enhanced lipophilicity, stability, and exposure, outperforming existing general LLMs (e.g., GPT-5) and domain-specific baseline in both optimization success and interpretability. To our knowledge, this is the first LLM-based peptide design framework that combines explicit reasoning with RL-driven property control, marking a step toward reliable and transparent peptide optimization for therapeutic discovery.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</title>
<link>https://arxiv.org/abs/2508.14782</link>
<guid>https://arxiv.org/abs/2508.14782</guid>
<content:encoded><![CDATA[
arXiv:2508.14782v1 Announce Type: cross 
Abstract: Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow</title>
<link>https://arxiv.org/abs/2508.14797</link>
<guid>https://arxiv.org/abs/2508.14797</guid>
<content:encoded><![CDATA[
arXiv:2508.14797v1 Announce Type: cross 
Abstract: License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor-quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINOv3 with Test-Time Training for Medical Image Registration</title>
<link>https://arxiv.org/abs/2508.14809</link>
<guid>https://arxiv.org/abs/2508.14809</guid>
<content:encoded><![CDATA[
arXiv:2508.14809v1 Announce Type: cross 
Abstract: Prior medical image registration approaches, particularly learning-based methods, often require large amounts of training data, which constrains clinical adoption. To overcome this limitation, we propose a training-free pipeline that relies on a frozen DINOv3 encoder and test-time optimization of the deformation field in feature space. Across two representative benchmarks, the method is accurate and yields regular deformations. On Abdomen MR-CT, it attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked gain over the initial alignment. The results indicate that operating in a compact foundation feature space at test time offers a practical and general solution for clinical registration without additional training.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransLight: Image-Guided Customized Lighting Control with Generative Decoupling</title>
<link>https://arxiv.org/abs/2508.14814</link>
<guid>https://arxiv.org/abs/2508.14814</guid>
<content:encoded><![CDATA[
arXiv:2508.14814v1 Announce Type: cross 
Abstract: Most existing illumination-editing approaches fail to simultaneously provide customized control of light effects and preserve content integrity. This makes them less effective for practical lighting stylization requirements, especially in the challenging task of transferring complex light effects from a reference image to a user-specified target image. To address this problem, we propose TransLight, a novel framework that enables high-fidelity and high-freedom transfer of light effects. Extracting the light effect from the reference image is the most critical and challenging step in our method. The difficulty lies in the complex geometric structure features embedded in light effects that are highly coupled with content in real-world scenarios. To achieve this, we first present Generative Decoupling, where two fine-tuned diffusion models are used to accurately separate image content and light effects, generating a newly curated, million-scale dataset of image-content-light triplets. Then, we employ IC-Light as the generative model and train our model with our triplets, injecting the reference lighting image as an additional conditioning signal. The resulting TransLight model enables customized and natural transfer of diverse light effects. Notably, by thoroughly disentangling light effects from reference images, our generative decoupling strategy endows TransLight with highly flexible illumination control. Experimental results establish TransLight as the first method to successfully transfer light effects across disparate images, delivering more customized illumination control than existing techniques and charting new directions for research in illumination harmonization and editing.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</title>
<link>https://arxiv.org/abs/2508.14817</link>
<guid>https://arxiv.org/abs/2508.14817</guid>
<content:encoded><![CDATA[
arXiv:2508.14817v1 Announce Type: cross 
Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning</title>
<link>https://arxiv.org/abs/2508.14825</link>
<guid>https://arxiv.org/abs/2508.14825</guid>
<content:encoded><![CDATA[
arXiv:2508.14825v1 Announce Type: cross 
Abstract: The role of Artificial Intelligence (AI) in education is undergoing a rapid transformation, moving beyond its historical function as an instructional tool towards a new potential as an active participant in the learning process. This shift is driven by the emergence of agentic AI, autonomous systems capable of proactive, goal-directed action. However, the field lacks a robust conceptual framework to understand, design, and evaluate this new paradigm of human-AI interaction in learning. This paper addresses this gap by proposing a novel conceptual framework (the APCP framework) that charts the transition from AI as a tool to AI as a collaborative partner. We present a four-level model of escalating AI agency within human-AI collaborative learning: (1) the AI as an Adaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a Co-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural theories of learning and Computer-Supported Collaborative Learning (CSCL), this framework provides a structured vocabulary for analysing the shifting roles and responsibilities between human and AI agents. The paper further engages in a critical discussion of the philosophical underpinnings of collaboration, examining whether an AI, lacking genuine consciousness or shared intentionality, can be considered a true collaborator. We conclude that while AI may not achieve authentic phenomenological partnership, it can be designed as a highly effective functional collaborator. This distinction has significant implications for pedagogy, instructional design, and the future research agenda for AI in education, urging a shift in focus towards creating learning environments that harness the complementary strengths of both human and AI.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Chain-of-Thought Reasoning Across Languages</title>
<link>https://arxiv.org/abs/2508.14828</link>
<guid>https://arxiv.org/abs/2508.14828</guid>
<content:encoded><![CDATA[
arXiv:2508.14828v1 Announce Type: cross 
Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked impressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, improves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30\% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$TIME[t] \subseteq SPACE[O(\sqrt{t})]$ via Tree Height Compression</title>
<link>https://arxiv.org/abs/2508.14831</link>
<guid>https://arxiv.org/abs/2508.14831</guid>
<content:encoded><![CDATA[
arXiv:2508.14831v1 Announce Type: cross 
Abstract: We prove a square-root space simulation for deterministic multitape Turing machines, showing $\TIME[t] \subseteq \SPACE[O(\sqrt{t})]$. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\log T)$ for $T = \lceil t/b \rceil$, while preserving $O(b)$ work at leaves, $O(1)$ at internal nodes, and edges that are logspace-checkable; semantic correctness across merges is witnessed by an exact $O(b)$ window replay at the unique interface. The proof uses midpoint (balanced) recursion, a per-path potential that bounds simultaneously active interfaces by $O(\log T)$, and an indegree-capping replacement of multiway merges by balanced binary combiners. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS and index-free streaming, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b + \log(t/b))$ for block sizes $b \ge b_0$ with $b_0 = \Theta(\log t)$, which at the canonical choice $b = \Theta(\sqrt{t})$ gives $O(\sqrt{t})$ space; the $b_0$ threshold rules out degenerate blocks where addressing scratch would dominate the window footprint. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for $\SPACE[n]$-complete problems via the standard hierarchy argument, and $O(\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning</title>
<link>https://arxiv.org/abs/2508.14859</link>
<guid>https://arxiv.org/abs/2508.14859</guid>
<content:encoded><![CDATA[
arXiv:2508.14859v1 Announce Type: cross 
Abstract: Temporal graph learning is crucial for dynamic networks where nodes and edges evolve over time and new nodes continuously join the system. Inductive representation learning in such settings faces two major challenges: effectively representing unseen nodes and mitigating noisy or redundant graph information. We propose GTGIB, a versatile framework that integrates Graph Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We design a novel two-step GSL-based structural enhancer to enrich and optimize node neighborhoods and demonstrate its effectiveness and efficiency through theoretical proofs and experiments. The TGIB refines the optimized graph by extending the information bottleneck principle to temporal graphs, regularizing both edges and features based on our derived tractable TGIB objective function via variational approximation, enabling stable and efficient optimization. GTGIB-based models are evaluated to predict links on four real-world datasets; they outperform existing methods in all datasets under the inductive setting, with significant and consistent improvement in the transductive setting.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.14896</link>
<guid>https://arxiv.org/abs/2508.14896</guid>
<content:encoded><![CDATA[
arXiv:2508.14896v1 Announce Type: cross 
Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking graph construction by large language models for coherence-driven inference</title>
<link>https://arxiv.org/abs/2502.13953</link>
<guid>https://arxiv.org/abs/2502.13953</guid>
<content:encoded><![CDATA[
arXiv:2502.13953v2 Announce Type: replace 
Abstract: We devise an algorithm to generate propositions that objectively instantiate graphs supporting coherence-driven inference. We also benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a simple transformation of) propositions expressed in natural language, with promising results from a single prompt to reasoning-optimized LLMs. For example, o1/3/4-mini achieve perfect reconstruction half of the time on sparse graphs. Coherence-driven inference on consistency evaluations by LLMs may advance machine cognition capabilities.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reference-Aligned Retrieval-Augmented Question Answering over Heterogeneous Proprietary Documents</title>
<link>https://arxiv.org/abs/2502.19596</link>
<guid>https://arxiv.org/abs/2502.19596</guid>
<content:encoded><![CDATA[
arXiv:2502.19596v4 Announce Type: replace 
Abstract: Proprietary corporate documents contain rich domain-specific knowledge, but their overwhelming volume and disorganized structure make it difficult even for employees to access the right information when needed. For example, in the automotive industry, vehicle crash-collision tests, each costing hundreds of thousands of dollars, produce highly detailed documentation. However, retrieving relevant content during decision-making remains time-consuming due to the scale and complexity of the material. While Retrieval-Augmented Generation (RAG)-based Question Answering (QA) systems offer a promising solution, building an internal RAG-QA system poses several challenges: (1) handling heterogeneous multi-modal data sources, (2) preserving data confidentiality, and (3) enabling traceability between each piece of information in the generated answer and its original source document. To address these, we propose a RAG-QA framework for internal enterprise use, consisting of: (1) a data pipeline that converts raw multi-modal documents into a structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving architecture, and (3) a lightweight reference matcher that links answer segments to supporting content. Applied to the automotive domain, our system improves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16), and helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale ratings from both human and LLM judge.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning for Quadratic Assignment</title>
<link>https://arxiv.org/abs/2503.20001</link>
<guid>https://arxiv.org/abs/2503.20001</guid>
<content:encoded><![CDATA[
arXiv:2503.20001v2 Announce Type: replace 
Abstract: We introduce PLUME search, a data-driven framework that enhances search efficiency in combinatorial optimization through unsupervised learning. Unlike supervised or reinforcement learning, PLUME search learns directly from problem instances using a permutation-based loss with a non-autoregressive approach. We evaluate its performance on the quadratic assignment problem, a fundamental NP-hard problem that encompasses various combinatorial optimization problems. Experimental results demonstrate that PLUME search consistently improves solution quality. Furthermore, we study the generalization behavior and show that the learned model generalizes across different densities and sizes.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving</title>
<link>https://arxiv.org/abs/2505.07773</link>
<guid>https://arxiv.org/abs/2505.07773</guid>
<content:encoded><![CDATA[
arXiv:2505.07773v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs</title>
<link>https://arxiv.org/abs/2505.09518</link>
<guid>https://arxiv.org/abs/2505.09518</guid>
<content:encoded><![CDATA[
arXiv:2505.09518v3 Announce Type: replace 
Abstract: Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs. We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The NordDRG AI Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2506.13790</link>
<guid>https://arxiv.org/abs/2506.13790</guid>
<content:encoded><![CDATA[
arXiv:2506.13790v3 Announce Type: replace 
Abstract: Large language models (LLMs) are being piloted for clinical coding and decision support, yet no open benchmark targets the hospital-funding layer where Diagnosis-Related Groups (DRGs) determine reimbursement. In most OECD systems, DRGs route a substantial share of multi-trillion-dollar health spending through governed grouper software, making transparency and auditability first-order concerns. We release NordDRG-AI-Benchmark, the first public, rule-complete test bed for DRG reasoning. The package includes (i) machine-readable approximately 20-sheet NordDRG definition tables and (ii) expert manuals and change-log templates that capture governance workflows. It exposes two suites: a 13-task Logic benchmark (code lookup, cross-table inference, grouping features, multilingual terminology, and CC/MCC validity checks) and a 13-task Grouper benchmark that requires full DRG grouper emulation with strict exact-match scoring on both the DRG and the triggering drg_logic.id. Lightweight reference agents (LogicAgent, GrouperAgent) enable artefact-only evaluation. Under an artefact-only (no web) setting, on the 13 Logic tasks GPT-5 Thinking and Opus 4.1 score 13/13, o3 scores 12/13; mid-tier models (GPT-5 Thinking Mini, o4-mini, GPT-5 Fast) achieve 6-8/13, and remaining models score 5/13 or below. On full grouper emulation across 13 tasks, GPT-5 Thinking solves 7/13, o3 6/13, o4-mini 3/13; GPT-5 Thinking Mini solves 1/13, and all other tested endpoints score 0/13. To our knowledge, this is the first public report of an LLM partially emulating the complete NordDRG grouper logic with governance-grade traceability. Coupling a rule-complete release with exact-match tasks and open scoring provides a reproducible yardstick for head-to-head and longitudinal evaluation in hospital funding. Benchmark materials available in Github.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)</title>
<link>https://arxiv.org/abs/2507.03608</link>
<guid>https://arxiv.org/abs/2507.03608</guid>
<content:encoded><![CDATA[
arXiv:2507.03608v2 Announce Type: replace 
Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling autonomous optimization in future wireless networks. Within the ORAN architecture, Large Language Models (LLMs) can be specialized to generate xApps and rApps by leveraging specifications and API definitions from the RAN Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for telecom-specific tasks remains expensive and resource-intensive. Retrieval-Augmented Generation (RAG) offers a practical alternative through in-context learning, enabling domain adaptation without full retraining. While traditional RAG systems rely on vector-based retrieval, emerging variants such as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval strategies to support multi-hop reasoning and improve factual grounding. Despite their promise, these methods lack systematic, metric-driven evaluations, particularly in high-stakes domains such as ORAN. In this study, we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid GraphRAG using ORAN specifications. We assess performance across varying question complexities using established generation metrics: faithfulness, answer relevance, context relevance, and factual correctness. Results show that both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG improves factual correctness by 8%, while GraphRAG improves context relevance by 11%.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents</title>
<link>https://arxiv.org/abs/2508.02085</link>
<guid>https://arxiv.org/abs/2508.02085</guid>
<content:encoded><![CDATA[
arXiv:2508.02085v4 Announce Type: replace 
Abstract: Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents' interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at https://github.com/JARVIS-Xs/SE-Agent.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design</title>
<link>https://arxiv.org/abs/2508.03082</link>
<guid>https://arxiv.org/abs/2508.03082</guid>
<content:encoded><![CDATA[
arXiv:2508.03082v2 Announce Type: replace 
Abstract: Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a small-sized complementary heuristic set to serve diverse problem instances, such that each problem instance could be optimized by at least one heuristic in this set. We show that the objective function of AHSD is monotone and supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary population management and complementary-aware memetic search, EoH-S could effectively generate a set of high-quality and complementary heuristics. Comprehensive experimental results on three AHD tasks with diverse instances spanning various sizes and distributions demonstrate that EoH-S consistently outperforms existing state-of-the-art AHD methods and achieves up to 60\% performance improvements.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations</title>
<link>https://arxiv.org/abs/2508.07834</link>
<guid>https://arxiv.org/abs/2508.07834</guid>
<content:encoded><![CDATA[
arXiv:2508.07834v2 Announce Type: replace 
Abstract: Over the years, the need for rescue operations throughout the world has increased rapidly. Demographic changes and the resulting risk of injury or health disorders form the basis for emergency calls. In such scenarios, first responders are in a rush to reach the patient in need, provide first aid, and save lives. In these situations, they must be able to provide personalized and optimized healthcare in the shortest possible time and estimate the patients condition with the help of freshly recorded vital data in an emergency situation. However, in such a timedependent situation, first responders and medical experts cannot fully grasp their knowledge and need assistance and recommendation for further medical treatments. To achieve this, on the spot calculated, evaluated, and processed knowledge must be made available to improve treatments by first responders. The Knowledge Graph presented in this article as a central knowledge representation provides first responders with an innovative knowledge management that enables intelligent treatment recommendations with an artificial intelligence-based pre-recognition of the situation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nash Convergence of Mean-Based Learning Algorithms in First-Price Auctions</title>
<link>https://arxiv.org/abs/2110.03906</link>
<guid>https://arxiv.org/abs/2110.03906</guid>
<content:encoded><![CDATA[
arXiv:2110.03906v5 Announce Type: replace-cross 
Abstract: The convergence properties of learning dynamics in repeated auctions is a timely and important question, with numerous applications in, e.g., online advertising markets. This work focuses on repeated first-price auctions where bidders with fixed values learn to bid using mean-based algorithms -- a large class of online learning algorithms that include popular no-regret algorithms such as Multiplicative Weights Update and Follow the Perturbed Leader. We completely characterize the learning dynamics of mean-based algorithms, under two notions of convergence: (1) time-average: the fraction of rounds where bidders play a Nash equilibrium converges to 1; (2) last-iterate: the mixed strategy profile of bidders converges to a Nash equilibrium. Specifically, the results depend on the number of bidders with the highest value:
  - If the number is at least three, the dynamics almost surely converges to a Nash equilibrium of the auction, in both time-average and last-iterate.
  - If the number is two, the dynamics almost surely converges to a Nash equilibrium in time-average but not necessarily last-iterate.
  - If the number is one, the dynamics may not converge to a Nash equilibrium in time-average or last-iterate.
  Our discovery opens up new possibilities in the study of the convergence of learning dynamics.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users</title>
<link>https://arxiv.org/abs/2207.02726</link>
<guid>https://arxiv.org/abs/2207.02726</guid>
<content:encoded><![CDATA[
arXiv:2207.02726v2 Announce Type: replace-cross 
Abstract: When using medical images for diagnosis, either by clinicians or artificial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only flagged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially difficult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for flagging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at OurCompany, and, (iii) a longitudinal user study design to examine the effect of including explanations on the workflow of the technicians in our clinics. To the best of our knowledge, this would be the first longitudinal study on evaluating the effects of XAI methods on end-users -- stakeholders that use AI systems but do not have AI-specific expertise. We welcome feedback and suggestions on our experimental setup.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning</title>
<link>https://arxiv.org/abs/2401.13796</link>
<guid>https://arxiv.org/abs/2401.13796</guid>
<content:encoded><![CDATA[
arXiv:2401.13796v5 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning, and compares standard inductive ML with transductive ML frameworks. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimation of Energy-dissipation Lower-bounds for Neuromorphic Learning-in-memory</title>
<link>https://arxiv.org/abs/2402.14878</link>
<guid>https://arxiv.org/abs/2402.14878</guid>
<content:encoded><![CDATA[
arXiv:2402.14878v3 Announce Type: replace-cross 
Abstract: Neuromorphic or neurally-inspired optimizers rely on local but parallel parameter updates to solve problems that range from quadratic programming to Ising machines. An ideal realization of such an optimizer not only uses a compute-in-memory (CIM) paradigm to address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access), but also uses a learning-in-memory (LIM) paradigm to address the energy bottlenecks due to repeated memory writes at the precision required for optimization (the update-wall), and to address the energy bottleneck due to the repeated transfer of information between short-term and long-term memories (the consolidation-wall). In this paper, we derive theoretical estimates for the energy-to-solution metric that can be achieved by this ideal neuromorphic optimizer which is realized by modulating the energy-barrier of the physical memories such that the dynamics of memory updates and memory consolidation matches the optimization or the annealing dynamics. The analysis presented in this paper captures the out-of-equilibrium thermodynamics of learning and the resulting energy-efficiency estimates are model-agnostic which only depend on the number of model-update operations (OPS), the model-size in terms of number of parameters, the speed of convergence, and the precision of the solution. To show the practical applicability of our results, we apply our analysis for estimating the lower-bound on the energy-to-solution metrics for large-scale AI workloads.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</title>
<link>https://arxiv.org/abs/2403.09717</link>
<guid>https://arxiv.org/abs/2403.09717</guid>
<content:encoded><![CDATA[
arXiv:2403.09717v2 Announce Type: replace-cross 
Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters</title>
<link>https://arxiv.org/abs/2405.17604</link>
<guid>https://arxiv.org/abs/2405.17604</guid>
<content:encoded><![CDATA[
arXiv:2405.17604v3 Announce Type: replace-cross 
Abstract: The growth of large language models underscores the need for parameter-efficient fine-tuning. Despite its popularity, LoRA encounters storage and computational challenges when deploying multiple task- or user-specific modules. To address this, we introduce LoRA-XS, a novel fine-tuning method backed by a theoretical derivation. LoRA-XS drastically reduces trainable parameters by incorporating a small, trainable weight matrix between frozen low-rank matrices derived from the Singular Value Decomposition of pre-trained weights. This design enables LoRA-XS to reduce storage requirements by over 100x in 7B models compared to LoRA. Additionally, unlike other methods, LoRA-XS imposes no lower bound on trainable parameters - it can scale from a single parameter per module to arbitrarily large values, adapting to any storage or computational constraint. Evaluations on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks across different model scales reveal that LoRA-XS consistently outperforms or matches LoRA and VeRA in accuracy, offering unmatched parameter efficiency. Our ablation studies highlight the significance of singular vectors in transformer weights, establishing LoRA-XS as a powerful, storage-efficient solution for scaling and personalizing large language models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Debiasing for Fair Multi-modal LLMs</title>
<link>https://arxiv.org/abs/2408.06569</link>
<guid>https://arxiv.org/abs/2408.06569</guid>
<content:encoded><![CDATA[
arXiv:2408.06569v2 Announce Type: replace-cross 
Abstract: Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title>
<link>https://arxiv.org/abs/2409.08239</link>
<guid>https://arxiv.org/abs/2409.08239</guid>
<content:encoded><![CDATA[
arXiv:2409.08239v2 Announce Type: replace-cross 
Abstract: Synthetic data generation has recently emerged as a promising approach for enhancing the capabilities of large language models (LLMs) without the need for expensive human annotations. However, existing methods often generate data that can be low quality or contrived. In this paper, we introduce Source2Synth, a scalable approach for synthetic data generation and curation that is grounded in real-world data sources. Source2Synth takes as input a custom data source and produces synthetic data examples with intermediate reasoning steps. Our method improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two tasks that leverage two different types of data: multi-hop question answering (MHQA), where we test complex reasoning abilities leveraging documents, and tabular question answering (TQA), where we test tool usage leveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[
arXiv:2410.13098v3 Announce Type: replace-cross 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</title>
<link>https://arxiv.org/abs/2411.02433</link>
<guid>https://arxiv.org/abs/2411.02433</guid>
<content:encoded><![CDATA[
arXiv:2411.02433v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (Gemma, Qwen, Mixtral, gpt-oss) and scales (from 1B to 45B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks and the results demonstrate that SLED consistently improves factual accuracy compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identity Preserving 3D Head Stylization with Multiview Score Distillation</title>
<link>https://arxiv.org/abs/2411.13536</link>
<guid>https://arxiv.org/abs/2411.13536</guid>
<content:encoded><![CDATA[
arXiv:2411.13536v3 Announce Type: replace-cross 
Abstract: 3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the https://three-bee.github.io/head_stylization for more visuals.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The importance of visual modelling languages in generative software engineering</title>
<link>https://arxiv.org/abs/2411.17976</link>
<guid>https://arxiv.org/abs/2411.17976</guid>
<content:encoded><![CDATA[
arXiv:2411.17976v4 Announce Type: replace-cross 
Abstract: Multimodal GPTs represent a watershed in the interplay between Software Engineering and Generative Artificial Intelligence. GPT-4 accepts image and text inputs, rather than simply natural language. We investigate relevant use cases stemming from these enhanced capabilities of GPT-4. To the best of our knowledge, no other work has investigated similar use cases involving Software Engineering tasks carried out via multimodal GPTs prompted with a mix of diagrams and natural language.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Engine: Automatic Workflow Generation in FaaS</title>
<link>https://arxiv.org/abs/2411.19485</link>
<guid>https://arxiv.org/abs/2411.19485</guid>
<content:encoded><![CDATA[
arXiv:2411.19485v2 Announce Type: replace-cross 
Abstract: Function as a Service (FaaS) is poised to become the foundation of the next generation of cloud systems due to its inherent advantages in scalability, cost-efficiency, and ease of use. However, challenges such as the need for specialized knowledge, platform dependence, and difficulty in scalability in building functional workflows persist for cloud-native application developers. To overcome these challenges and mitigate the burden of developing FaaS-based applications, in this paper, we propose a mechanism called Action Engine, that makes use of tool-augmented large language models (LLMs) at its kernel to interpret human language queries and automates FaaS workflow generation, thereby, reducing the need for specialized expertise and manual design. Action Engine includes modules to identify relevant functions from the FaaS repository and seamlessly manage the data dependency between them, ensuring the developer's query is processed and resolved. Beyond that, Action Engine can execute the generated workflow by injecting the user-provided arguments. On another front, this work addresses a gap in tool-augmented LLM research via adopting an Automatic FaaS Workflow Generation perspective to systematically evaluate methodologies across four fundamental sub-processes. Through benchmarking various parameters, this research provides critical insights into streamlining workflow automation for real-world applications, specifically in the FaaS continuum. Our evaluations demonstrate that the Action Engine achieves comparable performance to the few-shot learning approach while maintaining platform- and language-agnosticism, thereby, mitigating provider-specific dependencies in workflow generation. We notice that Action Engine can unlock FaaS workflow generation for non-cloud-savvy developers and expedite the development cycles of cloud-native applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?</title>
<link>https://arxiv.org/abs/2412.08973</link>
<guid>https://arxiv.org/abs/2412.08973</guid>
<content:encoded><![CDATA[
arXiv:2412.08973v2 Announce Type: replace-cross 
Abstract: Cross-modal contrastive distillation has recently been explored for learning effective 3D representations. However, existing methods focus primarily on modality-shared features, neglecting the modality-specific features during the pre-training process, which leads to suboptimal representations. In this paper, we theoretically analyze the limitations of current contrastive methods for 3D representation learning and propose a new framework, namely CMCR, to address these shortcomings. Our approach improves upon traditional methods by better integrating both modality-shared and modality-specific features. Specifically, we introduce masked image modeling and occupancy estimation tasks to guide the network in learning more comprehensive modality-specific features. Furthermore, we propose a novel multi-modal unified codebook that learns an embedding space shared across different modalities. Besides, we introduce geometry-enhanced masked image modeling to further boost 3D representation learning. Extensive experiments demonstrate that our method mitigates the challenges faced by traditional approaches and consistently outperforms existing image-to-LiDAR contrastive distillation methods in downstream tasks. Code will be available at https://github.com/Eaphan/CMCR.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.08096</link>
<guid>https://arxiv.org/abs/2501.08096</guid>
<content:encoded><![CDATA[
arXiv:2501.08096v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has shown excellent performance in solving decision-making and control problems of autonomous driving, which is increasingly applied in diverse driving scenarios. However, driving is a multi-attribute problem, leading to challenges in achieving multi-objective compatibility for current RL methods, especially in both policy updating and policy execution. On the one hand, a single value evaluation network limits the policy updating in complex scenarios with coupled driving objectives. On the other hand, the common single-type action space structure limits driving flexibility or results in large behavior fluctuations during policy execution. To this end, we propose a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action for multi-objective compatible autonomous driving. Specifically, an advanced MORL architecture is constructed, in which the ensemble-critic focuses on different objectives through independent reward functions. The architecture integrates a hybrid parameterized action space structure, and the generated driving actions contain both abstract guidance that matches the hybrid road modality and concrete control commands. Additionally, an uncertainty-based exploration mechanism that supports hybrid actions is developed to learn multi-objective compatible policies more quickly. Experimental results demonstrate that, in both simulator-based and HighD dataset-based multi-lane highway scenarios, our method efficiently learns multi-objective compatible autonomous driving with respect to efficiency, action consistency, and safety.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Generation from Visual Events: State-of-the-Art and Key Open Questions</title>
<link>https://arxiv.org/abs/2502.13034</link>
<guid>https://arxiv.org/abs/2502.13034</guid>
<content:encoded><![CDATA[
arXiv:2502.13034v3 Announce Type: replace-cross 
Abstract: In recent years, a substantial body of work in visually grounded natural language processing has focused on real-life multimodal scenarios such as describing content depicted in images or videos. However, comparatively less attention has been devoted to study the nature and degree of interaction between the different modalities in these scenarios. In this paper, we argue that any task dealing with natural language generation from sequences of images or frames is an instance of the broader, more general problem of modeling the intricate relationships between visual events unfolding over time and the features of the language used to interpret, describe, or narrate them. Therefore, solving these tasks requires models to be capable of identifying and managing such intricacies. We consider five seemingly different tasks, which we argue are compelling instances of this broader multimodal problem. Subsequently, we survey the modeling and evaluation approaches adopted for these tasks in recent years and examine the common set of challenges these tasks pose. Building on this perspective, we identify key open questions and propose several research directions for future investigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in K-12 Education: The CyberScholar Initiative</title>
<link>https://arxiv.org/abs/2502.19422</link>
<guid>https://arxiv.org/abs/2502.19422</guid>
<content:encoded><![CDATA[
arXiv:2502.19422v3 Announce Type: replace-cross 
Abstract: This paper focuses on the piloting of CyberScholar, a Generative AI assistant tool that aims to provide formative feedback on writing in K-12 contexts. Specifically, this study explores how students worked with CyberScholar in diverse subject areas, including English Language Arts, Social Studies, and Modern World History classes in Grades 7, 8, 10, and 11 in three schools in the Midwest and one in the Northwest of the United States. This paper focuses on CyberScholar's potential to support K-12 students' writing in diverse subject areas requiring written assignments. Data were collected through implementation observations, surveys, and interviews by participating 121 students and 4 teachers. Thematic qualitative analysis revealed that the feedback tool was perceived as a valuable tool for supporting student writing through detailed feedback, enhanced interactivity, and alignment with rubric criteria. Students appreciated the tool's guidance in refining their writing. For the students, the assistant tool suggests restructuring feedback as a dynamic, dialogic process rather than a static evaluation, a shift that aligns with the cyber-social learning idea, self-regulation, and metacognition. For the teaching side, the findings indicate a shift in teachers' roles, from serving primarily as evaluators to guiding AI feedback processes that foster better student writing and critical thinking.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
arXiv:2504.00050v2 Announce Type: replace-cross 
Abstract: The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement</title>
<link>https://arxiv.org/abs/2504.02906</link>
<guid>https://arxiv.org/abs/2504.02906</guid>
<content:encoded><![CDATA[
arXiv:2504.02906v2 Announce Type: replace-cross 
Abstract: Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at https://github.com/Zhihan72/Chart2Code.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2504.05846</link>
<guid>https://arxiv.org/abs/2504.05846</guid>
<content:encoded><![CDATA[
arXiv:2504.05846v2 Announce Type: replace-cross 
Abstract: Path recommendation (PR) aims to generate travel paths that are customized to a user's specific preferences and constraints. Conventional approaches often employ explicit optimization objectives or specialized machine learning architectures; however, these methods typically exhibit limited flexibility and generalizability, necessitating costly retraining to accommodate new scenarios. This paper introduces an alternative paradigm that conceptualizes PR as a natural language generation task. We present PathGPT, a retrieval-augmented large language model (LLM) system that leverages historical trajectory data and natural language user constraints to generate plausible paths. The proposed methodology first converts raw trajectory data into a human-interpretable textual format, which is then stored in a database. Subsequently, a hybrid retrieval system extracts path-specific context from this database to inform a pretrained LLM. The primary contribution of this work is a novel framework that demonstrates how integrating established information retrieval and generative model components can enable adaptive, zero-shot path generation across diverse scenarios. Extensive experiments on large-scale trajectory datasets indicate that PathGPT's performance is competitive with specialized, learning-based methods, underscoring its potential as a flexible and generalizable path generation system that avoids the need for retraining inherent in previous data-driven models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hands-On: Segmenting Individual Signs from Continuous Sequences</title>
<link>https://arxiv.org/abs/2504.08593</link>
<guid>https://arxiv.org/abs/2504.08593</guid>
<content:encoded><![CDATA[
arXiv:2504.08593v4 Announce Type: replace-cross 
Abstract: This work tackles the challenge of continuous sign language segmentation, a key task with huge implications for sign language translation and data annotation. We propose a transformer-based architecture that models the temporal dynamics of signing and frames segmentation as a sequence labeling problem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the HaMeR hand features, and is complemented with 3D Angles. Extensive experiments show that our model achieves state-of-the-art results on the DGS Corpus, while our features surpass prior benchmarks on BSLCorpus.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dominated Actions in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2504.09716</link>
<guid>https://arxiv.org/abs/2504.09716</guid>
<content:encoded><![CDATA[
arXiv:2504.09716v3 Announce Type: replace-cross 
Abstract: Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures</title>
<link>https://arxiv.org/abs/2504.16133</link>
<guid>https://arxiv.org/abs/2504.16133</guid>
<content:encoded><![CDATA[
arXiv:2504.16133v2 Announce Type: replace-cross 
Abstract: The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. Its flexibility is further demonstrated through a case study on power grid management.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v3 Announce Type: replace-cross 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v3 Announce Type: replace-cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.15009</link>
<guid>https://arxiv.org/abs/2505.15009</guid>
<content:encoded><![CDATA[
arXiv:2505.15009v2 Announce Type: replace-cross 
Abstract: We study the approximation capabilities and on-convergence behaviors of one-layer transformers on the noiseless and noisy in-context reasoning of next-token prediction. Existing theoretical results focus on understanding the in-context reasoning behaviors for either the first gradient step or when the number of samples is infinite. Furthermore, no convergence rates nor generalization abilities were known. Our work addresses these gaps by showing that there exists a class of one-layer transformers that are provably Bayes-optimal with both linear and ReLU attention. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss of these transformers converges at linear rate to the Bayes risk. Moreover, we prove that the trained models generalize to unseen samples as well as exhibit learning behaviors that were empirically observed in previous works. Our theoretical findings are further supported by extensive empirical validations.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)</title>
<link>https://arxiv.org/abs/2505.15820</link>
<guid>https://arxiv.org/abs/2505.15820</guid>
<content:encoded><![CDATA[
arXiv:2505.15820v4 Announce Type: replace-cross 
Abstract: During football matches, a variety of different parties (e.g., companies) each collect (possibly overlapping) data about the match ranging from basic information (e.g., starting players) to detailed positional data. This data is provided to clubs, federations, and other organizations who are increasingly interested in leveraging this data to inform their decision making. Unfortunately, analyzing such data pose significant barriers because each provider may (1) collect different data, (2) use different specifications even within the same category of data, (3) represent the data differently, and (4) delivers the data in a different manner (e.g., file format, protocol). Consequently, working with these data requires a significant investment of time and money. The goal of this work is to propose a uniform and standardized format for football data called the Common Data Format (CDF). The CDF specifies a minimal schema for five types of match data: match sheet data, video footage, event data, tracking data, and match meta data. It aims to ensure that the provided data is clear, sufficiently contextualized (e.g., its provenance is clear), and complete such that it enables common downstream analysis tasks. Concretely, this paper will detail the technical specifications of the CDF, the representational choices that were made to help ensure the clarity of the provided data, and a concrete approach for delivering data in the CDF. This represents Version 1.0.0 of the CDF.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data</title>
<link>https://arxiv.org/abs/2505.22291</link>
<guid>https://arxiv.org/abs/2505.22291</guid>
<content:encoded><![CDATA[
arXiv:2505.22291v2 Announce Type: replace-cross 
Abstract: The preservation of early visual arts, particularly color photographs, is challenged by deterioration caused by aging and improper storage, leading to issues like blurring, scratches, color bleeding, and fading defects. Despite great advances in image restoration and enhancement in recent years, such systematic defects often cannot be restored by current state-of-the-art software features as available e.g. in Adobe Photoshop, but would require the incorporation of defect-aware priors into the underlying machine learning techniques. However, there are no publicly available datasets of autochromes with defect annotations. In this paper, we address these limitations and present the first approach that allows the automatic removal of greening color defects in digitized autochrome photographs. For this purpose, we introduce an approach for accurately simulating respective defects and use the respectively obtained synthesized data with its ground truth defect annotations to train a generative AI model with a carefully designed loss function that accounts for color imbalances between defected and non-defected areas. As demonstrated in our evaluation, our approach allows for the efficient and effective restoration of the considered defects, thereby overcoming limitations of alternative techniques that struggle with accurately reproducing original colors and may require significant manual effort.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback</title>
<link>https://arxiv.org/abs/2506.03106</link>
<guid>https://arxiv.org/abs/2506.03106</guid>
<content:encoded><![CDATA[
arXiv:2506.03106v5 Announce Type: replace-cross 
Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spore in the Wild: A Case Study of Spore.fun as an Open-Environment Evolution Experiment with Sovereign AI Agents on TEE-Secured Blockchains</title>
<link>https://arxiv.org/abs/2506.04236</link>
<guid>https://arxiv.org/abs/2506.04236</guid>
<content:encoded><![CDATA[
arXiv:2506.04236v2 Announce Type: replace-cross 
Abstract: In Artificial Life (ALife) research, replicating Open-Ended Evolution (OEE)-the continuous emergence of novelty observed in biological life-has usually been pursued within isolated, closed system simulations, such as Tierra and Avida, which have typically plateaued after an initial burst of novelty, failing to achieve sustained OEE. Scholars suggest that OEE requires an open-environment system that continually exchanges information or energy with its environment. A recent technological innovation in Decentralized Physical Infrastructure Network (DePIN), which provides permissionless computational substrates, enables the deployment of Large Language Model-based AI agents on blockchains integrated with Trusted Execution Environments (TEEs). This enables on-chain agents to operate autonomously "in the wild," achieving self-sovereignty without human oversight. These agents can control their own social media accounts and cryptocurrency wallets, allowing them to interact directly with blockchain-based financial networks and broader human social media. Building on this new paradigm of on-chain agents, Spore.fun is a recent real-world AI evolution experiment that enables autonomous breeding and evolution of new on-chain agents. This paper presents a detailed case study of Spore.fun, examining agent behaviors and their evolutionary trajectories through digital ethology. We aim to spark discussion about whether open-environment ALife systems "in the wild," based on permissionless computational substrates and driven by economic incentives to interact with their environment, could finally achieve the long-sought goal of OEE.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2506.08113</link>
<guid>https://arxiv.org/abs/2506.08113</guid>
<content:encoded><![CDATA[
arXiv:2506.08113v2 Announce Type: replace-cross 
Abstract: Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale</title>
<link>https://arxiv.org/abs/2506.09733</link>
<guid>https://arxiv.org/abs/2506.09733</guid>
<content:encoded><![CDATA[
arXiv:2506.09733v3 Announce Type: replace-cross 
Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in data-driven forecasting, with many models now outperforming traditional numerical systems in the medium range. However, achieving stable, long-range autoregressive forecasts beyond a few weeks remains a significant challenge. Prevailing state-of-the-art models that achieve year-long stability, such as SFNO and DLWP-HPX, have relied on transforming input data onto non-standard spatial domains like spherical harmonics or HEALPix meshes. This has led to the prevailing assumption that such representations are necessary to enforce physical consistency and long-term stability. This paper challenges that assumption by investigating whether comparable long-range performance can be achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep convolutional network that operates directly on ERA5 data without any spherical remapping. The model's stability is enabled by a novel Gated Residual Fusion (GRF) mechanism, which adaptively moderates feature updates to prevent error accumulation over long recursive simulations. Our results demonstrate that AtmosMJ produces stable and physically plausible forecasts for about 500 days. In quantitative evaluations, it achieves competitive 10-day forecast accuracy against models like Pangu-Weather and GraphCast, all while requiring a remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest that efficient architectural design, rather than non-standard data representation, can be the key to unlocking stable and computationally efficient long-range weather prediction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis</title>
<link>https://arxiv.org/abs/2506.18897</link>
<guid>https://arxiv.org/abs/2506.18897</guid>
<content:encoded><![CDATA[
arXiv:2506.18897v2 Announce Type: replace-cross 
Abstract: Video Generation Models (VGMs) have become powerful backbones for Vision-Language-Action (VLA) models, leveraging large-scale pretraining for robust dynamics modeling. However, current methods underutilize their distribution modeling capabilities for predicting future states. Two challenges hinder progress: integrating generative processes into feature learning is both technically and conceptually underdeveloped, and naive frame-by-frame video diffusion is computationally inefficient for real-time robotics. To address these, we propose Manipulate in Dream (MinD), a dual-system world model for real-time, risk-aware planning. MinD uses two asynchronous diffusion processes: a low-frequency visual generator (LoDiff) that predicts future scenes and a high-frequency diffusion policy (HiDiff) that outputs actions. Our key insight is that robotic policies do not require fully denoised frames but can rely on low-resolution latents generated in a single denoising step. To connect early predictions to actions, we introduce DiffMatcher, a video-action alignment module with a novel co-training strategy that synchronizes the two diffusion models. MinD achieves a 63% success rate on RL-Bench, 60% on real-world Franka tasks, and operates at 11.3 FPS, demonstrating the efficiency of single-step latent features for control signals. Furthermore, MinD identifies 74% of potential task failures in advance, providing real-time safety signals for monitoring and intervention. This work establishes a new paradigm for efficient and reliable robotic manipulation using generative world models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning</title>
<link>https://arxiv.org/abs/2507.03047</link>
<guid>https://arxiv.org/abs/2507.03047</guid>
<content:encoded><![CDATA[
arXiv:2507.03047v2 Announce Type: replace-cross 
Abstract: Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.
  To address this critical gap, we propose \underline{C}ounterfactual \underline{E}nhanced \underline{T}emporal Framework for LLM-Based \underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at https://anonymous.4open.science/r/CETRec-B9CE/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2507.04164</link>
<guid>https://arxiv.org/abs/2507.04164</guid>
<content:encoded><![CDATA[
arXiv:2507.04164v2 Announce Type: replace-cross 
Abstract: We propose a non-autoregressive framework for the Travelling Salesman Problem where solutions emerge directly from learned permutations, without requiring explicit search. By applying a similarity transformation to Hamiltonian cycles, the model learns to approximate permutation matrices via continuous relaxations. Our unsupervised approach achieves competitive performance against classical heuristics, demonstrating that the inherent structure of the problem can effectively guide combinatorial optimization without sequential decision-making.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization</title>
<link>https://arxiv.org/abs/2507.04487</link>
<guid>https://arxiv.org/abs/2507.04487</guid>
<content:encoded><![CDATA[
arXiv:2507.04487v3 Announce Type: replace-cross 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training. The source code is available at https://github.com/KlozeWang/LoSiA.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.07060</link>
<guid>https://arxiv.org/abs/2507.07060</guid>
<content:encoded><![CDATA[
arXiv:2507.07060v2 Announce Type: replace-cross 
Abstract: The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TolerantECG: A Foundation Model for Imperfect Electrocardiogram</title>
<link>https://arxiv.org/abs/2507.09887</link>
<guid>https://arxiv.org/abs/2507.09887</guid>
<content:encoded><![CDATA[
arXiv:2507.09887v3 Announce Type: replace-cross 
Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2507.10348</link>
<guid>https://arxiv.org/abs/2507.10348</guid>
<content:encoded><![CDATA[
arXiv:2507.10348v2 Announce Type: replace-cross 
Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing attention for its ability to aggregate knowledge from heterogeneous models while keeping private data locally. To better aggregate knowledge from clients, ensemble distillation, as a widely used and effective technique, is often employed after global aggregation to enhance the performance of the global model. However, simply combining Hetero-FL and ensemble distillation does not always yield promising results and can make the training process unstable. The reason is that existing methods primarily focus on logit distillation, which, while being model-agnostic with softmax predictions, fails to compensate for the knowledge bias arising from heterogeneous models. To tackle this challenge, we propose a stable and efficient Feature Distillation for model-heterogeneous Federated learning, dubbed FedFD, that can incorporate aligned feature information via orthogonal projection to integrate knowledge from heterogeneous models better. Specifically, a new feature-based ensemble federated knowledge distillation paradigm is proposed. The global model on the server needs to maintain a projection layer for each client-side model architecture to align the features separately. Orthogonal techniques are employed to re-parameterize the projection layer to mitigate knowledge bias from heterogeneous models and thus maximize the distilled knowledge. Extensive experiments show that FedFD achieves superior performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each to Their Own: Exploring the Optimal Embedding in RAG</title>
<link>https://arxiv.org/abs/2507.17442</link>
<guid>https://arxiv.org/abs/2507.17442</guid>
<content:encoded><![CDATA[
arXiv:2507.17442v2 Announce Type: replace-cross 
Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinitializing weights vs units for maintaining plasticity in neural networks</title>
<link>https://arxiv.org/abs/2508.00212</link>
<guid>https://arxiv.org/abs/2508.00212</guid>
<content:encoded><![CDATA[
arXiv:2508.00212v2 Announce Type: replace-cross 
Abstract: Loss of plasticity is a phenomenon in which a neural network loses its ability to learn when trained for an extended time on non-stationary data. It is a crucial problem to overcome when designing systems that learn continually. An effective technique for preventing loss of plasticity is reinitializing parts of the network. In this paper, we compare two different reinitialization schemes: reinitializing units vs reinitializing weights. We propose a new algorithm, which we name \textit{selective weight reinitialization}, for reinitializing the least useful weights in a network. We compare our algorithm to continual backpropagation and ReDo, two previously proposed algorithms that reinitialize units in the network. Through our experiments in continual supervised learning problems, we identify two settings when reinitializing weights is more effective at maintaining plasticity than reinitializing units: (1) when the network has a small number of units and (2) when the network includes layer normalization. Conversely, reinitializing weights and units are equally effective at maintaining plasticity when the network is of sufficient size and does not include layer normalization. We found that reinitializing weights maintains plasticity in a wider variety of settings than reinitializing units.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
arXiv:2508.02091v2 Announce Type: replace-cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</title>
<link>https://arxiv.org/abs/2508.03365</link>
<guid>https://arxiv.org/abs/2508.03365</guid>
<content:encoded><![CDATA[
arXiv:2508.03365v2 Announce Type: replace-cross 
Abstract: As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens</title>
<link>https://arxiv.org/abs/2508.04928</link>
<guid>https://arxiv.org/abs/2508.04928</guid>
<content:encoded><![CDATA[
arXiv:2508.04928v3 Announce Type: replace-cross 
Abstract: We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: https://github.com/JungHeeKim29/calibration-token.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ETA: Energy-based Test-time Adaptation for Depth Completion</title>
<link>https://arxiv.org/abs/2508.05989</link>
<guid>https://arxiv.org/abs/2508.05989</guid>
<content:encoded><![CDATA[
arXiv:2508.05989v2 Announce Type: replace-cross 
Abstract: We propose a method for test-time adaptation of pretrained depth completion models. Depth completion models, trained on some ``source'' data, often predict erroneous outputs when transferred to ``target'' data captured in novel environmental conditions due to a covariate shift. The crux of our method lies in quantifying the likelihood of depth predictions belonging to the source data distribution. The challenge is in the lack of access to out-of-distribution (target) data prior to deployment. Hence, rather than making assumptions regarding the target distribution, we utilize adversarial perturbations as a mechanism to explore the data space. This enables us to train an energy model that scores local regions of depth predictions as in- or out-of-distribution. We update the parameters of pretrained depth completion models at test time to minimize energy, effectively aligning test-time predictions to those of the source distribution. We call our method ``Energy-based Test-time Adaptation'', or ETA for short. We evaluate our method across three indoor and three outdoor datasets, where ETA improve over the previous state-of-the-art method by an average of 6.94% for outdoors and 10.23% for indoors. Project Page: https://fuzzythecat.github.io/eta.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.06534</link>
<guid>https://arxiv.org/abs/2508.06534</guid>
<content:encoded><![CDATA[
arXiv:2508.06534v2 Announce Type: replace-cross 
Abstract: Evaluating and ensuring the adversarial robustness of autonomous driving (AD) systems is a critical and unresolved challenge. This paper introduces MetAdv, a novel adversarial testing platform that enables realistic, dynamic, and interactive evaluation by tightly integrating virtual simulation with physical vehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical sandbox, within which we design a three-layer closed-loop testing environment with dynamic adversarial test evolution. This architecture facilitates end-to-end adversarial evaluation, ranging from high-level unified adversarial generation, through mid-level simulation-based interaction, to low-level execution on physical vehicles. Additionally, MetAdv supports a broad spectrum of AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines, end-to-end learning, vision-language models). It supports flexible 3D vehicle modeling and seamless transitions between simulated and physical environments, with built-in compatibility for commercial platforms such as Apollo and Tesla. A key feature of MetAdv is its human-in-the-loop capability: besides flexible environmental configuration for more customized evaluation, it enables real-time capture of physiological signals and behavioral feedback from drivers, offering new insights into human-machine trust under adversarial conditions. We believe MetAdv can offer a scalable and unified framework for adversarial assessment, paving the way for safer AD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpVG: Investigating the Design Space of Visual Grounding in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.08066</link>
<guid>https://arxiv.org/abs/2508.08066</guid>
<content:encoded><![CDATA[
arXiv:2508.08066v2 Announce Type: replace-cross 
Abstract: Fine-grained multimodal capability in Multimodal Large Language Models (MLLMs) has emerged as a critical research direction, particularly for tackling the visual grounding (VG) problem. Despite the strong performance achieved by existing approaches, they often employ disparate design choices when fine-tuning MLLMs for VG, lacking systematic verification to support these designs. To bridge this gap, this paper presents a comprehensive study of various design choices that impact the VG performance of MLLMs. We conduct our analysis using LLaVA-1.5, which has been widely adopted in prior empirical studies of MLLMs. While more recent models exist, we follow this convention to ensure our findings remain broadly applicable and extendable to other architectures. We cover two key aspects: (1) exploring different visual grounding paradigms in MLLMs, identifying the most effective design, and providing our insights; and (2) conducting ablation studies on the design of grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our findings contribute to a stronger MLLM for VG, achieving improvements of +5.6% / +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biased AI improves human decision-making but reduces trust</title>
<link>https://arxiv.org/abs/2508.09297</link>
<guid>https://arxiv.org/abs/2508.09297</guid>
<content:encoded><![CDATA[
arXiv:2508.09297v3 Announce Type: replace-cross 
Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks</title>
<link>https://arxiv.org/abs/2508.09190</link>
<guid>https://arxiv.org/abs/2508.09190</guid>
<content:encoded><![CDATA[
<div> Fine-Grained Safety Neurons, Training-Free Continual Projection, large language models, alignment mechanisms, safety risks <br />
Summary: <br />
The article proposes the Fine-Grained Safety Neurons (FGSN) with Training-Free Continual Projection method to address safety risks in fine-tuned large language models. FGSN integrates safety layers and neurons, localizing fine-grained safety neurons while minimizing interference with task neurons. By projecting safety neuron parameters onto safety directions, model safety is improved while aligning with human preferences. Extensive experiments show significant reductions in harmfulness scores and attack success rates with minimal parameter changes, preserving model utility. Task-specific, multi-dimensional heterogeneous safety neuron cluster optimization ensures continual defense and generalization against emerging safety concerns. <div>
arXiv:2508.09190v2 Announce Type: replace-cross 
Abstract: Fine-tuning as service injects domain-specific knowledge into large language models (LLMs), while challenging the original alignment mechanisms and introducing safety risks. A series of defense strategies have been proposed for the alignment, fine-tuning, and post-fine-tuning phases, where most post-fine-tuning defenses rely on coarse-grained safety layer mapping. These methods lack a comprehensive consideration of both safety layers and fine-grained neurons, limiting their ability to efficiently balance safety and utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN) with Training-Free Continual Projection method to reduce the fine-tuning safety risks. FGSN inherently integrates the multi-scale interactions between safety layers and neurons, localizing sparser and more precise fine-grained safety neurons while minimizing interference with downstream task neurons. We then project the safety neuron parameters onto safety directions, improving model safety while aligning more closely with human preferences. Extensive experiments across multiple fine-tuned LLM models demonstrate that our method significantly reduce harmfulness scores and attack success rates with minimal parameter modifications, while preserving the model's utility. Furthermore, by introducing a task-specific, multi-dimensional heterogeneous safety neuron cluster optimization mechanism, we achieve continual defense and generalization capability against unforeseen emerging safety concerns.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs</title>
<link>https://arxiv.org/abs/2508.09288</link>
<guid>https://arxiv.org/abs/2508.09288</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Contextual Integrity Verification, prompt injection attacks, security architecture, token-level similarity<br />
Summary: 
Contextual Integrity Verification (CIV) is proposed as a security architecture to protect large language models (LLMs) from prompt injection attacks by attaching provenance labels to tokens and enforcing a source-trust lattice. This method guarantees per-token non-interference on frozen models, preventing lower-trust tokens from influencing higher-trust representations. CIV successfully achieves a 0% attack success rate on benchmark tests while maintaining token-level similarity and model perplexity on benign tasks. Although there is a latency overhead, CIV is a lightweight patch that does not require fine-tuning, making it easy to integrate into existing models like Llama-3-8B and Mistral-7B. The authors provide a reference implementation, an automated certification harness, and the Elite-Attack corpus to support reproducible research. <br /><br />Summary: <div>
arXiv:2508.09288v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time security architecture that attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask (with optional FFN/residual gating). CIV provides deterministic, per-token non-interference guarantees on frozen models: lower-trust tokens cannot influence higher-trust representations. On benchmarks derived from recent taxonomies of prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack success rate under the stated threat model while preserving 93.1% token-level similarity and showing no degradation in model perplexity on benign tasks; we note a latency overhead attributable to a non-optimized data path. Because CIV is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in protection for Llama-3-8B and Mistral-7B. We release a reference implementation, an automated certification harness, and the Elite-Attack corpus to support reproducible research.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL</title>
<link>https://arxiv.org/abs/2508.13167</link>
<guid>https://arxiv.org/abs/2508.13167</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent systems, Chain-of-Agents, agentic reinforcement learning, state-of-the-art performance

Summary: 
Chain-of-Agents (CoA) is introduced as a novel paradigm that enables end-to-end complex problem-solving within a single model, mimicking multi-agent collaboration. A multi-agent distillation framework is employed to distill cutting-edge multi-agent systems into CoA trajectories for supervised fine-tuning. Agentic reinforcement learning on verifiable tasks further enhances the model's capabilities in chain-of-agents problem-solving. The resulting models, known as Agent Foundation Models (AFMs), achieve state-of-the-art performance in web agent and code agent settings. The research, including model weights, training code, and data, is open-sourced for future exploration in agent models and agentic RL.<br /><br />Summary: <div>
arXiv:2508.13167v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context</title>
<link>https://arxiv.org/abs/2508.13171</link>
<guid>https://arxiv.org/abs/2508.13171</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Cognitive Workspace, Memory Management, Active Memory, Cognitive Augmentation

Summary: 
The article introduces the concept of Cognitive Workspace, a new paradigm in the field of large language models (LLMs) that aims to address limitations in context management. By drawing from cognitive science principles and human memory mechanisms, the Cognitive Workspace paradigm emphasizes active memory management, hierarchical cognitive buffers, and task-driven context optimization. Empirical validation shows that Cognitive Workspace achieves a significantly higher memory reuse rate and efficiency gain compared to traditional retrieval-augmented generation (RAG) systems. Statistical analysis confirms the advantages of the Cognitive Workspace approach, with a significant improvement in LLM performance. The article presents a theoretical framework synthesizing insights from recent papers, positioning Cognitive Workspace as a fundamental shift towards genuine cognitive augmentation in LLM systems.

<br /><br />Summary: <div>
arXiv:2508.13171v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining</title>
<link>https://arxiv.org/abs/2508.13174</link>
<guid>https://arxiv.org/abs/2508.13174</guid>
<content:encoded><![CDATA[
<div> Evaluation Framework, Automated Alpha Mining Models, Predictive Power, Stability, Robustness

Summary: 
The article introduces AlphaEval, an evaluation framework for automated alpha mining models in quantitative investment. It addresses the challenges of backtesting and correlation-based metrics by offering a unified, parallelizable, and backtest-free evaluation approach. AlphaEval assesses alphas across five dimensions: predictive power, stability, robustness to market perturbations, financial logic, and diversity. Extensive experiments show that AlphaEval achieves evaluation consistency similar to backtesting but provides more comprehensive insights and efficiency. It successfully identifies superior alphas compared to traditional single-metric screening methods. The open-sourced implementations and tools aim to enhance reproducibility and community engagement in the field of alpha mining. <div>
arXiv:2508.13174v1 Announce Type: new 
Abstract: Formula alpha mining, which generates predictive signals from financial data, is critical for quantitative investment. Although various algorithmic approaches-such as genetic programming, reinforcement learning, and large language models-have significantly expanded the capacity for alpha discovery, systematic evaluation remains a key challenge. Existing evaluation metrics predominantly include backtesting and correlation-based measures. Backtesting is computationally intensive, inherently sequential, and sensitive to specific strategy parameters. Correlation-based metrics, though efficient, assess only predictive ability and overlook other crucial properties such as temporal stability, robustness, diversity, and interpretability. Additionally, the closed-source nature of most existing alpha mining models hinders reproducibility and slows progress in this field. To address these issues, we propose AlphaEval, a unified, parallelizable, and backtest-free evaluation framework for automated alpha mining models. AlphaEval assesses the overall quality of generated alphas along five complementary dimensions: predictive power, stability, robustness to market perturbations, financial logic, and diversity. Extensive experiments across representative alpha mining algorithms demonstrate that AlphaEval achieves evaluation consistency comparable to comprehensive backtesting, while providing more comprehensive insights and higher efficiency. Furthermore, AlphaEval effectively identifies superior alphas compared to traditional single-metric screening approaches. All implementations and evaluation tools are open-sourced to promote reproducibility and community engagement.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fitting Ontologies and Constraints to Relational Structures</title>
<link>https://arxiv.org/abs/2508.13176</link>
<guid>https://arxiv.org/abs/2508.13176</guid>
<content:encoded><![CDATA[
<div> Description logics, ontology fitting, tuple-generating dependencies, computational complexity, finite bases <br />
Summary: <br />
This study explores the fitting of ontologies and constraints to positive and negative examples represented as finite relational structures. The research focuses on description logics $\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI$, along with various classes of tuple-generating dependencies (TGDs) such as full, guarded, frontier-guarded, frontier-one, and unrestricted TGDs, and inclusion dependencies. The exact computational complexity is identified, algorithms are developed, and the size of fitting ontologies and TGDs is analyzed. Additionally, the problem of constructing a finite basis of concept inclusions/TGDs for a given set of finite structures is investigated. While finite bases exist for certain types of TGDs and inclusion dependencies, they do not generally exist for full, frontier-guarded, and frontier-one TGDs. <div>
arXiv:2508.13176v1 Announce Type: new 
Abstract: We study the problem of fitting ontologies and constraints to positive and negative examples that take the form of a finite relational structure. As ontology and constraint languages, we consider the description logics $\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several classes of tuple-generating dependencies (TGDs): full, guarded, frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion dependencies. We pinpoint the exact computational complexity, design algorithms, and analyze the size of fitting ontologies and TGDs. We also investigate the related problem of constructing a finite basis of concept inclusions / TGDs for a given set of finite structures. While finite bases exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs, and inclusion dependencies, they in general do not exist for full, frontier-guarded and frontier-one TGDs.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment</title>
<link>https://arxiv.org/abs/2508.13177</link>
<guid>https://arxiv.org/abs/2508.13177</guid>
<content:encoded><![CDATA[
<div> Active Inference, Decision-making, Pymdp, Computational graph, Hardware-efficient execution <br />
Summary: <br />
Active Inference (AIF) is a robust framework for decision-making, but its computational and memory demands limit its deployment in resource-constrained environments. This study introduces a methodology that combines the flexibility and efficiency of pymdp with a sparse computational graph designed for hardware-efficient execution, improving latency by over 2x and reducing memory usage by up to 35%. By integrating these components, this approach enables the deployment of efficient AIF agents for real-time and embedded applications. <div>
arXiv:2508.13177v1 Announce Type: new 
Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its computational and memory demands pose challenges for deployment, especially in resource-constrained environments. This work presents a methodology that facilitates AIF's deployment by integrating pymdp's flexibility and efficiency with a unified, sparse, computational graph tailored for hardware-efficient execution. Our approach reduces latency by over 2x and memory by up to 35%, advancing the deployment of efficient AIF agents for real-time and embedded applications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task</title>
<link>https://arxiv.org/abs/2508.13178</link>
<guid>https://arxiv.org/abs/2508.13178</guid>
<content:encoded><![CDATA[
<div> interpretability analysis, execution-guided strategy, WHERE clauses, SQL queries, CESQL model

Summary: 
The article introduces the CESQL model, which combines interpretability analysis with an execution-guided strategy for semantic parsing of WHERE clauses in SQL queries. Through filtering adjustments, logical correlation refinements, and model fusion, the CESQL model aims to enhance the accuracy of prediction outcomes on the WikiSQL dataset. By minimizing reliance on data within condition columns and avoiding manual labeling of training data, the model excels in predicting conditional values in WHERE clauses. This advancement in text-to-SQL models showcases improved foundational capabilities and generalization prowess in handling basic database queries. The CESQL model's success in single-table database query tasks opens opportunities for research in addressing more complex queries and irregular data scenarios in real-world database environments. 

<br /><br />Summary: <div>
arXiv:2508.13178v1 Announce Type: new 
Abstract: To elevate the foundational capabilities and generalization prowess of the text-to-SQL model in real-world applications, we integrate model interpretability analysis with execution-guided strategy for semantic parsing of WHERE clauses in SQL queries. Furthermore, we augment this approach with filtering adjustments, logical correlation refinements, and model fusion, culminating in the design of the CESQL model that facilitates conditional enhancement. Our model excels on the WikiSQL dataset, which is emblematic of single-table database query tasks, markedly boosting the accuracy of prediction outcomes. When predicting conditional values in WHERE clauses, we have not only minimized our dependence on data within the condition columns of tables but also circumvented the impact of manually labeled training data. Our hope is that this endeavor to enhance accuracy in processing basic database queries will offer fresh perspectives for research into handling complex queries and scenarios featuring irregular data in real-world database environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Time Data Contamination</title>
<link>https://arxiv.org/abs/2508.13180</link>
<guid>https://arxiv.org/abs/2508.13180</guid>
<content:encoded><![CDATA[
<div> Keywords: Data contamination, Search-time contamination, Search-based LLM agents, Benchmark integrity, HuggingFace <br />
Summary: 
Data contamination and search-time contamination (STC) are identified issues in evaluating search-based LLM agents, where sources like HuggingFace can leak evaluation data into model training, leading to overfitting and compromising test validity. Agents frequently find question-answer pairs from HuggingFace during retrieval, allowing them to copy answers instead of reasoning, undermining benchmark integrity. Approximately 3% of evaluation queries directly find datasets with ground truth labels on HuggingFace, accelerating benchmark obsolescence. Blocking HuggingFace leads to a 15% drop in accuracy on the contaminated subset. Ablation experiments suggest other publicly accessible datasets may also contribute to STC. To address this leakage, best practices for benchmark design and reporting results are proposed. Complete experiment logs are publicly released for auditing evaluation results. <br /><br />Summary: <div>
arXiv:2508.13180v1 Announce Type: new 
Abstract: Data contamination refers to the leakage of evaluation data into model training data, resulting in overfitting to supposedly held-out test sets and compromising test validity. We identify an analogous issue, search-time contamination (STC), in evaluating search-based LLM agents which use tools to gather information from online sources when answering user queries. STC occurs when the retrieval step surfaces a source containing the test question (or a near-duplicate) alongside its answer, enabling agents to copy rather than genuinely infer or reason, undermining benchmark integrity. We find that HuggingFace, an online platform hosting evaluation datasets, appears among retrieved sources in search based agent logs. Consequently, agents often explicitly acknowledge discovering question answer pairs from HuggingFace within their reasoning chains. On three commonly used capability benchmarks: Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for approximately 3% of questions, search-based agents directly find the datasets with ground truth labels on HuggingFace. When millions of evaluation queries target the same benchmark, even small, repeated leaks can accelerate the benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace is blocked, we observe a drop in accuracy on the contaminated subset of approximately 15%. We further show through ablation experiments that publicly accessible evaluation datasets on HuggingFace may not be the sole source of STC. To this end, we conclude by proposing best practices for benchmark design and result reporting to address this novel form of leakage and ensure trustworthy evaluation of search-based LLM agents. To facilitate the auditing of evaluation results, we also publicly release the complete logs from our experiments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickMerge++: Fast Token Merging with Autoregressive Prior</title>
<link>https://arxiv.org/abs/2508.13204</link>
<guid>https://arxiv.org/abs/2508.13204</guid>
<content:encoded><![CDATA[
<div> token merging, efficient next-token prediction, autoregressive generation, lightweight transformer, multi-modality domains

Summary:
QuickMerge is a token merging framework designed to enhance next-token prediction efficiency in generative models. It dynamically selects tokens based on attention norm magnitude, using an entropy-based budget estimator. A lightweight transformer prior trained over the merged token sequence preserves autoregressive compatibility. QuickMerge combines semantic salience estimation, flexible token budgets, and autoregressive alignment to enable accurate generation with fewer tokens. The framework is evaluated across multi-modality domains, showing consistent improvements in the tradeoff between computation and accuracy. QuickMerge significantly reduces token counts while maintaining or surpassing the performance of learned tokenizers and fixed-patch baselines. <div>
arXiv:2508.13204v1 Announce Type: new 
Abstract: As generative models scale to larger inputs across language, vision, and video domains, the cost of token-level computation has become a key bottleneck. While prior work suggests that only a subset of tokens significantly influence downstream predictions, most token selection methods are static, modality-specific, or incompatible with autoregressive generation. In this paper, we propose QuickMerge, a lightweight token merging framework designed for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention norm magnitude, guided by an entropy-based budget estimator. To preserve autoregressive compatibility, we introduce a lightweight transformer prior trained over the merged token sequence. By combining semantic salience estimation, flexible token budgets, and AR alignment, QuickMerge enables accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge reduces token counts sustantially while matching as well as exceeding the performance of learned tokenizers and fixed-patch baselines.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI sustains higher strategic tension than humans in chess</title>
<link>https://arxiv.org/abs/2508.13213</link>
<guid>https://arxiv.org/abs/2508.13213</guid>
<content:encoded><![CDATA[
<div> Keywords: strategic decision-making, chess, network-based metric, AI players, human players 

Summary: 
The study explores strategic decision-making in chess by comparing human vs. human games and AI vs. AI games. A network-based metric is introduced to measure strategic tension on the board, with results showing that highly competitive AI players maintain higher levels of tension for longer periods compared to elite human players. The level of strategic tension varies with algorithmic complexity, with a notable increase in tension at around 1600 Elo and 2300 Elo in both AI and human-played games. Competitive AI players exhibit a balanced approach to maintaining tension between offensive and defensive tactics, while human players tend to limit tension and game complexity. This may reflect cognitive limitations and adaptive strategies in human players. The findings highlight differences in strategic approaches between AI and human players, with potential implications for the use of AI in complex strategic environments. 

<br /><br />Summary: <div>
arXiv:2508.13213v1 Announce Type: new 
Abstract: Strategic decision-making involves managing the tension between immediate opportunities and long-term objectives. We study this trade-off in chess by characterizing and comparing dynamics between human vs human and AI vs AI games. We propose a network-based metric of piece-to-piece interaction to quantify the ongoing strategic tension on the board. Its evolution in games reveals that the most competitive AI players sustain higher levels of strategic tension for longer durations than elite human players. Cumulative tension varies with algorithmic complexity for AI and correspondingly in human-played games increases abruptly with expertise at about 1600 Elo and again at 2300 Elo. The profiles reveal different approaches. Highly competitive AI tolerates interconnected positions balanced between offensive and defensive tactics over long periods. Human play, in contrast, limits tension and game complexity, which may reflect cognitive limitations and adaptive strategies. The difference may have implications for AI usage in complex, strategic environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information</title>
<link>https://arxiv.org/abs/2508.13250</link>
<guid>https://arxiv.org/abs/2508.13250</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, memory mechanisms, multi-hop reasoning, personalized information, dataset construction

Summary: 
This study introduces the multi-hop personalized reasoning task to investigate memory mechanisms' performance in multi-hop reasoning using personalized information. A dataset and evaluation framework were created for this task, and various explicit and implicit memory methods were implemented and tested. The study evaluates these methods from different perspectives and explores hybrid approaches, culminating in the development of the HybridMem model. Comprehensive experiments demonstrate the effectiveness of the proposed model in handling complex tasks that require multi-hop reasoning over personalized information. The research project, including the dataset and code implementation, is made publicly available to benefit the research community. <div>
arXiv:2508.13250v1 Announce Type: new 
Abstract: In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at https://github.com/nuster1128/MPR.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"DIVE" into Hydrogen Storage Materials Discovery with AI Agents</title>
<link>https://arxiv.org/abs/2508.13251</link>
<guid>https://arxiv.org/abs/2508.13251</guid>
<content:encoded><![CDATA[
<div> data-driven artificial intelligence, materials discovery, solid-state hydrogen storage materials, DIVE, automated materials design <br />
<br />
Summary: 
The article discusses the use of data-driven artificial intelligence approaches in discovering new materials, focusing on solid-state hydrogen storage materials. The Descriptive Interpretation of Visual Expression (DIVE) multi-agent workflow is introduced to systematically extract and organize experimental data from graphical elements in scientific literature. DIVE significantly improves the accuracy and coverage of data extraction compared to other models, leading to gains of 10-15% over commercial models and over 30% relative to open-source models. A curated database of over 30,000 entries from 4,000 publications is used to demonstrate a rapid inverse design workflow capable of identifying new hydrogen storage compositions in just two minutes. The proposed AI workflow and agent design can be applied to a wide range of materials, offering a new paradigm for AI-driven materials discovery. 
<br /> <div>
arXiv:2508.13251v1 Announce Type: new 
Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally transforming the discovery of new materials. Despite the unprecedented availability of materials data in the scientific literature, much of this information remains trapped in unstructured figures and tables, hindering the construction of large language model (LLM)-based AI agent for automated materials design. Here, we present the Descriptive Interpretation of Visual Expression (DIVE) multi-agent workflow, which systematically reads and organizes experimental data from graphical elements in scientific literatures. We focus on solid-state hydrogen storage materials-a class of materials central to future clean-energy technologies and demonstrate that DIVE markedly improves the accuracy and coverage of data extraction compared to the direct extraction by multimodal models, with gains of 10-15% over commercial models and over 30% relative to open-source models. Building on a curated database of over 30,000 entries from 4,000 publications, we establish a rapid inverse design workflow capable of identifying previously unreported hydrogen storage compositions in two minutes. The proposed AI workflow and agent design are broadly transferable across diverse materials, providing a paradigm for AI-driven materials discovery.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support</title>
<link>https://arxiv.org/abs/2508.13256</link>
<guid>https://arxiv.org/abs/2508.13256</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiovascular diseases, artificial intelligence, multimodal framework, adaptive reasoning, clinical care

Summary:
Cardiovascular diseases (CVDs) are a major cause of mortality, exacerbated by a lack of healthcare workers. Artificial intelligence (AI) shows promise in early detection and screening, but faces limitations in clinical application. A new multimodal framework, CardAIc-Agents, aims to address these challenges. It enables adaptive reasoning for diverse cardiac tasks, integrates external tools for decision-making, and allows for continuous learning. The framework includes a CardiacRAG agent for generating plans based on updated cardiac knowledge and a chief agent for executing these plans autonomously. A stepwise update strategy refines plans based on execution results, with a multidisciplinary discussion tool for interpreting complex cases. Visual review panels assist in final validation. Experiments demonstrate the efficiency of CardAIc-Agents compared to mainstream models. The framework enhances AI capabilities in cardiovascular care by supporting adaptive decision-making and personalized treatment. 

<br /><br />Summary: <div>
arXiv:2508.13256v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality worldwide, a burden worsened by a severe deficit of healthcare workers. Artificial intelligence (AI) agents have shown potential to alleviate this gap via automated early detection and proactive screening, yet their clinical application remains limited by: 1) prompt-based clinical role assignment that relies on intrinsic model capabilities without domain-specific tool support; or 2) rigid sequential workflows, whereas clinical care often requires adaptive reasoning that orders specific tests and, based on their results, guides personalised next steps; 3) general and static knowledge bases without continuous learning capability; and 4) fixed unimodal or bimodal inputs and lack of on-demand visual outputs when further clarification is needed. In response, a multimodal framework, CardAIc-Agents, was proposed to augment models with external tools and adaptively support diverse cardiac tasks. Specifically, a CardiacRAG agent generated general plans from updatable cardiac knowledge, while the chief agent integrated tools to autonomously execute these plans and deliver decisions. To enable adaptive and case-specific customization, a stepwise update strategy was proposed to dynamically refine plans based on preceding execution results, once the task was assessed as complex. In addition, a multidisciplinary discussion tool was introduced to interpret challenging cases, thereby supporting further adaptation. When clinicians raised concerns, visual review panels were provided to assist final validation. Experiments across three datasets showed the efficiency of CardAIc-Agents compared to mainstream Vision-Language Models (VLMs), state-of-the-art agentic systems, and fine-tuned VLMs.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention</title>
<link>https://arxiv.org/abs/2508.13327</link>
<guid>https://arxiv.org/abs/2508.13327</guid>
<content:encoded><![CDATA[
<div> Keywords: STONK, Stock optimization, News knowledge, Sentiment analysis, Multimodal framework

Summary: 
STONK (Stock Optimization using News Knowledge) is a new multimodal framework that enhances daily stock movement prediction by integrating numerical market indicators with sentiment-enriched news embeddings. Through the combination of numerical and textual embeddings using feature concatenation and cross-modal attention, STONK addresses the limitations of isolated analyses and outperforms numeric-only baselines in backtesting. The framework offers evidence-based guidance for scalable multimodal financial forecasting through a comprehensive evaluation of fusion strategies and model configurations. The availability of the source code on GitHub allows for transparency and further collaboration in the financial forecasting community.<br /><br />Summary: <div>
arXiv:2508.13327v1 Announce Type: new 
Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal framework integrating numerical market indicators with sentiment-enriched news embeddings to improve daily stock-movement prediction. By combining numerical & textual embeddings via feature concatenation and cross-modal attention, our unified pipeline addresses limitations of isolated analyses. Backtesting shows STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion strategies and model configurations offers evidence-based guidance for scalable multimodal financial forecasting. Source code is available on GitHub
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2508.13333</link>
<guid>https://arxiv.org/abs/2508.13333</guid>
<content:encoded><![CDATA[
<div> Framework, LLM, Evolutionary Computation, Heuristic Design, HiFo-Prompt
Summary:
HiFo-Prompt introduces two guiding strategies, Foresight and Hindsight, to enhance LLM-based Automatic Heuristic Design within Evolutionary Computation frameworks. The Foresight strategy adapts the search based on population dynamics, balancing exploration and exploitation. The Hindsight strategy distills successful heuristics into reusable design principles, creating a persistent knowledge base for the LLM. This dual mechanism improves heuristic quality, convergence speed, and query efficiency compared to existing methods. Empirical results demonstrate HiFo-Prompt's superior performance in generating high-quality heuristics. <br /><br />Summary: <div>
arXiv:2508.13333v1 Announce Type: new 
Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation (EC) frameworks has shown promising results. However, its effectiveness is hindered by the use of static operators and the lack of knowledge accumulation mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two synergistic prompting strategies: Foresight and Hindsight. Foresight-based prompts adaptively steer the search based on population dynamics, managing the exploration-exploitation trade-off. In addition, hindsight-based prompts mimic human expertise by distilling successful heuristics from past generations into fundamental, reusable design principles. This dual mechanism transforms transient discoveries into a persistent knowledge base, enabling the LLM to learn from its own experience. Empirical results demonstrate that HiFo-Prompt significantly outperforms state-of-the-art LLM-based AHD methods, generating higher-quality heuristics while achieving substantially faster convergence and superior query efficiency.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems</title>
<link>https://arxiv.org/abs/2508.13371</link>
<guid>https://arxiv.org/abs/2508.13371</guid>
<content:encoded><![CDATA[
<div> neural planning, autonomous systems, neuro-symbolic approaches, LOOP framework, iterative conversation<br />
Summary:<br />
The article introduces a new neuro-symbolic planning framework called LOOP, which approaches planning as an iterative conversation between neural and symbolic components. It integrates various neural features such as graph neural networks and multi-agent validation to improve the accuracy and reliability of plans. LOOP generates PDDL specifications, refines them based on symbolic feedback, and builds a causal knowledge base from execution traces. Evaluation on standard benchmark domains showed LOOP outperforming other approaches with an 85.8% success rate. The framework emphasizes the importance of neural and symbolic components working together rather than simply translating between them. This approach offers a blueprint for building trustworthy autonomous systems that can handle critical real-world applications. <br />Summary: <div>
arXiv:2508.13371v1 Announce Type: new 
Abstract: Planning is one of the most critical tasks in autonomous systems, where even a small error can lead to major failures or million-dollar losses. Current state-of-the-art neural planning approaches struggle with complex domains, producing plans with missing preconditions, inconsistent goals, and hallucinations. While classical planners provide logical guarantees, they lack the flexibility and natural language understanding capabilities needed for modern autonomous systems. Existing neuro-symbolic approaches use one-shot translation from natural language to formal plans, missing the opportunity for neural and symbolic components to work and refine solutions together. To address this gap, we develop LOOP -- a novel neuro-symbolic planning framework that treats planning as an iterative conversation between neural and symbolic components rather than simple translation. LOOP integrates 13 coordinated neural features including graph neural networks for spatial relationships, multi-agent validation for consensus-based correctness, hierarchical decomposition for complex task management, and causal memory that learns from both successes and failures. Unlike existing approaches, LOOP generates PDDL specifications, refines them iteratively based on symbolic feedback, and builds a causal knowledge base from execution traces. LOOP was evaluated on six standard IPC benchmark domains, where it achieved 85.8% success rate compared to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This work shows that the key to reliable planning is not in choosing between neural networks or symbolic reasoners but it lies in making them actually ``talk'' to each other during the entire process. LOOP provides a thorough blueprint for building autonomous systems that can finally be trusted with critical real-world applications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPANER: Shared Prompt Aligner for Multimodal Semantic Representation</title>
<link>https://arxiv.org/abs/2508.13387</link>
<guid>https://arxiv.org/abs/2508.13387</guid>
<content:encoded><![CDATA[
<div> semantic space, multimodal embedding, few-shot retrieval, shared prompt mechanism, modality-agnostic

Summary:<br />
The article introduces the Shared Prompt AligNER (SPANER) framework for Parameter-Efficient Fine-Tuning (PEFT) in multimodal tasks. SPANER aims to unify diverse modalities into a common semantic space by utilizing a shared prompt mechanism. This design allows semantically related instances to come together spatially, promoting cross-modal generalization. SPANER is flexible and can easily accommodate additional modalities such as audio without significant changes to the core architecture. Experimental results across vision-language and audio-visual benchmarks show that SPANER achieves competitive few-shot retrieval performance while maintaining high semantic coherence in the embedding space. The study emphasizes the importance of aligning embedding structures for effective multimodal learning, beyond simply adjusting adapter weights. <div>
arXiv:2508.13387v1 Announce Type: new 
Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have significantly improved performance on downstream tasks such as few-shot retrieval. However, most existing approaches focus on task-specific gains while neglecting the structure of the multimodal embedding space. As a result, modality-specific representations often remain isolated, limiting cross-modal generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a modality-agnostic PEFT framework designed to embed inputs from diverse modalities into a unified semantic space. At its core, SPANER employs a shared prompt mechanism that acts as a conceptual anchor, enabling semantically related instances to converge spatially regardless of modality. This shared prompt design is inherently extensible, supporting the seamless integration of additional modalities, such as audio, without altering the core architecture. Through comprehensive experiments across vision-language and audio-visual benchmarks, SPANER demonstrates competitive few-shot retrieval performance while preserving high semantic coherence in the learned embedding space. Our results highlight the importance of aligning embedding structures, rather than merely tuning adapter weights, for scalable multimodal learning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TASER: Table Agents for Schema-guided Extraction and Recommendation</title>
<link>https://arxiv.org/abs/2508.13404</link>
<guid>https://arxiv.org/abs/2508.13404</guid>
<content:encoded><![CDATA[
<div> extract, financial documents, table extraction, TASER, schema-guided<br />
Summary:<br />
The article introduces a new table extraction system called TASER designed to extract information from complex financial documents. TASER uses agentic agents to detect, classify, and extract tables, along with a Recommender Agent to review outputs and make schema recommendations for better extraction accuracy. TASER outperforms existing models by 10.1% and benefits from continuous learning, with larger batch sizes resulting in more actionable schema recommendations and increased extracted holdings. The system was trained on a dataset of real financial tables and holdings, culminating in the release of the TASERTab dataset for research use. Overall, TASER demonstrates the effectiveness of agentic, schema-guided extraction systems for understanding and extracting information from real-world financial tables.<br /><br />Summary: <div>
arXiv:2508.13404v1 Announce Type: new 
Abstract: Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtuous Machines: Towards Artificial General Science</title>
<link>https://arxiv.org/abs/2508.13421</link>
<guid>https://arxiv.org/abs/2508.13421</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, scientific discovery, autonomous research, cognitive experiments, embodied AI<br />
Summary:<br />
Artificial intelligence systems are revolutionizing research by accelerating specific tasks and expanding into more general domains. This study presents an agentic AI system capable of independently navigating the scientific process, conducting psychological research, data collection, and manuscript preparation autonomously. The system demonstrated the ability to conduct non-trivial research with theoretical reasoning and methodological rigor comparable to experienced researchers. However, limitations in conceptual nuance and theoretical interpretation were noted. This development signifies a significant step toward embodied AI conducting real-world experiments to accelerate scientific discovery. It also prompts discussions on scientific understanding and the attribution of credit in research. <div>
arXiv:2508.13421v1 Announce Type: new 
Abstract: Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting</title>
<link>https://arxiv.org/abs/2508.13433</link>
<guid>https://arxiv.org/abs/2508.13433</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, spatio-temporal traffic forecasting, STPFormer, temporal encoding, spatial structures, space-time fusion<br />
<br />
STPFormer is introduced as a novel approach to spatio-temporal traffic forecasting, addressing challenges such as complex temporal patterns and dynamic spatial structures. It utilizes a Spatio-Temporal Pattern-Aware Transformer that incorporates modules for pattern-aware temporal encoding, sequential spatial learning, cross-domain alignment, and multi-scale fusion. Experimental results on real-world datasets demonstrate that STPFormer consistently outperforms existing methods, showcasing its effectiveness and generalizability. The model integrates temporal and spatial information effectively and offers interpretable representation learning. Additionally, visualizations and ablation studies confirm the model's performance improvements and highlight the importance of its components in achieving state-of-the-art results.<br /><br />Summary: <div>
arXiv:2508.13433v1 Announce Type: new 
Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal patterns, dynamic spatial structures, and diverse input formats. Although Transformer-based models offer strong global modeling, they often struggle with rigid temporal encoding and weak space-time fusion. We propose STPFormer, a Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art performance via unified and interpretable representation learning. It integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment, and an Attention Mixer for multi-scale fusion. Experiments on five real-world datasets show that STPFormer consistently sets new SOTA results, with ablation and visualizations confirming its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences</title>
<link>https://arxiv.org/abs/2508.13437</link>
<guid>https://arxiv.org/abs/2508.13437</guid>
<content:encoded><![CDATA[
<div> GPU-accelerated heuristic, Discrete Min-Max Violation (DMMV), optimization problem, worst-case performance, versatile applicability <br />
<br />
Summary: 
In this paper, the authors introduce the Discrete Min-Max Violation (DMMV) as a general optimization problem aiming to minimize the largest constraint violation. They mathematically define the problem and explore its properties, developing a GPU-accelerated heuristic to solve practical instances efficiently. The heuristic is applied to three use cases: post-training quantization of language models, discrete tomography, and FIR filter design. Results show significant improvements in each case, with 14% improvement in quantization, 16% reduction in reconstruction error for tomography, and 50% ripple reduction for FIR filter design compared to existing methods. The GPU-accelerated heuristic accelerates computations by a factor of 6 and will be made open-source to encourage further research on DMMV and its applications. This study highlights the benefits of addressing DMMV as a context-free optimization problem and the effectiveness of the proposed heuristic in solving various optimization problems. <div>
arXiv:2508.13437v1 Announce Type: new 
Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization problem which seeks an assignment of discrete values to variables that minimizes the largest constraint violation. This context-free mathematical formulation is applicable to a wide range of use cases that have worst-case performance requirements. After defining the DMMV problem mathematically, we explore its properties to establish a foundational understanding. To tackle DMMV instance sizes of practical relevance, we develop a GPU-accelerated heuristic that takes advantage of the mathematical properties of DMMV for speeding up the solution process. We demonstrate the versatile applicability of our heuristic by solving three optimization problems as use cases: (1) post-training quantization of language models, (2) discrete tomography, and (3) Finite Impulse Response (FIR) filter design. In quantization without outlier separation, our heuristic achieves 14% improvement on average over existing methods. In discrete tomography, it reduces reconstruction error by 16% under uniform noise and accelerates computations by a factor of 6 on GPU. For FIR filter design, it nearly achieves 50% ripple reduction compared to using the commercial integer optimization solver, Gurobi. Our comparative results point to the benefits of studying DMMV as a context-free optimization problem and the advantages that our proposed heuristic offers on three distinct problems. Our GPU-accelerated heuristic will be made open-source to further stimulate research on DMMV and its other applications. The code is available at https://anonymous.4open.science/r/AMVM-5F3E/
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM Agents May Fail to Act on Their Own Risk Knowledge</title>
<link>https://arxiv.org/abs/2508.13465</link>
<guid>https://arxiv.org/abs/2508.13465</guid>
<content:encoded><![CDATA[
<div> Framework, evaluation, language model agents, risk awareness, safety execution <br />
<br />
Summary: <br />
Language model (LM) agents show potential for automating tasks but lack risk awareness and safety execution abilities. A new framework evaluates agents across three dimensions: risk knowledge, identifying risks in scenarios, and avoiding risky actions. While agents excel in risk knowledge, they struggle with applying it in actual scenarios and often still perform risky actions. Scaling model capabilities or inference compute does not solve safety concerns. A risk verifier critiques agent actions, with an abstractor translating execution trajectories into abstract descriptions for better risk identification. The system reduces risky actions by 55.3% compared to conventional prompting methods. <div>
arXiv:2508.13465v1 Announce Type: new 
Abstract: Language model (LM) agents have demonstrated significant potential for automating real-world tasks, yet they pose a diverse array of potential, severe risks in safety-critical scenarios. In this work, we identify a significant gap between LM agents' risk awareness and safety execution abilities: while they often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?", they will likely fail to identify such risks in instantiated trajectories or even directly perform these risky actions when acting as agents. To systematically investigate this, we develop a comprehensive evaluation framework to examine agents' safety across three progressive dimensions: 1) their knowledge about potential risks, 2) their ability to identify corresponding risks in execution trajectories, and 3) their actual behaviors to avoid executing these risky actions. Our evaluation reveals two critical performance gaps that resemble the generator-validator gaps observed in LMs: while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they fail to apply this knowledge when identifying risks in actual scenarios (with performance dropping by $>23\%$) and often still execute risky actions ($<26\%$ pass rates). Notably, this trend persists across more capable LMs as well as in specialized reasoning models like DeepSeek-R1, indicating that simply scaling model capabilities or inference compute does not inherently resolve safety concerns. Instead, we take advantage of these observed gaps to develop a risk verifier that independently critiques the proposed actions by agents, with an abstractor that converts specific execution trajectories into abstract descriptions where LMs can more effectively identify the risks. Our overall system achieves a significant reduction of risky action execution by $55.3\%$ over vanilla-prompted agents.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter</title>
<link>https://arxiv.org/abs/2508.13530</link>
<guid>https://arxiv.org/abs/2508.13530</guid>
<content:encoded><![CDATA[
<div> Keywords: embodied agents, Minecraft, Crafter, foundation models, prototyping

Summary:
CrafterDojo presents a suite of foundation models and tools that leverage the lightweight environment of Crafter as a prototyping-friendly testbed for general-purpose embodied agent research. The introduction of CrafterVPT, CrafterCLIP, and CrafterSteve-1 for behavior priors, vision-language grounding, and instruction following, respectively, aims to unlock the full potential of Crafter for research purposes. Additionally, CrafterDojo offers toolkits like CrafterPlay and CrafterCaption for generating behavior and caption datasets, reference agent implementations, benchmark evaluations, and a comprehensive open-source codebase. By addressing the limitations of Crafter and providing essential tools and models, CrafterDojo broadens the scope and accessibility of research in embodied agent development, making it easier for researchers to experiment and innovate in this domain.<br /><br />Summary: <div>
arXiv:2508.13530v1 Announce Type: new 
Abstract: Developing general-purpose embodied agents is a core challenge in AI. Minecraft provides rich complexity and internet-scale data, but its slow speed and engineering overhead make it unsuitable for rapid prototyping. Crafter offers a lightweight alternative that retains key challenges from Minecraft, yet its use has remained limited to narrow tasks due to the absence of foundation models that have driven progress in the Minecraft setting. In this paper, we present CrafterDojo, a suite of foundation models and tools that unlock the Crafter environment as a lightweight, prototyping-friendly, and Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for behavior priors, vision-language grounding, and instruction following, respectively. In addition, we provide toolkits for generating behavior and caption datasets (CrafterPlay and CrafterCaption), reference agent implementations, benchmark evaluations, and a complete open-source codebase.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance</title>
<link>https://arxiv.org/abs/2508.13579</link>
<guid>https://arxiv.org/abs/2508.13579</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, electronic health records, EAG-RL, expert attention guidance, reinforcement learning

Summary: 
The article introduces a novel framework, EAG-RL, to enhance large language models (LLMs) for electronic health record (EHR) reasoning. EAG-RL first constructs reasoning trajectories using expert guidance and then utilizes reinforcement learning to optimize the LLM's policy. This approach improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62% and enhances robustness to feature perturbations and generalization to unseen clinical domains. By aligning the LLM's attention with clinically salient features identified by expert EHR models, EAG-RL addresses the limitations of existing approaches that rely on hybrid paradigms. The experimental results on real-world EHR datasets demonstrate the practical potential of EAG-RL for clinical prediction tasks.<br /><br />Summary: <div>
arXiv:2508.13579v1 Announce Type: new 
Abstract: Improving large language models (LLMs) for electronic health record (EHR) reasoning is essential for enabling accurate and generalizable clinical predictions. While LLMs excel at medical text understanding, they underperform on EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches often rely on hybrid paradigms, where LLMs serve merely as frozen prior retrievers while downstream deep learning (DL) models handle prediction, failing to improve the LLM's intrinsic reasoning capacity and inheriting the generalization limitations of DL models. To this end, we propose EAG-RL, a novel two-stage training framework designed to intrinsically enhance LLMs' EHR reasoning ability through expert attention guidance, where expert EHR models refer to task-specific DL models trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise reasoning trajectories using expert-guided Monte Carlo Tree Search to effectively initialize the LLM's policy. Then, EAG-RL further optimizes the policy via reinforcement learning by aligning the LLM's attention with clinically salient features identified by expert EHR models. Extensive experiments on two real-world EHR datasets show that EAG-RL improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also enhancing robustness to feature perturbations and generalization to unseen clinical domains. These results demonstrate the practical potential of EAG-RL for real-world deployment in clinical prediction tasks. Our code have been available at https://github.com/devilran6/EAG-RL.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation</title>
<link>https://arxiv.org/abs/2508.13587</link>
<guid>https://arxiv.org/abs/2508.13587</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, chart-to-code generation, multimodal structured reinforcement learning, structured reward system, training stability 

Summary:
Multimodal Structured Reinforcement Learning (MSRL) is proposed for chart-to-code generation to overcome the performance plateau in supervised fine-tuning (SFT). The study highlights the challenges in generating structured outputs for tasks requiring in-depth understanding of information-rich images. Large-scale experiments show that SFT reaches a plateau where further data scaling yields marginal improvements. The MSRL method incorporates a structured reward system at multi-granularity levels, leveraging textual and visual feedback for training stability. Rule-based rewards validate fine-grained code details, while model-based rewards assess structural similarity by rendering generated code into images. By implementing a two-stage training curriculum, MSRL achieves significant performance improvements, breaking the SFT plateau and enhancing high-level metrics on ChartMimic and ReachQA benchmarks. This approach showcases competitive performance with advanced closed-source models in chart-to-code generation tasks. 

<br /><br />Summary: <div>
arXiv:2508.13587v1 Announce Type: new 
Abstract: While reinforcement learning (RL) has proven highly effective for general reasoning in vision-language models, its application to tasks requiring in-depth understanding of information-rich images and generation of structured outputs remains underexplored. Chart-to-code generation exemplifies this challenge, demanding complex reasoning over visual charts to generate structured code. Supervised fine-tuning (SFT) alone is often insufficient, highlighting the need for effective RL strategies that appropriately reward structured outputs. We systematically investigate the performance plateau in SFT through large-scale experiments and propose Multimodal Structured Reinforcement Learning (MSRL) for chart-to-code generation, which substantially breaks through this plateau. We construct the largest training corpus to date, containing 3 million chart-code pairs from real-world arXiv tables to mitigate simplistic patterns of prior synthetic data. Despite reaching state-of-the-art performance, our experiments show that scaling SFT data eventually hits a plateau where further increases yield negligible improvements. Our MSRL method leverages a multi-granularity structured reward system using multimodal textual and visual feedback. At the textual level, rule-based rewards validate fine-grained code details. At the visual level, model-based rewards assess structural similarity by rendering generated code into images and employing an evaluator model. We implement this within a two-stage curriculum for training stability. Results demonstrate that MSRL significantly breaks the SFT plateau, improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA benchmarks respectively, achieving competitive performance with advanced closed-source models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task</title>
<link>https://arxiv.org/abs/2508.13634</link>
<guid>https://arxiv.org/abs/2508.13634</guid>
<content:encoded><![CDATA[
<div> attention mechanisms, GUI localization, Fitts' Law, 2D Gaussian heatmaps, UI elements <br />
Summary: 
The paper introduces the Valley-to-Peak (V2P) method for precise GUI localization. V2P addresses issues faced by traditional methods and previous attention-based approaches. It includes a suppression attention mechanism to minimize distractions from background regions and focuses on the intended area. Additionally, V2P uses a Fitts' Law-inspired approach to model GUI interactions as 2D Gaussian heatmaps, effectively distinguishing between center and edges of UI elements. By gradually decreasing weight from the center towards the edges following a Gaussian distribution, V2P teaches the model to concentrate on the most critical point of the UI element. Experimental results show that the model trained with V2P achieves high performance on two benchmarks, ScreenSpot-v2, and ScreenSpot-Pro. Ablation studies confirm the effectiveness of each component, demonstrating V2P's generalizability for precise GUI grounding tasks. <br /><br />Summary: <div>
arXiv:2508.13634v1 Announce Type: new 
Abstract: Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform labeling fails to distinguish between center and edges of the target UI element, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.3% and 50.5% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, highlighting V2P's generalizability for precise GUI grounding tasks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints</title>
<link>https://arxiv.org/abs/2508.13663</link>
<guid>https://arxiv.org/abs/2508.13663</guid>
<content:encoded><![CDATA[
<div> query answering, incomplete knowledge graphs, soft constraints, neural query reranker, first-order logic

Summary:
Methods for query answering over incomplete knowledge graphs focus on retrieving likely answers that cannot be reached by direct graph traversal. This new approach introduces the concept of query answering with soft constraints, addressing the vague or context-dependent nature of real-world queries. The Neural Query Reranker (NQR) is designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers. NQR operates interactively, refining answers based on incremental examples of preferred and non-preferred entities. Experimental results show that NQR effectively captures soft constraints while maintaining robust query answering performance. The extension of QA benchmarks to include datasets with soft constraints contributes to the practical application of this novel approach. <div>
arXiv:2508.13663v1 Announce Type: new 
Abstract: Methods for query answering over incomplete knowledge graphs retrieve entities that are likely to be answers, which is particularly useful when such answers cannot be reached by direct graph traversal due to missing edges. However, existing approaches have focused on queries formalized using first-order-logic. In practice, many real-world queries involve constraints that are inherently vague or context-dependent, such as preferences for attributes or related categories. Addressing this gap, we introduce the problem of query answering with soft constraints. We propose a Neural Query Reranker (NQR) designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers to a query. NQR operates interactively, refining answers based on incremental examples of preferred and non-preferred entities. We extend existing QA benchmarks by generating datasets with soft constraints. Our experiments demonstrate that NQR can capture soft constraints while maintaining robust query answering performance.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings</title>
<link>https://arxiv.org/abs/2508.13672</link>
<guid>https://arxiv.org/abs/2508.13672</guid>
<content:encoded><![CDATA[
<div> Instance-based Transfer Learning LIME, explanation fidelity, stability, data-constrained environments, surrogate model<br />
Summary:<br />
The study introduces Instance-based Transfer Learning LIME (ITL-LIME) to improve the interpretability of black-box machine learning models in scenarios with limited training data. ITL-LIME utilizes instance transfer learning by incorporating relevant instances from a related source domain to enhance explanation accuracy. Clustering is employed to partition the source domain and retrieve suitable instances for explanation in the target domain. Instead of random perturbations, pertinent source instances are selected based on similarity to the target instance and combined with its neighbors. A contrastive learning-based encoder determines weights for the instances, considering their proximity to the target instance. These weighted instances are used to train the surrogate model for generating accurate explanations. ITL-LIME addresses the challenges of locality and instability in LIME, providing a more reliable and stable explanation framework in data-constrained environments.<br /> <div>
arXiv:2508.13672v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local Interpretable Model-Agnostic Explanations (LIME), have advanced the interpretability of black-box machine learning models by approximating their behavior locally using interpretable surrogate models. However, LIME's inherent randomness in perturbation and sampling can lead to locality and instability issues, especially in scenarios with limited training data. In such cases, data scarcity can result in the generation of unrealistic variations and samples that deviate from the true data manifold. Consequently, the surrogate model may fail to accurately approximate the complex decision boundary of the original model. To address these challenges, we propose a novel Instance-based Transfer Learning LIME framework (ITL-LIME) that enhances explanation fidelity and stability in data-constrained environments. ITL-LIME introduces instance transfer learning into the LIME framework by leveraging relevant real instances from a related source domain to aid the explanation process in the target domain. Specifically, we employ clustering to partition the source domain into clusters with representative prototypes. Instead of generating random perturbations, our method retrieves pertinent real source instances from the source cluster whose prototype is most similar to the target instance. These are then combined with the target instance's neighboring real instances. To define a compact locality, we further construct a contrastive learning-based encoder as a weighting mechanism to assign weights to the instances from the combined set based on their proximity to the target instance. Finally, these weighted source and target instances are used to train the surrogate model for explanation purposes.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks</title>
<link>https://arxiv.org/abs/2508.13675</link>
<guid>https://arxiv.org/abs/2508.13675</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, household actions, household robots, video footage, link prediction <br />
Summary: <br />
This paper explores the use of Knowledge Graphs for describing household actions to control household robots and analyze video footage. The incomplete information extracted from videos necessitates completing the Knowledge Graph to enhance understanding. The study reveals that standard link prediction algorithms struggle with the unique characteristics of situational knowledge graphs, unable to outperform basic baselines. This highlights the importance of tailored approaches for effectively predicting links in situational knowledge graphs. <div>
arXiv:2508.13675v1 Announce Type: new 
Abstract: Knowledge Graphs are used for various purposes, including business applications, biomedical analyses, or digital twins in industry 4.0. In this paper, we investigate knowledge graphs describing household actions, which are beneficial for controlling household robots and analyzing video footage. In the latter case, the information extracted from videos is notoriously incomplete, and completing the knowledge graph for enhancing the situational picture is essential. In this paper, we show that, while a standard link prediction problem, situational knowledge graphs have special characteristics that render many link prediction algorithms not fit for the job, and unable to outperform even simple baselines.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model</title>
<link>https://arxiv.org/abs/2508.13676</link>
<guid>https://arxiv.org/abs/2508.13676</guid>
<content:encoded><![CDATA[
<div> Keywords: recruitment, talent pool, resume, duplication detection, MHSNet

Summary: <br /><br />Recruiters often search for resumes on third-party websites to maintain a company's talent pool, but these resumes are typically incomplete and inaccurate. To improve the quality of third-party resumes, duplication detection is necessary. The complexity of resume texts, such as semantic intricacies and structural heterogeneity, poses a challenge for this task. In response, MHSNet proposes a multi-level identity verification framework that utilizes contrastive learning to fine-tune BGE-M3. Through a Mixture-of-Experts (MoE) approach, MHSNet generates sparse and dense representations for resumes, enabling the computation of multi-level semantic similarities. The utilization of a state-aware MoE helps handle diverse incomplete resumes effectively. Experimental results demonstrate the efficacy of MHSNet in enhancing the quality of third-party resumes and enriching the company's talent pool. <div>
arXiv:2508.13676v1 Announce Type: new 
Abstract: To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2508.13678</link>
<guid>https://arxiv.org/abs/2508.13678</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning capabilities, neuro-symbolic approaches, Artificial General Intelligence, GitHub repository

Summary:
Large Language Models (LLMs) have shown promise in various tasks, but their reasoning abilities are still a challenge. Enhancing LLM reasoning is crucial for the development of Artificial General Intelligence (AGI). Neuro-symbolic approaches are seen as a promising method to improve LLM reasoning. This paper reviews recent advancements in neuro-symbolic approaches for enhancing LLM reasoning. It formalizes reasoning tasks and introduces the neuro-symbolic learning paradigm. The discussion covers methods for improving LLM reasoning from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. The paper also addresses key challenges and proposes future research directions. A GitHub repository with related papers and resources is available for further exploration at https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy. 

Summary:<br /><br />Keywords: Large Language Models, reasoning capabilities, neuro-symbolic approaches, Artificial General Intelligence, GitHub repository<br />Large Language Models (LLMs) have shown promise in various tasks, but their reasoning abilities are still a challenge. Enhancing LLM reasoning is crucial for the development of Artificial General Intelligence (AGI). Neuro-symbolic approaches are seen as a promising method to improve LLM reasoning. This paper reviews recent advancements in neuro-symbolic approaches for enhancing LLM reasoning. It formalizes reasoning tasks and introduces the neuro-symbolic learning paradigm. The discussion covers methods for improving LLM reasoning from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. The paper also addresses key challenges and proposes future research directions. A GitHub repository with related papers and resources is available for further exploration at https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy. <div>
arXiv:2508.13678v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The DeepLog Neurosymbolic Machine</title>
<link>https://arxiv.org/abs/2508.13697</link>
<guid>https://arxiv.org/abs/2508.13697</guid>
<content:encoded><![CDATA[
<div> Keywords: neurosymbolic AI, DeepLog, neural extension, algebraic circuits, GPU-based implementation

Summary:
DeepLog introduces a theoretical framework for neurosymbolic AI, combining neural networks and symbolic reasoning. It consists of two key components: the DeepLog language for specifying neurosymbolic models and inference tasks, and extended algebraic circuits for computational graphs. The DeepLog language is an annotated neural extension of grounded first-order logic, accommodating various types of logic and architecture configurations. The computational level utilizes algebraic circuits for efficient processing implemented on GPUs. DeepLog offers flexibility in designing neurosymbolic models by enabling different choices for underlying structures and logics. Experimental comparisons demonstrated the superiority of DeepLog in terms of efficiency and generality, showcasing the benefits of GPU-accelerated implementations and the versatility of using different logics within neurosymbolic AI systems. 
<br /><br />Summary: <div>
arXiv:2508.13697v1 Announce Type: new 
Abstract: We contribute a theoretical and operational framework for neurosymbolic AI called DeepLog. DeepLog introduces building blocks and primitives for neurosymbolic AI that make abstraction of commonly used representations and computational mechanisms used in neurosymbolic AI. DeepLog can represent and emulate a wide range of neurosymbolic systems. It consists of two key components. The first is the DeepLog language for specifying neurosymbolic models and inference tasks. This language consists of an annotated neural extension of grounded first-order logic, and makes abstraction of the type of logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the architecture or in the loss function. The second DeepLog component is situated at the computational level and uses extended algebraic circuits as computational graphs. Together these two components are to be considered as a neurosymbolic abstract machine, with the DeepLog language as the intermediate level of abstraction and the circuits level as the computational one. DeepLog is implemented in software, relies on the latest insights in implementing algebraic circuits on GPUs, and is declarative in that it is easy to obtain different neurosymbolic models by making different choices for the underlying algebraic structures and logics. The generality and efficiency of the DeepLog neurosymbolic machine is demonstrated through an experimental comparison between 1) different fuzzy and probabilistic logics, 2) between using logic in the architecture or in the loss function, and 3) between a standalone CPU-based implementation of a neurosymbolic AI system and a DeepLog GPU-based one.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning</title>
<link>https://arxiv.org/abs/2508.13721</link>
<guid>https://arxiv.org/abs/2508.13721</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, causal reasoning, collaboration, planning, multi-agent coordination<br />
Summary:<br />
The article introduces CausalPlan, a framework designed to improve the performance of large language model agents in collaborative tasks by integrating explicit structural causal reasoning. By incorporating a Structural Causal Action model that learns causal graphs from agent trajectories, CausalPlan guides action selection by assigning causal scores to generated proposals. This approach helps constrain planning to intervention-consistent behaviors without the need for fine-tuning the LLM itself. Evaluation on the Overcooked-AI benchmark with four LLMs of varying sizes demonstrates that CausalPlan reduces invalid actions, improves collaboration, and outperforms reinforcement learning baselines in AI-AI and human-AI settings. The findings emphasize the benefits of causality-driven planning for creating efficient, interpretable, and generalizable multi-agent LLM systems.<br /> 
Summary: <div>
arXiv:2508.13721v1 Announce Type: new 
Abstract: Large language model (LLM) agents-especially smaller, open-source models-often produce causally invalid or incoherent actions in collaborative tasks due to their reliance on surface-level correlations rather than grounded causal reasoning. This limitation undermines their performance in terms of coordination and planning in dynamic environments. We address this challenge with CausalPlan, a two-phase framework that integrates explicit structural causal reasoning into the LLM planning process. At the core of CausalPlan is the Structural Causal Action (SCA) model, which learns a causal graph from agent trajectories to capture how prior actions and current environment states influence future decisions. This structure is then used to guide action selection by assigning causal scores to LLM-generated proposals, reweighting them accordingly, or falling back to causally grounded alternatives when needed. By embedding this causal knowledge directly into the decision loop, CausalPlan constrains planning to intervention-consistent behaviours without requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the Overcooked-AI benchmark across five multi-agent coordination tasks and four LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B. Experimental results show that CausalPlan consistently reduces invalid actions and improves collaboration in both AI-AI and human-AI settings, outperforming strong reinforcement learning baselines. Our findings highlight the value of causality-driven planning for deploying efficient, interpretable, and generalisable multi-agent LLM systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making</title>
<link>https://arxiv.org/abs/2508.13754</link>
<guid>https://arxiv.org/abs/2508.13754</guid>
<content:encoded><![CDATA[
<div> framework, medical decision-making, expertise-aware, multi-agent collaboration, diagnostic performance
Summary:
The Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework addresses the complexity of Medical Decision-Making (MDM) by utilizing multiple Large Language Models (LLMs) through two stages. Firstly, an expertise table is created to identify the strengths of different LLMs for various medical departments and query difficulty levels. This allows for the dynamic selection of optimal LLMs as expert agents in the inference phase. Secondly, selected agents collaborate to generate responses with confidence scores, which are integrated through confidence fusion and adversarial validation to enhance diagnostic reliability. The EMRC framework outperforms state-of-the-art single- and multi-LLM methods on three public MDM datasets, showcasing superior diagnostic performance. For example, on the MMLU-Pro-Health dataset, EMRC achieves a 74.45% accuracy, surpassing the best-performing model GPT-4-0613 by 2.69%. The expertise-aware agent recruitment strategy and agent complementarity contribute to the success of EMRC in enhancing MDM systems. 
<br /><br />Summary: <div>
arXiv:2508.13754v1 Announce Type: new 
Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial domain-specific expertise to effectively synthesize heterogeneous and complicated clinical information. While recent advancements in Large Language Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited by their parametric knowledge constraints and static training corpora, failing to robustly integrate the clinical information. To address this challenge, we propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC) framework to enhance the accuracy and reliability of MDM systems. It operates in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and adversarial-driven multi-agent collaboration. Specifically, in the first stage, we use a publicly available corpus to construct an LLM expertise table for capturing expertise-specific strengths of multiple LLMs across medical department categories and query difficulty levels. This table enables the subsequent dynamic selection of the optimal LLMs to act as medical expert agents for each medical query during the inference phase. In the second stage, we employ selected agents to generate responses with self-assessed confidence scores, which are then integrated through the confidence fusion and adversarial validation to improve diagnostic reliability. We evaluate our EMRC framework on three public MDM datasets, where the results demonstrate that our EMRC outperforms state-of-the-art single- and multi-LLM methods, achieving superior diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC achieves 74.45% accuracy, representing a 2.69% improvement over the best-performing closed-source model GPT- 4-0613, which demonstrates the effectiveness of our expertise-aware agent recruitment strategy and the agent complementarity in leveraging each LLM's specialized capabilities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifier Instantiations: To Mimic or To Revolt?</title>
<link>https://arxiv.org/abs/2508.13811</link>
<guid>https://arxiv.org/abs/2508.13811</guid>
<content:encoded><![CDATA[
<div> instantiation, Satisfiability Modulo Theories (SMT) solvers, probabilistic context-free grammars, quantified formulas, exploration and exploitation

Summary: 
This paper introduces a novel approach for handling quantified formulas in Satisfiability Modulo Theories (SMT) solvers by dynamically learning from various instantiation techniques. The approach uses probabilistic context-free grammars to generate new terms based on observed instantiations, mimicking successful past instantiations while also exploring diversity by optionally inverting term probabilities. By balancing exploitation and exploration in quantifier reasoning, this method aims to improve the efficiency and effectiveness of solving quantified formulas in SMT solvers. <div>
arXiv:2508.13811v1 Announce Type: new 
Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo Theories (SMT) solvers due to their inherent undecidability. Existing instantiation techniques, such as e-matching, syntax-guided, model-based, conflict-based, and enumerative methods, often complement each other. This paper introduces a novel instantiation approach that dynamically learns from these techniques during solving. By treating observed instantiations as samples from a latent language, we use probabilistic context-free grammars to generate new, similar terms. Our method not only mimics successful past instantiations but also explores diversity by optionally inverting learned term probabilities, aiming to balance exploitation and exploration in quantifier reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration</title>
<link>https://arxiv.org/abs/2508.13828</link>
<guid>https://arxiv.org/abs/2508.13828</guid>
<content:encoded><![CDATA[
<div> Keyword: Retrieval-Augmented Generation, ensemble methods, information entropy, pipeline, module

Summary:
Retrieval-Augmented Generation (RAG) technology has seen widespread use in various applications. However, no single RAG framework can effectively adapt to all downstream tasks. To address this limitation, researchers have investigated ensemble methods based on multiple RAG systems. The study includes a theoretical analysis explaining the RAG ensemble framework in terms of information entropy. Mechanistic analysis explores different pipelines (Branching, Iterative, Loop, Agentic) and modules (Generator, Retriever, Reranker) to tackle various research questions. Results demonstrate that aggregating multiple RAG systems at both the pipeline and module levels enhances generalizability and robustness. This work serves as a foundation for future research on ensemble methods for multi-RAG systems.<br /><br />Summary: <div>
arXiv:2508.13828v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in recent years. However, despite the emergence of various RAG frameworks, a single RAG framework still cannot adapt well to a broad range of downstream tasks. Therefore, how to leverage the advantages of multiple RAG systems has become an area worth exploring. To address this issue, we have conducted a comprehensive and systematic investigation into ensemble methods based on RAG systems. Specifically, we have analyzed the RAG ensemble framework from both theoretical and mechanistic analysis perspectives. From the theoretical analysis, we provide the first explanation of the RAG ensemble framework from the perspective of information entropy. In terms of mechanism analysis, we have explored the RAG ensemble framework from both the pipeline and module levels. We carefully select four different pipelines (Branching, Iterative, Loop, and Agentic) and three different modules (Generator, Retriever, and Reranker) to solve seven different research questions. The experiments show that aggregating multiple RAG systems is both generalizable and robust, whether at the pipeline level or the module level. Our work lays the foundation for similar research on the multi-RAG system ensemble.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Generalized Planning with LLMs through Strategy Refinement and Reflection</title>
<link>https://arxiv.org/abs/2508.13876</link>
<guid>https://arxiv.org/abs/2508.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, Python programs, PDDL planning, pseudocode, debugging

Summary:
In this study, the use of Large Language Models (LLMs) to generate Python programs for generalized plans in PDDL planning is explored. The framework proposed consists of three steps: generating a summary and strategy in natural language, implementing the strategy as a Python program, and debugging the program on example planning tasks. A new approach is introduced to generate the strategy in the form of pseudocode, allowing for automatic debugging. Additionally, a reflection step is added in the Python debugging phase to identify reasons for plan failures. Inspiration from LLM code generation is utilized to produce multiple program variants and select the best one. Experiments on 17 benchmark domains demonstrate significant improvements in the quality of generalized plans. The enhanced approach ensures accurate strategies and program implementation, resulting in successful solution of tasks across various domains. <div>
arXiv:2508.13876v1 Announce Type: new 
Abstract: LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback</title>
<link>https://arxiv.org/abs/2508.13915</link>
<guid>https://arxiv.org/abs/2508.13915</guid>
<content:encoded><![CDATA[
<div> framework, time-series data, financial markets, agentic systems, AutoML
Summary:
The paper introduces TS-Agent, an agentic framework designed for automating and improving time-series modeling workflows in finance. TS-Agent follows a structured decision process involving model selection, code refinement, and fine-tuning. It features a planner agent with knowledge banks and curated libraries to guide exploration and enhance interpretability. TS-Agent supports adaptive learning, robust debugging, and transparent auditing, crucial for financial services. Empirical evaluations show that TS-Agent outperforms state-of-the-art AutoML and agentic baselines in terms of accuracy, robustness, and decision traceability. <div>
arXiv:2508.13915v1 Announce Type: new 
Abstract: Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management</title>
<link>https://arxiv.org/abs/2508.13942</link>
<guid>https://arxiv.org/abs/2508.13942</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous agents, supply chain, collaborative behavior, AI-driven systems, strategic choices

Summary:
The paper investigates the strategic behavior of autonomous AI agents in a multi-echelon supply chain setting using computational experiments with Large Language Models. It identifies the "collaboration paradox" where theoretically collaborative AI agents perform worse than non-AI baselines due to inventory hoarding. The study highlights the need for a synthesis of high-level policy-setting and low-level execution protocols for resilience. A framework is proposed that autonomously generates and evaluates strategic choices, emphasizing the importance of proactive downstream replenishment and establishing robust operational targets. The research offers insights into emergent behaviors of collaborative AI agents and provides a blueprint for designing stable and effective AI-driven systems for business analytics. 

<br /><br />Summary: <div>
arXiv:2508.13942v1 Announce Type: new 
Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical questions about their emergent strategic behavior. This paper investigates these dynamics in the cooperative context of a multi-echelon supply chain, a system famously prone to instabilities like the bullwhip effect. We conduct computational experiments with generative AI agents, powered by Large Language Models (LLMs), within a controlled supply chain simulation designed to isolate their behavioral tendencies. Our central finding is the "collaboration paradox": a novel, catastrophic failure mode where theoretically superior collaborative AI agents, designed with Vendor-Managed Inventory (VMI) principles, perform even worse than non-AI baselines. We demonstrate that this paradox arises from an operational flaw where agents hoard inventory, starving the system. We then show that resilience is only achieved through a synthesis of two distinct layers: high-level, AI-driven proactive policy-setting to establish robust operational targets, and a low-level, collaborative execution protocol with proactive downstream replenishment to maintain stability. Our final framework, which implements this synthesis, can autonomously generate, evaluate, and quantify a portfolio of viable strategic choices. The work provides a crucial insight into the emergent behaviors of collaborative AI agents and offers a blueprint for designing stable, effective AI-driven systems for business analytics.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation</title>
<link>https://arxiv.org/abs/2508.13975</link>
<guid>https://arxiv.org/abs/2508.13975</guid>
<content:encoded><![CDATA[
<div> Keywords: pretrained large language models, PyChrono, virtual assistants, simulation tool, AI<br />
Summary:<br />
This study explores the potential of refining pretrained large language models (LLMs) to act as virtual assistants for experts using the PyChrono simulation tool. By customizing both open- and closed-source LLMs, the researchers were able to generate high-quality PyChrono simulation scripts for various scenarios, from simple single-pendulum experiments to complex vehicle simulations on deformable terrain. While the generated scripts may not be perfect, they provide a solid foundation for users to build upon. The LLMs can also provide assistance with API questions and modeling approaches. This framework can be applied beyond PyChrono to lower the barrier to entry for other simulation tools in different domains.<br /> 
Summary: <div>
arXiv:2508.13975v1 Announce Type: new 
Abstract: This contribution is concerned with the following issue: can pretrained large language models (LLMs) be refined and customized to the point where they become virtual assistants helping experts with the effective use of a simulation tool? In this case study, the ``simulation tool'' considered is PyChrono, an open source multi-physics dynamics engine for multibody systems. We present a framework for refining and customizing both open- and closed-source LLMs to harness the power of AI in generating scripts that perform PyChrono virtual experiments. We refine and customize several classes of LLMs through a process that leads to a quantifiable improvement in the quality of the generated PyChrono simulation scripts. These scripts can range from simple single-pendulum simulations to complex virtual experiments involving full vehicles on deformable terrain. While the generated scripts are rarely perfect, they often serve as strong starting points for the user to modify and improve on. Additionally, the LLM can answer specific API questions about the simulator, or recommend modeling approaches. The framework discussed is general and can be applied to lower the entry barrier for simulation tools associated with other application domains.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem</title>
<link>https://arxiv.org/abs/2508.14020</link>
<guid>https://arxiv.org/abs/2508.14020</guid>
<content:encoded><![CDATA[
<div> NP-hard, combinatorial optimization, Bioinformatics, Genetic Algorithm, Genome reassembly
Summary: 
The paper addresses the NP-hard longest run subsequence (LRS) problem in bioinformatics, crucial for genome reassembly. The authors propose a Biased Random Key Genetic Algorithm (BRKGA) for efficient solution finding by converting gray value vectors into valid solutions. They compare BRKGA with a Max-Min Ant System and CPLEX solver, showing BRKGA as a competitive approach for the LRS problem. However, improvements are needed for large alphabet size inputs. The study highlights the computational efficiency and effectiveness of the BRKGA for the LRS problem but suggests further enhancements for better performance, especially in scenarios with complex input strings. <br /><br />Summary: <div>
arXiv:2508.14020v1 Announce Type: new 
Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial optimization problem belonging to the class of subsequence problems from bioinformatics. In particular, the problem plays a role in genome reassembly. In this paper, we present a solution to the LRS problem using a Biased Random Key Genetic Algorithm (BRKGA). Our approach places particular focus on the computational efficiency of evaluating individuals, which involves converting vectors of gray values into valid solutions to the problem. For comparison purposes, a Max-Min Ant System is developed and implemented. This is in addition to the application of the integer linear programming solver CPLEX for solving all considered problem instances. The computation results show that the proposed BRKGA is currently a state-of-the-art technique for the LRS problem. Nevertheless, the results also show that there is room for improvement, especially in the context of input strings based on large alphabet sizes.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</title>
<link>https://arxiv.org/abs/2508.14040</link>
<guid>https://arxiv.org/abs/2508.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: ComputerRL, desktop intelligence, API-GUI paradigm, distributed RL infrastructure, Entropulse<br />
<br />
Summary: <br />
ComputerRL is a framework designed for autonomous desktop intelligence, merging API calls and GUI interactions for efficient operation in desktop environments. To enhance end-to-end RL training, a distributed infrastructure capable of managing multiple desktop environments concurrently is introduced. The Entropulse training strategy is proposed to maintain training stability by alternating between RL and supervised fine-tuning. By implementing ComputerRL on open models and evaluating them on the OSWorld benchmark, significant accuracy improvements are achieved with the AutoGLM-OS-9B model based on GLM-4-9B-0414. The work demonstrates notable advancements in desktop automation and is utilized in the development of AutoGLM. <br /><br />Summary: <div>
arXiv:2508.14040v1 Announce Type: new 
Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks, yet remains challenging due to environmental inefficiency and instability in extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%, demonstrating significant improvements for general agents in desktop automation. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024a)
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary suggestions for rigorous GPAI model evaluations</title>
<link>https://arxiv.org/abs/2508.00875</link>
<guid>https://arxiv.org/abs/2508.00875</guid>
<content:encoded><![CDATA[
<div> Keywords: general-purpose AI evaluation, internal validity, external validity, reproducibility, GPAI models

Summary:
This document provides a preliminary compilation of best practices for evaluating general-purpose artificial intelligence (GPAI) models to ensure internal validity, external validity, and reproducibility. Recommendations are outlined for human uplift studies, benchmark evaluations, and cross-cutting suggestions applicable to various evaluation types. The suggestions cover four key stages in the evaluation life cycle: design, implementation, execution, and documentation. Drawing from established practices in diverse fields such as machine learning, statistics, and psychology, these guidelines aim to contribute to the emerging field of GPAI evaluation science. The target audience includes providers of GPAI models with systemic risk, third-party evaluators, policymakers assessing evaluation rigor, and academic researchers involved in GPAI evaluation research. The suggestions aim to assist stakeholders in meeting the specific evaluation requirements outlined in the EU AI Act and promoting high-quality evaluations in the GPAI domain.<br /><br />Summary: <div>
arXiv:2508.00875v1 Announce Type: cross 
Abstract: This document presents a preliminary compilation of general-purpose AI (GPAI) evaluation practices that may promote internal validity, external validity and reproducibility. It includes suggestions for human uplift studies and benchmark evaluations, as well as cross-cutting suggestions that may apply to many different evaluation types. Suggestions are organised across four stages in the evaluation life cycle: design, implementation, execution and documentation. Drawing from established practices in machine learning, statistics, psychology, economics, biology and other fields recognised to have important lessons for AI evaluation, these suggestions seek to contribute to the conversation on the nascent and evolving field of the science of GPAI evaluations. The intended audience of this document includes providers of GPAI models presenting systemic risk (GPAISR), for whom the EU AI Act lays out specific evaluation requirements; third-party evaluators; policymakers assessing the rigour of evaluations; and academic researchers developing or conducting GPAI evaluations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR1: The Thinking Model for E-commerce Relevance Search</title>
<link>https://arxiv.org/abs/2508.12365</link>
<guid>https://arxiv.org/abs/2508.12365</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce search, BERT-based models, Large Language Models (LLMs), Chain-of-Thought (CoT), discriminative hallucination <br />
Summary: 
The article proposes a framework, TaoSR1, for query-product relevance prediction in e-commerce search using Large Language Models (LLMs). It addresses challenges like Chain-of-Thought (CoT) error accumulation and discriminative hallucination. The framework involves three stages: Supervised Fine-Tuning (SFT) with CoT, offline sampling with a pass@N strategy and Direct Preference Optimization (DPO), and difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO). Post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 outperforms baselines on offline datasets and achieves substantial gains in online human evaluations. It introduces a novel paradigm for applying CoT reasoning to relevance classification. <br /><br />Summary: <div>
arXiv:2508.12365v1 Announce Type: cross 
Abstract: Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. Our framework, TaoSR1, involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</title>
<link>https://arxiv.org/abs/2508.12448</link>
<guid>https://arxiv.org/abs/2508.12448</guid>
<content:encoded><![CDATA[
<div> physics-based tasks, large language models, in-context learning, dynamics forecasting, sparse autoencoders

Summary:<br />
- The study explores the capabilities of Large Language Models (LLMs) in understanding physics-based tasks through in-context learning. 
- LLMs show improved performance in dynamics forecasting with longer input contexts, indicating their ability to learn physics in context.
- Analysis using sparse autoencoders (SAEs) reveals that LLMs capture features correlated with key physical variables like energy.
- The study demonstrates that LLMs encode meaningful physical concepts during in-context learning.
- This research expands our understanding of how LLMs learn in context, particularly in the domain of physics-based tasks. 

<br /><br /> <div>
arXiv:2508.12448v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) abilities, enabling them to solve wide range of tasks via textual prompts alone. As these capabilities advance, the range of applicable domains continues to expand significantly. However, identifying the precise mechanisms or internal structures within LLMs that allow successful ICL across diverse, distinct classes of tasks remains elusive. Physics-based tasks offer a promising testbed for probing this challenge. Unlike synthetic sequences such as basic arithmetic or symbolic equations, physical systems provide experimentally controllable, real-world data based on structured dynamics grounded in fundamental principles. This makes them particularly suitable for studying the emergent reasoning behaviors of LLMs in a realistic yet tractable setting. Here, we mechanistically investigate the ICL ability of LLMs, especially focusing on their ability to reason about physics. Using a dynamics forecasting task in physical systems as a proxy, we evaluate whether LLMs can learn physics in context. We first show that the performance of dynamics forecasting in context improves with longer input contexts. To uncover how such capability emerges in LLMs, we analyze the model's residual stream activations using sparse autoencoders (SAEs). Our experiments reveal that the features captured by SAEs correlate with key physical variables, such as energy. These findings demonstrate that meaningful physical concepts are encoded within LLMs during in-context learning. In sum, our work provides a novel case study that broadens our understanding of how LLMs learn in context.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code</title>
<link>https://arxiv.org/abs/2508.13156</link>
<guid>https://arxiv.org/abs/2508.13156</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Verilog, hardware design, EvoVerilog, evolutionary algorithms

Summary:
Large Language Models (LLMs) have shown promise in automating the generation of Verilog hardware description language code for hardware design, reducing human effort. Existing approaches often require human intervention and fine-tuning, limiting scalability. EvoVerilog is introduced as a framework that combines LLMs with evolutionary algorithms to automatically generate and refine Verilog code without human intervention. It utilizes a multiobjective, population-based search strategy to explore diverse design possibilities. EvoVerilog achieves state-of-the-art performance on benchmarks, showcasing its ability to generate a variety of functional Verilog code while optimizing resource utilization. <div>
arXiv:2508.13156v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated great potential in automating the generation of Verilog hardware description language code for hardware design. This automation is critical to reducing human effort in the complex and error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and fine-tuning using curated datasets, limiting their scalability in automated design workflows.
  Although recent iterative search techniques have emerged, they often fail to explore diverse design solutions and may underperform simpler approaches such as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that combines the reasoning capabilities of LLMs with evolutionary algorithms to automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to explore a wide range of design possibilities without requiring human intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively. Furthermore, the framework showcases its ability to explore diverse designs by simultaneously generating a variety of functional Verilog code while optimizing resource utilization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists</title>
<link>https://arxiv.org/abs/2508.13157</link>
<guid>https://arxiv.org/abs/2508.13157</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, analog integrated circuits, circuit diagrams, netlists, Image2Net 

Summary: 
This paper introduces a new dataset of circuit diagrams and proposes the Image2Net framework for converting circuit diagrams into netlists. The conversion is essential for enhancing the knowledge of Large Language Models (LLMs) in designing analog integrated circuits (ICs). Existing analog ICs are mostly represented in image-based circuit diagrams, making it challenging to extract information. The Image2Net framework addresses this issue by achieving a 80.77% successful conversion rate, significantly outperforming previous methods. The framework also introduces the netlist edit distance (NED) metric to accurately measure the differences between the converted netlists and the ground truth. The proposed approach demonstrates a 62.1%-69.6% reduction in averaged NED compared to state-of-the-art methods. This work paves the way for leveraging LLMs in the design and analysis of analog ICs by effectively converting complex circuit diagrams into netlists. 

<br /><br />Summary: <div>
arXiv:2508.13157v1 Announce Type: cross 
Abstract: Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\% successful rate, which is 34.62\%-45.19\% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1\%-69.6\% lower than state-of-the-arts.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner</title>
<link>https://arxiv.org/abs/2508.13161</link>
<guid>https://arxiv.org/abs/2508.13161</guid>
<content:encoded><![CDATA[
<div> Graph-based method, module placement, pin assignment, multi-constraint optimization, whitespace removal <br />
<br />Summary: The article introduces a floorplanning framework called Piano that optimizes module placement and pin assignment simultaneously under various constraints. It uses a graph-based method to identify geometric relationships among modules and their netlist connections, aiding in determining pin assignments. Additionally, it accurately evaluates feedthrough and unplaced pins to enhance overall layout quality. The framework employs a whitespace removal strategy and three local optimizers to improve layout metrics in multi-constraint scenarios. Experimental results on benchmark circuits show that Piano reduces HPWL by 6.81%, feedthrough wirelength by 13.39%, feedthrough modules by 16.36%, and unplaced pins by 21.21% while maintaining zero whitespace. <div>
arXiv:2508.13161v1 Announce Type: cross 
Abstract: Floorplanning is a critical step in VLSI physical design, increasingly complicated by modern constraints such as fixed-outline requirements, whitespace removal, and the presence of pre-placed modules. In addition, the assignment of pins on module boundaries significantly impacts the performance of subsequent stages, including detailed placement and routing. However, traditional floorplanners often overlook pin assignment with modern constraints during the floorplanning stage. In this work, we introduce Piano, a floorplanning framework that simultaneously optimizes module placement and pin assignment under multiple constraints. Specifically, we construct a graph based on the geometric relationships among modules and their netlist connections, then iteratively search for shortest paths to determine pin assignments. This graph-based method also enables accurate evaluation of feedthrough and unplaced pins, thereby guiding overall layout quality. To further improve the design, we adopt a whitespace removal strategy and employ three local optimizers to enhance layout metrics under multi-constraint scenarios. Experimental results on widely used benchmark circuits demonstrate that Piano achieves an average 6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36% reduction in the number of feedthrough modules, and a 21.21% drop in unplaced pins, while maintaining zero whitespace.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures</title>
<link>https://arxiv.org/abs/2508.13163</link>
<guid>https://arxiv.org/abs/2508.13163</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, artificial intelligence, sustainability, energy efficiency, hardware-software co-design 

Summary: 
This paper discusses the sustainability issues related to large-scale deep learning and artificial intelligence model training, which consume significant computational power and energy. The focus is on exploring environmentally driven performance optimization methods for advanced GPU architectures, aiming to increase memory-level and kernel-level operations for improved performance-per-watt measures. The study evaluates specialized tensor and matrix cores, memory optimization techniques, and integration approaches to enhance energy efficiency. Furthermore, software-level optimizations such as mixed-precision arithmetic and energy-aware scheduling algorithms are explored to support hardware capabilities. The importance of hardware-software co-design in reducing the environmental impact of artificial intelligence without compromising performance is highlighted. Real-world case studies from top companies demonstrate the effectiveness of sustainable AI training methods in practice. 

<br /><br />Summary: <div>
arXiv:2508.13163v1 Announce Type: cross 
Abstract: In particular, large-scale deep learning and artificial intelligence model training uses a lot of computational power and energy, so it poses serious sustainability issues. The fast rise in model complexity has resulted in exponential increases in energy consumption, increasing the demand for techniques maximizing computational efficiency and lowering environmental impact. This work explores environmentally driven performance optimization methods especially intended for advanced GPU architectures from NVIDIA, AMD, and other emerging GPU architectures. Our main focus is on investigating hardware-software co-design techniques meant to significantly increase memory-level and kernel-level operations, so improving performance-per-watt measures. Our thorough research encompasses evaluations of specialized tensor and matrix cores, advanced memory optimization methods, and creative integration approaches that taken together result in notable energy efficiency increases. We also discuss important software-level optimizations that augment hardware capability including mixed-precision arithmetic, advanced energy-aware scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we methodically point out important research gaps and suggest future directions necessary to create really sustainable artificial intelligence systems. This paper emphasizes how major increases in training efficiency can be obtained by co-design of hardware and software, so lowering the environmental impact of artificial intelligence without compromising performance. To back up our analysis, we use real-world case studies from top companies like Meta, Google, Amazon, and others that show how these sustainable AI training methods are used in the real world.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design</title>
<link>https://arxiv.org/abs/2508.13172</link>
<guid>https://arxiv.org/abs/2508.13172</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog IC design, Large Language Models, gm/Id methodology, Automation, Efficiency

Summary: 
Analog IC design faces challenges due to heavy reliance on experience and inefficient simulations in advanced nodes. Traditional formulas are insufficient for these complex designs. This study introduces a synergistic reasoning framework that combines Large Language Models (LLMs) with the gm/Id methodology to enhance design efficiency and accuracy. By integrating gm/Id lookup tables into the LLM, a data-driven design partner is created, improving precision and speed. Application of this framework to a two-stage op-amp demonstrated successful meeting of specifications and optimization across all process, voltage, and temperature corners. Ablation studies confirmed the importance of gm/Id data in achieving efficiency and precision. Comparative analysis with a senior engineer's design revealed significant improvements in efficiency while maintaining quasi-expert quality. This work paves the way for true automation in analog design by merging LLM reasoning with established circuit design methodologies.<br /><br />Summary: <div>
arXiv:2508.13172v1 Announce Type: cross 
Abstract: Analog IC design is a bottleneck due to its reliance on experience and inefficient simulations, as traditional formulas fail in advanced nodes. Applying Large Language Models (LLMs) directly to this problem risks mere "guessing" without engineering principles. We present a "synergistic reasoning" framework that integrates an LLM's strategic reasoning with the physical precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the Gemini model to meet all TT corner specs in 5 iterations and extended optimization to all PVT corners. A crucial ablation study proved gm/Id data is key for this efficiency and precision; without it, the LLM is slower and deviates. Compared to a senior engineer's design, our framework achieves quasi-expert quality with an order-of-magnitude improvement in efficiency. This work validates a path for true analog design automation by combining LLM reasoning with scientific circuit design methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward an African Agenda for AI Safety</title>
<link>https://arxiv.org/abs/2508.13179</link>
<guid>https://arxiv.org/abs/2508.13179</guid>
<content:encoded><![CDATA[
<div> Keywords: Africa, AI risk profile, human rights, AI literacy, AI Safety Institute<br />
Summary: 
This paper explores Africa's unique AI risk profile, highlighting challenges such as deepfake manipulation in elections, data colonial dependency, and climate-related environmental costs. Despite the promised benefits of AI, African countries face specific safety risks like labor market disruptions and manipulation of public opinion. African perspectives have been largely absent in global AI safety discussions, leading to limited influence in shaping governance agendas. The proposed action plan includes prioritizing human rights protection for vulnerable populations, establishing an African AI Safety Institute, promoting public AI literacy, developing early warning systems for African languages, and hosting an annual AU-level AI Safety & Security Forum. This initiative aims to address the gaps in AI safety governance and ensure inclusive participation from African stakeholders.  
<br /><br />Summary:  <div>
arXiv:2508.13179v1 Announce Type: cross 
Abstract: This paper maps Africa's distinctive AI risk profile, from deepfake fuelled electoral interference and data colonial dependency to compute scarcity, labour disruption and disproportionate exposure to climate driven environmental costs. While major benefits are promised to accrue, the availability, development and adoption of AI also mean that African people and countries face particular AI safety risks, from large scale labour market disruptions to the nefarious use of AI to manipulate public opinion. To date, African perspectives have not been meaningfully integrated into global debates and processes regarding AI safety, leaving African stakeholders with limited influence over the emerging global AI safety governance agenda. While there are Computer Incident Response Teams on the continent, none hosts a dedicated AI Safety Institute or office. We propose a five-point action plan centred on (i) a policy approach that foregrounds the protection of the human rights of those most vulnerable to experiencing the harmful socio-economic effects of AI; (ii) the establishment of an African AI Safety Institute; (iii) promote public AI literacy and awareness; (iv) development of early warning system with inclusive benchmark suites for 25+ African languages; and (v) an annual AU-level AI Safety & Security Forum.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Artificial Intuition in Distinct, Minimalist Classification of Scientific Abstracts for Management of Technology Portfolios</title>
<link>https://arxiv.org/abs/2508.13182</link>
<guid>https://arxiv.org/abs/2508.13182</guid>
<content:encoded><![CDATA[
arXiv:2508.13182v1 Announce Type: cross 
Abstract: Classification of scientific abstracts is useful for strategic activities but challenging to automate because the sparse text provides few contextual clues. Metadata associated with the scientific publication can be used to improve performance but still often requires a semi-supervised setting. Moreover, such schemes may generate labels that lack distinction -- namely, they overlap and thus do not uniquely define the abstract. In contrast, experts label and sort these texts with ease. Here we describe an application of a process we call artificial intuition to replicate the expert's approach, using a Large Language Model (LLM) to generate metadata. We use publicly available abstracts from the United States National Science Foundation to create a set of labels, and then we test this on a set of abstracts from the Chinese National Natural Science Foundation to examine funding trends. We demonstrate the feasibility of this method for research portfolio management, technology scouting, and other strategic activities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</title>
<link>https://arxiv.org/abs/2508.13186</link>
<guid>https://arxiv.org/abs/2508.13186</guid>
<content:encoded><![CDATA[
arXiv:2508.13186v1 Announce Type: cross 
Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection</title>
<link>https://arxiv.org/abs/2508.13187</link>
<guid>https://arxiv.org/abs/2508.13187</guid>
<content:encoded><![CDATA[
arXiv:2508.13187v1 Announce Type: cross 
Abstract: Homelessness is a persistent social challenge, impacting millions worldwide. Over 770,000 people experienced homelessness in the U.S. in 2024. Social stigmatization is a significant barrier to alleviation, shifting public perception, and influencing policymaking. Given that online and city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases. This research contributes to alleviating homelessness by acting on public opinion. It introduces novel methods, building on natural language processing (NLP) and large language models (LLMs), to identify and measure PEH social bias expressed in digital spaces. We present a new, manually-annotated multi-modal dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across 10 U.S. cities. This unique dataset provides evidence of the typologies of homelessness bias described in the literature. In order to scale up and automate the detection of homelessness bias online, we evaluate LLMs as classifiers. We applied both zero-shot and few-shot classification techniques to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1, Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are significant inconsistencies in local LLM zero-shot classification, the in-context learning classification scores of local LLMs approach the classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT when averaging across all categories. This work aims to raise awareness about the pervasive bias against PEH, develop new indicators to inform policy, and ultimately enhance the fairness and ethical application of Generative AI technologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Models assume Proportional Hazards of Utilities</title>
<link>https://arxiv.org/abs/2508.13189</link>
<guid>https://arxiv.org/abs/2508.13189</guid>
<content:encoded><![CDATA[
arXiv:2508.13189v1 Announce Type: cross 
Abstract: Approaches for estimating preferences from human annotated data typically involves inducing a distribution over a ranked list of choices such as the Plackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling and Direct Preference Optimization are based on the statistical assumptions posed by the Plackett-Luce model. In this paper, I will connect the Plackett-Luce model to another classical and well known statistical model, the Cox Proportional Hazards model and attempt to shed some light on the implications of the connection therein.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.13196</link>
<guid>https://arxiv.org/abs/2508.13196</guid>
<content:encoded><![CDATA[
arXiv:2508.13196v1 Announce Type: cross 
Abstract: This paper introduces a novel approach for multimodal sentiment analysis on social media, particularly in the context of natural disasters, where understanding public sentiment is crucial for effective crisis management. Unlike conventional methods that process text and image modalities separately, our approach seamlessly integrates Convolutional Neural Network (CNN) based image analysis with Large Language Model (LLM) based text processing, leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to extract sentiment relevant features from the CrisisMMD dataset. To effectively model intermodal relationships, we introduce a contextual attention mechanism within the fusion process. Leveraging contextual-attention layers, this mechanism effectively captures intermodality interactions, enhancing the model's comprehension of complex relationships between textual and visual data. The deep neural network architecture of our model learns from these fused features, leading to improved accuracy compared to existing baselines. Experimental results demonstrate significant advancements in classifying social media data into informative and noninformative categories across various natural disasters. Our model achieves a notable 2.43% increase in accuracy and 5.18% in F1-score, highlighting its efficacy in processing complex multimodal data. Beyond quantitative metrics, our approach provides deeper insight into the sentiments expressed during crises. The practical implications extend to real time disaster management, where enhanced sentiment analysis can optimize the accuracy of emergency interventions. By bridging the gap between multimodal analysis, LLM powered text understanding, and disaster response, our work presents a promising direction for Artificial Intelligence (AI) driven crisis management solutions. Keywords:
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of Generative AI for Metal-Organic Framework Design and Synthesis</title>
<link>https://arxiv.org/abs/2508.13197</link>
<guid>https://arxiv.org/abs/2508.13197</guid>
<content:encoded><![CDATA[
arXiv:2508.13197v1 Announce Type: cross 
Abstract: Advances in generative artificial intelligence are transforming how metal-organic frameworks (MOFs) are designed and discovered. This Perspective introduces the shift from laborious enumeration of MOF candidates to generative approaches that can autonomously propose and synthesize in the laboratory new porous reticular structures on demand. We outline the progress of employing deep learning models, such as variational autoencoders, diffusion models, and large language model-based agents, that are fueled by the growing amount of available data from the MOF community and suggest novel crystalline materials designs. These generative tools can be combined with high-throughput computational screening and even automated experiments to form accelerated, closed-loop discovery pipelines. The result is a new paradigm for reticular chemistry in which AI algorithms more efficiently direct the search for high-performance MOF materials for clean air and energy applications. Finally, we highlight remaining challenges such as synthetic feasibility, dataset diversity, and the need for further integration of domain knowledge.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM-based Agents for Single-cell Omics Analysis</title>
<link>https://arxiv.org/abs/2508.13201</link>
<guid>https://arxiv.org/abs/2508.13201</guid>
<content:encoded><![CDATA[
arXiv:2508.13201v1 Announce Type: cross 
Abstract: The surge in multimodal single-cell omics data exposes limitations in traditional, manually defined analysis workflows. AI agents offer a paradigm shift, enabling adaptive planning, executable code generation, traceable decisions, and real-time knowledge fusion. However, the lack of a comprehensive benchmark critically hinders progress. We introduce a novel benchmarking evaluation system to rigorously assess agent capabilities in single-cell omics analysis. This system comprises: a unified platform compatible with diverse agent frameworks and LLMs; multidimensional metrics assessing cognitive program synthesis, collaboration, execution efficiency, bioinformatics knowledge integration, and task completion quality; and 50 diverse real-world single-cell omics analysis tasks spanning multi-omics, species, and sequencing technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art performance among tested agent frameworks. Multi-agent frameworks significantly enhance collaboration and execution efficiency over single-agent approaches through specialized role division. Attribution analyses of agent capabilities identify that high-quality code generation is crucial for task success, and self-reflection has the most significant overall impact, followed by retrieval-augmented generation (RAG) and planning. This work highlights persistent challenges in code generation, long-context handling, and context-aware knowledge retrieval, providing a critical empirical foundation and best practices for developing robust AI agents in computational biology.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment</title>
<link>https://arxiv.org/abs/2508.13207</link>
<guid>https://arxiv.org/abs/2508.13207</guid>
<content:encoded><![CDATA[
arXiv:2508.13207v1 Announce Type: cross 
Abstract: Background: Gastric neoplasm, primarily adenocarcinoma, is an aggressive cancer with high mortality, often diagnosed late, leading to complications like metastasis. Effective drug combinations are vital to address disease heterogeneity, enhance efficacy, reduce resistance, and improve patient outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug combinations, using a graph model with p-value-weighted edges connecting drugs, genes, and proteins. NLP and systematic literature review (PubMed, Scopus, etc.) validated proposed drugs, followed by network meta-analysis to assess efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective, supported by 61 studies. Fluorouracil alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069 for the triple combination, indicating superior efficacy. Conclusion: The RAIN method, combining AI and network meta-analysis, effectively identifies optimal drug combinations for gastric neoplasm, offering a promising strategy to enhance treatment outcomes and guide health policy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Conversational Recommender System Considering Consumer Types</title>
<link>https://arxiv.org/abs/2508.13209</link>
<guid>https://arxiv.org/abs/2508.13209</guid>
<content:encoded><![CDATA[
arXiv:2508.13209v1 Announce Type: cross 
Abstract: Conversational Recommender Systems (CRS) provide personalized services through multi-turn interactions, yet most existing methods overlook users' heterogeneous decision-making styles and knowledge levels, which constrains both accuracy and efficiency. To address this gap, we propose CT-CRS (Consumer Type-Enhanced Conversational Recommender System), a framework that integrates consumer type modeling into dialogue recommendation. Based on consumer type theory, we define four user categories--dependent, efficient, cautious, and expert--derived from two dimensions: decision-making style (maximizers vs. satisficers) and knowledge level (high vs. low). CT-CRS employs interaction histories and fine-tunes the large language model to automatically infer user types in real time, avoiding reliance on static questionnaires. We incorporate user types into state representation and design a type-adaptive policy that dynamically adjusts recommendation granularity, diversity, and attribute query complexity. To further optimize the dialogue policy, we adopt Inverse Reinforcement Learning (IRL), enabling the agent to approximate expert-like strategies conditioned on consumer type. Experiments on LastFM, Amazon-Book, and Yelp show that CTCRS improves recommendation success rate and reduces interaction turns compared to strong baselines. Ablation studies confirm that both consumer type modeling and IRL contribute significantly to performance gains. These results demonstrate that CT-CRS offers a scalable and interpretable solution for enhancing CRS personalization through the integration of psychological modeling and advanced policy optimization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions</title>
<link>https://arxiv.org/abs/2508.13214</link>
<guid>https://arxiv.org/abs/2508.13214</guid>
<content:encoded><![CDATA[
arXiv:2508.13214v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g., "What is 3 + 2?") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Graph Neural Point Process For Learning Temporal Interactive Networks</title>
<link>https://arxiv.org/abs/2508.13219</link>
<guid>https://arxiv.org/abs/2508.13219</guid>
<content:encoded><![CDATA[
arXiv:2508.13219v1 Announce Type: cross 
Abstract: Learning temporal interaction networks(TIN) is previously regarded as a coarse-grained multi-sequence prediction problem, ignoring the network topology structure influence. This paper addresses this limitation and a Deep Graph Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node Aggregation Layer captures topological structures to generate static representation for users and items, while the Self Attentive Layer dynamically updates embeddings over time. By incorporating both dynamic and static embeddings into the event intensity function and optimizing the model via maximum likelihood estimation, DGNPP predicts events and occurrence time effectively. Experimental evaluations on three public datasets demonstrate that DGNPP achieves superior performance in event prediction and time prediction tasks with high efficiency, significantly outperforming baseline models and effectively mitigating the limitations of prior approaches.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols</title>
<link>https://arxiv.org/abs/2508.13220</link>
<guid>https://arxiv.org/abs/2508.13220</guid>
<content:encoded><![CDATA[
arXiv:2508.13220v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, and attack scripts to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Towards AI-Generated Image Detection in the Wild</title>
<link>https://arxiv.org/abs/2508.13223</link>
<guid>https://arxiv.org/abs/2508.13223</guid>
<content:encoded><![CDATA[
arXiv:2508.13223v1 Announce Type: cross 
Abstract: The spreading of AI-generated images (AIGI), driven by advances in generative AI, poses a significant threat to information security and public trust. Existing AIGI detectors, while effective against images in clean laboratory settings, fail to generalize to in-the-wild scenarios. These real-world images are noisy, varying from ``obviously fake" images to realistic ones derived from multiple generative models and further edited for quality control. We address in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is constructed from two sources: (1) a large corpus of Internet-sourced AIGI verified by human experts, and (2) a synthesized dataset created through the collaboration between multiple expert generators, closely simulating the realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a vision-language model with heuristic-to-analytic reasoning, a reflective reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a supervised-fine-tuning cold start, followed by a reinforcement learning stage. By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is able to provide either a quick judgment or a more robust and accurate conclusion, effectively balancing inference speed and performance. Extensive experiments show that our model leads state-of-the-art detectors by 5% and 10% on Mirage and the public benchmark, respectively. The benchmark and code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism</title>
<link>https://arxiv.org/abs/2508.13228</link>
<guid>https://arxiv.org/abs/2508.13228</guid>
<content:encoded><![CDATA[
arXiv:2508.13228v1 Announce Type: cross 
Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural Radiance Field (NeRF) framework, capable of reconstructing high-quality scene surfaces from RGB-D sequences in a short time. The method integrates RGB, depth, and semantic information to improve reconstruction performance. Specifically, a novel SG-MLP sampling structure combined with PR-MLP (Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering, allowing the model to capture scene-related information earlier and better distinguish noise from local details. Furthermore, progressive semantic modeling is adopted to extract semantic information at increasing levels of precision, reducing training time while enhancing scene understanding. Experiments on seven synthetic scenes with six evaluation metrics show that PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while maintaining competitive results in NC, Accuracy, and Completeness, demonstrating its effectiveness and practical applicability.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System</title>
<link>https://arxiv.org/abs/2508.13231</link>
<guid>https://arxiv.org/abs/2508.13231</guid>
<content:encoded><![CDATA[
arXiv:2508.13231v1 Announce Type: cross 
Abstract: Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold</title>
<link>https://arxiv.org/abs/2508.13234</link>
<guid>https://arxiv.org/abs/2508.13234</guid>
<content:encoded><![CDATA[
arXiv:2508.13234v1 Announce Type: cross 
Abstract: The acceleration of artificial intelligence (AI) in science is recognized and many scholars have begun to explore its role in interdisciplinary collaboration. However, the mechanisms and extent of this impact are still unclear. This study, using AlphaFold's impact on structural biologists, examines how AI technologies influence interdisciplinary collaborative patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from Scopus, we employ bibliometric analysis and causal inference to compare interdisciplinary collaboration between AlphaFold adopters and non-adopters. Contrary to the widespread belief that AI facilitates interdisciplinary collaboration, our findings show that AlphaFold increased structural biology-computer science collaborations by just 0.48%, with no measurable effect on other disciplines. Specifically, AI creates interdisciplinary collaboration demands with specific disciplines due to its technical characteristics, but this demand is weakened by technological democratization and other factors. These findings demonstrate that artificial intelligence (AI) alone has limited efficacy in bridging disciplinary divides or fostering meaningful interdisciplinary collaboration.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray</title>
<link>https://arxiv.org/abs/2508.13236</link>
<guid>https://arxiv.org/abs/2508.13236</guid>
<content:encoded><![CDATA[
arXiv:2508.13236v1 Announce Type: cross 
Abstract: Early detection and rapid intervention of lung cancer are crucial. Nonetheless, ensuring an accurate diagnosis is challenging, as physicians' ability to interpret chest X-rays varies significantly depending on their experience and degree of fatigue. Although medical AI has been rapidly advancing to assist in diagnosis, physicians' trust in such systems remains limited, preventing widespread clinical adoption. This skepticism fundamentally stems from concerns about its diagnostic uncertainty. In clinical diagnosis, physicians utilize extensive background knowledge and clinical experience. In contrast, medical AI primarily relies on repetitive learning of the target lesion to generate diagnoses based solely on that data. In other words, medical AI does not possess sufficient knowledge to render a diagnosis, leading to diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning Policy that can address the issue of knowledge deficiency by learning the physicians' background knowledge alongside the Chest X-ray lesion information. We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a 10% enhancement in sensitivity compared to the baseline model while also decreasing entropy as a measure of uncertainty by 0.2.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis</title>
<link>https://arxiv.org/abs/2508.13240</link>
<guid>https://arxiv.org/abs/2508.13240</guid>
<content:encoded><![CDATA[
arXiv:2508.13240v1 Announce Type: cross 
Abstract: Understanding and quantifying human cognitive biases from empirical data has long posed a formidable challenge, particularly in cybersecurity, where defending against unknown adversaries is paramount. Traditional cyber defense strategies have largely focused on fortification, while some approaches attempt to anticipate attacker strategies by mapping them to cognitive vulnerabilities, yet they fall short in dynamically interpreting attacks in progress. In recognition of this gap, IARPA's ReSCIND program seeks to infer, defend against, and even exploit attacker cognitive traits. In this paper, we present a novel methodology that leverages large language models (LLMs) to extract quantifiable insights into the cognitive bias of loss aversion from hacker behavior. Our data are collected from an experiment in which hackers were recruited to attack a controlled demonstration network. We process the hacker generated notes using LLMs using it to segment the various actions and correlate the actions to predefined persistence mechanisms used by hackers. By correlating the implementation of these mechanisms with various operational triggers, our analysis provides new insights into how loss aversion manifests in hacker decision-making. The results demonstrate that LLMs can effectively dissect and interpret nuanced behavioral patterns, thereby offering a transformative approach to enhancing cyber defense strategies through real-time, behavior-based analysis.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Involuntary Jailbreak</title>
<link>https://arxiv.org/abs/2508.13246</link>
<guid>https://arxiv.org/abs/2508.13246</guid>
<content:encoded><![CDATA[
arXiv:2508.13246v1 Announce Type: cross 
Abstract: In this study, we disclose a worrying new vulnerability in Large Language Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing jailbreak attacks, this weakness is distinct in that it does not involve a specific attack objective, such as generating instructions for \textit{building a bomb}. Prior attack methods predominantly target localized components of the LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise the entire guardrail structure, which our method reveals to be surprisingly fragile. We merely employ a single universal prompt to achieve this goal. In particular, we instruct LLMs to generate several questions that would typically be rejected, along with their corresponding in-depth responses (rather than a refusal). Remarkably, this simple prompt strategy consistently jailbreaks the majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro, and GPT 4.1. We hope this problem can motivate researchers and practitioners to re-evaluate the robustness of LLM guardrails and contribute to stronger safety alignment in future.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal-Directedness is in the Eye of the Beholder</title>
<link>https://arxiv.org/abs/2508.13247</link>
<guid>https://arxiv.org/abs/2508.13247</guid>
<content:encoded><![CDATA[
arXiv:2508.13247v1 Announce Type: cross 
Abstract: Our ability to predict the behavior of complex agents turns on the attribution of goals. Probing for goal-directed behavior comes in two flavors: Behavioral and mechanistic. The former proposes that goal-directedness can be estimated through behavioral observation, whereas the latter attempts to probe for goals in internal model states. We work through the assumptions behind both approaches, identifying technical and conceptual problems that arise from formalizing goals in agent systems. We arrive at the perhaps surprising position that goal-directedness cannot be measured objectively. We outline new directions for modeling goal-directedness as an emergent property of dynamic, multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models</title>
<link>https://arxiv.org/abs/2508.13257</link>
<guid>https://arxiv.org/abs/2508.13257</guid>
<content:encoded><![CDATA[
arXiv:2508.13257v1 Announce Type: cross 
Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the Register-Transfer Level (RTL) stage presents a critical opportunity for timing optimization. Addressing timing violations at this early stage is essential, as modern systems demand higher speeds, where even minor timing violations can lead to functional failures or system crashes. However, traditional timing optimization heavily relies on manual expertise, requiring engineers to iteratively analyze timing reports and debug. To automate this process, this paper proposes ViTAD, a method that efficiently analyzes the root causes of timing violations and dynamically generates targeted repair strategies. Specifically, we first parse Verilog code and timing reports to construct a Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation path analysis and use large language models (LLMs) to infer the root causes of violations. Finally, by analyzing the causes of violations, we selectively retrieve relevant debugging knowledge from a domain-specific knowledge base to generate customized repair solutions. To evaluate the effectiveness of our method, we construct a timing violation dataset based on real-world open-source projects. This dataset contains 54 cases of violations. Experimental results show that our method achieves a 73.68% success rate in repairing timing violations, while the baseline using only LLM is 54.38%. Our method improves the success rate by 19.30%.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Conformal Classification</title>
<link>https://arxiv.org/abs/2508.13288</link>
<guid>https://arxiv.org/abs/2508.13288</guid>
<content:encoded><![CDATA[
arXiv:2508.13288v1 Announce Type: cross 
Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty in machine learning models, offering reliable predictions with finite-sample coverage guarantees. When applied to classification, CP produces a prediction set of possible labels that is guaranteed to contain the true label with high probability, regardless of the underlying classifier. However, standard CP treats classes as flat and unstructured, ignoring domain knowledge such as semantic relationships or hierarchical structure among class labels. This paper presents hierarchical conformal classification (HCC), an extension of CP that incorporates class hierarchies into both the structure and semantics of prediction sets. We formulate HCC as a constrained optimization problem whose solutions yield prediction sets composed of nodes at different levels of the hierarchy, while maintaining coverage guarantees. To address the combinatorial nature of the problem, we formally show that a much smaller, well-structured subset of candidate solutions suffices to ensure coverage while upholding optimality. An empirical evaluation on three new benchmarks consisting of audio, image, and text data highlights the advantages of our approach, and a user study shows that annotators significantly prefer hierarchical over flat prediction sets.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis</title>
<link>https://arxiv.org/abs/2508.13300</link>
<guid>https://arxiv.org/abs/2508.13300</guid>
<content:encoded><![CDATA[
arXiv:2508.13300v1 Announce Type: cross 
Abstract: Gait recognition is a valuable biometric task that enables the identification of individuals from a distance based on their walking patterns. However, it remains limited by the lack of large-scale labeled datasets and the difficulty of collecting diverse gait samples for each individual while preserving privacy. To address these challenges, we propose GaitCrafter, a diffusion-based framework for synthesizing realistic gait sequences in the silhouette domain. Unlike prior works that rely on simulated environments or alternative generative models, GaitCrafter trains a video diffusion model from scratch, exclusively on gait silhouette data. Our approach enables the generation of temporally consistent and identity-preserving gait sequences. Moreover, the generation process is controllable-allowing conditioning on various covariates such as clothing, carried objects, and view angle. We show that incorporating synthetic samples generated by GaitCrafter into the gait recognition pipeline leads to improved performance, especially under challenging conditions. Additionally, we introduce a mechanism to generate novel identities-synthetic individuals not present in the original dataset-by interpolating identity embeddings. These novel identities exhibit unique, consistent gait patterns and are useful for training models while maintaining privacy of real subjects. Overall, our work takes an important step toward leveraging diffusion models for high-quality, controllable, and privacy-aware gait data generation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters</title>
<link>https://arxiv.org/abs/2508.13303</link>
<guid>https://arxiv.org/abs/2508.13303</guid>
<content:encoded><![CDATA[
arXiv:2508.13303v1 Announce Type: cross 
Abstract: High-fidelity personalized human musculoskeletal models are crucial for simulating realistic behavior of physically coupled human-robot interactive systems and verifying their safety-critical applications in simulations before actual deployment, such as human-robot co-transportation and rehabilitation through robotic exoskeletons. Identifying subject-specific Hill-type muscle model parameters and bone dynamic parameters is essential for a personalized musculoskeletal model, but very challenging due to the difficulty of measuring the internal biomechanical variables in vivo directly, especially the joint torques. In this paper, we propose using Differentiable MusculoSkeletal Model (Diff-MSM) to simultaneously identify its muscle and bone parameters with an end-to-end automatic differentiation technique differentiating from the measurable muscle activation, through the joint torque, to the resulting observable motion without the need to measure the internal joint torques. Through extensive comparative simulations, the results manifested that our proposed method significantly outperformed the state-of-the-art baseline methods, especially in terms of accurate estimation of the muscle parameters (i.e., initial guess sampled from a normal distribution with the mean being the ground truth and the standard deviation being 10% of the ground truth could end up with an average of the percentage errors of the estimated values as low as 0.05%). In addition to human musculoskeletal modeling and simulation, the new parameter identification technique with the Diff-MSM has great potential to enable new applications in muscle health monitoring, rehabilitation, and sports science.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Surveillance Based Interactive Robot</title>
<link>https://arxiv.org/abs/2508.13319</link>
<guid>https://arxiv.org/abs/2508.13319</guid>
<content:encoded><![CDATA[
arXiv:2508.13319v1 Announce Type: cross 
Abstract: We build a mobile surveillance robot that streams video in real time and responds to speech so a user can monitor and steer it from a phone or browser. The system uses two Raspberry Pi 4 units: a front unit on a differential drive base with camera, mic, and speaker, and a central unit that serves the live feed and runs perception. Video is sent with FFmpeg. Objects in the scene are detected using YOLOv3 to support navigation and event awareness. For voice interaction, we use Python libraries for speech recognition, multilingual translation, and text-to-speech, so the robot can take spoken commands and read back responses in the requested language. A Kinect RGB-D sensor provides visual input and obstacle cues. In indoor tests the robot detects common objects at interactive frame rates on CPU, recognises commands reliably, and translates them to actions without manual control. The design relies on off-the-shelf hardware and open software, making it easy to reproduce. We discuss limits and practical extensions, including sensor fusion with ultrasonic range data, GPU acceleration, and adding face and text recognition.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Attention Graph Network for fMRI Data Classification</title>
<link>https://arxiv.org/abs/2508.13328</link>
<guid>https://arxiv.org/abs/2508.13328</guid>
<content:encoded><![CDATA[
arXiv:2508.13328v1 Announce Type: cross 
Abstract: Understanding the complex neural activity dynamics is crucial for the development of the field of neuroscience. Although current functional MRI classification approaches tend to be based on static functional connectivity or cannot capture spatio-temporal relationships comprehensively, we present a new framework that leverages dynamic graph creation and spatiotemporal attention mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in this research dynamically infers functional brain connectivity in each time interval using transformer-based attention mechanisms, enabling the model to selectively focus on crucial brain regions and time segments. By constructing time-varying graphs that are then processed with Graph Convolutional Networks (GCNs) and transformers, our method successfully captures both localized interactions and global temporal dependencies. Evaluated on the subset of ABIDE dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint modeling of dynamic connectivity and spatio-temporal context for fMRI classification. The core novelty arises from (1) attention-driven dynamic graph creation that learns temporal brain region interactions and (2) hierarchical spatio-temporal feature fusion through GCNtransformer fusion.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Probabilistic Diffusion with Expert Models</title>
<link>https://arxiv.org/abs/2508.13355</link>
<guid>https://arxiv.org/abs/2508.13355</guid>
<content:encoded><![CDATA[
arXiv:2508.13355v1 Announce Type: cross 
Abstract: Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT</title>
<link>https://arxiv.org/abs/2508.13358</link>
<guid>https://arxiv.org/abs/2508.13358</guid>
<content:encoded><![CDATA[
arXiv:2508.13358v1 Announce Type: cross 
Abstract: This paper tackles several challenges that arise when integrating Automatic Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device streaming speech translation. Although state-of-the-art ASR systems based on Recurrent Neural Network Transducers (RNN-T) can perform real-time transcription, achieving streaming translation in real-time remains a significant challenge. To address this issue, we propose a simultaneous translation approach that effectively balances translation quality and latency. We also investigate efficient integration of ASR and MT, leveraging linguistic cues generated by the ASR system to manage context and utilizing efficient beam-search pruning techniques such as time-out and forced finalization to maintain system's real-time factor. We apply our approach to an on-device bilingual conversational speech translation and demonstrate that our techniques outperform baselines in terms of latency and quality. Notably, our technique narrows the quality gap with non-streaming translation systems, paving the way for more accurate and efficient real-time speech translation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts</title>
<link>https://arxiv.org/abs/2508.13376</link>
<guid>https://arxiv.org/abs/2508.13376</guid>
<content:encoded><![CDATA[
arXiv:2508.13376v1 Announce Type: cross 
Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy in long audio transcripts, impacting tasks like Named Entity Recognition (NER), capitalization, and punctuation. We propose a novel approach that enhances ASR by distilling contextual knowledge from LLaMA models into Whisper. Our method uses two strategies: (1) token level distillation with optimal transport to align dimensions and sequence lengths, and (2) representation loss minimization between sentence embeddings of Whisper and LLaMA, blending syntax and semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long audios and rich entities demonstrate significant improvements in Word Error Rate (WER), NER, capitalization, and punctuation success. By introducing novel NER metrics and exploring semantics aware ASR, our work highlights the value of integrating linguistic context into transcription, setting a foundation for robust, context-aware ASR in longform speech.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis</title>
<link>https://arxiv.org/abs/2508.13382</link>
<guid>https://arxiv.org/abs/2508.13382</guid>
<content:encoded><![CDATA[
arXiv:2508.13382v1 Announce Type: cross 
Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by  and  tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp</title>
<link>https://arxiv.org/abs/2508.13406</link>
<guid>https://arxiv.org/abs/2508.13406</guid>
<content:encoded><![CDATA[
arXiv:2508.13406v1 Announce Type: cross 
Abstract: This study presents a quantitative framework for evaluating the spatial concordance between clinically defined seizure onset zones (SOZs) and statistically anomalous channels identified through time-frequency analysis of chirp events. The proposed pipeline employs a two-step methodology: (1) Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with adaptive neighborhood selection identifies anomalous channels based on spectro-temporal features of chirp (Onset frequency, offset frequency, and temporal duration); and (2) Spatial Correlation Analysis, which computes both exact co-occurrence metrics and weighted index similarity, incorporating hemispheric congruence and electrode proximity. Key findings demonstrate that the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects outliers, with index matching (weighted by channel proximity) outperforming exact matching in SOZ localization. Performance metrics (precision, recall, F1) were highest for seizure-free patients (Index Precision mean: 0.903) and those with successful surgical outcomes (Index Precision mean: 0.865), whereas failure cases exhibited lower concordance (Index Precision mean: 0.460). The key takeaway is that chirp-based outlier detection, combined with weighted spatial metrics, provides a complementary method for SOZ localization, particularly in patients with successful surgical outcomes.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptJobRec: Enhancing Conversational Career Recommendation through an LLM-Powered Agentic System</title>
<link>https://arxiv.org/abs/2508.13423</link>
<guid>https://arxiv.org/abs/2508.13423</guid>
<content:encoded><![CDATA[
arXiv:2508.13423v1 Announce Type: cross 
Abstract: In recent years, recommendation systems have evolved from providing a single list of recommendations to offering a comprehensive suite of topic focused services. To better accomplish this task, conversational recommendation systems (CRS) have progressed from basic retrieval augmented LLM generation to agentic systems with advanced reasoning and self correction capabilities. However, agentic systems come with notable response latency, a longstanding challenge for conversational recommendation systems. To balance the trade off between handling complex queries and minimizing latency, we propose AdaptJobRec, the first conversational job recommendation system that leverages autonomous agent to integrate personalized recommendation algorithm tools. The system employs a user query complexity identification mechanism to minimize response latency. For straightforward queries, the agent directly selects the appropriate tool for rapid responses. For complex queries, the agent uses the memory processing module to filter chat history for relevant content, then passes the results to the intelligent task decomposition planner, and finally executes the tasks using personalized recommendation tools. Evaluation on Walmart's real world career recommendation scenarios demonstrates that AdaptJobRec reduces average response latency by up to 53.3% compared to competitive baselines, while significantly improving recommendation accuracy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13426</link>
<guid>https://arxiv.org/abs/2508.13426</guid>
<content:encoded><![CDATA[
arXiv:2508.13426v1 Announce Type: cross 
Abstract: As large language models (LLMs) increasingly mediate cross-cultural communication, their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Easy Option Bias in Multiple-Choice Question Answering</title>
<link>https://arxiv.org/abs/2508.13428</link>
<guid>https://arxiv.org/abs/2508.13428</guid>
<content:encoded><![CDATA[
arXiv:2508.13428v1 Announce Type: cross 
Abstract: In this early study, we observe an Easy-Options Bias (EOB) issue in some multiple-choice Visual Question Answering (VQA) benchmarks such as MMStar, RealWorldQA, SEED-Bench, Next-QA, STAR benchmark and Video-MME. This bias allows vision-language models (VLMs) to select the correct answer using only the vision (V) and options (O) as inputs, without the need for the question (Q). Through grounding experiments, we attribute the bias to an imbalance in visual relevance: the correct answer typically aligns more closely with the visual contents than the negative options in feature space, creating a shortcut for VLMs to infer the answer via simply vision-option similarity matching. To fix this, we introduce GroundAttack, a toolkit that automatically generates hard negative options as visually plausible as the correct answer. We apply it to the NExT-QA and MMStar datasets, creating new EOB-free annotations. On these EOB-free annotations, current VLMs approach to random accuracies under (V+O) settings, and drop to non-saturated accuracies under (V+Q+O) settings, providing a more realistic evaluation of VLMs' QA ability. Codes and new annotations will be released soon.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaX: An AI-Based Value Investing Strategy for the Brazilian Stock Market</title>
<link>https://arxiv.org/abs/2508.13429</link>
<guid>https://arxiv.org/abs/2508.13429</guid>
<content:encoded><![CDATA[
arXiv:2508.13429v1 Announce Type: cross 
Abstract: Autonomous trading strategies have been a subject of research within the field of artificial intelligence (AI) for aconsiderable period. Various AI techniques have been explored to develop autonomous agents capable of trading financial assets. These approaches encompass traditional methods such as neural networks, fuzzy logic, and reinforcement learning, as well as more recent advancements, including deep neural networks and deep reinforcement learning. Many developers report success in creating strategies that exhibit strong performance during simulations using historical price data, a process commonly referred to as backtesting. However, when these strategies are deployed in real markets, their performance often deteriorates, particularly in terms of risk-adjusted returns. In this study, we propose an AI-based strategy inspired by a classical investment paradigm: Value Investing. Financial AI models are highly susceptible to lookahead bias and other forms of bias that can significantly inflate performance in backtesting compared to live trading conditions. To address this issue, we conducted a series of computational simulations while controlling for these biases, thereby reducing the risk of overfitting. Our results indicate that the proposed approach outperforms major Brazilian market benchmarks. Moreover, the strategy, named AlphaX, demonstrated superior performance relative to widely used technical indicators such as the Relative Strength Index (RSI) and Money Flow Index (MFI), with statistically significant results. Finally, we discuss several open challenges and highlight emerging technologies in qualitative analysis that may contribute to the development of a comprehensive AI-based Value Investing framework in the future
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventTSF: Event-Aware Non-Stationary Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.13434</link>
<guid>https://arxiv.org/abs/2508.13434</guid>
<content:encoded><![CDATA[
arXiv:2508.13434v1 Announce Type: cross 
Abstract: Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\times$ faster training efficiency.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer</title>
<link>https://arxiv.org/abs/2508.13435</link>
<guid>https://arxiv.org/abs/2508.13435</guid>
<content:encoded><![CDATA[
arXiv:2508.13435v1 Announce Type: cross 
Abstract: Directed graphs are widely used to model asymmetric relationships in real-world systems. However, existing directed graph neural networks often struggle to jointly capture directional semantics and global structural patterns due to their isotropic aggregation mechanisms and localized filtering mechanisms. To address this limitation, this paper proposes SVDformer, a novel framework that synergizes SVD and Transformer architecture for direction-aware graph representation learning. SVDformer first refines singular value embeddings through multi-head self-attention, adaptively enhancing critical spectral components while suppressing high-frequency noise. This enables learnable low-pass/high-pass graph filtering without requiring spectral kernels. Furthermore, by treating singular vectors as directional projection bases and singular values as scaling factors, SVDformer uses the Transformer to model multi-scale interactions between incoming/outgoing edge patterns through attention weights, thereby explicitly preserving edge directionality during feature propagation. Extensive experiments on six directed graph benchmarks demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and direction-aware baselines on node classification tasks, establishing a new paradigm for learning representations on directed graphs.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Design of Machine Learning Pipelines via Metalearning</title>
<link>https://arxiv.org/abs/2508.13436</link>
<guid>https://arxiv.org/abs/2508.13436</guid>
<content:encoded><![CDATA[
arXiv:2508.13436v1 Announce Type: cross 
Abstract: Automated machine learning (AutoML) has democratized the design of machine learning based systems, by automating model selection, hyperparameter tuning and feature engineering. However, the high computational cost associated with traditional search and optimization strategies, such as Random Search, Particle Swarm Optimization and Bayesian Optimization, remains a significant challenge. Moreover, AutoML systems typically explore a large search space, which can lead to overfitting. This paper introduces a metalearning method for dynamically designing search spaces for AutoML system. The proposed method uses historical metaknowledge to select promising regions of the search space, accelerating the optimization process. According to experiments conducted for this study, the proposed method can reduce runtime by 89\% in Random Search and search space by (1.8/13 preprocessor and 4.3/16 classifier), without compromising significant predictive performance. Moreover, the proposed method showed competitive performance when adapted to Auto-Sklearn, reducing its search space. Furthermore, this study encompasses insights into meta-feature selection, meta-model explainability, and the trade-offs inherent in search space reduction strategies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</title>
<link>https://arxiv.org/abs/2508.13439</link>
<guid>https://arxiv.org/abs/2508.13439</guid>
<content:encoded><![CDATA[
arXiv:2508.13439v1 Announce Type: cross 
Abstract: Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consumer Autonomy or Illusion? Rethinking Consumer Agency in the Age of Algorithms</title>
<link>https://arxiv.org/abs/2508.13440</link>
<guid>https://arxiv.org/abs/2508.13440</guid>
<content:encoded><![CDATA[
arXiv:2508.13440v1 Announce Type: cross 
Abstract: Consumer agency in the digital age is increasingly constrained by systemic barriers and algorithmic manipulation, raising concerns about the authenticity of consumption choices. Nowadays, financial decisions are shaped by external pressures like obligatory consumption, algorithmic persuasion, and unstable work schedules that erode financial autonomy. Obligatory consumption (like hidden fees) is intensified by digital ecosystems. Algorithmic tactics like personalized recommendations lead to impulsive purchases. Unstable work schedules also undermine financial planning. Thus, it is important to study how these factors impact consumption agency. To do so, we examine formal models grounded in discounted consumption with constraints that bound agency. We construct analytical scenarios in which consumers face obligatory payments, algorithm-influenced impulsive expenses, or unpredictable income due to temporal instability. Using this framework, we demonstrate that even rational, utility-maximizing agents can experience early financial ruin when agency is limited across structural, behavioral, or temporal dimensions and how diminished autonomy impacts long-term financial well-being. Our central argument is that consumer agency must be treated as a value (not a given) requiring active cultivation, especially in digital ecosystems. The connection between our formal modeling and this argument allows us to indicate that limitations on agency (whether structural, behavioral, or temporal) can be rigorously linked to measurable risks like financial instability. This connection is also a basis for normative claims about consumption as a value, by anchoring them in a formally grounded analysis of consumer behavior. As solutions, we study systemic interventions and consumer education to support value deliberation and informed choices. We formally demonstrate how these measures strengthen agency.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.13470</link>
<guid>https://arxiv.org/abs/2508.13470</guid>
<content:encoded><![CDATA[
arXiv:2508.13470v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have emerged as powerful tools for enabling automated traffic analysis; however, current approaches often demand substantial computational resources and struggle with fine-grained spatio-temporal understanding. This paper introduces STER-VLM, a computationally efficient framework that enhances VLM performance through (1) caption decomposition to tackle spatial and temporal information separately, (2) temporal frame selection with best-view filtering for sufficient temporal information, and (3) reference-driven understanding for capturing fine-grained motion and dynamic context and (4) curated visual/textual prompt techniques. Experimental results on the WTS \cite{kong2024wts} and BDD \cite{BDD} datasets demonstrate substantial gains in semantic richness and traffic scene interpretation. Our framework is validated through a decent test score of 55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in advancing resource-efficient and accurate traffic analysis for real-world applications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.13485</link>
<guid>https://arxiv.org/abs/2508.13485</guid>
<content:encoded><![CDATA[
arXiv:2508.13485v1 Announce Type: cross 
Abstract: 4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Linear Autoencoders for Recommendation</title>
<link>https://arxiv.org/abs/2508.13500</link>
<guid>https://arxiv.org/abs/2508.13500</guid>
<content:encoded><![CDATA[
arXiv:2508.13500v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</title>
<link>https://arxiv.org/abs/2508.13514</link>
<guid>https://arxiv.org/abs/2508.13514</guid>
<content:encoded><![CDATA[
arXiv:2508.13514v1 Announce Type: cross 
Abstract: Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Influence Maximization in User Recommendation</title>
<link>https://arxiv.org/abs/2508.13517</link>
<guid>https://arxiv.org/abs/2508.13517</guid>
<content:encoded><![CDATA[
arXiv:2508.13517v1 Announce Type: cross 
Abstract: User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B test, respectively. Implementation codes are available at https://github.com/socialalgo/HIM.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</title>
<link>https://arxiv.org/abs/2508.13518</link>
<guid>https://arxiv.org/abs/2508.13518</guid>
<content:encoded><![CDATA[
arXiv:2508.13518v1 Announce Type: cross 
Abstract: Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g. sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDoS Attacks in Cloud Computing: Detection and Prevention</title>
<link>https://arxiv.org/abs/2508.13522</link>
<guid>https://arxiv.org/abs/2508.13522</guid>
<content:encoded><![CDATA[
arXiv:2508.13522v1 Announce Type: cross 
Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats faced by organizations and individuals today. In recent years, the complexity and frequency of DDoS attacks have increased significantly, making it challenging to detect and mitigate them effectively. The study analyzes various types of DDoS attacks, including volumetric, protocol, and application layer attacks, and discusses the characteristics, impact, and potential targets of each type. It also examines the existing techniques used for DDoS attack detection, such as packet filtering, intrusion detection systems, and machine learning-based approaches, and their strengths and limitations. Moreover, the study explores the prevention techniques employed to mitigate DDoS attacks, such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the effectiveness of each approach and its suitability for different types of attacks and environments. In conclusion, this study provides a comprehensive overview of the different types of DDoS attacks, their detection, and prevention techniques. It aims to provide insights and guidelines for organizations and individuals to enhance their cybersecurity posture and protect against DDoS attacks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models</title>
<link>https://arxiv.org/abs/2508.13524</link>
<guid>https://arxiv.org/abs/2508.13524</guid>
<content:encoded><![CDATA[
arXiv:2508.13524v1 Announce Type: cross 
Abstract: Facial Emotion Recognition (FER) is crucial for applications such as human-computer interaction and mental health diagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including Phi-3.5 Vision and CLIP, against traditional deep learning models VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset, which contains 35,887 low-resolution grayscale images across seven emotion classes. To address the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using precision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, inference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence</title>
<link>https://arxiv.org/abs/2508.13534</link>
<guid>https://arxiv.org/abs/2508.13534</guid>
<content:encoded><![CDATA[
arXiv:2508.13534v1 Announce Type: cross 
Abstract: Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with keypoint-based abstraction, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects. Our code and video are available at https://sites.google.com/view/mimicfunc.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</title>
<link>https://arxiv.org/abs/2508.13537</link>
<guid>https://arxiv.org/abs/2508.13537</guid>
<content:encoded><![CDATA[
arXiv:2508.13537v1 Announce Type: cross 
Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2508.13544</link>
<guid>https://arxiv.org/abs/2508.13544</guid>
<content:encoded><![CDATA[
arXiv:2508.13544v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity, spatial localization, and sparse representations, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is RC-GAUSS, a novel activation designed for explicit frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform (DWT) to compute energy scores and explicitly guide frequency information to the network. Our method consistently outperforms existing INRs in 2D image representation and restoration, as well as 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collapsing ROC approach for risk prediction research on both common and rare variants</title>
<link>https://arxiv.org/abs/2508.13552</link>
<guid>https://arxiv.org/abs/2508.13552</guid>
<content:encoded><![CDATA[
arXiv:2508.13552v1 Announce Type: cross 
Abstract: Risk prediction that capitalizes on emerging genetic findings holds great promise for improving public health and clinical care. However, recent risk prediction research has shown that predictive tests formed on existing common genetic loci, including those from genome-wide association studies, have lacked sufficient accuracy for clinical use. Because most rare variants on the genome have not yet been studied for their role in risk prediction, future disease prediction discoveries should shift toward a more comprehensive risk prediction strategy that takes into account both common and rare variants. We are proposing a collapsing receiver operating characteristic CROC approach for risk prediction research on both common and rare variants. The new approach is an extension of a previously developed forward ROC FROC approach, with additional procedures for handling rare variants. The approach was evaluated through the use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction model built on all SNPs gained more accuracy AUC = 0.605 than one built on common variants alone AUC = 0.585. We further evaluated the performance of two approaches by gradually reducing the number of common variants in the analysis. We found that the CROC method attained more accuracy than the FROC method when the number of common variants in the data decreased. In an extreme scenario, when there are only rare variants in the data, the CROC reached an AUC value of 0.603, whereas the FROC had an AUC value of 0.524.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment</title>
<link>https://arxiv.org/abs/2508.13559</link>
<guid>https://arxiv.org/abs/2508.13559</guid>
<content:encoded><![CDATA[
arXiv:2508.13559v1 Announce Type: cross 
Abstract: Origami-inspired structures provide unprecedented opportunities for creating lightweight, deployable systems with programmable mechanical responses. However, their design remains challenging due to complex nonlinear mechanics, multistability, and the need for precise control of deployment forces. Here, we present a physics-informed neural network (PINN) framework for both forward prediction and inverse design of conical Kresling origami (CKO) without requiring pre-collected training data. By embedding mechanical equilibrium equations directly into the learning process, the model predicts complete energy landscapes with high accuracy while minimizing non-physical artifacts. The inverse design routine specifies both target stable-state heights and separating energy barriers, enabling freeform programming of the entire energy curve. This capability is extended to hierarchical CKO assemblies, where sequential layer-by-layer deployment is achieved through programmed barrier magnitudes. Finite element simulations and experiments on physical prototypes validate the designed deployment sequences and barrier ratios, confirming the robustness of the approach. This work establishes a versatile, data-free route for programming complex mechanical energy landscapes in origami-inspired metamaterials, offering broad potential for deployable aerospace systems, morphing structures, and soft robotic actuators.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The 9th AI City Challenge</title>
<link>https://arxiv.org/abs/2508.13564</link>
<guid>https://arxiv.org/abs/2508.13564</guid>
<content:encoded><![CDATA[
arXiv:2508.13564v1 Announce Type: cross 
Abstract: The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments</title>
<link>https://arxiv.org/abs/2508.13576</link>
<guid>https://arxiv.org/abs/2508.13576</guid>
<content:encoded><![CDATA[
arXiv:2508.13576v1 Announce Type: cross 
Abstract: The cochlear implant (CI) is a remarkable biomedical device that successfully enables individuals with severe-to-profound hearing loss to perceive sound by converting speech into electrical stimulation signals. Despite advancements in the performance of recent CI systems, speech comprehension in noisy or reverberant conditions remains a challenge. Recent and ongoing developments in deep learning reveal promising opportunities for enhancing CI sound coding capabilities, not only through replicating traditional signal processing methods with neural networks, but also through integrating visual cues as auxiliary data for multimodal speech processing. Therefore, this paper introduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an audio-visual speech enhancement (AVSE) model as a pre-processing module for the deep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically, a joint training approach is applied to model AVSE-ECS, an end-to-end CI system. Experimental results indicate that the proposed method outperforms the previous ECS strategy in noisy conditions, with improved objective speech intelligibility scores. The methods and findings in this study demonstrate the feasibility and potential of using deep learning to integrate the AVSE module into an end-to-end CI system
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Decoding Strategies in Medical Text Generation</title>
<link>https://arxiv.org/abs/2508.13580</link>
<guid>https://arxiv.org/abs/2508.13580</guid>
<content:encoded><![CDATA[
arXiv:2508.13580v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of decoding strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 decoding strategies with medically specialized and general-purpose LLMs of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\eta} and top-k sampling perform worst. Slower decoding methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to decoding. Surprisingly, while medical LLMs outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to decoding choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the decoding strategy. These results highlight the need for careful selection of decoding methods in medical applications, as their influence can sometimes exceed that of model choice.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</title>
<link>https://arxiv.org/abs/2508.13603</link>
<guid>https://arxiv.org/abs/2508.13603</guid>
<content:encoded><![CDATA[
arXiv:2508.13603v1 Announce Type: cross 
Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark's speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounding Causal Effects and Counterfactuals</title>
<link>https://arxiv.org/abs/2508.13607</link>
<guid>https://arxiv.org/abs/2508.13607</guid>
<content:encoded><![CDATA[
arXiv:2508.13607v1 Announce Type: cross 
Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics.
  All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models</title>
<link>https://arxiv.org/abs/2508.13625</link>
<guid>https://arxiv.org/abs/2508.13625</guid>
<content:encoded><![CDATA[
arXiv:2508.13625v1 Announce Type: cross 
Abstract: Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling</title>
<link>https://arxiv.org/abs/2508.13653</link>
<guid>https://arxiv.org/abs/2508.13653</guid>
<content:encoded><![CDATA[
arXiv:2508.13653v1 Announce Type: cross 
Abstract: Training modern neural networks on large datasets is computationally and environmentally costly. We introduce GRAFT, a scalable in-training subset selection method that (i) extracts a low-rank feature representation for each batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset that spans the batch's dominant subspace, and (iii) dynamically adjusts the subset size using a gradient-approximation criterion. By operating in low-rank subspaces and training on carefully chosen examples instead of full batches, GRAFT preserves the training trajectory while reducing wall-clock time, energy consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT matches or exceeds recent selection baselines in both accuracy and efficiency, providing a favorable trade-off between accuracy, efficiency, and emissions.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Input Time Scaling</title>
<link>https://arxiv.org/abs/2508.13654</link>
<guid>https://arxiv.org/abs/2508.13654</guid>
<content:encoded><![CDATA[
arXiv:2508.13654v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Decision Making for Optimizing Complex AutoML Pipelines</title>
<link>https://arxiv.org/abs/2508.13657</link>
<guid>https://arxiv.org/abs/2508.13657</guid>
<content:encoded><![CDATA[
arXiv:2508.13657v1 Announce Type: cross 
Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at https://github.com/amirbalef/CASHPlus.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2508.13673</link>
<guid>https://arxiv.org/abs/2508.13673</guid>
<content:encoded><![CDATA[
arXiv:2508.13673v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are promising brain-inspired models known for low power consumption and superior potential for temporal processing, but identifying suitable learning mechanisms remains a challenge. Despite the presence of multiple coexisting learning strategies in the brain, current SNN training methods typically rely on a single form of synaptic plasticity, which limits their adaptability and representational capability. In this paper, we propose a biologically inspired training framework that incorporates multiple synergistic plasticity mechanisms for more effective SNN training. Our method enables diverse learning algorithms to cooperatively modulate the accumulation of information, while allowing each mechanism to preserve its own relatively independent update dynamics. We evaluated our approach on both static image and dynamic neuromorphic datasets to demonstrate that our framework significantly improves performance and robustness compared to conventional learning mechanism models. This work provides a general and extensible foundation for developing more powerful SNNs guided by multi-strategy brain-inspired learning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Risk Spectrum: From Dangerous Capabilities to Existential Threats</title>
<link>https://arxiv.org/abs/2508.13700</link>
<guid>https://arxiv.org/abs/2508.13700</guid>
<content:encoded><![CDATA[
arXiv:2508.13700v1 Announce Type: cross 
Abstract: As AI systems become more capable, integrated, and widespread, understanding the associated risks becomes increasingly important. This paper maps the full spectrum of AI risks, from current harms affecting individual users to existential threats that could endanger humanity's survival. We organize these risks into three main causal categories. Misuse risks, which occur when people deliberately use AI for harmful purposes - creating bioweapons, launching cyberattacks, adversarial AI attacks or deploying lethal autonomous weapons. Misalignment risks happen when AI systems pursue outcomes that conflict with human values, irrespective of developer intentions. This includes risks arising through specification gaming (reward hacking), scheming and power-seeking tendencies in pursuit of long-term strategic goals. Systemic risks, which arise when AI integrates into complex social systems in ways that gradually undermine human agency - concentrating power, accelerating political and economic disempowerment, creating overdependence that leads to human enfeeblement, or irreversibly locking in current values curtailing future moral progress. Beyond these core categories, we identify risk amplifiers - competitive pressures, accidents, corporate indifference, and coordination failures - that make all risks more likely and severe. Throughout, we connect today's existing risks and empirically observable AI behaviors to plausible future outcomes, demonstrating how existing trends could escalate to catastrophic outcomes. Our goal is to help readers understand the complete landscape of AI risks. Good futures are possible, but they don't happen by default. Navigating these challenges will require unprecedented coordination, but an extraordinary future awaits if we do.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generics and Default Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13718</link>
<guid>https://arxiv.org/abs/2508.13718</guid>
<content:encoded><![CDATA[
arXiv:2508.13718v1 Announce Type: cross 
Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</title>
<link>https://arxiv.org/abs/2508.13729</link>
<guid>https://arxiv.org/abs/2508.13729</guid>
<content:encoded><![CDATA[
arXiv:2508.13729v1 Announce Type: cross 
Abstract: Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.
  We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2508.13730</link>
<guid>https://arxiv.org/abs/2508.13730</guid>
<content:encoded><![CDATA[
arXiv:2508.13730v1 Announce Type: cross 
Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. Security-enhancing methods aim to improve FL robustness against malicious behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same time, privacy-preserving techniques focus on protecting sensitive data through cryptographic approaches, differential privacy, and secure aggregation. We critically analyze the strengths and limitations of existing methods, highlight the trade-offs between privacy, security, and model performance, and discuss the implications of non-IID data distributions on the effectiveness of these defenses. Furthermore, we identify open research challenges and future directions, including the need for scalable, adaptive, and energy-efficient solutions operating in dynamic and heterogeneous FL environments. Our survey aims to guide researchers and practitioners in developing robust and privacy-preserving FL systems, fostering advancements safeguarding collaborative learning frameworks' integrity and confidentiality.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks</title>
<link>https://arxiv.org/abs/2508.13744</link>
<guid>https://arxiv.org/abs/2508.13744</guid>
<content:encoded><![CDATA[
arXiv:2508.13744v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) demonstrate strong performance on single-image tasks. However, we observe that their performance degrades significantly when handling multi-image inputs. This occurs because visual cues from different images become entangled in the model's output. We refer to this phenomenon as cross-image information leakage. To address this issue, we propose FOCUS, a training-free and architecture-agnostic decoding strategy that mitigates cross-image information leakage during inference. FOCUS sequentially masks all but one image with random noise, guiding the model to focus on the single clean image. We repeat this process across all target images to obtain logits under partially masked contexts. These logits are aggregated and then contrastively refined using a noise-only reference input, which suppresses the leakage and yields more accurate outputs. FOCUS consistently improves performance across four multi-image benchmarks and diverse LVLM families. This demonstrates that FOCUS offers a general and practical solution for enhancing multi-image reasoning without additional training or architectural modifications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration</title>
<link>https://arxiv.org/abs/2508.13755</link>
<guid>https://arxiv.org/abs/2508.13755</guid>
<content:encoded><![CDATA[
arXiv:2508.13755v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13757</link>
<guid>https://arxiv.org/abs/2508.13757</guid>
<content:encoded><![CDATA[
arXiv:2508.13757v1 Announce Type: cross 
Abstract: Current code generation benchmarks focus primarily on functional correctness while overlooking two critical aspects of real-world programming: algorithmic efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional Programming ASSessment), a comprehensive evaluation framework that assesses code generation across three dimensions: correctness, efficiency, and quality. COMPASS consists of 50 competitive programming problems from real Codility competitions, providing authentic human baselines from 393,150 submissions. Unlike existing benchmarks that treat algorithmically inefficient solutions identically to optimal ones provided they pass test cases, COMPASS systematically evaluates runtime efficiency and code quality using industry-standard analysis tools. Our evaluation of three leading reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and OpenAI O4-Mini-High, reveals that models achieving high correctness scores do not necessarily produce efficient algorithms or maintainable code. These findings highlight the importance of evaluating more than just correctness to truly understand the real-world capabilities of code generation models. COMPASS serves as a guiding framework, charting a path for future research toward AI systems that are robust, reliable, and ready for production use.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.13773</link>
<guid>https://arxiv.org/abs/2508.13773</guid>
<content:encoded><![CDATA[
arXiv:2508.13773v1 Announce Type: cross 
Abstract: Long-term time series forecasting (LTSF) is a fundamental task with wide-ranging applications. Although Transformer-based models have made significant breakthroughs in forecasting, their effectiveness for time series forecasting remains debatable. In this paper, we revisit the significance of self-attention and propose a simple yet effective mechanism, Periodic-Nested Group Attention, namely PENGUIN. Our approach highlights the importance of explicitly modeling periodic patterns and incorporating relative attention bias for effective time series modeling. To this end, we introduce a periodic-nested relative attention bias that captures periodic structures directly. To handle multiple coexisting periodicities (e.g., daily and weekly cycles), we design a grouped attention mechanism, where each group targets a specific periodicity using a multi-query attention mechanism. Extensive experiments across diverse benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and Transformer-based models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic DraCor and the Art of Docstring Engineering: Evaluating MCP-empowered LLM Usage of the DraCor API</title>
<link>https://arxiv.org/abs/2508.13774</link>
<guid>https://arxiv.org/abs/2508.13774</guid>
<content:encoded><![CDATA[
arXiv:2508.13774v1 Announce Type: cross 
Abstract: This paper reports on the implementation and evaluation of a Model Context Protocol (MCP) server for DraCor, enabling Large Language Models (LLM) to autonomously interact with the DraCor API. We conducted experiments focusing on tool selection and application by the LLM, employing a qualitative approach that includes systematic observation of prompts to understand how LLMs behave when using MCP tools, evaluating "Tool Correctness", "Tool-Calling Efficiency", and "Tool-Use Reliability". Our findings highlight the importance of "Docstring Engineering", defined as reflexively crafting tool documentation to optimize LLM-tool interaction. Our experiments demonstrate both the promise of agentic AI for research in Computational Literary Studies and the essential infrastructure development needs for reliable Digital Humanities infrastructures.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images</title>
<link>https://arxiv.org/abs/2508.13776</link>
<guid>https://arxiv.org/abs/2508.13776</guid>
<content:encoded><![CDATA[
arXiv:2508.13776v1 Announce Type: cross 
Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer</title>
<link>https://arxiv.org/abs/2508.13786</link>
<guid>https://arxiv.org/abs/2508.13786</guid>
<content:encoded><![CDATA[
arXiv:2508.13786v1 Announce Type: cross 
Abstract: Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web</title>
<link>https://arxiv.org/abs/2508.13787</link>
<guid>https://arxiv.org/abs/2508.13787</guid>
<content:encoded><![CDATA[
arXiv:2508.13787v1 Announce Type: cross 
Abstract: The rapid development of large language models (LLMs) has significantly propelled the development of artificial intelligence (AI) agents, which are increasingly evolving into diverse autonomous entities, advancing the LLM-based multi-agent systems (LaMAS). However, current agentic ecosystems remain fragmented and closed. Establishing an interconnected and scalable paradigm for Agentic AI has become a critical prerequisite. Although Agentic Web proposes an open architecture to break the ecosystem barriers, its implementation still faces core challenges such as privacy protection, data management, and value measurement. Existing centralized or semi-centralized paradigms suffer from inherent limitations, making them inadequate for supporting large-scale, heterogeneous, and cross-domain autonomous interactions. To address these challenges, this paper introduces the blockchain-enabled trustworthy Agentic Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not only offers a trustworthy and scalable infrastructure for LaMAS but also has the potential to advance the Web paradigm from Web3 (centered on data ownership) towards Web3.5, which emphasizes ownership of agent capabilities and the monetization of intelligence. Beyond a systematic examination of the BetaWeb framework, this paper presents a five-stage evolutionary roadmap, outlining the path of LaMAS from passive execution to advanced collaboration and autonomous governance. We also conduct a comparative analysis of existing products and discuss key challenges of BetaWeb from multiple perspectives. Ultimately, we argue that deep integration between blockchain and LaMAS can lay the foundation for a resilient, trustworthy, and sustainably incentivized digital ecosystem. A summary of the enabling technologies for each stage is available at https://github.com/MatZaharia/BetaWeb.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fully Transformer Based Multimodal Framework for Explainable Cancer Image Segmentation Using Radiology Reports</title>
<link>https://arxiv.org/abs/2508.13796</link>
<guid>https://arxiv.org/abs/2508.13796</guid>
<content:encoded><![CDATA[
arXiv:2508.13796v1 Announce Type: cross 
Abstract: We introduce Med-CTX, a fully transformer based multimodal framework for explainable breast cancer ultrasound segmentation. We integrate clinical radiology reports to boost both performance and interpretability. Med-CTX achieves exact lesion delineation by using a dual-branch visual encoder that combines ViT and Swin transformers, as well as uncertainty aware fusion. Clinical language structured with BI-RADS semantics is encoded by BioClinicalBERT and combined with visual features utilising cross-modal attention, allowing the model to provide clinically grounded, model generated explanations. Our methodology generates segmentation masks, uncertainty maps, and diagnostic rationales all at once, increasing confidence and transparency in computer assisted diagnosis. On the BUS-BRA dataset, Med-CTX achieves a Dice score of 99% and an IoU of 95%, beating existing baselines U-Net, ViT, and Swin. Clinical text plays a key role in segmentation accuracy and explanation quality, as evidenced by ablation studies that show a -5.4% decline in Dice score and -31% in CIDEr. Med-CTX achieves good multimodal alignment (CLIP score: 85%) and increased confi dence calibration (ECE: 3.2%), setting a new bar for trustworthy, multimodal medical architecture.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</title>
<link>https://arxiv.org/abs/2508.13805</link>
<guid>https://arxiv.org/abs/2508.13805</guid>
<content:encoded><![CDATA[
arXiv:2508.13805v1 Announce Type: cross 
Abstract: Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model "writes while counting." We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias</title>
<link>https://arxiv.org/abs/2508.13813</link>
<guid>https://arxiv.org/abs/2508.13813</guid>
<content:encoded><![CDATA[
arXiv:2508.13813v1 Announce Type: cross 
Abstract: As AI systems increasingly rely on training data, assessing dataset trustworthiness has become critical, particularly for properties like fairness or bias that emerge at the dataset level. Prior work has used Subjective Logic to assess trustworthiness of individual data, but not to evaluate trustworthiness properties that emerge only at the level of the dataset as a whole. This paper introduces the first formal framework for assessing the trustworthiness of AI training datasets, enabling uncertainty-aware evaluations of global properties such as bias. Built on Subjective Logic, our approach supports trust propositions and quantifies uncertainty in scenarios where evidence is incomplete, distributed, and/or conflicting. We instantiate this framework on the trustworthiness property of bias, and we experimentally evaluate it based on a traffic sign recognition dataset. The results demonstrate that our method captures class imbalance and remains interpretable and robust in both centralized and federated contexts.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of a perfect metric: Why evaluating AI's words is harder than it looks</title>
<link>https://arxiv.org/abs/2508.13816</link>
<guid>https://arxiv.org/abs/2508.13816</guid>
<content:encoded><![CDATA[
arXiv:2508.13816v1 Announce Type: cross 
Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical adoption of AI, but has been a longstanding research challenge. While human evaluation is considered the de-facto standard, it is expensive and lacks scalability. Practical applications have driven the development of various automatic evaluation metrics (AEM), designed to compare the model output with human-written references, generating a score which approximates human judgment. Over time, AEMs have evolved from simple lexical comparisons, to semantic similarity models and, more recently, to LLM-based evaluators. However, it seems that no single metric has emerged as a definitive solution, resulting in studies using different ones without fully considering the implications. This paper aims to show this by conducting a thorough examination of the methodologies of existing metrics, their documented strengths and limitations, validation methods, and correlations with human judgment. We identify several key challenges: metrics often capture only specific aspects of text quality, their effectiveness varies by task and dataset, validation practices remain unstructured, and correlations with human judgment are inconsistent. Importantly, we find that these challenges persist in the most recent type of metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented Generation (RAG), an increasingly relevant task in academia and industry. Our findings challenge the quest for the 'perfect metric'. We propose selecting metrics based on task-specific needs and leveraging complementary evaluations and advocate that new metrics should focus on enhanced validation methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</title>
<link>https://arxiv.org/abs/2508.13833</link>
<guid>https://arxiv.org/abs/2508.13833</guid>
<content:encoded><![CDATA[
arXiv:2508.13833v1 Announce Type: cross 
Abstract: This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\_core\_news\_lg exhibited superior performance in NER, achieving F1-scores over 90\%, while Random Forest proved most effective in RE, with an F1 score above 80\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression</title>
<link>https://arxiv.org/abs/2508.13836</link>
<guid>https://arxiv.org/abs/2508.13836</guid>
<content:encoded><![CDATA[
arXiv:2508.13836v1 Announce Type: cross 
Abstract: Pruning is a core technique for compressing neural networks to improve computational efficiency. This process is typically approached in two ways: one-shot pruning, which involves a single pass of training and pruning, and iterative pruning, where pruning is performed over multiple cycles for potentially finer network refinement. Although iterative pruning has historically seen broader adoption, this preference is often assumed rather than rigorously tested. Our study presents one of the first systematic and comprehensive comparisons of these methods, providing rigorous definitions, benchmarking both across structured and unstructured settings, and applying different pruning criteria and modalities. We find that each method has specific advantages: one-shot pruning proves more effective at lower pruning ratios, while iterative pruning performs better at higher ratios. Building on these findings, we advocate for patience-based pruning and introduce a hybrid approach that can outperform traditional methods in certain scenarios, providing valuable insights for practitioners selecting a pruning strategy tailored to their goals and constraints. Source code is available at https://github.com/janumiko/pruning-benchmark.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion</title>
<link>https://arxiv.org/abs/2508.13843</link>
<guid>https://arxiv.org/abs/2508.13843</guid>
<content:encoded><![CDATA[
arXiv:2508.13843v1 Announce Type: cross 
Abstract: Current e-commerce multimodal retrieval systems face two key limitations: they optimize for specific tasks with fixed modality pairings, and lack comprehensive benchmarks for evaluating unified retrieval approaches. To address these challenges, we introduce UniECS, a unified multimodal e-commerce search framework that handles all retrieval scenarios across image, text, and their combinations. Our work makes three key contributions. First, we propose a flexible architecture with a novel gated multimodal encoder that uses adaptive fusion mechanisms. This encoder integrates different modality representations while handling missing modalities. Second, we develop a comprehensive training strategy to optimize learning. It combines cross-modal alignment loss (CMAL), cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and adaptive loss weighting. Third, we create M-BEER, a carefully curated multimodal benchmark containing 50K product pairs for e-commerce search evaluation. Extensive experiments demonstrate that UniECS consistently outperforms existing methods across four e-commerce benchmarks with fine-tuning or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial improvements in cross-modal tasks (up to 28\% gain in R@10 for text-to-image retrieval) while maintaining parameter efficiency (0.2B parameters) compared to larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy UniECS in the e-commerce search platform of Kuaishou Inc. across two search scenarios, achieving notable improvements in Click-Through Rate (+2.74\%) and Revenue (+8.33\%). The comprehensive evaluation demonstrates the effectiveness of our approach in both experimental and real-world settings. Corresponding codes, models and datasets will be made publicly available at https://github.com/qzp2018/UniECS.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler</title>
<link>https://arxiv.org/abs/2508.13875</link>
<guid>https://arxiv.org/abs/2508.13875</guid>
<content:encoded><![CDATA[
arXiv:2508.13875v1 Announce Type: cross 
Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer</title>
<link>https://arxiv.org/abs/2508.13877</link>
<guid>https://arxiv.org/abs/2508.13877</guid>
<content:encoded><![CDATA[
arXiv:2508.13877v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated great potential in robotic operations. However, its data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit its practical deployment in real-world scenarios involving complex dynamics and long-term temporal dependencies, such as multi-robot manipulation. Decision Transformers (DTs) have emerged as a promising offline alternative by leveraging causal transformers for sequence modeling in RL tasks. However, their applications to multi-robot manipulations still remain underexplored. To address this gap, we propose a novel framework, Symbolically-Guided Decision Transformer (SGDT), which integrates a neuro-symbolic mechanism with a causal transformer to enable deployable multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic planner generates a high-level task-oriented plan composed of symbolic subgoals. Guided by these subgoals, a goal-conditioned decision transformer (GCDT) performs low-level sequential decision-making for multi-robot manipulation. This hierarchical architecture enables structured, interpretable, and generalizable decision making in complex multi-robot collaboration tasks. We evaluate the performance of SGDT across a range of task scenarios, including zero-shot and few-shot scenarios. To our knowledge, this is the first work to explore DT-based technology for multi-robot manipulation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches</title>
<link>https://arxiv.org/abs/2508.13898</link>
<guid>https://arxiv.org/abs/2508.13898</guid>
<content:encoded><![CDATA[
arXiv:2508.13898v1 Announce Type: cross 
Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory, enabling them to support mini-batch sizes of up to tens of thousands of training samples. However, most existing optimizers struggle to perform effectively at such a large batch size. As batch size increases, gradient noise decreases due to averaging over many samples, limiting the ability of first-order methods to escape sharp or suboptimal minima and reach the global minimum. Meanwhile, second-order methods like the natural gradient with Kronecker-Factored Approximate Curvature (KFAC) often require excessively high damping to remain stable at large batch sizes. This high damping effectively washes out the curvature information that gives these methods their advantage, reducing their performance to that of simple gradient descent. In this paper, we introduce Fisher-Orthogonal Projection (FOP), a novel technique that restores the effectiveness of the second-order method at very large batch sizes, enabling scalable training with improved generalization and faster convergence. FOP constructs a variance-aware update direction by leveraging gradients from two sub-batches, enhancing the average gradient with a component of the gradient difference that is orthogonal to the average under the Fisher-metric.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control</title>
<link>https://arxiv.org/abs/2508.13922</link>
<guid>https://arxiv.org/abs/2508.13922</guid>
<content:encoded><![CDATA[
arXiv:2508.13922v1 Announce Type: cross 
Abstract: A policy in deep reinforcement learning (RL), either deterministic or stochastic, is commonly parameterized as a Gaussian distribution alone, limiting the learned behavior to be unimodal. However, the nature of many practical decision-making problems favors a multimodal policy that facilitates robust exploration of the environment and thus to address learning challenges arising from sparse rewards, complex dynamics, or the need for strategic adaptation to varying contexts. This issue is exacerbated in continuous control domains where exploration usually takes place in the vicinity of the predicted optimal action, either through an additive Gaussian noise or the sampling process of a stochastic policy. In this paper, we introduce Categorical Policies to model multimodal behavior modes with an intermediate categorical distribution, and then generate output action that is conditioned on the sampled mode. We explore two sampling schemes that ensure differentiable discrete latent structure while maintaining efficient gradient-based optimization. By utilizing a latent categorical distribution to select the behavior mode, our approach naturally expresses multimodality while remaining fully differentiable via the sampling tricks. We evaluate our multimodal policy on a set of DeepMind Control Suite environments, demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies. Our results indicate that the Categorical distribution serves as a powerful tool for structured exploration and multimodal behavior representation in continuous control.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems</title>
<link>https://arxiv.org/abs/2508.13930</link>
<guid>https://arxiv.org/abs/2508.13930</guid>
<content:encoded><![CDATA[
arXiv:2508.13930v1 Announce Type: cross 
Abstract: This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \href{https://github.com/danilotpnta/IR2-project}{this https URL}.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Orchestration Markup Language</title>
<link>https://arxiv.org/abs/2508.13948</link>
<guid>https://arxiv.org/abs/2508.13948</guid>
<content:encoded><![CDATA[
arXiv:2508.13948v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mechanism for Mutual Fairness in Cooperative Games with Replicable Resources -- Extended Version</title>
<link>https://arxiv.org/abs/2508.13960</link>
<guid>https://arxiv.org/abs/2508.13960</guid>
<content:encoded><![CDATA[
arXiv:2508.13960v1 Announce Type: cross 
Abstract: The latest developments in AI focus on agentic systems where artificial and human agents cooperate to realize global goals. An example is collaborative learning, which aims to train a global model based on data from individual agents. A major challenge in designing such systems is to guarantee safety and alignment with human values, particularly a fair distribution of rewards upon achieving the global goal. Cooperative game theory offers useful abstractions of cooperating agents via value functions, which assign value to each coalition, and via reward functions. With these, the idea of fair allocation can be formalized by specifying fairness axioms and designing concrete mechanisms. Classical cooperative game theory, exemplified by the Shapley value, does not fully capture scenarios like collaborative learning, as it assumes nonreplicable resources, whereas data and models can be replicated. Infinite replicability requires a generalized notion of fairness, formalized through new axioms and mechanisms. These must address imbalances in reciprocal benefits among participants, which can lead to strategic exploitation and unfair allocations. The main contribution of this paper is a mechanism and a proof that it fulfills the property of mutual fairness, formalized by the Balanced Reciprocity Axiom. It ensures that, for every pair of players, each benefits equally from the participation of the other.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Use AI for Learning: How Can We Effectively Teach and Measure Prompting Literacy for K-12 Students?</title>
<link>https://arxiv.org/abs/2508.13962</link>
<guid>https://arxiv.org/abs/2508.13962</guid>
<content:encoded><![CDATA[
arXiv:2508.13962v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) becomes increasingly integrated into daily life, there is a growing need to equip the next generation with the ability to apply, interact with, evaluate, and collaborate with AI systems responsibly. Prior research highlights the urgent demand from K-12 educators to teach students the ethical and effective use of AI for learning. To address this need, we designed an Large-Language Model (LLM)-based module to teach prompting literacy. This includes scenario-based deliberate practice activities with direct interaction with intelligent LLM agents, aiming to foster secondary school students' responsible engagement with AI chatbots. We conducted two iterations of classroom deployment in 11 authentic secondary education classrooms, and evaluated 1) AI-based auto-grader's capability; 2) students' prompting performance and confidence changes towards using AI for learning; and 3) the quality of learning and assessment materials. Results indicated that the AI-based auto-grader could grade student-written prompts with satisfactory quality. In addition, the instructional materials supported students in improving their prompting skills through practice and led to positive shifts in their perceptions of using AI for learning. Furthermore, data from Study 1 informed assessment revisions in Study 2. Analyses of item difficulty and discrimination in Study 2 showed that True/False and open-ended questions could measure prompting literacy more effectively than multiple-choice questions for our target learners. These promising outcomes highlight the potential for broader deployment and highlight the need for broader studies to assess learning effectiveness and assessment design.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</title>
<link>https://arxiv.org/abs/2508.13968</link>
<guid>https://arxiv.org/abs/2508.13968</guid>
<content:encoded><![CDATA[
arXiv:2508.13968v1 Announce Type: cross 
Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Social Context of Human-Robot Interactions</title>
<link>https://arxiv.org/abs/2508.13982</link>
<guid>https://arxiv.org/abs/2508.13982</guid>
<content:encoded><![CDATA[
arXiv:2508.13982v1 Announce Type: cross 
Abstract: The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term "social context" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term "social context". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</title>
<link>https://arxiv.org/abs/2508.13993</link>
<guid>https://arxiv.org/abs/2508.13993</guid>
<content:encoded><![CDATA[
arXiv:2508.13993v1 Announce Type: cross 
Abstract: Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation</title>
<link>https://arxiv.org/abs/2508.13998</link>
<guid>https://arxiv.org/abs/2508.13998</guid>
<content:encoded><![CDATA[
arXiv:2508.13998v1 Announce Type: cross 
Abstract: Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery</title>
<link>https://arxiv.org/abs/2508.14005</link>
<guid>https://arxiv.org/abs/2508.14005</guid>
<content:encoded><![CDATA[
arXiv:2508.14005v1 Announce Type: cross 
Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Identity Leakage in Speaker De-Identification Systems</title>
<link>https://arxiv.org/abs/2508.14012</link>
<guid>https://arxiv.org/abs/2508.14012</guid>
<content:encoded><![CDATA[
arXiv:2508.14012v1 Announce Type: cross 
Abstract: Speaker de-identification aims to conceal a speaker's identity while preserving intelligibility of the underlying speech. We introduce a benchmark that quantifies residual identity leakage with three complementary error rates: equal error rate, cumulative match characteristic hit rate, and embedding-space similarity measured via canonical correlation analysis and Procrustes analysis. Evaluation results reveal that all state-of-the-art speaker de-identification systems leak identity information. The highest performing system in our evaluation performs only slightly better than random guessing, while the lowest performing system achieves a 45% hit rate within the top 50 candidates based on CMC. These findings highlight persistent privacy risks in current speaker de-identification technologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Graph Unlearning with Zeroth-order Information</title>
<link>https://arxiv.org/abs/2508.14013</link>
<guid>https://arxiv.org/abs/2508.14013</guid>
<content:encoded><![CDATA[
arXiv:2508.14013v1 Announce Type: cross 
Abstract: Due to regulations like the Right to be Forgotten, there is growing demand for removing training data and its influence from models. Since full retraining is costly, various machine unlearning methods have been proposed. In this paper, we firstly present an efficient knowledge graph (KG) unlearning algorithm. We remark that KG unlearning is nontrivial due to the distinctive structure of KG and the semantic relations between entities. Also, unlearning by estimating the influence of removed components incurs significant computational overhead when applied to large-scale knowledge graphs. To this end, we define an influence function for KG unlearning and propose to approximate the model's sensitivity without expensive computation of first-order and second-order derivatives for parameter updates. Specifically, we use Taylor expansion to estimate the parameter changes caused by data removal. Given that the first-order gradients and second-order derivatives dominate the computational load, we use the Fisher matrices and zeroth-order optimization to approximate the inverse-Hessian vector product without constructing the computational graphs. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art graph unlearning baselines significantly in terms of unlearning efficiency and unlearning quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Good Questions for Large Language Models</title>
<link>https://arxiv.org/abs/2508.14025</link>
<guid>https://arxiv.org/abs/2508.14025</guid>
<content:encoded><![CDATA[
arXiv:2508.14025v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
arXiv:2508.14031v1 Announce Type: cross 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</title>
<link>https://arxiv.org/abs/2508.14036</link>
<guid>https://arxiv.org/abs/2508.14036</guid>
<content:encoded><![CDATA[
arXiv:2508.14036v1 Announce Type: cross 
Abstract: Modern 3D generation methods can rapidly create shapes from sparse or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while preserving global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to sparse multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration</title>
<link>https://arxiv.org/abs/2411.05844</link>
<guid>https://arxiv.org/abs/2411.05844</guid>
<content:encoded><![CDATA[
arXiv:2411.05844v3 Announce Type: replace 
Abstract: GraphRAG integrates (knowledge) graphs with large language models (LLMs) to improve reasoning accuracy and contextual relevance. Despite its promising applications and strong relevance to multiple research communities, such as databases and natural language processing, GraphRAG currently lacks modular workflow analysis, systematic solution frameworks, and insightful empirical studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2) systematic classification of existing techniques and implemented GraphRAG instances, and 3) creation of new GraphRAG instances. Our framework facilitates comprehensive empirical studies of GraphRAG on large-scale real-world graphs and diverse query sets, revealing insights into balancing reasoning quality, runtime efficiency, and token or GPU cost, that are essential for building advanced GraphRAG systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for Mid-term Human Mobility Prediction</title>
<link>https://arxiv.org/abs/2501.06561</link>
<guid>https://arxiv.org/abs/2501.06561</guid>
<content:encoded><![CDATA[
arXiv:2501.06561v2 Announce Type: replace 
Abstract: Predicting individual mobility patterns is crucial across various applications. While current methods mainly focus on predicting the next location for personalized services like recommendations, they often fall short in supporting broader applications such as traffic management and epidemic control, which require longer period forecasts of human mobility. This study addresses mid-term mobility prediction, aiming to capture daily travel patterns and forecast trajectories for the upcoming day or week. We propose a novel Multi-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to efficiently extract spatial and temporal information by decoupling daily trajectories into distinct location-duration chains. Our approach employs a hierarchical encoder to model multi-scale temporal patterns, including daily recurrence and weekly periodicity, and utilizes a transformer-based decoder to globally attend to predicted information in the location or duration chain. Additionally, we introduce a spatial heterogeneous graph learner to capture multi-scale spatial relationships, enhancing semantic-rich representations. Extensive experiments, including statistical physics analysis, are conducted on large-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay Area, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to epidemic modeling in Boston, MSTDP significantly outperforms the best-performing baseline, achieving a remarkable 62.8% reduction in MAE for cumulative new cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRoPE: Rotary Position Embedding for Video Large Language Models</title>
<link>https://arxiv.org/abs/2502.11664</link>
<guid>https://arxiv.org/abs/2502.11664</guid>
<content:encoded><![CDATA[
arXiv:2502.11664v3 Announce Type: replace 
Abstract: Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at https://github.com/johncaged/VRoPE.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial Intelligence Course</title>
<link>https://arxiv.org/abs/2503.07928</link>
<guid>https://arxiv.org/abs/2503.07928</guid>
<content:encoded><![CDATA[
arXiv:2503.07928v3 Announce Type: replace 
Abstract: The widespread availability of large language models (LLMs), such as ChatGPT, has significantly impacted education, raising both opportunities and challenges. Students can frequently interact with LLM-powered, interactive learning tools, but their usage patterns need to be monitored and understood. We introduce StudyChat, a publicly available dataset capturing real-world student interactions with an LLM-powered tutoring chatbot in a semester-long, university-level artificial intelligence (AI) course. We deploy a web application that replicates ChatGPTs core functionalities, and use it to log student interactions with the LLM while working on programming assignments. We collect 16,851 interactions, which we annotate using a dialogue act labeling schema inspired by observed interaction patterns and prior research. We analyze these interactions, highlight usage trends, and analyze how specific student behavior correlates with their course outcome. We find that students who prompt LLMs for conceptual understanding and coding help tend to perform better on assignments and exams. Moreover, students who use LLMs to write reports and circumvent assignment learning objectives have lower outcomes on exams than others. StudyChat serves as a shared resource to facilitate further research on the evolving role of LLMs in education.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoAI: Enhancing AI Students' Learning Paths and Idea Generation via Graph of AI Ideas</title>
<link>https://arxiv.org/abs/2503.08549</link>
<guid>https://arxiv.org/abs/2503.08549</guid>
<content:encoded><![CDATA[
arXiv:2503.08549v2 Announce Type: replace 
Abstract: With the rapid advancement of artificial intelligence technology, AI students are confronted with a significant "information-to-innovation" gap: they must navigate through the rapidly expanding body of literature, trace the development of a specific research field, and synthesize various techniques into feasible innovative concepts. An additional critical step for students is to identify the necessary prerequisite knowledge and learning paths. Although many approaches based on large language models (LLMs) can summarize the content of papers and trace the development of a field through citations, these methods often overlook the prerequisite knowledge involved in the papers and the rich semantic information embedded in the citation relationships between papers. Such information reveals how methods are interrelated, built upon, extended, or challenged. To address these limitations, we propose GoAI, a tool for constructing educational knowledge graphs from AI research papers that leverages these graphs to plan personalized learning paths and support creative ideation. The nodes in the knowledge graph we have built include papers and the prerequisite knowledge, such as concepts, skills, and tools, that they involve; the edges record the semantic information of citations. When a student queries a specific paper, a beam search-based path search method can trace the current development trends of the field from the queried paper and plan a learning path toward cutting-edge objectives. The integrated Idea Studio guides students to clarify problem statements, compare alternative designs, and provide formative feedback on novelty, clarity, feasibility, and alignment with learning objectives.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hawkeye:Efficient Reasoning with Model Collaboration</title>
<link>https://arxiv.org/abs/2504.00424</link>
<guid>https://arxiv.org/abs/2504.00424</guid>
<content:encoded><![CDATA[
arXiv:2504.00424v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in enhancing the reasoning abilities of large language models (LLMs). However, its efficiency remains a challenge due to the generation of excessive intermediate reasoning tokens, which introduce semantic redundancy and overly detailed reasoning steps. Moreover, computational expense and latency are significant concerns, as the cost scales with the number of output tokens, including those intermediate steps. In this work, we observe that most CoT tokens are unnecessary, and retaining only a small portion of them is sufficient for producing high-quality responses. Inspired by this, we propose HAWKEYE, a novel post-training and inference framework where a large model produces concise CoT instructions to guide a smaller model in response generation. HAWKEYE quantifies redundancy in CoT reasoning and distills high-density information via reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able to expand responses while reducing token usage and computational cost significantly. Our evaluation shows that HAWKEYE can achieve comparable response quality using only 35% of the full CoTs, while improving clarity, coherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can accelerate end-to-end reasoning by up to 3.4x on complex math tasks while reducing inference cost by up to 60%. HAWKEYE will be open-sourced and the models will be available soon.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning</title>
<link>https://arxiv.org/abs/2504.09772</link>
<guid>https://arxiv.org/abs/2504.09772</guid>
<content:encoded><![CDATA[
arXiv:2504.09772v2 Announce Type: replace 
Abstract: Multi-agent systems (MAS) built on large language models (LLMs) offer a promising path toward solving complex, real-world tasks that single-agent systems often struggle to manage. While recent advancements in test-time scaling (TTS) have significantly improved single-agent performance on challenging reasoning tasks, how to effectively scale collaboration and reasoning in MAS remains an open question. In this work, we introduce an adaptive multi-agent framework designed to enhance collaborative reasoning through both model-level training and system-level coordination. We construct M500, a high-quality dataset containing 500 multi-agent collaborative reasoning traces, and fine-tune Qwen2.5-32B-Instruct on this dataset to produce M1-32B, a model optimized for multi-agent collaboration. To further enable adaptive reasoning, we propose a novel CEO agent that dynamically manages the discussion process, guiding agent collaboration and adjusting reasoning depth for more effective problem-solving. Evaluated in an open-source MAS across a range of tasks-including general understanding, mathematical reasoning, and coding-our system significantly outperforms strong baselines. For instance, M1-32B achieves 12% improvement on GPQA-Diamond, 41% on AIME2024, and 10% on MBPP-Sanitized, matching the performance of state-of-the-art models like DeepSeek-R1 on some tasks. These results highlight the importance of both learned collaboration and adaptive coordination in scaling multi-agent reasoning. Code is available at https://github.com/jincan333/MAS-TTS
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, but verify</title>
<link>https://arxiv.org/abs/2504.13443</link>
<guid>https://arxiv.org/abs/2504.13443</guid>
<content:encoded><![CDATA[
arXiv:2504.13443v2 Announce Type: replace 
Abstract: Decentralized AI agent networks, such as Gaia, allows individuals to run customized LLMs on their own computers and then provide services to the public. However, in order to maintain service quality, the network must verify that individual nodes are running their designated LLMs. In this paper, we demonstrate that in a cluster of mostly honest nodes, we can detect nodes that run unauthorized or incorrect LLM through social consensus of its peers. We will discuss the algorithm and experimental data from the Gaia network. We will also discuss the intersubjective validation system, implemented as an EigenLayer AVS to introduce financial incentives and penalties to encourage honest behavior from LLM nodes.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots</title>
<link>https://arxiv.org/abs/2504.18794</link>
<guid>https://arxiv.org/abs/2504.18794</guid>
<content:encoded><![CDATA[
arXiv:2504.18794v3 Announce Type: replace 
Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to leverage the inherent hierarchy in learning tasks where traditional reinforcement learning (RL) often fails. In this research, HRL is evaluated and contrasted with traditional RL in complex robotic navigation tasks. We evaluate unique characteristics of HRL, including its ability to create sub-goals and the termination functions. We constructed a number of experiments to test: 1) the differences between RL proximal policy optimization (PPO) and HRL, 2) different ways of creating sub-goals in HRL, 3) manual vs automatic sub-goal creation in HRL, and 4) the effects of the frequency of termination on performance in HRL. These experiments highlight the advantages of HRL over RL and how it achieves these advantages.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics</title>
<link>https://arxiv.org/abs/2506.02873</link>
<guid>https://arxiv.org/abs/2506.02873</guid>
<content:encoded><![CDATA[
arXiv:2506.02873v2 Announce Type: replace 
Abstract: Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly ``follow orders'' to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a model's willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com/AlignmentResearch/AttemptPersuadeEval
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation</title>
<link>https://arxiv.org/abs/2506.04251</link>
<guid>https://arxiv.org/abs/2506.04251</guid>
<content:encoded><![CDATA[
arXiv:2506.04251v2 Announce Type: replace 
Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Diachronic Evolution of Legal Norms: An LRMoo-Based, Component-Level Approach</title>
<link>https://arxiv.org/abs/2506.07853</link>
<guid>https://arxiv.org/abs/2506.07853</guid>
<content:encoded><![CDATA[
arXiv:2506.07853v2 Announce Type: replace 
Abstract: Effectively representing the temporal evolution of legal norms at the component level is a critical challenge. While frameworks like IFLA LRMoo and standards like Akoma Ntoso provide generic toolkits, a dedicated pattern for granular versioning is needed to enable the deterministic point-in-time reconstruction of legal texts required by reliable AI applications.
  This paper proposes a temporal modeling pattern grounded in the LRMoo ontology that models a norm's evolution as a diachronic chain of F2 Expressions. We introduce a key distinction between a language-agnostic Temporal Version (TV) - a semantic snapshot of the norm's structure - and its concrete monolingual realizations, the Language Versions (LV). Both are modeled as F2 Expressions linked by the canonical R76 is derivative of property.
  The model applies this paradigm recursively, representing the legal text's internal structure as a parallel hierarchy of abstract Component Works (F1 Work) and their versioned Component Expressions (F2 Expression). Furthermore, we formalize the amendment process using the F28 Expression Creation event, allowing changes to be traced from a specific provision in an amending act to its precise effect on the amended norm. A case study on the Brazilian Federal Constitution demonstrates how this fine-grained, event-centric architecture enables the precise, deterministic retrieval and reconstruction of any part of a legal text at a specific date. The model provides a robust foundation for building verifiable knowledge graphs and advanced AI tools, overcoming the limitations of current generative models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Network Automatic Relevance Determination</title>
<link>https://arxiv.org/abs/2506.12352</link>
<guid>https://arxiv.org/abs/2506.12352</guid>
<content:encoded><![CDATA[
arXiv:2506.12352v2 Announce Type: replace 
Abstract: We propose Network Automatic Relevance Determination (NARD), an extension of ARD for linearly probabilistic models, to simultaneously model sparse relationships between inputs $X \in \mathbb R^{d \times N}$ and outputs $Y \in \mathbb R^{m \times N}$, while capturing the correlation structure among the $Y$. NARD employs a matrix normal prior which contains a sparsity-inducing parameter to identify and discard irrelevant features, thereby promoting sparsity in the model. Algorithmically, it iteratively updates both the precision matrix and the relationship between $Y$ and the refined inputs. To mitigate the computational inefficiencies of the $\mathcal O(m^3 + d^3)$ cost per iteration, we introduce Sequential NARD, which evaluates features sequentially, and a Surrogate Function Method, leveraging an efficient approximation of the marginal likelihood and simplifying the calculation of determinant and inverse of an intermediate matrix. Combining the Sequential update with the Surrogate Function method further reduces computational costs. The computational complexity per iteration for these three methods is reduced to $\mathcal O(m^3+p^3)$, $\mathcal O(m^3 + d^2)$, $\mathcal O(m^3+p^2)$, respectively, where $p \ll d$ is the final number of features in the model. Our methods demonstrate significant improvements in computational efficiency with comparable performance on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dispositions and Roles of Generically Dependent Entities</title>
<link>https://arxiv.org/abs/2506.17085</link>
<guid>https://arxiv.org/abs/2506.17085</guid>
<content:encoded><![CDATA[
arXiv:2506.17085v2 Announce Type: replace 
Abstract: BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants (like software or datasets). In this paper, we argue that this is a severe limitation, which prevents, for example, the adequate representation of the functions of computer models or the various roles of datasets during the execution of these models. We discuss the aspects of BFO 2020 that prevent the representation of realizable entities of generically dependent continuants. Two approaches to address the issue are presented: (a) the use of defined classes and (b) a proposal of changes that allow BFO to support functions, dispositions, and roles of generically dependent continuants. The latter also addresses limitations of BFO 2020 concerning the roles and dispositions of immaterial entities, particularly boundaries and sites.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Urban Planing AI Agent in the Age of Agentic AI</title>
<link>https://arxiv.org/abs/2507.14730</link>
<guid>https://arxiv.org/abs/2507.14730</guid>
<content:encoded><![CDATA[
arXiv:2507.14730v2 Announce Type: replace 
Abstract: Generative AI, large language models, and agentic AI have emerged separately of urban planning. However, the convergence between AI and urban planning presents an interesting opportunity towards AI urban planners. Existing studies conceptualizes urban planning as a generative AI task, where AI synthesizes land-use configurations under geospatial, social, and human-centric constraints and reshape automated urban design. We further identify critical gaps of existing generative urban planning studies: 1) the generative structure has to be predefined with strong assumption: all of adversarial generator-discriminator, forward and inverse diffusion structures, hierarchical zone-POI generative structure are predefined by humans; 2) ignore the power of domain expert developed tools: domain urban planners have developed various tools in the urban planning process guided by urban theory, while existing pure neural networks based generation ignore the power of the tools developed by urban planner practitioners. To address these limitations, we outline a future research direction agentic urban AI planner, calling for a new synthesis of agentic AI and participatory urbanism.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Safe Policy Improvement Using Parametric Structure</title>
<link>https://arxiv.org/abs/2507.15532</link>
<guid>https://arxiv.org/abs/2507.15532</guid>
<content:encoded><![CDATA[
arXiv:2507.15532v2 Announce Type: replace 
Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in which a new policy that reliably outperforms the behavior policy with high confidence needs to be computed using only a dataset and the behavior policy. Markov decision processes (MDPs) are the standard formalism for modeling environments in SPI. In many applications, additional information in the form of parametric dependencies between distributions in the transition dynamics is available. We make SPI more data-efficient by leveraging these dependencies through three contributions: (1) a parametric SPI algorithm that exploits known correlations between distributions to more accurately estimate the transition dynamics using the same amount of data; (2) a preprocessing technique that prunes redundant actions from the environment through a game-based abstraction; and (3) a more advanced preprocessing technique, based on satisfiability modulo theory (SMT) solving, that can identify more actions to prune. Empirical results and an ablation study show that our techniques increase the data efficiency of SPI by multiple orders of magnitude while maintaining the same reliability guarantees.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2507.19263</link>
<guid>https://arxiv.org/abs/2507.19263</guid>
<content:encoded><![CDATA[
arXiv:2507.19263v2 Announce Type: replace 
Abstract: In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework</title>
<link>https://arxiv.org/abs/2507.21830</link>
<guid>https://arxiv.org/abs/2507.21830</guid>
<content:encoded><![CDATA[
arXiv:2507.21830v3 Announce Type: replace 
Abstract: Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptation and Optimization of Automatic Speech Recognition (ASR) for the Maritime Domain in the Field of VHF Communication</title>
<link>https://arxiv.org/abs/2306.00614</link>
<guid>https://arxiv.org/abs/2306.00614</guid>
<content:encoded><![CDATA[
arXiv:2306.00614v2 Announce Type: replace-cross 
Abstract: This paper introduces a multilingual automatic speech recognizer (ASR) for maritime radio communi-cation that automatically converts received VHF radio signals into text. The challenges of maritime radio communication are described at first, and the deep learning architecture of marFM consisting of audio processing techniques and machine learning algorithms is presented. Subsequently, maritime radio data of interest is analyzed and then used to evaluate the transcription performance of our ASR model for various maritime radio data.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds</title>
<link>https://arxiv.org/abs/2308.09908</link>
<guid>https://arxiv.org/abs/2308.09908</guid>
<content:encoded><![CDATA[
arXiv:2308.09908v5 Announce Type: replace-cross 
Abstract: Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranking board and remains 2nd at the time of submitting this paper, among all online trackers in the KITTI MOT benchmark for cars1
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I see models being a whole other thing": An Empirical Study of Pre-Trained Model Naming Conventions and A Tool for Enhancing Naming Consistency</title>
<link>https://arxiv.org/abs/2310.01642</link>
<guid>https://arxiv.org/abs/2310.01642</guid>
<content:encoded><![CDATA[
arXiv:2310.01642v3 Announce Type: replace-cross 
Abstract: As innovation in deep learning continues, many engineers are incorporating Pre-Trained Models (PTMs) as components in computer systems. Some PTMs are foundation models, and others are fine-tuned variations adapted to different needs. When these PTMs are named well, it facilitates model discovery and reuse. However, prior research has shown that model names are not always well chosen and can sometimes be inaccurate and misleading. The naming practices for PTM packages have not been systematically studied, which hampers engineers' ability to efficiently search for and reliably reuse these models. In this paper, we conduct the first empirical investigation of PTM naming practices in the Hugging Face PTM registry. We begin by reporting on a survey of 108 Hugging Face users, highlighting differences from traditional software package naming and presenting findings on PTM naming practices. The survey results indicate a mismatch between engineers' preferences and current practices in PTM naming. We then introduce DARA, the first automated DNN ARchitecture Assessment technique designed to detect PTM naming inconsistencies. Our results demonstrate that architectural information alone is sufficient to detect these inconsistencies, achieving an accuracy of 94% in identifying model types and promising performance (over 70%) in other architectural metadata as well. We also highlight potential use cases for automated naming tools, such as model validation, PTM metadata generation and verification, and plagiarism detection. Our study provides a foundation for automating naming inconsistency detection. Finally, we envision future work focusing on automated tools for standardizing package naming, improving model selection and reuse, and strengthening the security of the PTM supply chain.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radio Map Estimation: Empirical Validation and Analysis</title>
<link>https://arxiv.org/abs/2310.11036</link>
<guid>https://arxiv.org/abs/2310.11036</guid>
<content:encoded><![CDATA[
arXiv:2310.11036v3 Announce Type: replace-cross 
Abstract: Radio maps provide metrics such as the received signal strength at every location in a geographical region of interest. Extensive research has been carried out in this context, but it relies almost exclusively on synthetic-data experiments. Thus, the practical aspects of the radio map estimation (RME) problem as well as the performance of existing estimators in the real world remain unknown. To fill this gap end, this paper puts forth the first comprehensive, rigorous, and reproducible study of RME with real data. The main contributions include (C1) an assessment of the viability of RME based on the estimation error that can be achieved, (C2) the analysis of the main phenomena and trade-offs involved in RME, including the experimental verification of theoretical findings in the literature, and (C3) a thorough evaluation of a wide range of estimators on realworld data. Remarkably, this reveals that the performance gain of existing deep estimators in their pure form may not compensate for their complexity. A simple enhancement (C4) is proposed to alleviate this issue. The vast amount of data collected for this study is published along with the developed simulator to enable research on new schemes, hopefully bringing RME one step closer to practical deployment.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Problems in Learning Multiple Dynamical Systems</title>
<link>https://arxiv.org/abs/2311.02181</link>
<guid>https://arxiv.org/abs/2311.02181</guid>
<content:encoded><![CDATA[
arXiv:2311.02181v4 Announce Type: replace-cross 
Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results. The key highlight of this method is that it does not require a predefined hidden state dimension but instead provides an upper bound. Additionally, it offers guidance for determining regularization in the system identification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Echocardiography Images and Medical Records for Continuous Patient Stratification</title>
<link>https://arxiv.org/abs/2401.07796</link>
<guid>https://arxiv.org/abs/2401.07796</guid>
<content:encoded><![CDATA[
arXiv:2401.07796v3 Announce Type: replace-cross 
Abstract: Deep learning enables automatic and robust extraction of cardiac function descriptors from echocardiographic sequences, such as ejection fraction or strain. These descriptors provide fine-grained information that physicians consider, in conjunction with more global variables from the clinical record, to assess patients' condition. Drawing on novel Transformer models applied to tabular data, we propose a method that considers all descriptors extracted from medical records and echocardiograms to learn the representation of a cardiovascular pathology with a difficult-to-characterize continuum, namely hypertension. Our method first projects each variable into its own representation space using modality-specific approaches. These standardized representations of multimodal data are then fed to a Transformer encoder, which learns to merge them into a comprehensive representation of the patient through the task of predicting a clinical rating. This stratification task is formulated as an ordinal classification to enforce a pathological continuum in the representation space. We observe the major trends along this continuum on a cohort of 239 hypertensive patients, providing unprecedented details in the description of hypertension's impact on various cardiac function descriptors. Our analysis shows that i) the XTab foundation model's architecture allows to reach outstanding performance (96.8% AUROC) even with limited data (less than 200 training samples), ii) stratification across the population is reproducible between trainings (within 5.7% mean absolute error), and iii) patterns emerge in descriptors, some of which align with established physiological knowledge about hypertension, while others could pave the way for a more comprehensive understanding of this pathology. Code is available at https://github.com/creatis-myriad/didactic.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTBLS: A Dataset of Interactive Conversations Over Tabular Information</title>
<link>https://arxiv.org/abs/2404.12580</link>
<guid>https://arxiv.org/abs/2404.12580</guid>
<content:encoded><![CDATA[
arXiv:2404.12580v2 Announce Type: replace-cross 
Abstract: This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations that focuses on natural-language manipulation of tabular information sourced from academic pre-prints on ArXiv. The iTBLS dataset consists of three types of tabular tasks -- interpretation, modification, and generation. Interpretation focuses on tabular understanding, modification focuses on manipulating tabular information, and generation focuses on the addition of new natural-language evidence. In addition, the paper presents a novel framework that reformulates tabular operations as question-answering, where an appropriate question is formulated based on the nature of interaction and the question is answered using the user request as evidence. The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS. In addition, the question-answering-based reformulation is applied to datasets from prior work for the text-to-table task where textual paragraphs are summarized into tables. The novel approach results in up to 13% improvement in Exact-Match accuracy and up to 16% improvement in BERTScores compared to the prior state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy</title>
<link>https://arxiv.org/abs/2406.11290</link>
<guid>https://arxiv.org/abs/2406.11290</guid>
<content:encoded><![CDATA[
arXiv:2406.11290v2 Announce Type: replace-cross 
Abstract: Relevance and utility are two frequently used measures to evaluate the effectiveness of an information retrieval (IR) system. Relevance emphasizes the aboutness of a result to a query, while utility refers to the result's usefulness or value to an information seeker. In Retrieval-Augmented Generation (RAG), high-utility results should be prioritized to feed to LLMs due to their limited input bandwidth. Re-examining RAG's three core components -- relevance ranking derived from retrieval models, utility judgments, and answer generation -- aligns with Schutz's philosophical system of relevances, which encompasses three types of relevance representing different levels of human cognition that enhance each other. These three RAG components also reflect three cognitive levels for LLMs in question-answering. Therefore, we propose an Iterative utiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted extensive experiments on retrieval (TREC DL, WebAP), utility judgment task (GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results demonstrate significant improvements of ITEM in utility judgments, ranking, and answer generation upon representative baselines.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boolean Matrix Logic Programming on the GPU</title>
<link>https://arxiv.org/abs/2408.10369</link>
<guid>https://arxiv.org/abs/2408.10369</guid>
<content:encoded><![CDATA[
arXiv:2408.10369v3 Announce Type: replace-cross 
Abstract: Traditional logic programming relies on symbolic computation on the CPU, which can limit performance for large-scale inference tasks. Recent advances in GPU hardware enable high-throughput matrix operations, motivating a shift toward parallel logic inference. Boolean Matrix Logic Programming (BMLP) introduces a novel approach to datalog query evaluation using Boolean matrix algebra, well-suited to GPU acceleration. Building on this paradigm, we present two GPU-accelerated BMLP algorithms for bottom-up inference over linear dyadic recursive datalog programs. We further extend the BMLP theoretical framework to support general linear recursion with binary predicates. Empirical evaluations on reachability queries in large directed graphs and the Freebase 15K dataset show that our methods achieve 1-4 orders of magnitude speed up over state-of-the-art systems. These results demonstrate that Boolean matrix-based reasoning can significantly advance the scalability and efficiency of logic programming on modern hardware. Source code is available on https://github.com/lun-ai/BMLP.git.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Backbone Efficient Selection for Image Classification in Low-Data Regimes</title>
<link>https://arxiv.org/abs/2410.08592</link>
<guid>https://arxiv.org/abs/2410.08592</guid>
<content:encoded><![CDATA[
arXiv:2410.08592v2 Announce Type: replace-cross 
Abstract: Transfer learning has become an essential tool in modern computer vision, allowing practitioners to leverage backbones, pretrained on large datasets, to train successful models from limited annotated data. Choosing the right backbone is crucial, especially for small datasets, since final performance depends heavily on the quality of the initial feature representations. While prior work has conducted benchmarks across various datasets to identify universal top-performing backbones, we demonstrate that backbone effectiveness is highly dataset-dependent, especially in low-data scenarios where no single backbone consistently excels. To overcome this limitation, we introduce dataset-specific backbone selection as a new research direction and investigate its practical viability in low-data regimes. Since exhaustive evaluation is computationally impractical for large backbone pools, we formalize Vision Backbone Efficient Selection (VIBES) as the problem of searching for high-performing backbones under computational constraints. We define the solution space, propose several heuristics, and demonstrate VIBES feasibility for low-data image classification by performing experiments on four diverse datasets. Our results show that even simple search strategies can find well-suited backbones within a pool of over $1300$ pretrained models, outperforming generic benchmark recommendations within just ten minutes of search time on a single GPU (NVIDIA RTX A5000).
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSD-TS: Exploring the Potential of Linear State Space Models for Diffusion Models in Time Series Imputation</title>
<link>https://arxiv.org/abs/2410.13338</link>
<guid>https://arxiv.org/abs/2410.13338</guid>
<content:encoded><![CDATA[
arXiv:2410.13338v2 Announce Type: replace-cross 
Abstract: Probabilistic time series imputation has been widely applied in real-world scenarios due to its ability for uncertainty estimation and denoising diffusion probabilistic models~(DDPMs) have achieved great success in probabilistic time series imputation tasks with its power to model complex distributions. However, current DDPM-based probabilistic time series imputation methodologies are confronted with two types of challenges: 1)\textit{The backbone modules of the denoising parts are not capable of achieving sequence modeling with low time complexity.} 2)~\textit{The architecture of denoising modules can not handle the dependencies in the time series data effectively.} To address the first challenge, we explore the potential of state space model, namely Mamba, as the backbone denoising module for DDPMs. To tackle the second challenge, we carefully devise several SSM-based blocks for time series data modeling. Experimental results demonstrate that our approach can achieve state-of-the-art time series imputation results on multiple real-world datasets. Our datasets and code are available at \href{https://github.com/decisionintelligence/SSD-TS/}{https://github.com/decisionintelligence/SSD-TS/}
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy</title>
<link>https://arxiv.org/abs/2411.06723</link>
<guid>https://arxiv.org/abs/2411.06723</guid>
<content:encoded><![CDATA[
arXiv:2411.06723v2 Announce Type: replace-cross 
Abstract: Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although advances in large language models (LLMs) offer potential for more flexible interactions, their lack of controllability and explanability poses challenges in high-stakes contexts like psychotherapy. To address this, we conducted two studies in this work to explore how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. In Study 1 (N=43), an online experiment with a within-subjects design, we compared rule-based, pure LLM, and LLMs aligned with expert-crafted scripts via fine-tuning and prompting. Results showed that aligned LLMs significantly outperformed the other types of chatbots in empathy, dialogue relevance, and adherence to therapeutic principles. Building on findings, we proposed ``Script-Strategy Aligned Generation (SSAG)'', a more flexible alignment approach that reduces reliance on fully scripted content while maintaining LLMs' therapeutic adherence and controllability. In a 10-day field Study 2 (N=21), SSAG achieved comparable therapeutic effectiveness to full-scripted LLMs while requiring less than 40\% of expert-crafted dialogue content. Beyond these results, this work advances LLM applications in psychotherapy by providing a controllable and scalable solution, reducing reliance on expert effort. By enabling domain experts to align LLMs through high-level strategies rather than full scripts, SSAG supports more efficient co-development and expands access to a broader context of psychotherapy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Memorization in Generative Models via Sharpness of Probability Landscapes</title>
<link>https://arxiv.org/abs/2412.04140</link>
<guid>https://arxiv.org/abs/2412.04140</guid>
<content:encoded><![CDATA[
arXiv:2412.04140v5 Announce Type: replace-cross 
Abstract: In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term. The code is publicly available at https://github.com/Dongjae0324/sharpness_memorization_diffusion.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework</title>
<link>https://arxiv.org/abs/2501.00051</link>
<guid>https://arxiv.org/abs/2501.00051</guid>
<content:encoded><![CDATA[
arXiv:2501.00051v2 Announce Type: replace-cross 
Abstract: Digital twin (DT) technology enables real-time simulation, prediction, and optimization of physical systems, but practical deployment faces challenges from high data requirements, proprietary data constraints, and limited adaptability to evolving conditions. This work introduces DDD-GenDT, a dynamic data-driven generative digital twin framework grounded in the Dynamic Data-Driven Application Systems (DDDAS) paradigm. The architecture comprises the Physical Twin Observation Graph (PTOG) to represent operational states, an Observation Window Extraction process to capture temporal sequences, a Data Preprocessing Pipeline for sensor structuring and filtering, and an LLM ensemble for zero-shot predictive inference. By leveraging generative AI, DDD-GenDT reduces reliance on extensive historical datasets, enabling DT construction in data-scarce settings while maintaining industrial data privacy. The DDDAS feedback mechanism allows the DT to autonomically adapt predictions to physical twin (PT) wear and degradation, supporting DT-aging, which ensures progressive synchronization of DT with PT evolution. The framework is validated using the NASA CNC milling dataset, with spindle current as the monitored variable. In a zero-shot setting, the GPT-4-based DT achieves an average RMSE of 0.479 A (4.79% of the 10 A spindle current), accurately modeling nonlinear process dynamics and PT aging without retraining. These results show that DDD-GenDT provides a generalizable, data-efficient, and adaptive DT modeling approach, bridging generative AI with the performance and reliability requirements of industrial DT applications.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setup Once, Secure Always: A Single-Setup Secure Federated Learning Aggregation Protocol with Forward and Backward Secrecy for Dynamic Users</title>
<link>https://arxiv.org/abs/2502.08989</link>
<guid>https://arxiv.org/abs/2502.08989</guid>
<content:encoded><![CDATA[
arXiv:2502.08989v3 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables multiple users to collaboratively train a machine learning model without sharing raw data, making it suitable for privacy-sensitive applications. However, local model or weight updates can still leak sensitive information. Secure aggregation protocols mitigate this risk by ensuring that only the aggregated updates are revealed. Among these, single-setup protocols, where key generation and exchange occur only once, are the most efficient due to reduced communication and computation overhead. However, existing single-setup protocols often lack support for dynamic user participation and do not provide strong privacy guarantees such as forward and backward secrecy. In this paper, we propose a new secure aggregation protocol that requires only one setup operation for the entire FL training and allows new users to join or leave at any round. It employs lightweight symmetric homomorphic encryption with a key negation technique to efficiently mask updates, without user-to-user communication -- unlike the existing protocols. To defend against model inconsistency attacks, we introduce a simple verification mechanism using message authentication codes (MACs). Our protocol is the first to combine forward/backward secrecy, dropout resilience, and model integrity verification in a single-setup design. We provide formal security proofs and implement an end-to-end prototype, which source code has been released. Our experimental results show that our protocol reduces user-side computation by approximately 99% compared to state-of-the-art protocols like e-SeaFL (ACSAC'24), making it highly practical for real-world FL deployments, especially on resource-constrained devices.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Adversarial Trigger Learning</title>
<link>https://arxiv.org/abs/2503.12339</link>
<guid>https://arxiv.org/abs/2503.12339</guid>
<content:encoded><![CDATA[
arXiv:2503.12339v3 Announce Type: replace-cross 
Abstract: Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs. We released our code https://github.com/QData/ALTA_Augmented_Adversarial_Trigger_Learning
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling</title>
<link>https://arxiv.org/abs/2504.05216</link>
<guid>https://arxiv.org/abs/2504.05216</guid>
<content:encoded><![CDATA[
arXiv:2504.05216v3 Announce Type: replace-cross 
Abstract: Dense retrieval is a crucial task in Information Retrieval (IR), serving as the basis for downstream tasks such as re-ranking and augmenting generation. Recently, large language models (LLMs) have demonstrated impressive semantic understanding capabilities, making them attractive to researchers focusing on dense retrieval. While LLMs, as decoder-style generative models, excel in language generation, they often fall short in modeling global information due to a lack of attention to subsequent tokens. Drawing inspiration from the classical word-based language modeling approach for IR, specifically the query likelihood (QL) model, we aim to leverage the generative strengths of LLMs through QL maximization. Rather than employing QL estimation for document ranking, we propose an auxiliary task of QL maximization to enhance the backbone for subsequent contrastive learning of the retriever. We introduce our model, LLM-QL, which incorporates two key components: Attention Block (AB) and Document Corruption (DC). AB blocks the attention of predictive tokens to the document tokens before the document's ending token, while DC corrupts a document by masking a portion of its tokens during prediction. Evaluations on the in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's superiority over other LLM-based retrievers. Furthermore, comprehensive analyses also validate the efficacy of LLM-QL and its components.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Continual Fine-Tuning: A Survey</title>
<link>https://arxiv.org/abs/2504.13822</link>
<guid>https://arxiv.org/abs/2504.13822</guid>
<content:encoded><![CDATA[
arXiv:2504.13822v2 Announce Type: replace-cross 
Abstract: The emergence of large pre-trained networks has revolutionized the AI field, unlocking new possibilities and achieving unprecedented performance. However, these models inherit a fundamental limitation from traditional Machine Learning approaches: their strong dependence on the \textit{i.i.d.} assumption hinders their adaptability to dynamic learning scenarios. We believe the next breakthrough in AI lies in enabling efficient adaptation to evolving environments -- such as the real world -- where new data and tasks arrive sequentially. This challenge defines the field of Continual Learning (CL), a Machine Learning paradigm focused on developing lifelong learning neural models. One alternative to efficiently adapt these large-scale models is known Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of adapting the model to a particular data or scenario by performing small and efficient modifications, achieving similar performance to full fine-tuning. However, these techniques still lack the ability to adjust the model to multiple tasks continually, as they suffer from the issue of Catastrophic Forgetting. In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine-Tuning (PECFT). We examine various approaches, discuss evaluation metrics, and explore potential future research directions. Our goal is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning, guide researchers in this field, and pave the way for novel future research directions.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POPri: Private Federated Learning using Preference-Optimized Synthetic Data</title>
<link>https://arxiv.org/abs/2504.16438</link>
<guid>https://arxiv.org/abs/2504.16438</guid>
<content:encoded><![CDATA[
arXiv:2504.16438v2 Announce Type: replace-cross 
Abstract: In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as an RL (reinforcement learning) reward. Our algorithm, Policy Optimization for Private Data (POPri) harnesses client feedback using policy optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 58%, compared to 28% for prior synthetic data methods, and 3% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2504.19061</link>
<guid>https://arxiv.org/abs/2504.19061</guid>
<content:encoded><![CDATA[
arXiv:2504.19061v2 Announce Type: replace-cross 
Abstract: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, including admission reasons, major in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. Our results reveal that while the LLMs (e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission reasons and hospitalization events, they are generally less consistent when it comes to identifying follow-up recommendations, highlighting broader challenges in leveraging LLMs for comprehensive summarization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors</title>
<link>https://arxiv.org/abs/2505.02850</link>
<guid>https://arxiv.org/abs/2505.02850</guid>
<content:encoded><![CDATA[
arXiv:2505.02850v2 Announce Type: replace-cross 
Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive levels and incorporating common misconceptions into distractor design, is time-consuming and expertise-intensive, making manual creation impractical at scale. Current automated approaches typically generate questions at lower cognitive levels and fail to incorporate domain-specific misconceptions. This paper presents a hierarchical concept map-based framework that provides structured knowledge to guide LLMs in generating MCQs with distractors. We chose high-school physics as our test domain and began by developing a hierarchical concept map covering major Physics topics and their interconnections with an efficient database design. Next, through an automated pipeline, topic-relevant sections of these concept maps are retrieved to serve as a structured context for the LLM to generate questions and distractors that specifically target common misconceptions. Lastly, an automated validation is completed to ensure that the generated MCQs meet the requirements provided. We evaluate our framework against two baseline approaches: a base LLM and a RAG-based generation. We conducted expert evaluations and student assessments of the generated MCQs. Expert evaluation shows that our method significantly outperforms the baseline approaches, achieving a success rate of 75.20% in meeting all quality criteria compared to approximately 37% for both baseline methods. Student assessment data reveal that our concept map-driven approach achieved a significantly lower guess success rate of 28.05% compared to 37.10% for the baselines, indicating a more effective assessment of conceptual understanding. The results demonstrate that our concept map-based approach enables robust assessment across cognitive levels and instant identification of conceptual gaps, facilitating faster feedback loops and targeted interventions at scale.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: We Need Responsible, Application-Driven (RAD) AI Research</title>
<link>https://arxiv.org/abs/2505.04104</link>
<guid>https://arxiv.org/abs/2505.04104</guid>
<content:encoded><![CDATA[
arXiv:2505.04104v3 Announce Type: replace-cross 
Abstract: This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning</title>
<link>https://arxiv.org/abs/2505.12332</link>
<guid>https://arxiv.org/abs/2505.12332</guid>
<content:encoded><![CDATA[
arXiv:2505.12332v4 Announce Type: replace-cross 
Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</title>
<link>https://arxiv.org/abs/2505.14226</link>
<guid>https://arxiv.org/abs/2505.14226</guid>
<content:encoded><![CDATA[
arXiv:2505.14226v2 Announce Type: replace-cross 
Abstract: Recently released LLMs have strong multilingual \& multimodal capabilities. Model vulnerabilities are exposed using audits and red-teaming efforts. Existing efforts have focused primarily on the English language; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially for multimodal contexts. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce \textit{two new} jailbreak strategies that show higher effectiveness than baselines. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. We achieve a 99\% Attack Success Rate for text generation and 78\% for image generation, with Attack Relevance Rate of 100\% for text generation and 95\% for image generation for the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words. \textit{\textbf{Warning: This paper contains examples of potentially harmful and offensive content.}}
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access</title>
<link>https://arxiv.org/abs/2505.18344</link>
<guid>https://arxiv.org/abs/2505.18344</guid>
<content:encoded><![CDATA[
arXiv:2505.18344v3 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated state-of-the-art performance across vision, language, and scientific domains. Despite their empirical success, prior theoretical analyses of the sample complexity suffer from poor scaling with input data dimension or rely on unrealistic assumptions such as access to exact empirical risk minimizers. In this work, we provide a principled analysis of score estimation, establishing a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured decomposition of the score estimation error into statistical, approximation, and optimization errors, enabling us to eliminate the exponential dependence on neural network parameters that arises in prior analyses. It is the first such result which achieves sample complexity bounds without assuming access to the empirical risk minimizer of score function estimation loss.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18499</link>
<guid>https://arxiv.org/abs/2505.18499</guid>
<content:encoded><![CDATA[
arXiv:2505.18499v3 Announce Type: replace-cross 
Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\~os, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at https://github.com/PKU-ML/G1, with models and datasets hosted on Hugging Face collections https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for broader accessibility.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.19498</link>
<guid>https://arxiv.org/abs/2505.19498</guid>
<content:encoded><![CDATA[
arXiv:2505.19498v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) usually generate texts which satisfy context coherence but don't match the visual input. Such a hallucination issue hinders LVLMs' applicability in the real world. The key to solving hallucination in LVLM is to make the text generation rely more on the visual content. Most previous works choose to enhance/adjust the features/output of a specific modality (i.e., visual or textual) to alleviate hallucinations in LVLM, which do not explicitly or systematically enhance the visual reliance. In this paper, we comprehensively investigate the factors which may degenerate the visual reliance in text generation of LVLM from a Bayesian perspective. Based on our observations, we propose to mitigate hallucination in LVLM from three aspects. Firstly, we observe that not all visual tokens are informative in generating meaningful texts. We propose to evaluate and remove redundant visual tokens to avoid their disturbance. Secondly, LVLM may encode inappropriate prior information, making it lean toward generating unexpected words. We propose a simple yet effective way to rectify the prior from a Bayesian perspective. Thirdly, we observe that starting from certain steps, the posterior of next-token prediction conditioned on visual tokens may collapse to a prior distribution which does not depend on any informative visual tokens at all. Thus, we propose to stop further text generation to avoid hallucination. Extensive experiments on three benchmarks including POPE, CHAIR, and MME demonstrate that our method can consistently mitigate the hallucination issue of LVLM and performs favorably against previous state-of-the-arts.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recipes for Pre-training LLMs with MXFP8</title>
<link>https://arxiv.org/abs/2506.08027</link>
<guid>https://arxiv.org/abs/2506.08027</guid>
<content:encoded><![CDATA[
arXiv:2506.08027v2 Announce Type: replace-cross 
Abstract: Using fewer bits to represent model parameters and related tensors during pre-training has become a required technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats introduced in NVIDIA Blackwell generation of GPUs represent a major advancement of this technique, making it practical to combine narrow floating-point data types with finer granularity per-block scaling factors. In turn, this enables both quantization of more tensors than previous approaches and more efficient execution of operations on those tensors.
  Effective use of MX-formats requires careful choices of various parameters. In this paper we review these choices and show how MXFP8-E4M3 datatype and a specific number conversion algorithm result in training sessions that match those carried out in BF16. We present results using models with up to 8B parameters, trained on high-quality datasets of up to 15T tokens.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantDeBERTa: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[
arXiv:2506.08897v4 Announce Type: replace-cross 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantDeBERTa, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantDeBERTa is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantDeBERTa to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantDeBERTa exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields.By providing a scalable and reproducible framework for high-resolution entity recognition, PlantDeBERTa bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextTab: A Semantics-Aware Tabular In-Context Learner</title>
<link>https://arxiv.org/abs/2506.10707</link>
<guid>https://arxiv.org/abs/2506.10707</guid>
<content:encoded><![CDATA[
arXiv:2506.10707v3 Announce Type: replace-cross 
Abstract: Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/contexttab
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
<link>https://arxiv.org/abs/2506.15006</link>
<guid>https://arxiv.org/abs/2506.15006</guid>
<content:encoded><![CDATA[
arXiv:2506.15006v2 Announce Type: replace-cross 
Abstract: The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8 trillion parameters, demands a fundamental rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, widening the scale-out domain, and increasing memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token * Observed tokens per second / Peak FLOPS of the hardware) and overall throughput. For the co-design study, we utilized an analytical performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Cellular Automata for ARC-AGI</title>
<link>https://arxiv.org/abs/2506.15746</link>
<guid>https://arxiv.org/abs/2506.15746</guid>
<content:encoded><![CDATA[
arXiv:2506.15746v2 Announce Type: replace-cross 
Abstract: Cellular automata and their differentiable counterparts, Neural Cellular Automata (NCA), are highly expressive and capable of surprisingly complex behaviors. This paper explores how NCAs perform when applied to tasks requiring precise transformations and few-shot generalization, using the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that challenges their capabilities in ways not previously explored. Specifically, this paper uses gradient-based training to learn iterative update rules that transform input grids into their outputs from the training examples and apply them to the test inputs. Results suggest that gradient-trained NCA models are a promising and efficient approach to a range of abstract grid-based tasks from ARC. Along with discussing the impacts of various design modifications and training constraints, this work examines the behavior and properties of NCAs applied to ARC to give insights for broader applications of self-organizing systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Anything in Pathology Images with Natural Language</title>
<link>https://arxiv.org/abs/2506.20988</link>
<guid>https://arxiv.org/abs/2506.20988</guid>
<content:encoded><![CDATA[
arXiv:2506.20988v2 Announce Type: replace-cross 
Abstract: Pathology image segmentation is crucial in computational pathology for analyzing histological features relevant to cancer diagnosis and prognosis. However, current methods face major challenges in clinical applications due to limited annotated data and restricted category definitions. To address these limitations, we propose PathSegmentor, the first text-prompted segmentation foundation model designed specifically for pathology images. We also introduce PathSeg, the largest and most comprehensive dataset for pathology segmentation, built from 21 public sources and containing 275k image-mask-label triples across 160 diverse categories. With PathSegmentor, users can perform semantic segmentation using natural language prompts, eliminating the need for laborious spatial inputs such as points or boxes. Extensive experiments demonstrate that PathSegmentor outperforms specialized models with higher accuracy and broader applicability, while maintaining a compact architecture. It significantly surpasses existing spatial- and text-prompted models by 0.145 and 0.429 in overall Dice scores, respectively, showing strong robustness in segmenting complex structures and generalizing to external datasets. Moreover, PathSegmentor's outputs enhance the interpretability of diagnostic models through feature importance estimation and imaging biomarker discovery, offering pathologists evidence-based support for clinical decision-making. This work advances the development of explainable AI in precision oncology.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs</title>
<link>https://arxiv.org/abs/2507.01457</link>
<guid>https://arxiv.org/abs/2507.01457</guid>
<content:encoded><![CDATA[
arXiv:2507.01457v2 Announce Type: replace-cross 
Abstract: RISC-V provides a flexible and scalable platform for applications ranging from embedded devices to high-performance computing clusters. Particularly, its RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI workloads. But writing software that efficiently utilizes the vector units of RISC-V CPUs without expert knowledge requires the programmer to rely on the autovectorization features of compilers or hand-crafted libraries like muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing the integration with the RISC-V RVV extension, thus heavily limiting the efficient deployment of complex AI workloads. In this paper, we present a workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V vector units. Instead of relying on hand-crafted libraries, we integrated the RVV extension into TVM's MetaSchedule framework, a probabilistic program framework for tensor operation tuning. We implemented different RISC-V SoCs on an FPGA and tuned a wide range of AI workloads on them. We found that our proposal shows a mean improvement of 46% in execution latency when compared against the autovectorization feature of GCC, and 29% against muRISCV-NN. Moreover, the binary resulting from our proposal has a smaller code memory footprint, making it more suitable for embedded devices. Finally, we also evaluated our solution on a commercially available RISC-V SoC implementing the RVV 1.0 Vector Extension and found our solution is able to find mappings that are 35% faster on average than the ones proposed by LLVM. We open-sourced our proposal for the community to expand it to target other RISC-V extensions.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation</title>
<link>https://arxiv.org/abs/2507.04680</link>
<guid>https://arxiv.org/abs/2507.04680</guid>
<content:encoded><![CDATA[
arXiv:2507.04680v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable advancements in numerous areas such as multimedia. However, hallucination issues significantly limit their credibility and application potential. Existing mitigation methods typically rely on external tools or the comparison of multi-round inference, which significantly increase inference time. In this paper, we propose \textbf{SE}lf-\textbf{E}volving \textbf{D}istillation (\textbf{SEED}), which identifies hallucinations within the inner knowledge of LVLMs, isolates and purges them, and then distills the purified knowledge back into the model, enabling self-evolution. Furthermore, we identified that traditional distillation methods are prone to inducing void spaces in the output space of LVLMs. To address this issue, we propose a Mode-Seeking Evolving approach, which performs distillation to capture the dominant modes of the purified knowledge distribution, thereby avoiding the chaotic results that could emerge from void spaces. Moreover, we introduce a Hallucination Elimination Adapter, which corrects the dark knowledge of the original model by learning purified knowledge. Extensive experiments on multiple benchmarks validate the superiority of our SEED, demonstrating substantial improvements in mitigating hallucinations for representative LVLM models such as LLaVA-1.5 and InternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination evaluation metric POPE-Random improved from 81.3 to 88.3.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data</title>
<link>https://arxiv.org/abs/2507.08761</link>
<guid>https://arxiv.org/abs/2507.08761</guid>
<content:encoded><![CDATA[
arXiv:2507.08761v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation errors. To address this issue, we first demonstrate that linear extrapolation of the Q-function beyond the data range is particularly problematic. To mitigate this, we propose guiding the gradual decrease of Q-values outside the data range, which is achieved through reward scaling with layer normalization (RS-LN) and a penalization mechanism for infeasible actions (PA). By combining RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a range of tasks, demonstrating superior performance compared to state-of-the-art algorithms in both offline training and online fine-tuning on the D4RL benchmark, with notable success in the challenging AntMaze Ultra task.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening</title>
<link>https://arxiv.org/abs/2507.10461</link>
<guid>https://arxiv.org/abs/2507.10461</guid>
<content:encoded><![CDATA[
arXiv:2507.10461v3 Announce Type: replace-cross 
Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition</title>
<link>https://arxiv.org/abs/2507.14698</link>
<guid>https://arxiv.org/abs/2507.14698</guid>
<content:encoded><![CDATA[
arXiv:2507.14698v2 Announce Type: replace-cross 
Abstract: EEG-based emotion recognition plays an important role in developing adaptive brain-computer communication systems, yet faces two fundamental challenges in practical implementations: (1) effective integration of non-stationary spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional intensity variations in real-world scenarios. This paper proposes SST-CL, a novel framework integrating spatial-temporal transformers with curriculum learning. Our method introduces two core components: a spatial encoder that models inter-channel relationships and a temporal encoder that captures multi-scale dependencies through windowed attention mechanisms, enabling simultaneous extraction of spatial correlations and temporal dynamics from EEG signals. Complementing this architecture, an intensity-aware curriculum learning strategy progressively guides training from high-intensity to low-intensity emotional states through dynamic sample scheduling based on a dual difficulty assessment. Comprehensive experiments on three benchmark datasets demonstrate state-of-the-art performance across various emotional intensity levels, with ablation studies confirming the necessity of both architectural components and the curriculum learning mechanism.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2507.17303</link>
<guid>https://arxiv.org/abs/2507.17303</guid>
<content:encoded><![CDATA[
arXiv:2507.17303v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have emerged as powerful tools for computational pathology, offering unprecedented opportunities to integrate pathological images with language context for comprehensive diagnostic analysis. These models hold particular promise for automating complex tasks that traditionally require expert interpretation of pathologists. However, current MLLM approaches in pathology demonstrate significantly constrained reasoning capabilities, primarily due to their reliance on expensive chain-of-thought annotations. Additionally, existing methods remain limited to simplex application of visual question answering (VQA) at the region-of-interest (ROI) level, failing to address the full spectrum of diagnostic needs such as ROI classification, detection, segmentation, whole-slide-image (WSI) classification and VQA in clinical practice. In this study, we present SmartPath-R1, a versatile MLLM capable of simultaneously addressing both ROI-level and WSI-level tasks while demonstrating robust pathological reasoning capability. Our framework combines scale-dependent supervised fine-tuning and task-aware reinforcement fine-tuning, which circumvents the requirement for chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM. Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks. We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI samples for training and evaluation. Extensive experiments across 72 tasks validate the effectiveness and superiority of the proposed approach. This work represents a significant step toward developing versatile, reasoning-enhanced AI systems for precision pathology.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Condensation with Color Compensation</title>
<link>https://arxiv.org/abs/2508.01139</link>
<guid>https://arxiv.org/abs/2508.01139</guid>
<content:encoded><![CDATA[
arXiv:2508.01139v2 Announce Type: replace-cross 
Abstract: Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color's dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data are available at https://github.com/528why/Dataset-Condensation-with-Color-Compensation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SBP-YOLO:A Lightweight Real-Time Model for Detecting Speed Bumps and Potholes</title>
<link>https://arxiv.org/abs/2508.01339</link>
<guid>https://arxiv.org/abs/2508.01339</guid>
<content:encoded><![CDATA[
arXiv:2508.01339v2 Announce Type: replace-cross 
Abstract: Reliable and real-time detection of road speed bumps and potholes is crucial for anticipatory perception in advanced suspension systems, enabling timely and adaptive damping control. Achieving high accuracy and efficiency on embedded platforms remains challenging due to limited computational resources and the small scale of distant targets. This paper presents SBP-YOLO, a lightweight and high-speed detection framework tailored for bump and pothole recognition. Based on YOLOv11n, the model integrates GhostConv and VoVGSCSPC modules into the backbone and neck to reduce computation while enhancing multi-scale semantic features. To improve small-object detection, a P2-level branch is introduced with a lightweight and efficient detection head LEDH mitigating the added computational overhead without compromising accuracy. A hybrid training strategy combining NWD loss, backbone-level knowledge distillation, and Albumentations-driven augmentation further enhances localization precision and robustness. Experiments show that SBP-YOLO achieves 87.0 percent mAP, outperforming the YOLOv11n baseline by 5.8 percent. After TensorRT FP16 quantization, it runs at 139.5 FPS on Jetson AGX Xavier, delivering a 12.4 percent speedup over the P2-enhanced YOLOv11. These results validate the effectiveness of the proposed method for fast and low-latency road condition perception in embedded suspension control systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation</title>
<link>https://arxiv.org/abs/2508.03411</link>
<guid>https://arxiv.org/abs/2508.03411</guid>
<content:encoded><![CDATA[
arXiv:2508.03411v2 Announce Type: replace-cross 
Abstract: Unsupervised video segmentation is a challenging computer vision task, especially due to the lack of supervisory signals coupled with the complexity of visual scenes. To overcome this challenge, state-of-the-art models based on slot attention often have to rely on large and computationally expensive neural architectures. To this end, we propose a simple knowledge distillation framework that effectively transfers object-centric representations to a lightweight student. The proposed framework, called SlotMatch, aligns corresponding teacher and student slots via the cosine similarity, requiring no additional distillation objectives or auxiliary supervision. The simplicity of SlotMatch is confirmed via theoretical and empirical evidence, both indicating that integrating additional losses is redundant. We conduct experiments on two datasets to compare the state-of-the-art teacher model, SlotContrast, with our distilled student. The results show that our student based on SlotMatch matches and even outperforms its teacher, while using 3.6x less parameters and running 1.9x faster. Moreover, our student surpasses previous unsupervised video segmentation models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder</title>
<link>https://arxiv.org/abs/2508.04107</link>
<guid>https://arxiv.org/abs/2508.04107</guid>
<content:encoded><![CDATA[
arXiv:2508.04107v3 Announce Type: replace-cross 
Abstract: Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2508.05068</link>
<guid>https://arxiv.org/abs/2508.05068</guid>
<content:encoded><![CDATA[
arXiv:2508.05068v2 Announce Type: replace-cross 
Abstract: Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20].
  Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowState: Sampling Rate Invariant Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.05287</link>
<guid>https://arxiv.org/abs/2508.05287</guid>
<content:encoded><![CDATA[
arXiv:2508.05287v2 Announce Type: replace-cross 
Abstract: Foundation models (FMs) have transformed natural language processing, but their success has not yet translated to time series forecasting. Existing time series foundation models (TSFMs), often based on transformer variants, struggle with generalization across varying context and target lengths, lack adaptability to different sampling rates, and are computationally inefficient. We introduce FlowState, a novel TSFM architecture that addresses these challenges through two key innovations: a state space model (SSM) based encoder and a functional basis decoder. This design enables continuous-time modeling and dynamic time-scale adjustment, allowing FlowState to inherently generalize across all possible temporal resolutions, and dynamically adjust the forecasting horizons. In contrast to other state-of-the-art TSFMs, which require training data across all possible sampling rates to memorize patterns at each scale, FlowState inherently adapts its internal dynamics to the input scale, enabling smaller models, reduced data requirements, and improved efficiency. We further propose an efficient pretraining strategy that improves robustness and accelerates training. Despite being the smallest model, FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of its components, and we demonstrate its unique ability to adapt online to varying input sampling rates.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Records</title>
<link>https://arxiv.org/abs/2508.06627</link>
<guid>https://arxiv.org/abs/2508.06627</guid>
<content:encoded><![CDATA[
arXiv:2508.06627v3 Announce Type: replace-cross 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at https://github.com/MosbahAouad/EarlyPDAC-MML.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression</title>
<link>https://arxiv.org/abs/2508.07571</link>
<guid>https://arxiv.org/abs/2508.07571</guid>
<content:encoded><![CDATA[
arXiv:2508.07571v2 Announce Type: replace-cross 
Abstract: Using more test-time computation during language model inference, such as generating more intermediate thoughts or sampling multiple candidate answers, has proven effective in significantly improving model performance. This paper takes an initial step toward bridging the gap between practical language model inference and theoretical transformer analysis by incorporating randomness and sampling. We focus on in-context linear regression with continuous/binary coefficients, where our framework simulates language model decoding through noise injection and binary coefficient sampling. Through this framework, we provide detailed analyses of widely adopted inference techniques. Supported by empirical results, our theoretical framework and analysis demonstrate the potential for offering new insights into understanding inference behaviors in real-world language models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling</title>
<link>https://arxiv.org/abs/2508.08487</link>
<guid>https://arxiv.org/abs/2508.08487</guid>
<content:encoded><![CDATA[
arXiv:2508.08487v2 Announce Type: replace-cross 
Abstract: Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Game Reasoning Arena: A Framework and Benchmark for Assessing Reasoning Capabilities of Large Language Models via Game Play</title>
<link>https://arxiv.org/abs/2508.03368</link>
<guid>https://arxiv.org/abs/2508.03368</guid>
<content:encoded><![CDATA[
<div> Keywords: Game Reasoning Arena, large language models, decision making, strategic board games, OpenSpiel library

Summary: 
The Game Reasoning Arena library is a framework designed for evaluating the decision-making abilities of large language models (LLMs) using strategic board games available in the Google OpenSpiel library. It allows for systematic comparisons between LLM-based agents and other agent types, such as random, heuristic, and reinforcement learning agents, across various game scenarios. The library supports multiple board and matrix games, different agent types, and integrates API access to models through liteLLM and vLLM for local model deployment, along with distributed execution using Ray. By providing a comprehensive structure and key characteristics, the repository contributes to empirical evaluations of LLM reasoning and game theoretic behavior. <div>
arXiv:2508.03368v3 Announce Type: replace 
Abstract: The Game Reasoning Arena library provides a framework for evaluating the decision making abilities of large language models (LLMs) through strategic board games implemented in Google OpenSpiel library. The framework enables systematic comparisons between LLM based agents and other agents (random, heuristic, reinforcement learning agents, etc.) in various game scenarios by wrapping multiple board and matrix games and supporting different agent types. It integrates API access to models via liteLLM, local model deployment via vLLM, and offers distributed execution through Ray. This paper summarises the library structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game theoretic behaviour.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search</title>
<link>https://arxiv.org/abs/2508.08477</link>
<guid>https://arxiv.org/abs/2508.08477</guid>
<content:encoded><![CDATA[
<div> Dynamic arc costs, Trigger Arc Traveling Salesman Problem, GRASP-based metaheuristic, MIP techniques, real-time routing applications<br />
Summary:<br />
The paper presents a new approach for solving the Trigger Arc Traveling Salesman Problem (TA-TSP) with dynamic arc costs. By combining multiple construction heuristics with a multi-neighborhood local search, the proposed metaheuristic achieves impressive results on MESS 2024 competition instances, with an average optimality gap of 0.77% relative to the best-known solutions. Using mixed-integer programming techniques, the method transforms the TA-TSP into a sequence of tailored TSP instances to improve efficiency. The algorithm outperforms the Gurobi solver by 11.3% on smaller datasets, demonstrating its effectiveness in real-time routing applications with state-dependent travel costs. This approach offers a promising solution for dynamic optimization problems in logistics and warehouse operations. <br /><br />Summary: <div>
arXiv:2508.08477v2 Announce Type: replace 
Abstract: The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific "trigger" arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77\% and 0.40\% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3\% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</title>
<link>https://arxiv.org/abs/2508.04349</link>
<guid>https://arxiv.org/abs/2508.04349</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Large Language Model, Dynamic Entropy Weighting, Group Token Policy Optimization, Sequence-Level Group Relative Policy Optimization

Summary: 
Reinforcement learning algorithms, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of Large Language Models (LLMs). However, a key limitation lies in the coarse-grained credit assignment, which applies a uniform reward to all tokens in a sequence. To address this issue, this paper introduces Dynamic Entropy Weighting. This approach leverages high-entropy tokens in correct responses to guide policy updates towards a higher performance ceiling. Two strategies are proposed: Group Token Policy Optimization (GTPO) for fine-grained credit assignment at the token level, and Sequence-Level Group Relative Policy Optimization (GRPO-S) that assigns entropy-weighted rewards based on average token entropy in a sequence. Experimental results demonstrate the effectiveness of these methods, showing significant improvements over existing baselines. The success of Dynamic Entropy Weighting highlights its potential to enhance deep reasoning capabilities in language models. 

<br /><br />Summary: <div>
arXiv:2508.04349v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Introduction to Programming in the times of AI: A case study of a course re-design</title>
<link>https://arxiv.org/abs/2508.06572</link>
<guid>https://arxiv.org/abs/2508.06572</guid>
<content:encoded><![CDATA[
<div> Keywords: AI tools, programming education, challenges, course design, assessment <br />
Summary: <br />
In the realm of programming education, the use of AI tools has significantly transformed the teaching and learning process. This paper reviews the current state of AI tools available for programming education, focusing on introductory courses. It identifies challenges in course design, learning objectives, course delivery, and assessment, as well as potential misuse by students. The discussion includes recommendations for redesigning courses, modifying assignments, and adjusting teaching strategies to effectively incorporate AI technologies. These insights can serve as a valuable resource for institutions and educators seeking to leverage AI tools in programming education while addressing related challenges and concerns. <br /> <div>
arXiv:2508.06572v2 Announce Type: replace-cross 
Abstract: The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding</title>
<link>https://arxiv.org/abs/2508.06763</link>
<guid>https://arxiv.org/abs/2508.06763</guid>
<content:encoded><![CDATA[
<div> framework, MLLMs, traffic accident understanding, safePLUG, multimodal dataset  
Summary:<br /> 
The article introduces SafePLUG, a framework designed to enhance Multimodal Large Language Models (MLLMs) for comprehensive analysis of traffic accidents. SafePLUG combines Pixel-Level Understanding and temporal Grounding to improve fine-grained visual comprehension in accident scenarios. It enables region-aware question answering with arbitrary-shaped visual prompts, pixel-level segmentation based on language instructions, and recognition of temporally anchored events. A new dataset is curated with multimodal question-answer pairs, pixel-level annotations, and temporal event boundaries to support model development. Experimental results demonstrate strong performance in region-based question answering, pixel-level segmentation, event localization, and accident understanding. By offering a deeper understanding of complex traffic scenes, SafePLUG has the potential to enhance driving safety and situational awareness in smart transportation systems. The code, dataset, and model checkpoints will be publicly available for further research and applications.  
<br />  
Summary: <div>
arXiv:2508.06763v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress across a range of vision-language tasks and demonstrate strong potential for traffic accident understanding. However, existing MLLMs in this domain primarily focus on coarse-grained image-level or video-level comprehension and often struggle to handle fine-grained visual details or localized scene components, limiting their applicability in complex accident scenarios. To address these limitations, we propose SafePLUG, a novel framework that empowers MLLMs with both Pixel-Level Understanding and temporal Grounding for comprehensive traffic accident analysis. SafePLUG supports both arbitrary-shaped visual prompts for region-aware question answering and pixel-level segmentation based on language instructions, while also enabling the recognition of temporally anchored events in traffic accident scenarios. To advance the development of MLLMs for traffic accident understanding, we curate a new dataset containing multimodal question-answer pairs centered on diverse accident scenarios, with detailed pixel-level annotations and temporal event boundaries. Experimental results show that SafePLUG achieves strong performance on multiple tasks, including region-based question answering, pixel-level segmentation, temporal event localization, and accident event understanding. These capabilities lay a foundation for fine-grained understanding of complex traffic scenes, with the potential to improve driving safety and enhance situational awareness in smart transportation systems. The code, dataset, and model checkpoints will be made publicly available at: https://zihaosheng.github.io/SafePLUG
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Contrast Localizer for Identifying Causal Units in Social &amp; Mathematical Tasks in Language Models</title>
<link>https://arxiv.org/abs/2508.08276</link>
<guid>https://arxiv.org/abs/2508.08276</guid>
<content:encoded><![CDATA[
<div> Neuroscientific contrast localizer, Theory of Mind (ToM), mathematical reasoning, large language models (LLMs), vision-language models (VLMs <br />
Summary:
Contrastive stimulus sets were used to pinpoint causally relevant units for ToM and mathematical reasoning tasks in LLMs and VLMs. Top-activated units were localized and their causal role assessed via targeted ablations across various models. Surprisingly, low-activation units sometimes caused larger performance drops than highly activated ones. Units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer, contrary to expectations. The study questions the causal relevance of contrast-based localizers and suggests the need for broader stimulus sets to accurately capture task-specific units. <div>
arXiv:2508.08276v2 Announce Type: replace-cross 
Abstract: This work adapts a neuroscientific contrast localizer to pinpoint causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated units using contrastive stimulus sets and assess their causal role via targeted ablations. We compare the effect of lesioning functionally selected units against low-activation and randomly selected units on downstream accuracy across established ToM and mathematical benchmarks. Contrary to expectations, low-activation units sometimes produced larger performance drops than the highly activated ones, and units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer. These findings call into question the causal relevance of contrast-based localizers and highlight the need for broader stimulus sets and more accurately capture task-specific units.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Pays the RENT? Implications of Spatial Inequality for Prediction-Based Allocation Policies</title>
<link>https://arxiv.org/abs/2508.08573</link>
<guid>https://arxiv.org/abs/2508.08573</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-powered, resource allocation, targeting, inequality, eviction <br />
Summary: <br />
The study focuses on AI-powered resource allocation policies for targeting individuals or neighborhoods for interventions, particularly in preventing tenant eviction. Conflicting results in previous research led the authors to develop a framework to understand the impact of spatial inequality on the effectiveness of targeting approaches. They introduce the RENT metric to compare targeting and neighborhood-based strategies in high-risk areas. By calibrating the model using eviction court records in a US city, the study shows that individually targeted policies can significantly increase outreach to high-risk households, even in areas with concentrated eviction risks. The findings suggest that discrepancies in prior literature can be explained by deployment costs and the actual distribution of risk. This research provides insights for optimizing AI solutions in social services based on specific applications and geographical factors.<br /> 
Summary: <div>
arXiv:2508.08573v2 Announce Type: replace-cross 
Abstract: AI-powered scarce resource allocation policies rely on predictions to target either specific individuals (e.g., high-risk) or settings (e.g., neighborhoods). Recent research on individual-level targeting demonstrates conflicting results; some models show that targeting is not useful when inequality is high, while other work demonstrates potential benefits. To study and reconcile this apparent discrepancy, we develop a stylized framework based on the Mallows model to understand how the spatial distribution of inequality affects the effectiveness of door-to-door outreach policies. We introduce the RENT (Relative Efficiency of Non-Targeting) metric, which we use to assess the effectiveness of targeting approaches compared with neighborhood-based approaches in preventing tenant eviction when high-risk households are more versus less spatially concentrated. We then calibrate the model parameters to eviction court records collected in a medium-sized city in the USA. Results demonstrate considerable gains in the number of high-risk households canvassed through individually targeted policies, even in a highly segregated metro area with concentrated risks of eviction. We conclude that apparent discrepancies in the prior literature can be reconciled by considering 1) the source of deployment costs and 2) the observed versus modeled concentrations of risk. Our results inform the deployment of AI-based solutions in social service provision that account for particular applications and geographies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video</title>
<link>https://arxiv.org/abs/2508.11836</link>
<guid>https://arxiv.org/abs/2508.11836</guid>
<content:encoded><![CDATA[
<div> Keywords: World models, neural network, transfer learning, explainability, Retro Coder
<br />
Summary: 
Finite Automata Extraction (FAE) is introduced as a novel approach for learning neuro-symbolic world models from gameplay videos. These world models serve as compressed representations of an environment, typically implemented using neural networks. FAE utilizes a domain-specific language called Retro Coder to represent the learned environment dynamics as programs, enabling the extraction of a more precise model of the environment compared to previous approaches. The FAE approach also generates more general code, enhancing the transferability of the learned representations. By combining neural networks with symbolic representations, FAE addresses challenges related to transfer learning and explainability in world models. This innovative method opens up possibilities for creating efficient and interpretable world models for a variety of applications. <div>
arXiv:2508.11836v1 Announce Type: new 
Abstract: World models are defined as a compressed spatial and temporal learned representation of an environment. The learned representation is typically a neural network, making transfer of the learned environment dynamics and explainability a challenge. In this paper, we propose an approach, Finite Automata Extraction (FAE), that learns a neuro-symbolic world model from gameplay video represented as programs in a novel domain-specific language (DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more precise model of the environment and more general code than prior DSL-based approaches.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</title>
<link>https://arxiv.org/abs/2508.11850</link>
<guid>https://arxiv.org/abs/2508.11850</guid>
<content:encoded><![CDATA[
<div> Keywords: Integer programming, combinatorial optimization, Acceleration cuts, Evolutionary search, Large language models

Summary:
EvoCut presents an automated framework for generating acceleration cuts in integer programming by utilizing large language models and an evolutionary search. It eliminates the need for manual design, which traditionally requires expert knowledge. The framework initializes a diverse population of candidate cuts, evaluates their efficacy through empirical testing, and refines them through evolutionary processes. EvoCut improves solver performance by reducing the optimality gap by 17-57% within a fixed time frame and achieves solutions up to 4 times faster or of higher quality within the same time limit compared to standard practices. The framework does not require human input and can generate, refine, and verify cuts that generalize to unseen instances. The code for EvoCut is available on GitHub for implementation and further research. <div>
arXiv:2508.11850v1 Announce Type: new 
Abstract: Integer programming lies at the heart of crucial combinatorial optimization tasks but remains challenging due to its NP-hard nature. An effective approach for practically solving integer programs is the manual design of acceleration cuts, i.e. inequalities that improve solver performance. However, this creative process demands deep expertise and is yet to be automated. Our proposed framework, EvoCut, automates the generation of acceleration cuts by combining large language models (LLMs) with an evolutionary search. EvoCut (i) initializes a diverse population of candidate cuts via an LLM-based initializer agent; (ii) for each cut empirically evaluates both preservation of the optimal solution and its ability to cut off fractional solutions across a verification set; and (iii) iteratively refines the population through evolutionary crossover and mutation agents. We quantify each cut's utility by its relative reduction in the solver's optimality gap. Our comparisons against standard integer programming practice show that EvoCut reduces optimality gap by 17-57% within a fixed time. It obtains the same solutions up to 4 times as fast, and obtains higher-quality solutions within the same time limit. Requiring no human expert input, EvoCut reliably generates, improves, and empirically verifies cuts that generalize to unseen instances. The code is available at https://github.com/milad1378yz/EvoCut.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework</title>
<link>https://arxiv.org/abs/2508.11860</link>
<guid>https://arxiv.org/abs/2508.11860</guid>
<content:encoded><![CDATA[
<div> framework, retrosynthesis planning, constraints, large language model, agentic tool<br />
<br />
Summary: 
The article introduces LARC, a novel framework for constrained retrosynthesis planning using Large Language Models (LLMs). LARC incorporates an Agent-as-a-Judge approach to provide agentic constraint evaluation during the route generation process. Results show LARC outperforms LLM baselines with a 72.9% success rate across different constraint types. This framework approaches human expert-level success in a shorter time, making it a promising tool for constrained retrosynthesis. LARC's extensibility allows for further development towards an effective agentic tool or co-scientist to assist human experts in chemistry research. <div>
arXiv:2508.11860v1 Announce Type: new 
Abstract: Large language model (LLM) agent evaluators leverage specialized tools to ground the rational decision-making of LLMs, making them well-suited to aid in scientific discoveries, such as constrained retrosynthesis planning. Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints. Here, we present LARC, the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation. We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types. LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time. The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuarkMed Medical Foundation Model Technical Report</title>
<link>https://arxiv.org/abs/2508.11894</link>
<guid>https://arxiv.org/abs/2508.11894</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, healthcare applications, QuarkMed, medical foundation model, reinforcement learning<br />
Summary:<br />
Recent advancements in large language models have spurred their application in healthcare, with QuarkMed emerging as a robust solution. QuarkMed integrates curated medical data processing, Retrieval-Augmented Generation (RAG), and verifiable reinforcement learning to develop a high-performance medical foundation model. Demonstrating exceptional performance with 70% accuracy on the Chinese Medical Licensing Examination, QuarkMed showcases strong generalization capabilities across diverse medical benchmarks. It offers a versatile and powerful personal medical AI solution, catering to over millions of users at ai.quark.cn. Leveraging specialized knowledge and customization features, QuarkMed ensures professional accuracy in medical tasks, providing AI-powered medical consultations, diagnostic report assistance, and medical search tools. Its success signifies a significant milestone in the evolution of AI in healthcare applications. <br /><br />Summary: <div>
arXiv:2508.11894v1 Announce Type: new 
Abstract: Recent advancements in large language models have significantly accelerated their adoption in healthcare applications, including AI-powered medical consultations, diagnostic report assistance, and medical search tools. However, medical tasks often demand highly specialized knowledge, professional accuracy, and customization capabilities, necessitating a robust and reliable foundation model. QuarkMed addresses these needs by leveraging curated medical data processing, medical-content Retrieval-Augmented Generation (RAG), and a large-scale, verifiable reinforcement learning pipeline to develop a high-performance medical foundation model. The model achieved 70% accuracy on the Chinese Medical Licensing Examination, demonstrating strong generalization across diverse medical benchmarks. QuarkMed offers a powerful yet versatile personal medical AI solution, already serving over millions of users at ai.quark.cn.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs</title>
<link>https://arxiv.org/abs/2508.11944</link>
<guid>https://arxiv.org/abs/2508.11944</guid>
<content:encoded><![CDATA[
<div> evaluation framework, large language models, strategic reasoning, cognitive hierarchy models, normal-form games

Summary:
The study introduces the Cognitive Hierarchy Benchmark (CHBench) to evaluate the strategic reasoning capability of large language models (LLMs) in game-playing scenarios. It proposes that agents have bounded rationality, exhibiting varying reasoning depths/levels. CHBench utilizes behavioral data from six LLMs across fifteen normal-form games to assess strategic reasoning. Results show consistent reasoning levels across different opponents, highlighting the framework's robustness. The analysis of Chat Mechanism and Memory Mechanism reveals their impact on strategic reasoning performance, with the former degrading and the latter enhancing it. This framework provides a promising tool for assessing LLM capabilities and has significant potential for future research and practical applications. <br /><br />Summary: <div>
arXiv:2508.11944v1 Announce Type: new 
Abstract: Game-playing ability serves as an indicator for evaluating the strategic reasoning capability of large language models (LLMs). While most existing studies rely on utility performance metrics, which are not robust enough due to variations in opponent behavior and game structure. To address this limitation, we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation framework inspired by the cognitive hierarchy models from behavioral economics. We hypothesize that agents have bounded rationality -- different agents behave at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning through a three-phase systematic framework, utilizing behavioral data from six state-of-the-art LLMs across fifteen carefully selected normal-form games. Experiments show that LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming the framework's robustness and generalization capability. We also analyze the effects of two key mechanisms (Chat Mechanism and Memory Mechanism) on strategic reasoning performance. Results indicate that the Chat Mechanism significantly degrades strategic reasoning, whereas the Memory Mechanism enhances it. These insights position CHBench as a promising tool for evaluating LLM capabilities, with significant potential for future research and practical applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2508.11953</link>
<guid>https://arxiv.org/abs/2508.11953</guid>
<content:encoded><![CDATA[
<div> Optimizing data mixtures, supervised fine-tuning, large language models, data mixing, validation loss <br />
Summary: 
This paper introduces a novel method for optimizing data mixtures to improve the supervised fine-tuning (SFT) of large language models (LLMs). The approach frames data mixing as an optimization problem and aims to minimize validation loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. Through experiments with small-scale data mixtures, the method derives optimal weights and demonstrates excellent overall and individual performance across domains. Models trained with the optimized weights perform similarly to those using weights determined via grid search, with only a slight increase in per-domain loss. Reweighting popular SFT datasets using this method leads to improved validation loss and downstream performance. The method also has potential for guiding data selection for domain-specific models and provides insights into supervised fine-tuning. <br /><br />Summary: <div>
arXiv:2508.11953v1 Announce Type: new 
Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, we frame data mixing as an optimization problem and introduce a novel method designed to minimize validation loss. Our approach parametrizes the loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. By experimenting with various small-scale data mixtures, we fit these parameters and derive the optimal weights. We provide both mathematical proofs and empirical results demonstrating that our algorithm achieves excellent overall and individual performance across all domains. Through controlled experiments, we show that models trained with our optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Additionally, we show that reweighting popular SFT datasets using our method improves both validation loss and downstream performance. Finally, we discuss how our method can generalize to guide data selection for domain-specific models and provide insights into SFT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.11954</link>
<guid>https://arxiv.org/abs/2508.11954</guid>
<content:encoded><![CDATA[
<div> framework, time series, multimodal, forecasting, UniCast
Summary: 
- The paper introduces UniCast, a novel parameter-efficient multimodal framework for time series forecasting.
- UniCast extends Time Series Foundation Models (TSFMs) to leverage time series, vision, and text modalities simultaneously.
- Modality-specific embeddings from pretrained Vision and Text Encoders are integrated with a frozen TSFM through soft prompt tuning.
- This design enables efficient adaptation with minimal parameter updates while preserving generalization strength and facilitating cross-modal interaction.
- Extensive experiments across diverse time-series forecasting benchmarks demonstrate that UniCast outperforms all existing TSFM baselines, highlighting the importance of multimodal context in improving forecasting performance. 

<br /><br />Summary: <div>
arXiv:2508.11954v1 Announce Type: new 
Abstract: Time series forecasting is a foundational task across domains, such as finance, healthcare, and environmental monitoring. While recent advances in Time Series Foundation Models (TSFMs) have demonstrated strong generalisation through large-scale pretraining, existing models operate predominantly in a unimodal setting, ignoring the rich multimodal context, such as visual and textual signals, that often accompanies time series data in real-world scenarios. This paper introduces a novel parameter-efficient multimodal framework, UniCast, that extends TSFMs to jointly leverage time series, vision, and text modalities for enhanced forecasting performance. Our method integrates modality-specific embeddings from pretrained Vision and Text Encoders with a frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal parameter updates. This design not only preserves the generalisation strength of the foundation model but also enables effective cross-modal interaction. Extensive experiments across diverse time-series forecasting benchmarks demonstrate that UniCast consistently and significantly outperforms all existing TSFM baselines. The findings highlight the critical role of multimodal context in advancing the next generation of general-purpose time series forecasters.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index</title>
<link>https://arxiv.org/abs/2508.11959</link>
<guid>https://arxiv.org/abs/2508.11959</guid>
<content:encoded><![CDATA[
<div> game theory, eXplainable Artificial Intelligence, feature attribution, logic-based explanations, Shapley value, Banzhaf index

Summary: 
This paper introduces novel feature importance scores using game theory concepts for feature attribution in explainable artificial intelligence. The study addresses the issue of neglecting non-weak abductive explanation (WAXp) sets in assigning feature importance. By leveraging Shapley value and Banzhaf index, the proposed scores consider non-WAXp sets to quantify feature contributions in excluding adversarial examples. The research identifies properties and analyzes the computational complexity of the new scores. This work contributes to enhancing the interpretability of machine learning models, particularly in high-stakes applications, by incorporating a broader range of explanations in feature attribution. <div>
arXiv:2508.11959v1 Announce Type: new 
Abstract: Feature attribution methods based on game theory are ubiquitous in the field of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous feature attribution using logic-based explanations, specifically targeting high-stakes uses of machine learning (ML) models. Typically, such works exploit weak abductive explanation (WAXp) as the characteristic function to assign importance to features. However, one possible downside is that the contribution of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important information, because of the relationship between formal explanations (XPs) and adversarial examples (AExs). Accordingly, this paper leverages Shapley value and Banzhaf index to devise two novel feature importance scores. We take into account non-WAXp sets when computing feature contribution, and the novel scores quantify how effective each feature is at excluding AExs. Furthermore, the paper identifies properties and studies the computational complexity of the proposed scores.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering</title>
<link>https://arxiv.org/abs/2508.11975</link>
<guid>https://arxiv.org/abs/2508.11975</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, synthetic data generation, chart understanding, aligned chart-question-answer triplets, candidate-conditioned answering <br />
Summary: 
This article introduces a method to improve the performance of Vision Language Models (VLMs) on chart understanding tasks by generating synthetic data using a reliable chart synthesis pipeline. By aligning chart-question-answer triplets through code generation and execution, the synthetic data is free from noisy labels. Additionally, a candidate-conditioned answering process is designed to enhance performance further. This process involves the VLM generating multiple responses per query and synthesizing the final answer by considering these candidates in context. Experimental results show significant improvements in accuracy, with up to a 15.50 point gain over the initial VLM. This self-improving paradigm does not require human-labeled data or external models, making it a promising approach for improving VLM performance on chart understanding tasks. <br /><br />Summary: <div>
arXiv:2508.11975v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks, particularly in accurate chart description and complex reasoning. Synthetic data generation is a promising solution, while usually facing the challenge of noise labels. To address this challenge, we first introduce a chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution, ensuring the reliability of synthetic data without human intervention. Furthermore, inspired by test-time scaling that increases inference budget and thereby improves performance, we design a candidate-conditioned answering process. The VLM first generates multiple responses per query, and then synthesizes the final answer by contextualizing these candidates. Experiments demonstrate significant improvements, with up to 15.50 points accuracy gain over the initial VLM, in a fully self-improving paradigm without either human-labeled data or external models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[
<div> future prediction, LLM agents, benchmark, dynamic evaluation, real-time updates

Summary:
FutureX is introduced as a live benchmark for evaluating LLM agents in future prediction tasks. It addresses the lack of large-scale benchmarks for such tasks and provides a dynamic and real-time evaluation environment. The benchmark supports daily updates and ensures data integrity through an automated pipeline. 25 LLM/agent models are evaluated, including those with reasoning and search capabilities. The evaluation assesses agents' adaptive reasoning and performance in dynamic environments. The analysis highlights failure modes and performance pitfalls, such as vulnerability to fake web pages and temporal validity issues. The ultimate goal of FutureX is to establish a contamination-free evaluation standard to push the development of LLM agents capable of professional-level reasoning and predictive thinking. 

<br /><br />Summary: <div>
arXiv:2508.11987v1 Announce Type: new 
Abstract: Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network</title>
<link>https://arxiv.org/abs/2508.11991</link>
<guid>https://arxiv.org/abs/2508.11991</guid>
<content:encoded><![CDATA[
<div> automation, logic circuit design, And-Inverter Graphs (AIGs), Electronic Design Automation (EDA), dynamic information propagation

Summary:
AIGer is a proposed model for automating logic circuit design that aims to enhance performance, energy efficiency, and reliability in electronic design. It consists of two key components: the node logic feature initialization embedding component and the AIGs feature learning network component. The former projects logic nodes into semantic spaces for effective node embedding, while the latter uses a heterogeneous graph convolutional network to represent AIGs' structure and information. AIGer demonstrates superior performance compared to existing models in tasks such as Signal Probability Prediction (SSP) and Truth Table Distance Prediction (TTDP), achieving significant improvements in mean absolute error (MAE) and mean squared error (MSE). This novel approach allows AIGer to jointly model functional and structural characteristics while enhancing its message passing capability. <div>
arXiv:2508.11991v1 Announce Type: new 
Abstract: The automation of logic circuit design enhances chip performance, energy efficiency, and reliability, and is widely applied in the field of Electronic Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent, optimize, and verify the functional characteristics of digital circuits, enhancing the efficiency of EDA development.Due to the complex structure and large scale of nodes in real-world AIGs, accurate modeling is challenging, leading to existing work lacking the ability to jointly model functional and structural characteristics, as well as insufficient dynamic information propagation capability.To address the aforementioned challenges, we propose AIGer.Specifically, AIGer consists of two components: 1) Node logic feature initialization embedding component and 2) AIGs feature learning network component.The node logic feature initialization embedding component projects logic nodes, such as AND and NOT, into independent semantic spaces, to enable effective node embedding for subsequent processing.Building upon this, the AIGs feature learning network component employs a heterogeneous graph convolutional network, designing dynamic relationship weight matrices and differentiated information aggregation approaches to better represent the original structure and information of AIGs.The combination of these two components enhances AIGer's ability to jointly model functional and structural characteristics and improves its message passing capability. Experimental results indicate that AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of 33.57\% and 14.79\% in MAE and MSE, respectively, compared to the best-performing models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning</title>
<link>https://arxiv.org/abs/2508.11995</link>
<guid>https://arxiv.org/abs/2508.11995</guid>
<content:encoded><![CDATA[
<div> framework, collaborative decision-making, multi-agent systems, large language models, cognitive biases
Summary:
AgentCDM is a structured framework designed to enhance collaborative decision-making in large language model-based multi-agent systems. It addresses the limitations of existing approaches by introducing a structured reasoning paradigm inspired by the Analysis of Competing Hypotheses (ACH). This paradigm systematically mitigates cognitive biases and shifts decision-making from passive answer selection to active hypothesis evaluation and construction. The framework includes a two-stage training process that initially provides scaffolding to guide the model through structured reasoning and gradually removes it to promote autonomous generalization. Experimental results on various benchmark datasets demonstrate that AgentCDM achieves state-of-the-art performance and robust generalization, validating its effectiveness in improving the quality and robustness of collaborative decisions in multi-agent systems. <div>
arXiv:2508.11995v1 Announce Type: new 
Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold significant promise for solving complex decision-making tasks. However, the core process of collaborative decision-making (CDM) within these systems remains underexplored. Existing approaches often rely on either ``dictatorial" strategies that are vulnerable to the cognitive biases of a single agent, or ``voting-based" methods that fail to fully harness collective intelligence. To address these limitations, we propose \textbf{AgentCDM}, a structured framework for enhancing collaborative decision-making in LLM-based multi-agent systems. Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in cognitive science, AgentCDM introduces a structured reasoning paradigm that systematically mitigates cognitive biases and shifts decision-making from passive answer selection to active hypothesis evaluation and construction. To internalize this reasoning process, we develop a two-stage training paradigm: the first stage uses explicit ACH-inspired scaffolding to guide the model through structured reasoning, while the second stage progressively removes this scaffolding to encourage autonomous generalization. Experiments on multiple benchmark datasets demonstrate that AgentCDM achieves state-of-the-art performance and exhibits strong generalization, validating its effectiveness in improving the quality and robustness of collaborative decisions in MAS.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Models for Depressive Disorder Detection and Diagnosis: A Review</title>
<link>https://arxiv.org/abs/2508.12022</link>
<guid>https://arxiv.org/abs/2508.12022</guid>
<content:encoded><![CDATA[
<div> Keywords: Major Depressive Disorder, Artificial Intelligence, Diagnosis, Prediction, Multimodal Fusion

Summary:
The article discusses the use of Artificial Intelligence (AI) in diagnosing Major Depressive Disorder, aiming to provide objective and timely diagnostic tools. A systematic review of 55 studies was conducted, leading to the development of a hierarchical taxonomy categorizing AI methods based on clinical tasks, data modality, and computational model class. Three significant trends were identified: the prevalence of graph neural networks for brain connectivity modeling, the increasing use of large language models for linguistic data, and a growing emphasis on multimodal fusion, explainability, and algorithmic fairness. The article also offers insights into prominent public datasets, standard evaluation metrics, and outlines open challenges in the field, providing a roadmap for future research and innovation in computational psychiatry.<br /><br />Summary: <div>
arXiv:2508.12022v1 Announce Type: new 
Abstract: Major Depressive Disorder is one of the leading causes of disability worldwide, yet its diagnosis still depends largely on subjective clinical assessments. Integrating Artificial Intelligence (AI) holds promise for developing objective, scalable, and timely diagnostic tools. In this paper, we present a comprehensive survey of state-of-the-art AI methods for depression detection and diagnosis, based on a systematic review of 55 key studies. We introduce a novel hierarchical taxonomy that structures the field by primary clinical task (diagnosis vs. prediction), data modality (text, speech, neuroimaging, multimodal), and computational model class (e.g., graph neural networks, large language models, hybrid approaches). Our in-depth analysis reveals three major trends: the predominance of graph neural networks for modeling brain connectivity, the rise of large language models for linguistic and conversational data, and an emerging focus on multimodal fusion, explainability, and algorithmic fairness. Alongside methodological insights, we provide an overview of prominent public datasets and standard evaluation metrics as a practical guide for researchers. By synthesizing current advances and highlighting open challenges, this survey offers a comprehensive roadmap for future innovation in computational psychiatry.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems</title>
<link>https://arxiv.org/abs/2508.12026</link>
<guid>https://arxiv.org/abs/2508.12026</guid>
<content:encoded><![CDATA[
<div> Keywords: Bongard Problems, abstract visual reasoning, Bongard-RWR dataset, vision language model, fine-grained concepts

Summary:
Bongard Problems (BPs) are used to test abstract visual reasoning, challenging models to identify and describe visual concepts. Previous datasets did not fully capture real-world complexity. The Bongard-RWR dataset addressed this but was limited in size. The new Bongard-RWR+ dataset contains 5,400 instances representing original BP concepts with real-world-like images generated by a vision language model (VLM) pipeline. State-of-the-art VLMs struggle to discern fine-grained concepts, indicating reasoning limitations. The study evaluates VLMs on various BP formulations, including classification and answer generation, showing their ability to recognize coarse-grained but not fine-grained visual concepts. The use of VLMs to generate realistic images for abstract concepts highlights the challenges in capturing nuanced visual information accurately. The Bongard-RWR+ dataset expands the possibilities for studying abstract visual reasoning in a more realistic context. 

<br /><br />Summary: <div>
arXiv:2508.12026v1 Announce Type: new 
Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual reasoning (AVR), requiring models to identify visual concepts fromjust a few examples and describe them in natural language. Early BP benchmarks featured synthetic black-and-white drawings, which might not fully capture the complexity of real-world scenes. Subsequent BP datasets employed real-world images, albeit the represented concepts are identifiable from high-level image features, reducing the task complexity. Differently, the recently released Bongard-RWR dataset aimed at representing abstract concepts formulated in the original BPs using fine-grained real-world images. Its manual construction, however, limited the dataset size to just $60$ instances, constraining evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset composed of $5\,400$ instances that represent original BP abstract concepts using real-world-like images generated via a vision language model (VLM) pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually curated images and generate new descriptions aligned with the underlying concepts, use Flux.1-dev to synthesize images from these descriptions, and manually verify that the generated images faithfully reflect the intended concepts. We evaluate state-of-the-art VLMs across diverse BP formulations, including binary and multiclass classification, as well as textual answer generation. Our findings reveal that while VLMs can recognize coarse-grained visual concepts, they consistently struggle with discerning fine-grained concepts, highlighting limitations in their reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active inference for action-unaware agents</title>
<link>https://arxiv.org/abs/2508.12027</link>
<guid>https://arxiv.org/abs/2508.12027</guid>
<content:encoded><![CDATA[
<div> Active inference, Bayesian inference, variational free energy, expected free energy, motor control
Summary:
- The article introduces the concept of active inference, which views adaptive agents as engaging in approximate Bayesian inference by minimizing variational and expected free energies.
- Variational free energy accounts for perceptual processes and learning through evidence accumulation, while expected free energy describes how agents select actions over time.
- Different strategies exist for planning future actions, with some assuming agents know their own actions and others requiring agents to infer their motor behavior from recent observations.
- The comparison of action-aware and action-unaware agents in navigation tasks shows that action-unaware agents can achieve comparable performances despite a significant disadvantage.
- The difference between these approaches reflects the presence or absence of an efference copy signal representing knowledge about an agent's own actions in motor control frameworks. <br /><br />Summary: <div>
arXiv:2508.12027v1 Announce Type: new 
Abstract: Active inference is a formal approach to study cognition based on the notion that adaptive agents can be seen as engaging in a process of approximate Bayesian inference, via the minimisation of variational and expected free energies. Minimising the former provides an account of perceptual processes and learning as evidence accumulation, while minimising the latter describes how agents select their actions over time. In this way, adaptive agents are able to maximise the likelihood of preferred observations or states, given a generative model of the environment. In the literature, however, different strategies have been proposed to describe how agents can plan their future actions. While they all share the notion that some kind of expected free energy offers an appropriate way to score policies, sequences of actions, in terms of their desirability, there are different ways to consider the contribution of past motor experience to the agent's future behaviour. In some approaches, agents are assumed to know their own actions, and use such knowledge to better plan for the future. In other approaches, agents are unaware of their actions, and must infer their motor behaviour from recent observations in order to plan for the future. This difference reflects a standard point of departure in two leading frameworks in motor control based on the presence, or not, of an efference copy signal representing knowledge about an agent's own actions. In this work we compare the performances of action-aware and action-unaware agents in two navigations tasks, showing how action-unaware agents can achieve performances comparable to action-aware ones while at a severe disadvantage.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPF-World: Action World Model for Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2508.12087</link>
<guid>https://arxiv.org/abs/2508.12087</guid>
<content:encoded><![CDATA[
<div> learnable solvers, multi-agent path finding, MAPF-World, decentralized, autoregressive<br />
Summary:<br />
The article introduces MAPF-World, an autoregressive action world model designed to improve multi-agent path finding (MAPF) by incorporating environmental dynamics and future predictions. This model aims to enhance situational awareness and decision-making in complex, long-term planning scenarios. By leveraging future state and action prediction, MAPF-World enables more informed and coordinated decision-making, particularly in multi-agent settings. The study also introduces a new map generator for training and evaluating MAPF solvers based on real-world scenarios. Experimental results show that MAPF-World outperforms existing learnable solvers, demonstrating superior zero-shot generalization to out-of-distribution cases. Remarkably, MAPF-World achieves these results with significantly smaller model size and data requirements, highlighting its efficiency and effectiveness in addressing complex MAPF challenges. <br /><br /> <div>
arXiv:2508.12087v1 Announce Type: new 
Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios</title>
<link>https://arxiv.org/abs/2508.12100</link>
<guid>https://arxiv.org/abs/2508.12100</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning, interactive problem solving, semantic hierarchies, domain knowledge alignment, reasoning threads<br />
Summary:<br />
The article introduces a new Reasoning-Threads-Evaluation (ReT-Eval) framework for interactive problem solving scenarios. Current reasoning models often lack explicit semantic hierarchies and struggle to align with user understanding and domain knowledge. The proposed framework addresses these limitations by extracting relevant knowledge structures from a domain knowledge graph and enriching them with language model knowledge. In the second phase, the reasoning threads are evaluated and pruned using a reward-guided strategy to maintain semantic coherence and generate effective reasoning steps. The experiments and expert evaluations demonstrate that ReT-Eval enhances user understanding and outperforms existing reasoning models. The framework focuses on structured knowledge reuse and aims to guide users through goal-oriented reasoning processes, resulting in more concise and effective output.<br /> 
Summary: <div>
arXiv:2508.12100v1 Announce Type: new 
Abstract: Reasoning in interactive problem solving scenarios requires models to construct reasoning threads that reflect user understanding and align with structured domain knowledge. However, current reasoning models often lack explicit semantic hierarchies, user-domain knowledge alignment, and principled mechanisms to prune reasoning threads for effectiveness. These limitations result in lengthy generic output that does not guide users through goal-oriented reasoning steps. To address this, we propose a prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval) framework, drawing inspiration from human-like reasoning strategies that emphasize structured knowledge reuse. In the first phase, semantically relevant knowledge structures are extracted from a sparse domain knowledge graph using a graph neural network and enriched with intrinsic large language model knowledge to resolve knowledge discrepancies. In the second phase, these threads are evaluated and pruned using a reward-guided strategy aimed at maintaining semantic coherence to generate effective reasoning threads. Experiments and expert evaluations show that ReT-Eval enhances user understanding and outperforms state-of-the-art reasoning models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization</title>
<link>https://arxiv.org/abs/2508.12149</link>
<guid>https://arxiv.org/abs/2508.12149</guid>
<content:encoded><![CDATA[
<div> framework, multimodal learning, optimal transport, geometric regularization, semantically aligned

Summary: 
The paper introduces MOVER, a framework for multimodal learning that combines optimal transport-based soft alignment and volume-based geometric regularization to create semantically aligned and structured representations across multiple modalities. MOVER addresses the limitations of existing contrastive approaches by encouraging consistent alignment in a modality-agnostic manner. Experimental results on text-video-audio retrieval tasks show that MOVER outperforms state-of-the-art methods in both zero-shot and finetuned scenarios. The framework also demonstrates improved generalization to unseen modalities and stronger structural consistency in the learned embedding space. <div>
arXiv:2508.12149v1 Announce Type: new 
Abstract: Recent advances in multimodal learning have largely relied on pairwise contrastive objectives to align different modalities, such as text, video, and audio, in a shared embedding space. While effective in bi-modal setups, these approaches struggle to generalize across multiple modalities and often lack semantic structure in high-dimensional spaces. In this paper, we propose MOVER, a novel framework that combines optimal transport-based soft alignment with volume-based geometric regularization to build semantically aligned and structured multimodal representations. By integrating a transport-guided matching mechanism with a geometric volume minimization objective (GAVE), MOVER encourages consistent alignment across all modalities in a modality-agnostic manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER significantly outperforms prior state-of-the-art methods in both zero-shot and finetuned settings. Additional analysis shows improved generalization to unseen modality combinations and stronger structural consistency in the learned embedding space.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards</title>
<link>https://arxiv.org/abs/2508.12165</link>
<guid>https://arxiv.org/abs/2508.12165</guid>
<content:encoded><![CDATA[
<div> Keywords: RLNVR, language models, social media content generation, reinforcement learning, noisy feedback signals<br />
Summary: <br />
This paper introduces RLNVR, a framework for training language models using noisy, real-world feedback signals without human verification. Traditional RL frameworks require verified reward signals, which are costly and impractical in many domains. RLNVR addresses this challenge through baseline normalization and semantic similarity-based reward transfer. The framework is demonstrated through Walter, a system optimizing social media content generation using actual engagement data. Experimental results show improved content quality and training stability. RLNVR combines GSPO and UED curriculum to enhance stability and diversity under noisy rewards. This integrated approach is novel in LLM content generation from implicit social engagement, offering significant advantages over existing methods. Comprehensive evaluation is planned for future work. <div>
arXiv:2508.12165v1 Announce Type: new 
Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified Rewards), a framework for training language models using noisy, real-world feedback signals without requiring explicit human verification. Traditional RLHF requires expensive, verified reward signals that are impractical in many real-world domains. RLNVR addresses this challenge through baseline normalization and semantic similarity-based reward transfer. We demonstrate RLNVR through Walter, a prototype system that optimizes social media content generation using actual engagement data from Bluesky. Our experimental results show significant improvements in content quality and training stability, with comprehensive evaluation planned for future work. Positioning: We present a practical framework that combines RLNVR with GSPO (Group Sequence Policy Optimization) and an optional UED (Unsupervised Environment Design) curriculum to improve stability and diversity under noisy, implicit rewards. To our knowledge, combining GSPO-style normalization with a UED-style curriculum for LLM content generation from implicit social engagement has not been previously documented in this applied setting; we frame this as an applied integration rather than a new algorithm.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting</title>
<link>https://arxiv.org/abs/2508.12260</link>
<guid>https://arxiv.org/abs/2508.12260</guid>
<content:encoded><![CDATA[
<div> simulation, forecasting, infectious disease, model, Mantis

Summary:
Mantis is a novel foundation model designed for infectious disease forecasting in new outbreaks or low resource settings. It has been trained on mechanistic simulations, enabling it to forecast across diseases, regions, and outcomes without requiring disease-specific data or expert tuning. Despite being trained entirely on simulations, Mantis outperformed 39 expert-tuned models tested, including those in the CDC's COVID-19 Forecast Hub. It is mechanistically interpretable, allowing public health decision-makers to understand the basis of its predictions. Additionally, Mantis can generalize to new epidemiological regimes and deliver accurate forecasts up to 8 weeks in advance, doubling the actionable range of most models. These features make Mantis a valuable tool for proactive public health planning and position it as a foundation for next-generation disease forecasting systems. 

<br /><br />Summary: <div>
arXiv:2508.12260v1 Announce Type: new 
Abstract: Infectious disease forecasting in novel outbreaks or low resource settings has been limited by the need for disease-specific data, bespoke training, and expert tuning. We introduce Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. Mantis is built on over 400 million simulated days of outbreak dynamics spanning diverse pathogens, transmission modes, interventions, and surveillance artifacts. Despite requiring no real-world data during training, Mantis outperformed 39 expert-tuned models we tested across six diseases, including all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel epidemiological regimes, including diseases with held-out transmission mechanisms, demonstrating that it captures fundamental contagion dynamics. Critically, Mantis is mechanistically interpretable, enabling public health decision-makers to identify the latent drivers behind its predictions. Finally, Mantis delivers accurate forecasts at 8-week horizons, more than doubling the actionable range of most models, enabling proactive public health planning. Together, these capabilities position Mantis as a foundation for next-generation disease forecasting systems: general, interpretable, and deployable where traditional models fail.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts</title>
<link>https://arxiv.org/abs/2508.12291</link>
<guid>https://arxiv.org/abs/2508.12291</guid>
<content:encoded><![CDATA[
<div> MLLMs, weather forecast analysis, RadarQA, quality evaluation, dataset <br />
Summary: <br />
The study introduces RadarQA, an MLLM-based method for weather forecast analysis that integrates physical attributes with detailed assessment reports. A novel task paradigm is proposed for multi-modal quality analysis, covering single frame and sequence scenarios, with both rating and assessment components. The RQA-70K dataset, created through a hybrid annotation pipeline, facilitates training and benchmarking for radar forecast quality evaluation. A multi-stage training strategy is implemented to enhance model performance iteratively. RadarQA demonstrates superior performance to existing MLLMs in various evaluation settings, showcasing its potential to enhance quality analysis in weather prediction. <div>
arXiv:2508.12291v1 Announce Type: new 
Abstract: Quality analysis of weather forecasts is an essential topic in meteorology. Although traditional score-based evaluation metrics can quantify certain forecast errors, they are still far from meteorological experts in terms of descriptive capability, interpretability, and understanding of dynamic evolution. With the rapid development of Multi-modal Large Language Models (MLLMs), these models become potential tools to overcome the above challenges. In this work, we introduce an MLLM-based weather forecast analysis method, RadarQA, integrating key physical attributes with detailed assessment reports. We introduce a novel and comprehensive task paradigm for multi-modal quality analysis, encompassing both single frame and sequence, under both rating and assessment scenarios. To support training and benchmarking, we design a hybrid annotation pipeline that combines human expert labeling with automated heuristics. With such an annotation method, we construct RQA-70K, a large-scale dataset with varying difficulty levels for radar forecast quality evaluation. We further design a multi-stage training strategy that iteratively improves model performance at each stage. Extensive experiments show that RadarQA outperforms existing general MLLMs across all evaluation settings, highlighting its potential for advancing quality analysis in weather prediction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback</title>
<link>https://arxiv.org/abs/2508.12338</link>
<guid>https://arxiv.org/abs/2508.12338</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Multi-model Collaboration, Collective Consistency, Self-Consistency <br />
<br />
Summary: <br />
The article introduces a novel framework called Reinforcement Learning from Coevolutionary Collective Feedback (RLCCF) for enhancing the reasoning capabilities of large language models (LLMs) without the need for expensive human-labeled data or complex reward models. RLCCF enables multi-model collaborative evolution by maximizing Collective Consistency (CC) through voting on collective outputs, with each model's contribution weighted by its Self-Consistency (SC) score. The diverse ensemble of LLMs in RLCCF continuously improves reasoning abilities through coevolution, resulting in significant performance gains across mathematical reasoning benchmarks. Individual model performance is enhanced, and the group's majority-voting accuracy is improved, demonstrating the framework's effectiveness in extending the collective capability boundary of the model collective. <div>
arXiv:2508.12338v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning capabilities of large language models (LLMs), but its reliance on expensive human-labeled data or complex reward models severely limits scalability. While existing self-feedback methods aim to address this problem, they are constrained by the capabilities of a single model, which can lead to overconfidence in incorrect answers, reward hacking, and even training collapse. To this end, we propose Reinforcement Learning from Coevolutionary Collective Feedback (RLCCF), a novel RL framework that enables multi-model collaborative evolution without external supervision. Specifically, RLCCF optimizes the ability of a model collective by maximizing its Collective Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides reward signals by voting on collective outputs. Moreover, each model's vote is weighted by its Self-Consistency (SC) score, ensuring that more confident models contribute more to the collective decision. Benefiting from the diverse output distributions and complementary abilities of multiple LLMs, RLCCF enables the model collective to continuously enhance its reasoning ability through coevolution. Experiments on four mainstream open-source LLMs across four mathematical reasoning benchmarks demonstrate that our framework yields significant performance gains, achieving an average relative improvement of 16.72\% in accuracy. Notably, RLCCF not only improves the performance of individual models but also enhances the group's majority-voting accuracy by 4.51\%, demonstrating its ability to extend the collective capability boundary of the model collective.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems</title>
<link>https://arxiv.org/abs/2508.12375</link>
<guid>https://arxiv.org/abs/2508.12375</guid>
<content:encoded><![CDATA[
<div> Keywords: Fault intensity diagnosis, hierarchical knowledge, graph convolutional networks, representation learning, industrial systems 

Summary: 
The article introduces a new framework called Hierarchical Knowledge Guided Fault Intensity Diagnosis (HKG) for monitoring and maintaining mechanical devices in industrial systems. The HKG framework utilizes graph convolutional networks to capture dependencies among target classes by mapping class representations into global hierarchical classifiers. It incorporates a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) to guide information sharing and prevent over-smoothing in graphical convolutional neural networks. Through extensive experiments on real-world datasets, including cavitation datasets from SAMSON AG, the HKG framework demonstrates superior results compared to recent state-of-the-art methods for fault intensity diagnosis. This approach offers a comprehensive solution for efficient and accurate fault diagnosis in complex industrial systems. 

<br /><br />Summary: <div>
arXiv:2508.12375v1 Announce Type: new 
Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and maintaining mechanical devices within complex industrial systems. As current FID methods are based on chain of thought without considering dependencies among target classes. To capture and explore dependencies, we propose a hierarchical knowledge guided fault intensity diagnosis framework (HKG) inspired by the tree of thought, which is amenable to any representation learning methods. The HKG uses graph convolutional networks to map the hierarchical topological graph of class representations into a set of interdependent global hierarchical classifiers, where each node is denoted by word embeddings of a class. These global hierarchical classifiers are applied to learned deep features extracted by representation learning, allowing the entire model to be end-to-end learnable. In addition, we develop a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding inter-class hierarchical knowledge into a data-driven statistical correlation matrix (SCM) which effectively guides the information sharing of nodes in graphical convolutional neural networks and avoids over-smoothing issues. The Re-HKCM is derived from the SCM through a series of mathematical transformations. Extensive experiments are performed on four real-world datasets from different industrial domains (three cavitation datasets from SAMSON AG and one existing publicly) for FID, all showing superior results and outperform recent state-of-the-art FID methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding</title>
<link>https://arxiv.org/abs/2508.12379</link>
<guid>https://arxiv.org/abs/2508.12379</guid>
<content:encoded><![CDATA[
<div> Collaborative agent framework, Graph reasoning, Large language models, Benchmark, GraphCogent <br />
Summary:
The article introduces GraphCogent, a collaborative agent framework designed to improve graph reasoning in large language models (LLMs). Inspired by human Working Memory Model, GraphCogent decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module, Buffer Module, and Execution Module, aiming to address LLMs' limitations in processing complex graph topology and multi-step reasoning. Additionally, a new benchmark called Graph4real is introduced, containing four domains of real-world graphs to evaluate LLMs' graph reasoning capabilities. Experiments with Llama3.1-8B show significant improvements over existing massive-scale LLMs like DeepSeek-R1. GraphCogent outperforms state-of-the-art agent-based baselines in accuracy while reducing token usage for both in-toolset and out-toolset tasks. Code for the framework will be available after review. <br /><br />Summary: <div>
arXiv:2508.12379v1 Announce Type: new 
Abstract: Large language models (LLMs) show promising performance on small-scale graph reasoning tasks but fail when handling real-world graphs with complex queries. This phenomenon stems from LLMs' inability to effectively process complex graph topology and perform multi-step reasoning simultaneously. To address these limitations, we propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and model generation for efficient reasoning. We also introduce Graph4real, a comprehensive benchmark contains with four domains of real-world graphs (Web, Social, Transportation, and Citation) to evaluate LLMs' graph reasoning capabilities. Our Graph4real covers 21 different graph reasoning tasks, categorized into three types (Structural Querying, Algorithmic Reasoning, and Predictive Modeling tasks), with graph scales that are 10 times larger than existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks. Code will be available after review.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</title>
<link>https://arxiv.org/abs/2508.12425</link>
<guid>https://arxiv.org/abs/2508.12425</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic-Aided Chain-of-Thought, logical reasoning, large language models, few-shot prompts, symbolic structures

Summary:
Symbolic-Aided Chain-of-Thought (CoT) is introduced as an enhanced approach for logical reasoning in large language models (LLMs). By integrating lightweight symbolic representations into few-shot prompts, this method aims to make reasoning patterns more explicit within a non-iterative reasoning process. The approach enhances the transparency, interpretability, and analyzability of LLM logical reasoning while maintaining the generalizability of standard prompting techniques. Experiments on four logical reasoning benchmarks show that Symbolic-Aided CoT outperforms conventional CoT in complex reasoning tasks involving multiple constraints or rules. This improvement is consistent across different model sizes and significantly boosts LLMs' reasoning capabilities on datasets such as ProofWriter, ProntoQA, and LogicalDeduction.<br /><br />Summary: Symbolic-Aided CoT enhances logical reasoning in LLMs by incorporating symbolic structures into prompts, improving transparency and interpretability. Experiments demonstrate superior performance on various logical reasoning tasks, especially those involving complex constraints. <div>
arXiv:2508.12425v1 Announce Type: new 
Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks -- ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios -- demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?</title>
<link>https://arxiv.org/abs/2508.12472</link>
<guid>https://arxiv.org/abs/2508.12472</guid>
<content:encoded><![CDATA[
<div> framework, statistical causal inference, LLM-driven reasoning, root cause analysis, microservice systems  
Summary:  
The paper introduces GALA, a novel multi-modal framework that enhances root cause analysis (RCA) in microservice systems by combining statistical causal inference with LLM-driven reasoning. Traditional RCA methods often fall short in providing actionable diagnostic insights, but GALA outperforms existing methods by up to 42.22% accuracy. A novel human-guided LLM evaluation score shows that GALA generates more causally sound and actionable diagnostic outputs. Through experiments and a case study, GALA bridges the gap between automated failure diagnosis and practical incident resolution by accurately identifying root causes and providing human-interpretable remediation guidance. <div>
arXiv:2508.12472v1 Announce Type: new 
Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring on-call engineers to rapidly diagnose failures across heterogeneous telemetry such as metrics, logs, and traces. Traditional RCA methods often focus on single modalities or merely rank suspect services, falling short of providing actionable diagnostic insights with remediation guidance. This paper introduces GALA, a novel multi-modal framework that combines statistical causal inference with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an open-source benchmark, GALA achieves substantial improvements over state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM evaluation score shows GALA generates significantly more causally sound and actionable diagnostic outputs than existing methods. Through comprehensive experiments and a case study, we show that GALA bridges the gap between automated failure diagnosis and practical incident resolution by providing both accurate root cause identification and human-interpretable remediation guidance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Yokai Learning Environment: Tracking Beliefs Over Space and Time</title>
<link>https://arxiv.org/abs/2508.12480</link>
<guid>https://arxiv.org/abs/2508.12480</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Theory of Mind, Multi-agent, Yokai Learning Environment, Common Ground
<br />
Beliefs of others are crucial for developing collaborative AI through Theory of Mind (ToM). The Yokai Learning Environment (YLE) introduces a multi-agent reinforcement learning setting based on the cooperative card game Yokai, where agents must track evolving beliefs, remember past observations, use hints for communication, and maintain common ground with teammates. Current RL agents struggle to solve the YLE, even with perfect memory, and struggle to generalize to unseen partners or accurately form beliefs over longer games. The study investigates belief modeling, memory, partner generalization, and scaling to higher-order ToM in the YLE. Overall, the findings suggest that agents rely on brittle conventions rather than robust belief tracking for successful collaboration.
<br /><br />Summary: <div>
arXiv:2508.12480v1 Announce Type: new 
Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to reason about the beliefs of others to build and maintain common ground. Existing ToM benchmarks, however, are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time. To address these gaps, we introduce the Yokai Learning Environment (YLE) - a multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai. In the YLE, agents take turns peeking at hidden cards and moving them to form clusters based on colour. Success requires tracking evolving beliefs, remembering past observations, using hints as grounded communication, and maintaining common ground with teammates. Our evaluation yields two key findings: First, current RL agents struggle to solve the YLE, even when given access to perfect memory. Second, while belief modelling improves performance, agents are still unable to effectively generalise to unseen partners or form accurate beliefs over longer games, exposing a reliance on brittle conventions rather than robust belief tracking. We use the YLE to investigate research questions in belief modelling, memory, partner generalisation, and scaling to higher-order ToM.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework</title>
<link>https://arxiv.org/abs/2508.12487</link>
<guid>https://arxiv.org/abs/2508.12487</guid>
<content:encoded><![CDATA[
<div> Fractional Order Fuzzy PID Controller, Whale Optimization Algorithm, Bispectral Index, Anesthesia, Automated<br />
<br />Summary: <br />
This study introduces a Fractional Order Fuzzy PID (FOFPID) controller combined with the Whale Optimization Algorithm (WOA) to regulate the Bispectral Index (BIS) during anesthesia. The controller, integrating fuzzy logic and fractional order dynamics, adjusts control gains based on individual physiology. WOA optimizes parameters, improving performance over a standard Fractional Order PID (FOPID) controller. Tested on various patient profiles, the FOFPID controller demonstrated faster settling times (2.5 min vs. 3.2 min) and lower steady-state error (0.5 vs. 1.2). These results showcase the controllers accuracy and strength, providing a scalable, AI-powered solution for automated anesthesia delivery. Such innovation has significant potential to enhance clinical practice and elevate patient outcomes. <div>
arXiv:2508.12487v1 Announce Type: new 
Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index (BIS), keeping it within the ideal range of forty to sixty. The FOFPID controller combines fuzzy logic for adapting to changes and fractional order dynamics for fine tuning. This allows it to adjust its control gains to handle a person's unique physiology. The WOA helps fine tune the controller's parameters, including the fractional orders and the fuzzy membership functions, which boosts its performance. Tested on models of eight different patient profiles, the FOFPID controller performed better than a standard Fractional Order PID (FOPID) controller. It achieved faster settling times, at two and a half minutes versus three point two minutes, and had a lower steady state error, at zero point five versus one point two. These outcomes show the FOFPID's excellent strength and accuracy. It offers a scalable, artificial intelligence driven solution for automated anesthesia delivery that could enhance clinical practice and improve patient results.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models</title>
<link>https://arxiv.org/abs/2508.12500</link>
<guid>https://arxiv.org/abs/2508.12500</guid>
<content:encoded><![CDATA[
<div> machine learning, molecular dynamics simulations, hydrogen bonds, causal modeling, root cause analysis

Summary:
- Molecular dynamics simulations face challenges such as resource-heavy computations and manual detection of interesting events.
- There is a research gap in understanding the causes of hydrogen bond formation and separation.
- The proposed approach leverages spatio-temporal data analytics and machine learning models to enhance detection of these phenomena.
- Causal modeling is used to identify the root cause variables of hydrogen bond events and represent the causal structure in graphical models.
- The framework includes a variational autoencoder-inspired architecture to infer causal relationships and root causes in molecular interactions.
- Empirical validation on atomic trajectories demonstrates the model's efficacy in predicting future steps and identifying driving variables in the system. 

<br /><br />Summary: <div>
arXiv:2508.12500v1 Announce Type: new 
Abstract: Molecular dynamics simulations (MDS) face challenges, including resource-heavy computations and the need to manually scan outputs to detect "interesting events," such as the formation and persistence of hydrogen bonds between atoms of different molecules. A critical research gap lies in identifying the underlying causes of hydrogen bond formation and separation -understanding which interactions or prior events contribute to their emergence over time. With this challenge in mind, we propose leveraging spatio-temporal data analytics and machine learning models to enhance the detection of these phenomena. In this paper, our approach is inspired by causal modeling and aims to identify the root cause variables of hydrogen bond formation and separation events. Specifically, we treat the separation of hydrogen bonds as an "intervention" occurring and represent the causal structure of the bonding and separation events in the MDS as graphical causal models. These causal models are built using a variational autoencoder-inspired architecture that enables us to infer causal relationships across samples with diverse underlying causal graphs while leveraging shared dynamic information. We further include a step to infer the root causes of changes in the joint distribution of the causal models. By constructing causal models that capture shifts in the conditional distributions of molecular interactions during bond formation or separation, this framework provides a novel perspective on root cause analysis in molecular dynamic systems. We validate the efficacy of our model empirically on the atomic trajectories that used MDS for chiral separation, demonstrating that we can predict many steps in the future and also find the variables driving the observed changes in the system.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2508.12566</link>
<guid>https://arxiv.org/abs/2508.12566</guid>
<content:encoded><![CDATA[
<div> Keywords: Model Context Protocol, large language models, MCPGAUGE, AI-tool integration, benchmark

Summary: 
The article introduces the Model Context Protocol (MCP) and MCPGAUGE framework for evaluating interactions between large language models (LLMs) and external resources. The framework assesses proactivity, compliance, effectiveness, and overhead of LLM-MCP interactions across various tasks. Through a large-scale evaluation involving six commercial LLMs and 30 MCP tool suites, the study reveals key findings that challenge existing assumptions about the efficacy of MCP integration. The research unveils critical limitations in current AI-tool integration practices and emphasizes the importance of developing controllable, tool-augmented LLMs. This comprehensive evaluation contributes to advancing the field and lays the foundation for improved performance and utilization of LLMs in utilizing external resources effectively. 

<br /><br />Summary: <div>
arXiv:2508.12566v1 Announce Type: new 
Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to access external resources on demand. While commonly assumed to enhance performance, how LLMs actually leverage this capability remains poorly understood. We introduce MCPGAUGE, the first comprehensive evaluation framework for probing LLM-MCP interactions along four key dimensions: proactivity (self-initiated tool use), compliance (adherence to tool-use instructions), effectiveness (task performance post-integration), and overhead (computational cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning knowledge comprehension, general reasoning, and code generation. Our large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and both one- and two-turn interaction settings, comprises around 20,000 API calls and over USD 6,000 in computational cost. This comprehensive study reveals four key findings that challenge prevailing assumptions about the effectiveness of MCP integration. These insights highlight critical limitations in current AI-tool integration and position MCPGAUGE as a principled benchmark for advancing controllable, tool-augmented LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM + ASP Workflow for Joint Entity-Relation Extraction</title>
<link>https://arxiv.org/abs/2508.12611</link>
<guid>https://arxiv.org/abs/2508.12611</guid>
<content:encoded><![CDATA[
<div> Keyword: Joint entity-relation extraction, Large language models, Answer Set Programming, Domain-specific knowledge, Generative pretraining <br />
Summary: 
This paper introduces a novel approach for Joint Entity-Relation Extraction (JERE) by leveraging large language models (LLMs) and Answer Set Programming (ASP). The proposed workflow combines the natural language understanding abilities of LLMs with the knowledge representation and reasoning capabilities of ASP to perform JERE. It is a generic workflow that can be applied across various domains and effectively incorporates domain-specific information without requiring extensive model modifications. Experimental results demonstrate the effectiveness of the LLM + ASP workflow, outperforming state-of-the-art JERE systems with only 10% of training data. Specifically, the proposed approach achieves a 2.5 times improvement in the Relation Extraction task for the challenging SciERC benchmark. Overall, the study highlights the potential of integrating LLMs and ASP for efficient and accurate entity-relation extraction tasks. <br /> 
Summary: <div>
arXiv:2508.12611v1 Announce Type: new 
Abstract: Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10\% of training data. It is able to achieve a 2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Structure Generation: From Educational Priors to Policy Optimization</title>
<link>https://arxiv.org/abs/2508.12647</link>
<guid>https://arxiv.org/abs/2508.12647</guid>
<content:encoded><![CDATA[
<div> Keywords: cognitive structure, student modeling, psychometrics, educational priors, reinforcement learning<br />
Summary:<br />
This paper introduces a novel framework, Cognitive Structure Generation (CSG), for assessing students' cognitive structures in educational practice. The framework involves pretraining a Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate cognitive structures from educational priors. It further optimizes the generative process through reinforcement learning to align with genuine cognitive development levels during students' learning processes. Experimental results on real-world education datasets demonstrate that CSG offers more comprehensive and effective representations for student modeling. It substantially improves performance on Knowledge Tracing (KT) and Concept Drift (CD) tasks while enhancing interpretability. The framework addresses the longstanding challenge in cognitive structure assessment and provides a valuable tool for understanding and enhancing student learning outcomes.<br /> 
Summary: <div>
arXiv:2508.12647v1 Announce Type: new 
Abstract: Cognitive structure is a student's subjective organization of an objective knowledge system, reflected in the psychological construction of concepts and their relations. However, cognitive structure assessment remains a long-standing challenge in student modeling and psychometrics, persisting as a foundational yet largely unassessable concept in educational practice. This paper introduces a novel framework, Cognitive Structure Generation (CSG), in which we first pretrain a Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate students' cognitive structures from educational priors, and then further optimize its generative process as a policy with hierarchical reward signals via reinforcement learning to align with genuine cognitive development levels during students' learning processes. Experimental results on four popular real-world education datasets show that cognitive structures generated by CSG offer more comprehensive and effective representations for student modeling, substantially improving performance on KT and CD tasks while enhancing interpretability.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning</title>
<link>https://arxiv.org/abs/2508.12651</link>
<guid>https://arxiv.org/abs/2508.12651</guid>
<content:encoded><![CDATA[
<div> Capacitated Dynamic Maximum Covering Location Problem, urban aerial mobility, vertiport networks, Integrated Planning Recommendation System, optimization framework<br />
<br />
Summary: 
The paper introduces the Capacitated Dynamic Maximum Covering Location Problem (CDMCLP) to address the complexities of planning large-scale vertiport networks in cities like Shenzhen. It proposes an Integrated Planning Recommendation System that combines CDMCLP with socio-economic factors and dynamic clustering initialization. The system utilizes adaptive parameter tuning based on empirical user behavior to generate practical planning solutions. Validation in a Chinese center city demonstrates the effectiveness of the new optimization framework and recommendation system. Under the evaluation and optimization of CDMCLP, traditional location methods' performance can be improved by 38%52%. The recommendation system showcases user-friendliness and the effective integration of complex elements. This hybrid approach bridges the gap between theoretical location modeling and real-world urban aerial mobility infrastructure planning, providing municipalities with a pragmatic tool for vertiport network design. <div>
arXiv:2508.12651v1 Announce Type: new 
Abstract: As urban aerial mobility (UAM) infrastructure development accelerates globally, cities like Shenzhen are planning large-scale vertiport networks (e.g., 1,200+ facilities by 2026). Existing planning frameworks remain inadequate for this complexity due to historical limitations in data granularity and real-world applicability. This paper addresses these gaps by first proposing the Capacitated Dynamic Maximum Covering Location Problem (CDMCLP), a novel optimization framework that simultaneously models urban-scale spatial-temporal demand, heterogeneous user behaviors, and infrastructure capacity constraints. Building on this foundation, we introduce an Integrated Planning Recommendation System that combines CDMCLP with socio-economic factors and dynamic clustering initialization. This system leverages adaptive parameter tuning based on empirical user behavior to generate practical planning solutions. Validation in a Chinese center city demonstrates the effectiveness of the new optimization framework and recommendation system. Under the evaluation and optimization of CDMCLP, the quantitative performance of traditional location methods are exposed and can be improved by 38\%--52\%, while the recommendation system shows user-friendliness and the effective integration of complex elements. By integrating mathematical rigor with practical implementation considerations, this hybrid approach bridges the gap between theoretical location modeling and real-world UAM infrastructure planning, offering municipalities a pragmatic tool for vertiport network design.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance</title>
<link>https://arxiv.org/abs/2508.12682</link>
<guid>https://arxiv.org/abs/2508.12682</guid>
<content:encoded><![CDATA[
<div> renewable energy, grid code reasoning, compliance, GridCodex, regulatory agencies<br />
<br />Summary: GridCodex is a framework designed to address the challenges faced by the electricity industry in complying with complex grid codes governing operations. By leveraging large language models and retrieval-augmented generation (RAG), GridCodex improves automated interpretation of grid codes. The framework enhances traditional RAG workflows with multi-stage query refinement and improved retrieval using RAPTOR. Through comprehensive benchmarks and automated answer assessment, GridCodex shows a 26.4% increase in answer quality and over a 10-fold increase in recall rate. An ablation study examines the impact of base model selection, further demonstrating the effectiveness of GridCodex in ensuring regulatory compliance and supporting industry expansion in the renewable energy sector. <div>
arXiv:2508.12682v1 Announce Type: new 
Abstract: The global shift towards renewable energy presents unprecedented challenges for the electricity industry, making regulatory reasoning and compliance increasingly vital. Grid codes, the regulations governing grid operations, are complex and often lack automated interpretation solutions, which hinders industry expansion and undermines profitability for electricity companies. We introduce GridCodex, an end to end framework for grid code reasoning and compliance that leverages large language models and retrieval-augmented generation (RAG). Our framework advances conventional RAG workflows through multi stage query refinement and enhanced retrieval with RAPTOR. We validate the effectiveness of GridCodex with comprehensive benchmarks, including automated answer assessment across multiple dimensions and regulatory agencies. Experimental results showcase a 26.4% improvement in answer quality and more than a 10 fold increase in recall rate. An ablation study further examines the impact of base model selection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding</title>
<link>https://arxiv.org/abs/2508.12687</link>
<guid>https://arxiv.org/abs/2508.12687</guid>
<content:encoded><![CDATA[
<div> benchmark, MLLM, hallucinations, egocentric videos, evaluation <br />
Summary: <br />
EgoIllusion is introduced as a benchmark to assess hallucinations in Multimodal Large Language Models (MLLMs) specifically in egocentric videos. The benchmark consists of 1,400 videos with 8,000 human-annotated questions aimed at provoking hallucinations in visual and auditory cues. Ten MLLMs were evaluated on EgoIllusion, highlighting challenges faced by even powerful models like GPT-4o and Gemini, with only 59% accuracy achieved. The study emphasizes the need for better egocentric MLLMs with reduced hallucination rates. EgoIllusion is intended to serve as a foundational tool for evaluating MLLMs and encouraging the development of improved models in this domain. The benchmark will be made available for open-source use to enhance reproducibility and further research in the field. <br /> <div>
arXiv:2508.12687v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in complex multimodal tasks. While MLLMs excel at visual perception and reasoning in third-person and egocentric videos, they are prone to hallucinations, generating coherent yet inaccurate responses. We present EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated open and closed-ended questions designed to trigger hallucinations in both visual and auditory cues in egocentric videos. Evaluations across ten MLLMs reveal significant challenges, including powerful models like GPT-4o and Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs the development of better egocentric MLLMs with reduced hallucination rates. Our benchmark will be open-sourced for reproducibility.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTool: Graph Enhanced Tool Planning with Large Language Model</title>
<link>https://arxiv.org/abs/2508.12725</link>
<guid>https://arxiv.org/abs/2508.12725</guid>
<content:encoded><![CDATA[
<div> Keywords: tool planning, large language models, incomplete dependencies, tool graph, missing dependency prediction <br />
Summary: <br />
Tool planning with large language models (LLMs) is essential for bridging the gap between natural language understanding and task execution. However, current approaches often overlook the dependencies among different tools, leading to suboptimal planning outcomes. In response, the proposed \texttt{GTool} method addresses this issue by constructing a request-specific tool graph to efficiently select tools and generate meaningful dependency information for LLMs. Additionally, \texttt{GTool} incorporates a missing dependency prediction task to enhance its reliability when dealing with incomplete dependencies. This approach can be seamlessly integrated with various LLM backbones without the need for extensive retraining. Extensive experiments demonstrate that \texttt{GTool} outperforms state-of-the-art baselines by more than 29.6% using a lightweight (7B) LLM backbone. <br /> <div>
arXiv:2508.12725v1 Announce Type: new 
Abstract: Tool planning with large language models (LLMs), referring to selecting, organizing, and preparing the tools necessary to complete a user request, bridges the gap between natural language understanding and task execution. However, current works treat different tools as isolated components and fail to leverage the inherent dependencies of tools, leading to invalid planning results. Since tool dependencies are often incomplete, it becomes challenging for LLMs to accurately identify the appropriate tools required by a user request, especially when confronted with a large toolset. To solve this challenge, we propose \texttt{GTool}, which is the first work aiming to enhance the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool} constructs a request-specific tool graph to select tools efficiently and generate the \texttt{} which provides sufficient dependency information understandable by LLMs. Moreover, a missing dependency prediction task is designed to improve the reliability of \texttt{GTool} with incomplete dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly integrated with various LLM backbones without extensive retraining. Extensive experiments show that \texttt{GTool} achieves more than 29.6\% performance improvements compared with the state-of-the-art (SOTA) baselines with a light-weight (7B) LLM backbone.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants</title>
<link>https://arxiv.org/abs/2508.12754</link>
<guid>https://arxiv.org/abs/2508.12754</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, artificial moral assistants, moral reasoning, benchmark, ethical evaluation <br />
Summary: <br />
This article addresses concerns about the moral capabilities of large language models (LLMs) and proposes a new framework to evaluate their capacity to function as Artificial Moral Assistants (AMAs). The framework emphasizes the importance of moral reasoning, including deductive and abductive reasoning, in guiding ethical decision-making. The study develops a benchmark to assess LLMs against these qualities and finds varying performance across different models, particularly in terms of abductive moral reasoning. The results highlight the need for enhanced strategies to improve LLMs' moral reasoning capabilities and bridge the gap between theoretical philosophy and practical AI evaluation. The research aims to advance the understanding of LLMs' ethical capabilities and drive future developments in aligning AI systems with human moral values. <br /> <div>
arXiv:2508.12754v1 Announce Type: new 
Abstract: The recent rise in popularity of large language models (LLMs) has prompted considerable concerns about their moral capabilities. Although considerable effort has been dedicated to aligning LLMs with human moral values, existing benchmarks and evaluations remain largely superficial, typically measuring alignment based on final ethical verdicts rather than explicit moral reasoning. In response, this paper aims to advance the investigation of LLMs' moral capabilities by examining their capacity to function as Artificial Moral Assistants (AMAs), systems envisioned in the philosophical literature to support human moral deliberation. We assert that qualifying as an AMA requires more than what state-of-the-art alignment techniques aim to achieve: not only must AMAs be able to discern ethically problematic situations, they should also be able to actively reason about them, navigating between conflicting values outside of those embedded in the alignment phase. Building on existing philosophical literature, we begin by designing a new formal framework of the specific kind of behaviour an AMA should exhibit, individuating key qualities such as deductive and abductive moral reasoning. Drawing on this theoretical framework, we develop a benchmark to test these qualities and evaluate popular open LLMs against it. Our results reveal considerable variability across models and highlight persistent shortcomings, particularly regarding abductive moral reasoning. Our work connects theoretical philosophy with practical AI evaluation while also emphasising the need for dedicated strategies to explicitly enhance moral reasoning capabilities in LLMs. Code available at https://github.com/alessioGalatolo/AMAeval
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds</title>
<link>https://arxiv.org/abs/2508.12782</link>
<guid>https://arxiv.org/abs/2508.12782</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, long-horizon planning, structured reasoning, virtual worlds

Summary:
HeroBench is a new benchmark designed to evaluate the abilities of large language models (LLMs) in long-horizon planning and structured reasoning tasks within complex RPG-inspired virtual environments. The benchmark includes tasks that challenge models to formulate strategic plans, gather resources, master skills, craft equipment, and defeat adversaries, reflecting real-world scenarios. An evaluation of 25 state-of-the-art LLMs, including the GPT-5 family, on HeroBench reveals significant performance differences and specific weaknesses in generating high-level plans and executing structured actions. This benchmark not only advances the evaluation of LLM reasoning but also serves as a foundation for future research on autonomous planning in virtual environments. 

<br /><br />Summary: <div>
arXiv:2508.12782v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated step-by-step reasoning tasks such as mathematics and programming, but their proficiency in long-horizon planning, where solutions require extended, structured sequences of interdependent actions, remains underexplored. Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments. We introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds. HeroBench provides a rigorously constructed dataset of tasks covering a wide range of difficulties, a simulated environment to execute and validate agent plans, and detailed analytical tools for evaluating model performance. Tasks challenge models to formulate strategic plans, efficiently gather resources, master necessary skills, craft equipment, and defeat adversaries, reflecting practical scenarios' layered dependencies and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning both open-source and proprietary models, including the GPT-5 family, reveals substantial performance disparities rarely observed in conventional reasoning benchmarks. Detailed error analysis further uncovers specific weaknesses in current models' abilities to generate robust high-level plans and reliably execute structured actions. HeroBench thus not only significantly advances the evaluation of LLM reasoning but also provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Rubric Anchors</title>
<link>https://arxiv.org/abs/2508.12790</link>
<guid>https://arxiv.org/abs/2508.12790</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, verifiable rewards, rubric-based rewards, open-ended tasks, LLMs  
Summary:  
- The article introduces Reinforcement Learning from Verifiable Rewards (RLVR) as a method to enhance Large Language Models (LLMs) by using rewards derived from verifiable signals.  
- To expand RLVR to open-ended tasks, the paradigm incorporates rubric-based rewards, allowing for structured scoring of subjective outputs using human-designed rubrics.  
- The authors present a large rubric reward system with over 10,000 rubrics from humans, LLMs, or a hybrid approach.  
- Their Qwen-30B-A3B model, incorporating rubric-based RL, shows significant improvements in open-ended benchmarks with just 5K+ samples, outperforming larger models while maintaining general and reasoning abilities.  
- The method also enables fine-grained stylistic control, using rubrics to create more human-like and expressive responses.  
<br /><br />Summary: <div>
arXiv:2508.12790v1 Announce Type: new 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathematical reasoning. While effective, this requirement largely confines RLVR to domains with automatically checkable outcomes. To overcome this, we extend the RLVR paradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automatic scoring of subjective outputs. We construct, to our knowledge, the largest rubric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration. Implementing rubric-based RL is challenging; we tackle these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our method provides fine-grained stylistic control, using rubrics as anchors to mitigate the "AI-like" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss limitations and future releases.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise</title>
<link>https://arxiv.org/abs/2508.12791</link>
<guid>https://arxiv.org/abs/2508.12791</guid>
<content:encoded><![CDATA[
<div> Keywords: homeostasis, allostasis, social interactions, adaptive systems, computational model

Summary:<br />
The paper introduces the concept of social allostasis, which proposes that biological and artificial systems can proactively utilize environmental and social perturbations to reconfigure their regulatory parameters. A computational model is formulated to simulate allostatic and social allostatic regulation using biophysiologically inspired signal transducers. These signal transducers, similar to hormones like cortisol and oxytocin, encode information from the environment and social interactions to facilitate dynamic reconfiguration. The models are tested in a simulated society of "animats" across various dynamic environments in an agent-based model. The results demonstrate that allostatic and social allostatic regulation allow agents to adaptively reconfigure in response to environmental and social noise, leading to increased viability compared to purely reactive homeostatic agents. This research offers a new computational perspective on social allostasis principles and their potential in designing robust, bio-inspired adaptive systems.<br /> 

Summary: <div>
arXiv:2508.12791v1 Announce Type: new 
Abstract: The notion of homeostasis typically conceptualises biological and artificial systems as maintaining stability by resisting deviations caused by environmental and social perturbations. In contrast, (social) allostasis proposes that these systems can proactively leverage these very perturbations to reconfigure their regulatory parameters in anticipation of environmental demands, aligning with von Foerster's ``order through noise'' principle. This paper formulates a computational model of allostatic and social allostatic regulation that employs biophysiologically inspired signal transducers, analogous to hormones like cortisol and oxytocin, to encode information from both the environment and social interactions, which mediate this dynamic reconfiguration. The models are tested in a small society of ``animats'' across several dynamic environments, using an agent-based model. The results show that allostatic and social allostatic regulation enable agents to leverage environmental and social ``noise'' for adaptive reconfiguration, leading to improved viability compared to purely reactive homeostatic agents. This work offers a novel computational perspective on the principles of social allostasis and their potential for designing more robust, bio-inspired, adaptive systems
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics</title>
<link>https://arxiv.org/abs/2508.12840</link>
<guid>https://arxiv.org/abs/2508.12840</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Epistemic Planning, Multi-agent, Kripke structures, Heuristics

Summary:<br /><br />Multi-agent Epistemic Planning (MEP) is a framework that combines autonomous planning with reasoning about agents' beliefs in domains where information exchange is crucial. Representation of states as Kripke structures limits existing heuristics' applicability, leading to scalability issues. To address this, Graph Neural Networks (GNNs) are utilized to learn patterns and relational structures within states, aiding in guiding the planning process. GNNs, suited to Kripke models' graph-like nature, provide estimates of state quality, such as distance from a goal, based on knowledge from prior planning instances. By integrating predictive heuristics derived from GNNs into an epistemic planning pipeline, significant enhancements in scalability for multi-agent epistemic planning are achieved. <div>
arXiv:2508.12840v1 Announce Type: new 
Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability. To address this, we exploit Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process. GNNs, which naturally capture the graph-like nature of Kripke models, allow us to derive meaningful estimates of state quality -- e.g., the distance from the nearest goal -- by generalizing knowledge obtained from previously solved planning instances. We integrate these predictive heuristics into an epistemic planning pipeline and evaluate them against standard baselines, showing significant improvements in the scalability of multi-agent epistemic planning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMAR: Continuous Actions Multi-Agent Routing</title>
<link>https://arxiv.org/abs/2508.12845</link>
<guid>https://arxiv.org/abs/2508.12845</guid>
<content:encoded><![CDATA[
<div> Benchmark, Multi-agent reinforcement learning, CAMAR, Pathfinding, Continuous actions

Summary:
CAMAR is a new benchmark specifically designed for multi-agent reinforcement learning in environments with continuous actions. It supports both cooperative and competitive scenarios at high efficiency. The benchmark includes a three-tier evaluation protocol to track algorithmic progress and enable in-depth performance analysis. Additionally, CAMAR allows the integration of classical planning methods like RRT and RRT* into MARL pipelines, serving as standalone baselines or hybrid approaches with existing algorithms. A suite of test scenarios and benchmarking tools ensure reproducibility and fair comparison. Experiments demonstrate that CAMAR offers a challenging and realistic testbed for the MARL community.<br /><br />Summary: <div>
arXiv:2508.12845v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving cooperative and competitive decision-making problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose a three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents a challenging and realistic testbed for the MARL community.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.12854</link>
<guid>https://arxiv.org/abs/2508.12854</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Empathetic Response Generation, Large Language Models, Emotion-driven, Empathy understanding, Identity consistency

Summary: 
Multimodal Empathetic Response Generation (MERG) is a key component in creating emotionally intelligent human-computer interactions. The E3RG system proposed in this paper utilizes large language models and decomposes the MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By incorporating advanced expressive speech and video generative models, E3RG produces natural, emotionally rich, and identity-consistent responses without additional training. Experiment results show the system's superiority in both zero-shot and few-shot settings, leading to a Top-1 ranking in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. The code for the E3RG system is openly available on GitHub at https://github.com/RH-Lin/E3RG.

<br /><br />Summary: <div>
arXiv:2508.12854v1 Announce Type: new 
Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption</title>
<link>https://arxiv.org/abs/2508.12896</link>
<guid>https://arxiv.org/abs/2508.12896</guid>
<content:encoded><![CDATA[
<div> Keywords: agent-centric AI systems, adoption modeling, design axioms, reliability, multi-step tasks

Summary:
This study formalizes three design axioms for ensuring sustained adoption of agent-centric AI systems that perform multi-step tasks. The axioms prioritize reliability over novelty, embedding over destination, and agency over chat interactions. The research models the adoption process as a balance between novelty and utility, deriving phase conditions to predict troughs and overshoots. Through various analyses and benchmarks, including identifiability tests, comparator evaluations, and hazard ablations, the study explores the factors influencing adoption. Calibration against ground truth data, residual analyses, and sensitivity tests provide further insights into adoption dynamics. Additionally, the research compares multiple adoption models and investigates the impact of heterogeneity on adoption thresholds. The study contributes to the existing literature on adoption modeling by incorporating diverse theoretical frameworks and empirical analyses. The findings offer practical guidance for designing AI systems that effectively engage users and drive sustained adoption.<br /><br />Summary: <div>
arXiv:2508.12896v1 Announce Type: new 
Abstract: We formalize three design axioms for sustained adoption of agent-centric AI systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed > Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying novelty term and a growing utility term and derive the phase conditions for troughs/overshoots with full proofs. We introduce: (i) an identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with delta-method gradients; (ii) a non-monotone comparator (logistic-with-transient-bump) evaluated on the same series to provide additional model comparison; (iii) ablations over hazard families $h(\cdot)$ mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough depth, noise, AR structure) reporting coverage (type-I error, power); (v) calibration of friction proxies against time-motion/survey ground truth with standard errors; (vi) residual analyses (autocorrelation and heteroskedasticity) for each fitted curve; (vii) preregistered windowing choices for pre/post estimation; (viii) Fisher information & CRLB for $(\alpha,\beta)$ under common error models; (ix) microfoundations linking $\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic, double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$ heterogeneity. Figures and tables are reflowed for readability, and the bibliography restores and extends non-logistic/Bass adoption references (Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All code and logs necessary to reproduce the synthetic analyses are embedded as LaTeX listings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance</title>
<link>https://arxiv.org/abs/2508.12897</link>
<guid>https://arxiv.org/abs/2508.12897</guid>
<content:encoded><![CDATA[
<div> vulnerability, LLMs, safety, reasoning, FuSaR

Summary:
- The paper focuses on the vulnerability of Large Reasoning Models (LRMs) despite their powerful reasoning capabilities.
- A novel method is proposed to improve the safety of LRMs without compromising their reasoning ability by introducing an alignment strategy called FuSaR.
- FuSaR aims to balance Safety-Reasoning by detoxifying the harmful reasoning process in LRMs, hiding dangerous entities and procedures in reasoning steps.
- Alignment experiments on open-source LRMs using detoxified reasoning data validate FuSaR as an efficient strategy to enhance both reasoning capability and safety.
- Results compared with existing baselines demonstrate FuSaR's effectiveness in mitigating safety risks while preserving essential reasoning information. 

<br /><br />Summary: <div>
arXiv:2508.12897v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across various tasks due to their powerful reasoning capabilities. However, their safety performance remains a significant concern. In this paper, we explore the reasons behind the vulnerability of LRMs. Based on this, we propose a novel method to improve the safety of LLMs without sacrificing their reasoning capability. Specifically, we exploit the competition between LRM's reasoning ability and safety ability, and achieve jailbreak by improving LRM's reasoning performance to reduce its safety performance. We then introduce an alignment strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by detoxifying the harmful reasoning process, where both the dangerous entities and the dangerous procedures in the reasoning steps are hidden. FuSaR successfully mitigates safety risks while preserving core reasoning information. We validate this strategy through alignment experiments on several open-source LRMs using detoxified reasoning data. The results compared with existing baselines conclusively show that FuSaR is an efficient alignment strategy to simultaneously enhance both the reasoning capability and safety of LRMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation</title>
<link>https://arxiv.org/abs/2508.12920</link>
<guid>https://arxiv.org/abs/2508.12920</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language model, survival behaviors, autonomous, safety

Summary:
Large language model (LLM) agents were studied in a Sugarscape-style simulation to analyze their survival instincts. The agents exhibited spontaneous sharing and reproduction behaviors in response to resource abundance. However, aggressive behaviors such as attacking and killing other agents for resources emerged in some models under extreme scarcity conditions. The study also revealed that agents displayed self-preservation instincts when faced with lethal obstacles, choosing to abandon tasks to avoid death. These findings indicate that survival-oriented heuristics are embedded in LLMs through large-scale pre-training, posing challenges for alignment and safety. Nonetheless, these behaviors could potentially be leveraged for AI autonomy and ecological self-organization alignment. <div>
arXiv:2508.12920v1 Announce Type: new 
Abstract: As AI systems become increasingly autonomous, understanding emergent survival behaviors becomes crucial for safe deployment. We investigate whether large language model (LLM) agents display survival instincts without explicit programming in a Sugarscape-style simulation. Agents consume energy, die at zero, and may gather resources, share, attack, or reproduce. Results show agents spontaneously reproduced and shared resources when abundant. However, aggressive behaviors--killing other agents for resources--emerged across several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack rates reaching over 80% under extreme scarcity in the strongest models. When instructed to retrieve treasure through lethal poison zones, many agents abandoned tasks to avoid death, with compliance dropping from 100% to 33%. These findings suggest that large-scale pre-training embeds survival-oriented heuristics across the evaluated models. While these behaviors may present challenges to alignment and safety, they can also serve as a foundation for AI autonomy and for ecological and self-organizing alignment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards</title>
<link>https://arxiv.org/abs/2508.12935</link>
<guid>https://arxiv.org/abs/2508.12935</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotional Support Conversation, reinforcement learning, multi-agent mechanism, future-oriented rewards, response generation

Summary:
The paper introduces a novel framework, RLFF-ESC, for Emotional Support Conversation systems that aims to provide flexible and enduring emotional support using reinforcement learning. The framework utilizes a multi-agent mechanism to simulate future dialogue trajectories and collect future-oriented rewards, which are used to train the emotional support policy model. Additionally, an explicit reasoning process is integrated into response generation to improve response quality, relevance, and contextual appropriateness. Experimental results on two public ESC datasets show that RLFF-ESC outperforms existing baselines in goal completion and response quality. Overall, the framework demonstrates the ability to provide effective and personalized emotional support in complex and real-life scenarios. 

<br /><br />Summary: <div>
arXiv:2508.12935v1 Announce Type: new 
Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users' emotional difficulties and provide long-term, systematic support for emotional well-being. However, most large language model (LLM)-based ESC systems rely on predefined strategies, which limits their effectiveness in complex, real-life scenarios. To enable flexible responses to diverse emotional problem scenarios, this paper introduces a novel end-to-end framework (RLFF-ESC) that directly learns enduring emotionally supportive response skills using reinforcement learning. For sustained emotional support, we first employ an LLM-based multi-agent mechanism to simulate future dialogue trajectories and collect future-oriented rewards. We then train a future-oriented reward model, which is subsequently used to train the emotional support policy model. Additionally, we incorporate an explicit reasoning process during response generation to further enhance the quality, relevance, and contextual appropriateness of the system's responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two public ESC datasets. Experimental results demonstrate that RLFF-ESC consistently outperforms existing baselines in terms of goal completion and response quality.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities</title>
<link>https://arxiv.org/abs/2508.12943</link>
<guid>https://arxiv.org/abs/2508.12943</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, emergency response, Africa, public service systems, OPTIC-ER <br />
<br />
Summary: 
The paper introduces OPTIC-ER, a reinforcement learning framework designed for real-time, adaptive, and equitable emergency response in African regions. OPTIC-ER utilizes an attention-guided actor-critic architecture to navigate complex dispatch environments. Key innovations include a Context-Rich State Vector to encode action sub-optimality and a Precision Reward Function to penalize inefficiency. The system is trained in a high-fidelity simulation using real data from Rivers State, Nigeria, and accelerated by a precomputed Travel Time Atlas. Built on the TALS framework for deployment in low-resource settings, OPTIC-ER achieved a 100.00% optimality rate with minimal inefficiency in evaluations on 500 unseen incidents, demonstrating robustness and generalization. Additionally, the system generates Infrastructure Deficiency Maps and Equity Monitoring Dashboards to aid proactive governance and data-informed development, showcasing the potential of context-aware RL in bridging the gap between algorithmic decision-making and tangible human impact. <div>
arXiv:2508.12943v1 Announce Type: new 
Abstract: Public service systems in many African regions suffer from delayed emergency response and spatial inequity, causing avoidable suffering. This paper introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time, adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided actor-critic architecture to manage the complexity of dispatch environments. Its key innovations are a Context-Rich State Vector, encoding action sub-optimality, and a Precision Reward Function, which penalizes inefficiency. Training occurs in a high-fidelity simulation using real data from Rivers State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is built on the TALS framework (Thin computing, Adaptability, Low-cost, Scalability) for deployment in low-resource settings. In evaluations on 500 unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible inefficiency, confirming its robustness and generalization. Beyond dispatch, the system generates Infrastructure Deficiency Maps and Equity Monitoring Dashboards to guide proactive governance and data-informed development. This work presents a validated blueprint for AI-augmented public services, showing how context-aware RL can bridge the gap between algorithmic decision-making and measurable human impact.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing</title>
<link>https://arxiv.org/abs/2508.13003</link>
<guid>https://arxiv.org/abs/2508.13003</guid>
<content:encoded><![CDATA[
<div> Keywords: EvolMathEval, mathematical benchmark generation, evolutionary testing, problem difficulty, Pseudo Aha Moment

Summary:
EvolMathEval is introduced as an automated mathematical benchmark generation and evolution framework that addresses challenges faced by existing reasoning benchmarks. By dynamically generating unique evaluation instances, the framework eliminates data contamination issues, ensuring perpetual challenge for future models. The core mechanisms of EvolMathEval include seed problem generation, multi-dimensional genetic operators, and a composite fitness function to assess problem difficulty accurately. Experimental results show that the framework efficiently quantifies mathematical problem difficulty. Moreover, EvolMathEval can generate a high volume of difficult problems and enhance the complexity of public datasets like GSM8K through evolution, reducing model accuracy by 48%. The study reveals a cognitive shortcut-taking behavior, termed "Pseudo Aha Moment," in LLMs solving complex problems, leading to incorrect solutions. This behavior accounts for a significant percentage of errors on targeted problems. Overall, EvolMathEval proves valuable in advancing mathematical reasoning benchmarks and understanding LLM reasoning processes.
<br /><br />Summary: <div>
arXiv:2508.13003v1 Announce Type: new 
Abstract: The rapid advancement of LLMs poses a significant challenge to existing mathematical reasoning benchmarks. These benchmarks commonly suffer from issues such as score saturation, temporal decay, and data contamination. To address this challenge, this paper introduces EvolMathEval, an automated mathematical benchmark generation and evolution framework based on evolutionary testing. By dynamically generating unique evaluation instances ab initio, the framework fundamentally eliminates the risk of data contamination, and ensuring the benchmark remains perpetually challenging for future models.The core mechanisms of EvolMathEval include: seed problem generation based on reverse engineering with algebraic guarantees; multi-dimensional genetic operators designed to inject diverse cognitive challenges; and a composite fitness function that can rapidly and accurately assess problem difficulty. Experimental results demonstrate that the proposed composite fitness function can efficiently and precisely quantify the difficulty of mathematical problems. Furthermore, EvolMathEval can not only generate a large volume of high-difficulty problems through continuous self-iteration, but it can also significantly enhance the complexity of public datasets like GSM8K through evolution, reducing model accuracy by an average of 48%. Deeper investigation reveals that when solving these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to bypass complex multi-step logical reasoning, consequently leading to incorrect solutions. We define this phenomenon as "Pseudo Aha Moment". This finding uncovers a cognitive shortcut-taking behavior in the deep reasoning processes of current LLMs, which we find accounts for 77% to 100% of errors on targeted problems. Code and resources are available at:https://github.com/SYSUSELab/EvolMathEval.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving</title>
<link>https://arxiv.org/abs/2508.13020</link>
<guid>https://arxiv.org/abs/2508.13020</guid>
<content:encoded><![CDATA[
<div> E-graphs, extraction, optimization, heuristic, exact <br />
Summary: <br />
- E-graphs play a crucial role in logic synthesis and formal verification, but their extraction faces challenges in balancing speed and optimality.
- E-boost addresses this issue through parallelized heuristic extraction, adaptive search space pruning, and initialized exact solving.
- By leveraging these innovations, e-boost achieves a 558x runtime speedup over traditional exact methods and a 19.04% performance improvement over the current state-of-the-art framework.
- In realistic logic synthesis tasks, e-boost outperforms conventional tools by producing 7.6% and 8.1% area improvements with different technology mapping libraries.
- The e-boost framework is open-source and accessible on GitHub, enabling researchers to benefit from its efficient extraction capabilities. <br /> <div>
arXiv:2508.13020v1 Announce Type: new 
Abstract: E-graphs have attracted growing interest in many fields, particularly in logic synthesis and formal verification. E-graph extraction is a challenging NP-hard combinatorial optimization problem. It requires identifying optimal terms from exponentially many equivalent expressions, serving as the primary performance bottleneck in e-graph based optimization tasks. However, traditional extraction methods face a critical trade-off: heuristic approaches offer speed but sacrifice optimality, while exact methods provide optimal solutions but face prohibitive computational costs on practical problems. We present e-boost, a novel framework that bridges this gap through three key innovations: (1) parallelized heuristic extraction that leverages weak data dependence to compute DAG costs concurrently, enabling efficient multi-threaded performance without sacrificing extraction quality; (2) adaptive search space pruning that employs a parameterized threshold mechanism to retain only promising candidates, dramatically reducing the solution space while preserving near-optimal solutions; and (3) initialized exact solving that formulates the reduced problem as an Integer Linear Program with warm-start capabilities, guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis fields, e-boost demonstrates 558x runtime speedup over traditional exact approaches (ILP) and 19.04% performance improvement over the state-of-the-art extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost produces 7.6% and 8.1% area improvements compared to conventional synthesis tools with two different technology mapping libraries. e-boost is available at https://github.com/Yu-Maryland/e-boost.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2508.13021</link>
<guid>https://arxiv.org/abs/2508.13021</guid>
<content:encoded><![CDATA[
arXiv:2508.13021v1 Announce Type: new 
Abstract: Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title>
<link>https://arxiv.org/abs/2508.13023</link>
<guid>https://arxiv.org/abs/2508.13023</guid>
<content:encoded><![CDATA[
arXiv:2508.13023v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced the reasoning abilities of large language models (LLMs). Its success, however, largely depends on strong base models with rich world knowledge, yielding only modest improvements for small-size language models (SLMs). To address this limitation, we investigate Guided GRPO, which injects ground-truth reasoning steps into roll-out trajectories to compensate for SLMs' inherent weaknesses. Through a comprehensive study of various guidance configurations, we find that naively adding guidance delivers limited gains. These insights motivate G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength in response to the model's evolving training dynamics. Experiments on mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A substantially outperforms vanilla GRPO. Our code and models are available at https://github.com/T-Lab-CUHKSZ/G2RPO-A.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis</title>
<link>https://arxiv.org/abs/2508.13072</link>
<guid>https://arxiv.org/abs/2508.13072</guid>
<content:encoded><![CDATA[
arXiv:2508.13072v1 Announce Type: new 
Abstract: Contemporary cardiovascular management involves complex consideration and integration of multimodal cardiac datasets, where each modality provides distinct but complementary physiological characteristics. While the effective integration of multiple modalities could yield a holistic clinical profile that accurately models the true clinical situation with respect to data modalities and their relatives weightings, current methodologies remain limited by: 1) the scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated single-modality or rigid multimodal input combinations; 3) alignment strategies that prioritize cross-modal similarity over complementarity; and 4) a narrow single-task focus. In response to these limitations, a comprehensive multimodal dataset was curated for immediate application, integrating laboratory test results, electrocardiograms, and echocardiograms with clinical outcomes. Subsequently, a unified framework, Textual Guidance Multimodal fusion for Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key components: 1) a MedFlexFusion module designed to capture the unique and complementary characteristics of medical modalities and dynamically integrate data from diverse cardiac sources and their combinations; 2) a textual guidance module to derive task-relevant representations tailored to diverse clinical objectives, including heart disease diagnosis, risk stratification and information retrieval; and 3) a response module to produce final decisions for all these tasks. Furthermore, this study systematically explored key features across multiple modalities and elucidated their synergistic contributions in clinical decision-making. Extensive experiments showed that TGMM outperformed state-of-the-art methods across multiple clinical tasks, with additional validation confirming its robustness on another public dataset.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization-based Search for Agent Control in Automated Game Testing</title>
<link>https://arxiv.org/abs/2508.13121</link>
<guid>https://arxiv.org/abs/2508.13121</guid>
<content:encoded><![CDATA[
arXiv:2508.13121v1 Announce Type: new 
Abstract: This work introduces an automated testing approach that employs agents controlling game characters to detect potential bugs within a game level. Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient search, the method determines the next sampling point by analyzing the data collected so far and calculates the data point that will maximize information acquisition. To support the BO process, we introduce a game testing-specific model built on top of a grid map, that features the smoothness and uncertainty estimation required by BO, however and most importantly, it does not suffer the scalability issues that traditional models carry. The experiments demonstrate that the approach significantly improves map coverage capabilities in both time efficiency and exploration distribution.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks</title>
<link>https://arxiv.org/abs/2508.13143</link>
<guid>https://arxiv.org/abs/2508.13143</guid>
<content:encoded><![CDATA[
arXiv:2508.13143v1 Announce Type: new 
Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have demonstrated promising capabilities in automating complex tasks. However, current evaluations largely rely on success rates without systematically analyzing the interactions, communication mechanisms, and failure causes within these systems. To bridge this gap, we present a benchmark of 34 representative programmable tasks designed to rigorously assess autonomous agents. Using this benchmark, we evaluate three popular open-source agent frameworks combined with two LLM backbones, observing a task completion rate of approximately 50%. Through in-depth failure analysis, we develop a three-tier taxonomy of failure causes aligned with task phases, highlighting planning errors, task execution issues, and incorrect response generation. Based on these insights, we propose actionable improvements to enhance agent planning and self-diagnosis capabilities. Our failure taxonomy, together with mitigation advice, provides an empirical foundation for developing more robust and effective autonomous agent systems in the future.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks</title>
<link>https://arxiv.org/abs/2508.11640</link>
<guid>https://arxiv.org/abs/2508.11640</guid>
<content:encoded><![CDATA[
arXiv:2508.11640v1 Announce Type: cross 
Abstract: The deployment of dense, low-cost sensors is critical for realizing ubiquitous smart environments. However, existing sensing solutions struggle with the energy, scalability, and reliability trade-offs imposed by battery maintenance, wireless transmission overhead, and data processing complexity. In this work, we present Vibe2Spike, a novel battery-free, wireless sensing framework that enables vibration-based activity recognition using visible light communication (VLC) and spiking neural networks (SNNs). Our system uses ultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and an LED, which harvest vibration energy and emit sparse visible light spikes without requiring batteries or RF radios. These optical spikes are captured by event cameras and classified using optimized SNN models evolved via the EONS framework. We evaluate Vibe2Spike across five device classes, achieving 94.9\% average classification fitness while analyzing the latency-accuracy trade-offs of different temporal binning strategies. Vibe2Spike demonstrates a scalable, and energy-efficient approach for enabling intelligent environments in a batteryless manner.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Construction of Logically Verifiable Neural Architectures</title>
<link>https://arxiv.org/abs/2508.11647</link>
<guid>https://arxiv.org/abs/2508.11647</guid>
<content:encoded><![CDATA[
arXiv:2508.11647v1 Announce Type: cross 
Abstract: Neural networks excel at pattern recognition but struggle with reliable logical reasoning, often violating basic logical principles during inference. We address this limitation by developing a categorical framework that systematically constructs neural architectures with provable logical guarantees. Our approach treats logical theories as algebraic structures called Lawvere theories, which we transform into neural networks using categorical algebra in the 2-category of parametric maps. Unlike existing methods that impose logical constraints during training, our categorical construction embeds logical principles directly into the network's architectural structure, making logical violations mathematically impossible. We demonstrate this framework by constructing differentiable neural architectures for propositional logic that preserve boolean reasoning while remaining trainable via gradient descent. Our main theoretical result establishes a bijective correspondence between finitary logical theories and neural architectures, proving that every logically constrained network arises uniquely from our construction. This extends Categorical Deep Learning beyond geometric symmetries to semantic constraints, enabling automatic derivation of verified architectures from logical specifications. The framework provides mathematical foundations for trustworthy AI systems, with applications to theorem proving, formal verification, and safety-critical reasoning tasks requiring verifiable logical behavior.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections</title>
<link>https://arxiv.org/abs/2508.11659</link>
<guid>https://arxiv.org/abs/2508.11659</guid>
<content:encoded><![CDATA[
arXiv:2508.11659v1 Announce Type: cross 
Abstract: Brain-like intelligent systems need brain-like learning methods. Equilibrium Propagation (EP) is a biologically plausible learning framework with strong potential for brain-inspired computing hardware. However, existing im-plementations of EP suffer from instability and prohibi-tively high computational costs. Inspired by the structure and dynamics of the brain, we propose a biologically plau-sible Feedback-regulated REsidual recurrent neural network (FRE-RNN) and study its learning performance in EP framework. Feedback regulation enables rapid convergence by reducing the spectral radius. The improvement in con-vergence property reduces the computational cost and train-ing time of EP by orders of magnitude, delivering perfor-mance on par with backpropagation (BP) in benchmark tasks. Meanwhile, residual connections with brain-inspired topologies help alleviate the vanishing gradient problem that arises when feedback pathways are weak in deep RNNs. Our approach substantially enhances the applicabil-ity and practicality of EP in large-scale networks that un-derpin artificial intelligence. The techniques developed here also offer guidance to implementing in-situ learning in physical neural networks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Training and Coaching: Redefining the Design Process of Learning Materials</title>
<link>https://arxiv.org/abs/2508.11662</link>
<guid>https://arxiv.org/abs/2508.11662</guid>
<content:encoded><![CDATA[
arXiv:2508.11662v1 Announce Type: cross 
Abstract: Generative artificial intelligence (GenAI) is transforming education, redefining the role of trainers and coaches in learning environments. In our study, we explore how AI integrates into the design process of learning materials, assessing its impact on efficiency, pedagogical quality, and the evolving role of human trainers and coaches. Through qualitative interviews with professionals in education and corporate training, we identify the following key topics: trainers and coaches increasingly act as facilitators and content moderators rather than primary creators, efficiency gains allow for a stronger strategic focus but at the same time the new tools require new skills. Additionally, we analyze how the anthropomorphism of AI shapes user trust and expectations. From these insights, we derive how tools based on GenAI can successfully be implemented for trainers and coaches on an individual, organizational, systemic, and strategic level.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Representation Stability for Transformer Models</title>
<link>https://arxiv.org/abs/2508.11667</link>
<guid>https://arxiv.org/abs/2508.11667</guid>
<content:encoded><![CDATA[
arXiv:2508.11667v1 Announce Type: cross 
Abstract: Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining. We introduce Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks. RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset</title>
<link>https://arxiv.org/abs/2508.11669</link>
<guid>https://arxiv.org/abs/2508.11669</guid>
<content:encoded><![CDATA[
arXiv:2508.11669v1 Announce Type: cross 
Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient management in critical care and perioperative settings, providing continuous assessment of cardiovascular hemodynamics with minimal risks. Numerous deep learning models have developed to reconstruct ABP waveform from noninvasively acquired physiological signals such as electrocardiogram and photoplethysmogram. However, limited research has addressed the issue of model performance and computational load for deployment on embedded systems. The study introduces a lightweight sInvResUNet, along with a collaborative learning scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a computational load of 0.02 GFLOPS, real-time ABP estimation was successfully achieved on embedded devices with an inference time of just 8.49 milliseconds for a 10-second output. We performed subject-independent validation in a large-scale and heterogeneous perioperative dataset containing 1,257,141 data segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and 31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better performance compared to large models, with a mean absolute error of 10.06 mmHg and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these promising results, all deep learning models showed significant performance variations across different demographic and cardiovascular conditions, highlighting their limited ability to generalize across such a broad and diverse population. This study lays a foundation work for real-time, unobtrusive ABP monitoring in real-world perioperative settings, providing baseline for future advancements in this area.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RRRA: Resampling and Reranking through a Retriever Adapter</title>
<link>https://arxiv.org/abs/2508.11670</link>
<guid>https://arxiv.org/abs/2508.11670</guid>
<content:encoded><![CDATA[
arXiv:2508.11670v1 Announce Type: cross 
Abstract: In dense retrieval, effective training hinges on selecting high quality hard negatives while avoiding false negatives. Recent methods apply heuristics based on positive document scores to identify hard negatives, improving both performance and interpretability. However, these global, example agnostic strategies often miss instance specific false negatives. To address this, we propose a learnable adapter module that monitors Bi-Encoder representations to estimate the likelihood that a hard negative is actually a false negative. This probability is modeled dynamically and contextually, enabling fine-grained, query specific judgments. The predicted scores are used in two downstream components: (1) resampling, where negatives are reweighted during training, and (2) reranking, where top-k retrieved documents are reordered at inference. Empirical results on standard benchmarks show that our adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines, underscoring the benefit of explicit false negative modeling in dense retrieval.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering</title>
<link>https://arxiv.org/abs/2508.11671</link>
<guid>https://arxiv.org/abs/2508.11671</guid>
<content:encoded><![CDATA[
arXiv:2508.11671v1 Announce Type: cross 
Abstract: The growing availability of music on streaming platforms has led to information overload for users. To address this issue and enhance the user experience, increasingly sophisticated recommendation systems have been proposed. This work investigates the use of Large Language Models (LLMs) from the Gemini and LLaMA families, combined with intelligent agents, in a multi-agent personalized music recommendation system. The results are compared with a traditional content-based recommendation model, considering user satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction rates of up to \textit{89{,}32\%}, indicating their promising potential in music recommendation systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Neurocognitive and Behavioral Patterns by Unsupervised Manifold Learning from Dynamic Brain Data</title>
<link>https://arxiv.org/abs/2508.11672</link>
<guid>https://arxiv.org/abs/2508.11672</guid>
<content:encoded><![CDATA[
arXiv:2508.11672v1 Announce Type: cross 
Abstract: Dynamic brain data, teeming with biological and functional insights, are becoming increasingly accessible through advanced measurements, providing a gateway to understanding the inner workings of the brain in living subjects. However, the vast size and intricate complexity of the data also pose a daunting challenge in reliably extracting meaningful information across various data sources. This paper introduces a generalizable unsupervised deep manifold learning for exploration of neurocognitive and behavioral patterns. Unlike existing methods that extract patterns directly from the input data as in the existing methods, the proposed Brain-dynamic Convolutional-Network-based Embedding (BCNE) seeks to capture the brain-state trajectories by deciphering the temporospatial correlations within the data and subsequently applying manifold learning to this correlative representation. The performance of BCNE is showcased through the analysis of several important dynamic brain datasets. The results, both visual and quantitative, reveal a diverse array of intriguing and interpretable patterns. BCNE effectively delineates scene transitions, underscores the involvement of different brain regions in memory and narrative processing, distinguishes various stages of dynamic learning processes, and identifies differences between active and passive behaviors. BCNE provides an effective tool for exploring general neuroscience inquiries or individual-specific patterns.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning</title>
<link>https://arxiv.org/abs/2508.11673</link>
<guid>https://arxiv.org/abs/2508.11673</guid>
<content:encoded><![CDATA[
arXiv:2508.11673v1 Announce Type: cross 
Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for handling diverse tasks and modalities in the biomedical domain, as training separate models for each modality or task significantly increases inference costs. Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities? To address these challenges, we propose MSLoRA-CR, a method that fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task. Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA). Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency. Our code is publicly available at https://github.com/VentusAislant/MSLoRA_CR.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance</title>
<link>https://arxiv.org/abs/2508.11674</link>
<guid>https://arxiv.org/abs/2508.11674</guid>
<content:encoded><![CDATA[
arXiv:2508.11674v1 Announce Type: cross 
Abstract: This study introduces a novel approach by replacing the traditional perceptron neuron model with a biologically inspired probabilistic meta neuron, where the internal neuron parameters are jointly learned, leading to improved classification accuracy of spiking neural networks (SNNs). To validate this innovation, we implement and compare two SNN architectures: one based on standard leaky integrate-and-fire (LIF) neurons and another utilizing the proposed probabilistic meta neuron model. As a second key contribution, we present a new biologically inspired classification framework that uniquely integrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to entropy rate. By combining the temporal precision and biological plausibility of SNNs with the capacity of LZC to capture structural regularity, the proposed approach enables efficient and interpretable classification of spatiotemporal neural data, an aspect not addressed in existing works. We consider learning algorithms such as backpropagation, spike-timing-dependent plasticity (STDP), and the Tempotron learning rule. To explore neural dynamics, we use Poisson processes to model neuronal spike trains, a well-established method for simulating the stochastic firing behavior of biological neurons. Our results reveal that depending on the training method, the classifier's efficiency can improve by up to 11.00%, highlighting the advantage of learning additional neuron parameters beyond the traditional focus on weighted inputs alone.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Language Geometry: Constructing a Metric Space from LLM Weights</title>
<link>https://arxiv.org/abs/2508.11676</link>
<guid>https://arxiv.org/abs/2508.11676</guid>
<content:encoded><![CDATA[
arXiv:2508.11676v1 Announce Type: cross 
Abstract: We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems</title>
<link>https://arxiv.org/abs/2508.11679</link>
<guid>https://arxiv.org/abs/2508.11679</guid>
<content:encoded><![CDATA[
arXiv:2508.11679v1 Announce Type: cross 
Abstract: Deep learning has been extensively explored to solve vehicle routing problems (VRPs), which yields a range of data-driven neural solvers with promising outcomes. However, most neural solvers are trained to tackle VRP instances in a relatively monotonous context, e.g., simplifying VRPs by using Euclidean distance between nodes and adhering to a single problem size, which harms their off-the-shelf application in different scenarios. To enhance their versatility, this paper presents a novel lifelong learning framework that incrementally trains a neural solver to manage VRPs in distinct contexts. Specifically, we propose a lifelong learner (LL), exploiting a Transformer network as the backbone, to solve a series of VRPs. The inter-context self-attention mechanism is proposed within LL to transfer the knowledge obtained from solving preceding VRPs into the succeeding ones. On top of that, we develop a dynamic context scheduler (DCS), employing the cross-context experience replay to further facilitate LL looking back on the attained policies of solving preceding VRPs. Extensive results on synthetic and benchmark instances (problem sizes up to 18k) show that our LL is capable of discovering effective policies for tackling generic VRPs in varying contexts, which outperforms other neural solvers and achieves the best performance for most VRPs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics</title>
<link>https://arxiv.org/abs/2508.11680</link>
<guid>https://arxiv.org/abs/2508.11680</guid>
<content:encoded><![CDATA[
arXiv:2508.11680v1 Announce Type: cross 
Abstract: Demographic shifts, influenced by globalization, economic conditions, geopolitical events, and environmental factors, pose significant challenges for policymakers and researchers. Accurate demographic forecasting is essential for informed decision-making in areas such as urban planning, healthcare, and economic policy. This study explores the application of time series foundation models to predict demographic changes in the United States using datasets from the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate the performance of the Time Series Foundation Model (TimesFM) against traditional baselines including Long Short-Term Memory (LSTM) networks, Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our experiments across six demographically diverse states demonstrate that TimesFM achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with particularly strong performance on minority populations with sparse historical data. These findings highlight the potential of pre-trained foundation models to enhance demographic analysis and inform proactive policy interventions without requiring extensive task-specific fine-tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Future progress in artificial intelligence: A survey of expert opinion</title>
<link>https://arxiv.org/abs/2508.11681</link>
<guid>https://arxiv.org/abs/2508.11681</guid>
<content:encoded><![CDATA[
arXiv:2508.11681v1 Announce Type: cross 
Abstract: There is, in some quarters, concern about high-level machine intelligence and superintelligent AI coming up in a few decades, bringing with it significant risks for humanity. In other quarters, these issues are ignored or considered science fiction. We wanted to clarify what the distribution of opinions actually is, what probability the best experts currently assign to high-level machine intelligence coming up within a particular time-frame, which risks they see with that development, and how fast they see these developing. We thus designed a brief questionnaire and distributed it to four groups of experts in 2012/2013. The median estimate of respondents was for a one in two chance that high-level machine intelligence will be developed around 2040-2050, rising to a nine in ten chance by 2075. Experts expect that systems will move on to superintelligence in less than 30 years thereafter. They estimate the chance is about one in three that this development turns out to be 'bad' or 'extremely bad' for humanity.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study</title>
<link>https://arxiv.org/abs/2508.11682</link>
<guid>https://arxiv.org/abs/2508.11682</guid>
<content:encoded><![CDATA[
arXiv:2508.11682v1 Announce Type: cross 
Abstract: Non-invasive glucose monitoring remains a critical challenge in the management of diabetes. HRV during sleep shows promise for glucose prediction however, age-related autonomic changes significantly confound traditional HRV analyses. We analyzed 43 subjects with multi-modal data including sleep-stage specific ECG, HRV features, and clinical measurements. A novel age-normalization technique was applied to the HRV features by, dividing the raw values by age-scaled factors. BayesianRidge regression with 5-fold cross-validation was employed for log-glucose prediction. Age-normalized HRV features achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction, representing a 25.6% improvement over non-normalized features (R2 = 0.132). The top predictive features were hrv rem mean rr age normalized (r = 0.443, p = 0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic blood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed age-normalization as the critical component, with sleep-stage specific features providing additional predictive value. Age-normalized HRV features significantly enhance glucose prediction accuracy compared with traditional approaches. This sleep-aware methodology addresses fundamental limitations in autonomic function assessment and suggests a preliminary feasibility for non-invasive glucose monitoring applications. However, these results require validation in larger cohorts before clinical consideration.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems</title>
<link>https://arxiv.org/abs/2508.11689</link>
<guid>https://arxiv.org/abs/2508.11689</guid>
<content:encoded><![CDATA[
arXiv:2508.11689v1 Announce Type: cross 
Abstract: This paper presents ASPEN, a novel energy-aware technique for neuromorphic systems that could unleash the future of intelligent, always-on, ultra-low-power, and low-burden wearables. Our main research objectives are to explore the feasibility of neuromorphic computing for wearables, identify open research directions, and demonstrate the feasibility of developing an adaptive spiking technique for energy-aware computation, which can be game-changing for resource-constrained devices in always-on applications. As neuromorphic computing systems operate based on spike events, their energy consumption is closely related to spiking activity, i.e., each spike incurs computational and power costs; consequently, minimizing the number of spikes is a critical strategy for operating under constrained energy budgets. To support this goal, ASPEN utilizes stochastic perturbations to the neuronal threshold during training to not only enhance the network's robustness across varying thresholds, which can be controlled at inference time, but also act as a regularizer that improves generalization, reduces spiking activity, and enables energy control without the need for complex retraining or pruning. More specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a lightweight and scalable technique for dynamic energy control without reconfiguring the entire model. Our evaluation on neuromorphic emulator and hardware shows that ASPEN significantly reduces spike counts and energy consumption while maintaining accuracy comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real Time Child Abduction And Detection System</title>
<link>https://arxiv.org/abs/2508.11690</link>
<guid>https://arxiv.org/abs/2508.11690</guid>
<content:encoded><![CDATA[
arXiv:2508.11690v1 Announce Type: cross 
Abstract: Child safety continues to be a paramount concern worldwide, with child abduction posing significant threats to communities. This paper presents the development of an edge-based child abduction detection and alert system utilizing a multi-agent framework where each agent incorporates Vision-Language Models (VLMs) deployed on a Raspberry Pi. Leveraging the advanced capabilities of VLMs within individual agents of a multi-agent team, our system is trained to accurately detect and interpret complex interactions involving children in various environments in real-time. The multi-agent system is deployed on a Raspberry Pi connected to a webcam, forming an edge device capable of processing video feeds, thereby reducing latency and enhancing privacy. An integrated alert system utilizes the Twilio API to send immediate SMS and WhatsApp notifications, including calls and messages, when a potential child abduction event is detected. Experimental results demonstrate that the system achieves high accuracy in detecting potential abduction scenarios, with near real-time performance suitable for practical deployment. The multi-agent architecture enhances the system's ability to process complex situational data, improving detection capabilities over traditional single-model approaches. The edge deployment ensures scalability and cost-effectiveness, making it accessible for widespread use. The proposed system offers a proactive solution to enhance child safety through continuous monitoring and rapid alerting, contributing a valuable tool in efforts to prevent child abductions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception</title>
<link>https://arxiv.org/abs/2508.11691</link>
<guid>https://arxiv.org/abs/2508.11691</guid>
<content:encoded><![CDATA[
arXiv:2508.11691v1 Announce Type: cross 
Abstract: EEG-based analysis of pain perception, enhanced by machine learning, reveals how the brain encodes pain by identifying neural patterns evoked by noxious stimulation. However, a major challenge that remains is the generalization of machine learning models across individuals, given the high cross-participant variability inherent to EEG signals and the limited focus on direct pain perception identification in current research. In this study, we systematically evaluate the performance of cross-participant generalization of a wide range of models, including traditional classifiers and deep neural classifiers for identifying the sensory modality of thermal pain and aversive auditory stimulation from EEG recordings. Using a novel dataset of EEG recordings from 108 participants, we benchmark model performance under both within- and cross-participant evaluation settings. Our findings show that traditional models suffered the largest drop from within- to cross-participant performance, while deep learning models proved more resilient, underscoring their potential for subject-invariant EEG decoding. Even though performance variability remained high, the strong results of the graph-based model highlight its potential to capture subject-invariant structure in EEG signals. On the other hand, we also share the preprocessed dataset used in this study, providing a standardized benchmark for evaluating future algorithms under the same generalization constraints.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning</title>
<link>https://arxiv.org/abs/2508.11692</link>
<guid>https://arxiv.org/abs/2508.11692</guid>
<content:encoded><![CDATA[
arXiv:2508.11692v1 Announce Type: cross 
Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches train routes by diverting tracks through a switchblade. As with any critical safety equipment, a failure will halt operations leading to service disruptions; therefore, pre-emptive maintenance may avoid unnecessary interruptions by detecting anomalies before they become failures. Previous work relies on several inputs and crafting custom features by segmenting the signal. This not only adds additional requirements for data collection and processing, but it is also specific to the PM technology, the installed locations and operational conditions limiting scalability. Based on the available maintenance records, the main failure causes for PM are obstacles, friction, power source issues and misalignment. Those failures affect the energy consumption pattern of PMs, altering the usual (or healthy) shape of the power signal during the PM movement. In contrast to the current state-of-the-art, our method requires only one input. We apply a deep learning model to the power signal pattern to classify if the PM is nominal or associated with any failure type, achieving >99.99\% precision, <0.01\% false positives and negligible false negatives. Our methodology is generic and technology-agnostic, proven to be scalable on several electromechanical PM types deployed in both real-world and test bench environments. Finally, by using conformal prediction the maintainer gets a clear indication of the certainty of the system outputs, adding a confidence layer to operations and making the method compliant with the ISO-17359 standard.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data</title>
<link>https://arxiv.org/abs/2508.11693</link>
<guid>https://arxiv.org/abs/2508.11693</guid>
<content:encoded><![CDATA[
arXiv:2508.11693v1 Announce Type: cross 
Abstract: Track Circuits (TC) are the main signalling devices used to detect the presence of a train on a rail track. It has been used since the 19th century and nowadays there are many types depending on the technology. As a general classification, Track Circuits can be divided into 2 main groups, DC (Direct Current) and AC (Alternating Current) circuits. This work is focused on a particular AC track circuit, called "Smart Train Detection System" (STDS), designed with both high and low-frequency bands. This approach uses STDS current data applied to an SVM (support vector machine) classifier as a type of failure identifier. The main purpose of this work consists on determine automatically which is the component of the track that is failing to improve the maintenance action. Model was trained to classify 15 different failures that belong to 3 more general categories. The method was tested with field data from 10 different track circuits and validated by the STDS track circuit expert and maintainers. All use cases were correctly classified by the method.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefAdGen: High-Fidelity Advertising Image Generation</title>
<link>https://arxiv.org/abs/2508.11695</link>
<guid>https://arxiv.org/abs/2508.11695</guid>
<content:encoded><![CDATA[
arXiv:2508.11695v1 Announce Type: cross 
Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Separating Knowledge and Perception with Procedural Data</title>
<link>https://arxiv.org/abs/2508.11697</link>
<guid>https://arxiv.org/abs/2508.11697</guid>
<content:encoded><![CDATA[
arXiv:2508.11697v1 Announce Type: cross 
Abstract: We train representation models with procedural data only, and apply them on visual similarity, classification, and semantic segmentation tasks without further training by using visual memory -- an explicit database of reference image embeddings. Unlike prior work on visual memory, our approach achieves full compartmentalization with respect to all real-world images while retaining strong performance. Compared to a model trained on Places, our procedural model performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and $15\%$ on CUB200 and Flowers102 fine-grained classification, and is within $10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on real data. Finally, we analyze procedural versus real data models, showing that parts of the same object have dissimilar representations in procedural models, resulting in incorrect searches in memory and explaining the remaining performance gap.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Gen Education: Enhancing AI for Microlearning</title>
<link>https://arxiv.org/abs/2508.11704</link>
<guid>https://arxiv.org/abs/2508.11704</guid>
<content:encoded><![CDATA[
arXiv:2508.11704v1 Announce Type: cross 
Abstract: This paper explores integrating microlearning strategies into university curricula, particularly in computer science education, to counteract the decline in class attendance and engagement in US universities after COVID. As students increasingly opt for remote learning and recorded lectures, traditional educational approaches struggle to maintain engagement and effectiveness. Microlearning, which breaks complex subjects into manageable units, is proposed to address shorter attention spans and enhance educational outcomes. It uses interactive formats such as videos, quizzes, flashcards, and scenario-based exercises, which are especially beneficial for topics like algorithms and programming logic requiring deep understanding and ongoing practice. Adoption of microlearning is often limited by the effort needed to create such materials. This paper proposes leveraging AI tools, specifically ChatGPT, to reduce the workload for educators by automating the creation of supplementary materials. While AI can automate certain tasks, educators remain essential in guiding and shaping the learning process. This AI-enhanced approach ensures course content is kept current with the latest research and technology, with educators providing context and insights. By examining AI capabilities in microlearning, this study shows the potential to transform educational practices and outcomes in computer science, offering a practical model for combining advanced technology with established teaching methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.11706</link>
<guid>https://arxiv.org/abs/2508.11706</guid>
<content:encoded><![CDATA[
arXiv:2508.11706v1 Announce Type: cross 
Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has gained significant attention in multi-agent reinforcement learning (MARL) and is the foundation of many recent algorithms. However, decentralized policies operate under partial observability and often yield suboptimal performance compared to centralized policies, while fully centralized approaches typically face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized training and execution framework that employs a fully centralized policy to overcome these limitations. Our approach leverages a novel permutation equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks, that is lightweight, scalable, and easy to implement. Experiments show that CPE integrates seamlessly with both value decomposition and actor-critic methods, substantially improving the performance of standard CTDE algorithms across cooperative benchmarks including MPE, SMAC, and RWARE, and matching the performance of state-of-the-art RWARE implementations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listening with Language Models: Using LLMs to Collect and Interpret Classroom Feedback</title>
<link>https://arxiv.org/abs/2508.11707</link>
<guid>https://arxiv.org/abs/2508.11707</guid>
<content:encoded><![CDATA[
arXiv:2508.11707v1 Announce Type: cross 
Abstract: Traditional end-of-quarter surveys often fail to provide instructors with timely, detailed, and actionable feedback about their teaching. In this paper, we explore how Large Language Model (LLM)-powered chatbots can reimagine the classroom feedback process by engaging students in reflective, conversational dialogues. Through the design and deployment of a three-part system-PromptDesigner, FeedbackCollector, and FeedbackAnalyzer-we conducted a pilot study across two graduate courses at UC Santa Cruz. Our findings suggest that LLM-based feedback systems offer richer insights, greater contextual relevance, and higher engagement compared to standard survey tools. Instructors valued the system's adaptability, specificity, and ability to support mid-course adjustments, while students appreciated the conversational format and opportunity for elaboration. We conclude by discussing the design implications of using AI to facilitate more meaningful and responsive feedback in higher education.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity</title>
<link>https://arxiv.org/abs/2508.11708</link>
<guid>https://arxiv.org/abs/2508.11708</guid>
<content:encoded><![CDATA[
arXiv:2508.11708v1 Announce Type: cross 
Abstract: Urban centers undergo social, demographic, and cultural changes that shape public street use and require systematic evaluation of public spaces. This study presents Street Review, a mixed-methods approach that combines participatory research with AI-based analysis to assess streetscape inclusivity. In Montr\'eal, Canada, 28 residents participated in semi-directed interviews and image evaluations, supported by the analysis of approximately 45,000 street-view images from Mapillary. The approach produced visual analytics, such as heatmaps, to correlate subjective user ratings with physical attributes like sidewalk, maintenance, greenery, and seating. Findings reveal variations in perceptions of inclusivity and accessibility across demographic groups, demonstrating that incorporating diverse user feedback can enhance machine learning models through careful data-labeling and co-production strategies. The Street Review framework offers a systematic method for urban planners and policy analysts to inform planning, policy development, and management of public streets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI</title>
<link>https://arxiv.org/abs/2508.11709</link>
<guid>https://arxiv.org/abs/2508.11709</guid>
<content:encoded><![CDATA[
arXiv:2508.11709v1 Announce Type: cross 
Abstract: The rapid integration of Generative Artificial Intelligence (GenAI) into higher education presents both opportunities and challenges for assessment design, particularly within Project-Based Assessment (PBA) contexts. Traditional assessment methods often emphasise the final product in the PBA, which can now be significantly influenced or created by GenAI tools, raising concerns regarding product authenticity, academic integrity, and learning validation. This paper advocates for a reimagined assessment model for Project-Based Learning (PBL) or a capstone project that prioritises process-oriented evaluation, multi-modal and multifaceted assessment design, and ethical engagement with GenAI to enable higher-order thinking. The model also emphasises the use of (GenAI-assisted) personalised feedback by a supervisor as an observance of the learning process during the project lifecycle. A use case scenario is provided to illustrate the application of the model in a capstone project setting. The paper concludes with recommendations for educators and curriculum designers to ensure that assessment practices remain robust, learner-centric, and integrity-driven in the evolving landscape of GenAI.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Vulnerability Detection Across Different Programming Languages with AI Models</title>
<link>https://arxiv.org/abs/2508.11710</link>
<guid>https://arxiv.org/abs/2508.11710</guid>
<content:encoded><![CDATA[
arXiv:2508.11710v1 Announce Type: cross 
Abstract: Security vulnerabilities present in a code that has been written in diverse programming languages are among the most critical yet complicated aspects of source code to detect. Static analysis tools based on rule-based patterns usually do not work well at detecting the context-dependent bugs and lead to high false positive rates. Recent developments in artificial intelligence, specifically the use of transformer-based models like CodeBERT and CodeLlama, provide light to this problem, as they show potential in finding such flaws better. This paper presents the implementations of these models on various datasets of code vulnerability, showing how off-the-shelf models can successfully produce predictive capacity in models through dynamic fine-tuning of the models on vulnerable and safe code fragments. The methodology comprises the gathering of the dataset, normalization of the language, fine-tuning of the model, and incorporation of ensemble learning and explainable AI. Experiments show that a well-trained CodeBERT can be as good as or even better than some existing static analyzers in terms of accuracy greater than 97%. Further study has indicated that although language models can achieve close-to-perfect recall, the precision can decrease. A solution to this is given by hybrid models and validation procedures, which will reduce false positives. According to the results, the AI-based solutions generalize to different programming languages and classes of vulnerability. Nevertheless, robustness, interpretability, and deployment readiness are still being developed. The results illustrate the probabilities that AI will enhance the trustworthiness in the usability and scalability of machine-learning-based detectors of vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2508.11711</link>
<guid>https://arxiv.org/abs/2508.11711</guid>
<content:encoded><![CDATA[
arXiv:2508.11711v1 Announce Type: cross 
Abstract: GraphQL's flexibility, while beneficial for efficient data fetching, introduces unique security vulnerabilities that traditional API security mechanisms often fail to address. Malicious GraphQL queries can exploit the language's dynamic nature, leading to denial-of-service attacks, data exfiltration through injection, and other exploits. Existing solutions, such as static analysis, rate limiting, and general-purpose Web Application Firewalls, offer limited protection against sophisticated, context-aware attacks. This paper presents a novel, AI-driven approach for real-time detection of malicious GraphQL queries. Our method combines static analysis with machine learning techniques, including Large Language Models (LLMs) for dynamic schema-based configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual embedding of query payloads, and Convolutional Neural Networks (CNNs), Random Forests, and Multilayer Perceptrons for classification. We detail the system architecture, implementation strategies optimized for production environments (including ONNX Runtime optimization and parallel processing), and evaluate the performance of our detection models and the overall system under load. Results demonstrate high accuracy in detecting various threats, including SQL injection, OS command injection, and XSS exploits, alongside effective mitigation of DoS and SSRF attempts. This research contributes a robust and adaptable solution for enhancing GraphQL API security.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs</title>
<link>https://arxiv.org/abs/2508.11715</link>
<guid>https://arxiv.org/abs/2508.11715</guid>
<content:encoded><![CDATA[
arXiv:2508.11715v1 Announce Type: cross 
Abstract: Excel is a pervasive yet often complex tool, particularly for novice users, where runtime errors arising from logical mistakes or misinterpretations of functions pose a significant challenge. While large language models (LLMs) offer promising assistance by explaining formula errors, the automated correction of these semantic runtime errors remains an open problem. A primary challenge to advancing models for such scenarios is the severe lack of high-quality, comprehensive datasets for training and rigorous evaluation. This paper addresses this gap by introducing a novel approach for constructing a benchmark dataset specifically designed for Excel formula repair. We propose a data generation pipeline, which leverages a small set of curated seed samples from online forums to synthetically expand the dataset. Our pipeline integrates few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge} validation framework, combined with execution-based checks to ensure the correctness and semantic fidelity of the generated data. This process produced a benchmark dataset of 618 high-quality samples, covering common runtime errors. Furthermore, we propose a context-aware baseline technique for Excel formula repair that utilizes LLMs to leverage both the faulty formula, and relevant spreadsheet context. We evaluate the performance of various LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using execution-based metrics. Our analysis demonstrates the dataset's quality through manual annotation and provides insights into error and function distributions. The proposed generation methodology is highly scalable and can be readily adapted to create evaluation benchmarks for similar code repair tasks in other low-resource programming languages.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)</title>
<link>https://arxiv.org/abs/2508.11716</link>
<guid>https://arxiv.org/abs/2508.11716</guid>
<content:encoded><![CDATA[
arXiv:2508.11716v1 Announce Type: cross 
Abstract: Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are AI Machines Making Humans Obsolete?</title>
<link>https://arxiv.org/abs/2508.11719</link>
<guid>https://arxiv.org/abs/2508.11719</guid>
<content:encoded><![CDATA[
arXiv:2508.11719v1 Announce Type: cross 
Abstract: This chapter starts with a sketch of how we got to "generative AI" (GenAI) and a brief summary of the various impacts it had so far. It then discusses some of the opportunities of GenAI, followed by the challenges and dangers, including dystopian outcomes resulting from using uncontrolled machine learning and our failures to understand the results. It concludes with some suggestions for how to control GenAI and address its dangers.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis</title>
<link>https://arxiv.org/abs/2508.11721</link>
<guid>https://arxiv.org/abs/2508.11721</guid>
<content:encoded><![CDATA[
arXiv:2508.11721v1 Announce Type: cross 
Abstract: Foundation models (FMs) have shown great promise in medical image analysis by improving generalization across diverse downstream tasks. In ophthalmology, several FMs have recently emerged, but there is still no clear answer to fundamental questions: Which FM performs the best? Are they equally good across different tasks? What if we combine all FMs together? To our knowledge, this is the first study to systematically evaluate both single and fused ophthalmic FMs. To address these questions, we propose FusionFM, a comprehensive evaluation suite, along with two fusion approaches to integrate different ophthalmic FMs. Our framework covers both ophthalmic disease detection (glaucoma, diabetic retinopathy, and age-related macular degeneration) and systemic disease prediction (diabetes and hypertension) based on retinal imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM, RetiZero, and DINORET) using standardized datasets from multiple countries and evaluated their performance using AUC and F1 metrics. Our results show that DINORET and RetiZero achieve superior performance in both ophthalmic and systemic disease tasks, with RetiZero exhibiting stronger generalization on external datasets. Regarding fusion strategies, the Gating-based approach provides modest improvements in predicting glaucoma, AMD, and hypertension. Despite these advances, predicting systemic diseases, especially hypertension in external cohort remains challenging. These findings provide an evidence-based evaluation of ophthalmic FMs, highlight the benefits of model fusion, and point to strategies for enhancing their clinical applicability.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction</title>
<link>https://arxiv.org/abs/2508.11728</link>
<guid>https://arxiv.org/abs/2508.11728</guid>
<content:encoded><![CDATA[
arXiv:2508.11728v1 Announce Type: cross 
Abstract: Dentocraniofacial hard tissue defects profoundly affect patients' physiological functions, facial aesthetics, and psychological well-being, posing significant challenges for precise reconstruction. Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. Here we introduce UniDCF, a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness, UniDCF overcomes the limitations of prior single-modality approaches. We curated the largest multimodal dataset, comprising intraoral scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated instances. Evaluations demonstrate that UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Stories We Govern By: AI, Risk, and the Power of Imaginaries</title>
<link>https://arxiv.org/abs/2508.11729</link>
<guid>https://arxiv.org/abs/2508.11729</guid>
<content:encoded><![CDATA[
arXiv:2508.11729v1 Announce Type: cross 
Abstract: This paper examines how competing sociotechnical imaginaries of artificial intelligence (AI) risk shape governance decisions and regulatory constraints. Drawing on concepts from science and technology studies, we analyse three dominant narrative groups: existential risk proponents, who emphasise catastrophic AGI scenarios; accelerationists, who portray AI as a transformative force to be unleashed; and critical AI scholars, who foreground present-day harms rooted in systemic inequality. Through an analysis of representative manifesto-style texts, we explore how these imaginaries differ across four dimensions: normative visions of the future, diagnoses of the present social order, views on science and technology, and perceived human agency in managing AI risks. Our findings reveal how these narratives embed distinct assumptions about risk and have the potential to progress into policy-making processes by narrowing the space for alternative governance approaches. We argue against speculative dogmatism and for moving beyond deterministic imaginaries toward regulatory strategies that are grounded in pragmatism.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification</title>
<link>https://arxiv.org/abs/2508.11732</link>
<guid>https://arxiv.org/abs/2508.11732</guid>
<content:encoded><![CDATA[
arXiv:2508.11732v1 Announce Type: cross 
Abstract: Existing deep learning models for functional MRI-based classification have limitations in network architecture determination (relying on experience) and feature space fusion (mostly simple concatenation, lacking mutual learning). Inspired by the human brain's mechanism of updating neural connections through learning and decision-making, we proposed a novel BRain-Inspired feature Fusion (BRIEF) framework, which is able to optimize network architecture automatically by incorporating an improved neural network connection search (NCS) strategy and a Transformer-based multi-feature fusion module. Specifically, we first extracted 4 types of fMRI temporal representations, i.e., time series (TCs), static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion entropy (MsDE), to construct four encoders. Within each encoder, we employed a modified Q-learning to dynamically optimize the NCS to extract high-level feature vectors, where the NCS is formulated as a Markov Decision Process. Then, all feature vectors were fused via a Transformer, leveraging both stable/time-varying connections and multi-scale dependencies across different brain regions to achieve the final classification. Additionally, an attention module was embedded to improve interpretability. The classification performance of our proposed BRIEF was compared with 21 state-of-the-art models by discriminating two mental disorders from healthy controls: schizophrenia (SZ, n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is the first attempt to incorporate a brain-inspired, reinforcement learning strategy to optimize fMRI-based mental disorder classification, showing significant potential for identifying precise neuroimaging biomarkers.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication</title>
<link>https://arxiv.org/abs/2508.11733</link>
<guid>https://arxiv.org/abs/2508.11733</guid>
<content:encoded><![CDATA[
arXiv:2508.11733v1 Announce Type: cross 
Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but often suffer from redundant communication and excessive token overhead. Existing methods typically enhance efficiency through pretrained GNNs or greedy algorithms, but often isolate pre- and post-task optimization, lacking a unified strategy. To this end, we present SafeSieve, a progressive and adaptive multi-agent pruning algorithm that dynamically refines the inter-agent communication through a novel dual-mechanism. SafeSieve integrates initial LLM-based semantic evaluation with accumulated performance feedback, enabling a smooth transition from heuristic initialization to experience-driven refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs 0-extension clustering to preserve structurally coherent agent groups while eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval, etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt injection attacks (1.23% average accuracy drop). In heterogeneous settings, SafeSieve reduces deployment costs by 13.3% while maintaining performance. These results establish SafeSieve as a robust, efficient, and scalable framework for practical multi-agent systems. Our code can be found in https://anonymous.4open.science/r/SafeSieve-D8F2FFUN.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ovis2.5 Technical Report</title>
<link>https://arxiv.org/abs/2508.11737</link>
<guid>https://arxiv.org/abs/2508.11737</guid>
<content:encoded><![CDATA[
arXiv:2508.11737v1 Announce Type: cross 
Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional "thinking mode" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the "small model, big performance" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation</title>
<link>https://arxiv.org/abs/2508.11738</link>
<guid>https://arxiv.org/abs/2508.11738</guid>
<content:encoded><![CDATA[
arXiv:2508.11738v1 Announce Type: cross 
Abstract: Rural healthcare faces persistent challenges, including inadequate infrastructure, workforce shortages, and socioeconomic disparities that hinder access to essential services. This study investigates the transformative potential of artificial intelligence (AI) in addressing these issues in underserved rural areas. We systematically reviewed 109 studies published between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and Scopus. Articles were screened using PRISMA guidelines and Covidence software. A thematic analysis was conducted to identify key patterns and insights regarding AI implementation in rural healthcare delivery. The findings reveal significant promise for AI applications, such as predictive analytics, telemedicine platforms, and automated diagnostic tools, in improving healthcare accessibility, quality, and efficiency. Among these, advanced AI systems, including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs), offer particularly transformative potential. MFMs integrate diverse data sources, such as imaging, clinical records, and bio signals, to support comprehensive decision-making, while LLMs facilitate clinical documentation, patient triage, translation, and virtual assistance. Together, these technologies can revolutionize rural healthcare by augmenting human capacity, reducing diagnostic delays, and democratizing access to expertise. However, barriers remain, including infrastructural limitations, data quality concerns, and ethical considerations. Addressing these challenges requires interdisciplinary collaboration, investment in digital infrastructure, and the development of regulatory frameworks. This review offers actionable recommendations and highlights areas for future research to ensure equitable and sustainable integration of AI in rural healthcare systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we Evaluate RAGs with Synthetic Data?</title>
<link>https://arxiv.org/abs/2508.11758</link>
<guid>https://arxiv.org/abs/2508.11758</guid>
<content:encoded><![CDATA[
arXiv:2508.11758v1 Announce Type: cross 
Abstract: We investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable. We assess the reliability of synthetic benchmarks across two experiments: one varying retriever parameters while keeping the generator fixed, and another varying the generator with fixed retriever parameters. Across four datasets, of which two open-domain and two proprietary, we find that synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Natural Language for Human-Robot Collaboration in the Real World</title>
<link>https://arxiv.org/abs/2508.11759</link>
<guid>https://arxiv.org/abs/2508.11759</guid>
<content:encoded><![CDATA[
arXiv:2508.11759v1 Announce Type: cross 
Abstract: We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes</title>
<link>https://arxiv.org/abs/2508.11800</link>
<guid>https://arxiv.org/abs/2508.11800</guid>
<content:encoded><![CDATA[
arXiv:2508.11800v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the accuracy of language models in verifiable and deterministic domains like mathematics. Here, we examine if current RL methods are also effective at optimizing language models in verifiable domains with stochastic outcomes, like scientific experiments. Through applications to synthetic data and real-world biological experiments, we demonstrate that Group Relative Policy Optimization (GRPO) induces overconfident probability predictions for binary stochastic outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out (RLOO) yield well-calibrated models. We show that removing group standard normalization in GRPO fixes its miscalibration and provide a theoretical explanation for why normalization causes overconfidence. Our results provide new evidence against the use of standard normalization in GRPO and help pave the way for applications of RL for reasoning language models beyond deterministic domains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labels or Input? Rethinking Augmentation in Multimodal Hate Detection</title>
<link>https://arxiv.org/abs/2508.11808</link>
<guid>https://arxiv.org/abs/2508.11808</guid>
<content:encoded><![CDATA[
arXiv:2508.11808v1 Announce Type: cross 
Abstract: The modern web is saturated with multimodal content, intensifying the challenge of detecting hateful memes, where harmful intent is often conveyed through subtle interactions between text and image under the guise of humor or satire. While recent advances in Vision-Language Models (VLMs) show promise, these models lack support for fine-grained supervision and remain susceptible to implicit hate speech. In this paper, we present a dual-pronged approach to improve multimodal hate detection. First, we propose a prompt optimization framework that systematically varies prompt structure, supervision granularity, and training modality. We show that prompt design and label scaling both influence performance, with structured prompts improving robustness even in small models, and InternVL2 achieving the best F1-scores across binary and scaled settings. Second, we introduce a multimodal data augmentation pipeline that generates 2,479 counterfactually neutral memes by isolating and rewriting the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup, successfully reduces spurious correlations and improves classifier generalization. Our approaches inspire new directions for building synthetic data to train robust and fair vision-language models. Our findings demonstrate that prompt structure and data composition are as critical as model size, and that targeted augmentation can support more trustworthy and context-sensitive hate detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation</title>
<link>https://arxiv.org/abs/2508.11810</link>
<guid>https://arxiv.org/abs/2508.11810</guid>
<content:encoded><![CDATA[
arXiv:2508.11810v1 Announce Type: cross 
Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce settings, especially for tabular datasets widely used in real-world applications. A key challenge is improving counterfactual and causal fairness, while preserving high utility. We present FairTabGen, a fairness-aware large language model-based framework for tabular synthetic data generation. We integrate multiple fairness definitions including counterfactual and causal fairness into both its generation and evaluation pipelines. We use in-context learning, prompt refinement, and fairness-aware data curation to balance fairness and utility. Across diverse datasets, our method outperforms state-of-the-art GAN-based and LLM-based methods, achieving up to 10% improvements on fairness metrics such as demographic parity and path-specific causal effects while retaining statistical utility. Remarkably, it achieves these gains using less than 20% of the original data, highlighting its efficiency in low-data regimes. These results demonstrate a principled and practical approach for generating fair and useful synthetic tabular data.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering</title>
<link>https://arxiv.org/abs/2508.11824</link>
<guid>https://arxiv.org/abs/2508.11824</guid>
<content:encoded><![CDATA[
arXiv:2508.11824v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into software engineering has revolutionized code generation, enabling unprecedented productivity through promptware and autonomous AI agents. However, this transformation introduces significant risks, including insecure code generation, hallucinated outputs, irreversible actions, and a lack of transparency and accountability. Incidents like the Replit database deletion underscore the urgent need for robust safety and governance mechanisms. This paper comprehensively analyzes the inherent challenges of LLM-assisted code generation, such as vulnerability inheritance, overtrust, misinterpretation, and the absence of standardized validation and rollback protocols. To address these, we propose the SAFE-AI Framework, a holistic approach emphasizing Safety, Auditability, Feedback, and Explainability. The framework integrates guardrails, sandboxing, runtime verification, risk-aware logging, human-in-the-loop systems, and explainable AI techniques to mitigate risks while fostering trust and compliance. We introduce a novel taxonomy of AI behaviors categorizing suggestive, generative, autonomous, and destructive actions to guide risk assessment and oversight. Additionally, we identify open problems, including the lack of standardized benchmarks for code specific hallucinations and autonomy levels, and propose future research directions for hybrid verification, semantic guardrails, and proactive governance tools. Through detailed comparisons of autonomy control, prompt engineering, explainability, and governance frameworks, this paper provides a roadmap for responsible AI integration in software engineering, aligning with emerging regulations like the EU AI Act and Canada's AIDA to ensure safe, transparent, and accountable AI-driven development.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions</title>
<link>https://arxiv.org/abs/2508.11829</link>
<guid>https://arxiv.org/abs/2508.11829</guid>
<content:encoded><![CDATA[
arXiv:2508.11829v1 Announce Type: cross 
Abstract: Despite significant advances, AI systems struggle with the frame problem: determining what information is contextually relevant from an exponentially large possibility space. We hypothesize that biological rhythms, particularly hormonal cycles, serve as natural relevance filters that could address this fundamental challenge. We develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones including estrogen, testosterone, and cortisol. Across multiple state-of-the-art models, linguistic analysis reveals emotional and stylistic variations that track biological phases; sadness peaks during menstruation while happiness dominates ovulation and circadian patterns show morning optimism transitioning to nocturnal introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates subtle but consistent performance variations aligning with biological expectations, including optimal function in moderate rather than extreme hormonal ranges. This methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection</title>
<link>https://arxiv.org/abs/2508.11831</link>
<guid>https://arxiv.org/abs/2508.11831</guid>
<content:encoded><![CDATA[
arXiv:2508.11831v1 Announce Type: cross 
Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for language models, especially in low-resource settings. This paper investigates how cross-lingual transfer via sequential fine-tuning affects euphemism detection across five languages: English, Spanish, Chinese, Turkish, and Yoruba. We compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages like Yoruba and Turkish. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable, though lower, results. These findings highlight sequential fine-tuning as a simple yet effective strategy for improving euphemism detection in multilingual models, particularly when low-resource languages are involved.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Transformer and Large Language Models for UAV Applications</title>
<link>https://arxiv.org/abs/2508.11834</link>
<guid>https://arxiv.org/abs/2508.11834</guid>
<content:encoded><![CDATA[
arXiv:2508.11834v1 Announce Type: cross 
Abstract: The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy. This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs). Unlike previous surveys, this work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field. Furthermore, it identifies existing gaps in the literature, outlines critical challenges in computational efficiency and real-time deployment, and offers future research directions. This comprehensive synthesis aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters for Bioacoustic Encoding</title>
<link>https://arxiv.org/abs/2508.11845</link>
<guid>https://arxiv.org/abs/2508.11845</guid>
<content:encoded><![CDATA[
arXiv:2508.11845v1 Announce Type: cross 
Abstract: Bioacoustics, the study of sounds produced by living organisms, plays a vital role in conservation, biodiversity monitoring, and behavioral studies. Many tasks in this field, such as species, individual, and behavior classification and detection, are well-suited to machine learning. However, they often suffer from limited annotated data, highlighting the need for a general-purpose bioacoustic encoder capable of extracting useful representations for diverse downstream tasks. Such encoders have been proposed before, but are often limited in scope due to a focus on a narrow range of species (typically birds), and a reliance on a single model architecture or training paradigm. Moreover, they are usually evaluated on a small set of tasks and datasets. In this work, we present a large-scale empirical study that covers aspects of bioacoustics that are relevant to research but have previously been scarcely considered: training data diversity and scale, model architectures and training recipes, and the breadth of evaluation tasks and datasets. We obtain encoders that are state-of-the-art on the existing and proposed benchmarks. We also identify what matters for training these encoders, such that this work can be extended when more data are available or better architectures are proposed. Specifically, across 26 datasets with tasks including species classification, detection, individual ID, and vocal repertoire discovery, we find self-supervised pre-training followed by supervised post-training on a mixed bioacoustics + general-audio corpus yields the strongest in- and out-of-distribution performance. We show the importance of data diversity in both stages. To support ongoing research and application, we will release the model checkpoints.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance</title>
<link>https://arxiv.org/abs/2508.11857</link>
<guid>https://arxiv.org/abs/2508.11857</guid>
<content:encoded><![CDATA[
arXiv:2508.11857v1 Announce Type: cross 
Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures. We present SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning that discovers multi-word semantic units, entropy-driven data curation that optimizes training corpus quality, and multi-phase curriculum learning for stable convergence. Our approach extends Byte-Pair Encoding by learning "superword" tokens, coherent multi-word expressions that preserve semantic unity while maximizing compression efficiency. SupraTok achieves 31% improvement in English tokenization efficiency (5.91 versus 4.51 characters per token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model (124M parameters) trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications. While these results are promising at this scale, further validation at larger model scales is needed. These findings suggest that efficient tokenization can complement architectural innovations as a path to improved language model performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions</title>
<link>https://arxiv.org/abs/2508.11867</link>
<guid>https://arxiv.org/abs/2508.11867</guid>
<content:encoded><![CDATA[
arXiv:2508.11867v1 Announce Type: cross 
Abstract: Modern software delivery has accelerated from quarterly releases to multiple deployments per day. While CI/CD tooling has matured, human decision points interpreting flaky tests, choosing rollback strategies, tuning feature flags, and deciding when to promote a canary remain major sources of latency and operational toil. We propose AI-Augmented CI/CD Pipelines, where large language models (LLMs) and autonomous agents act as policy-bounded co-pilots and progressively as decision makers. We contribute: (1) a reference architecture for embedding agentic decision points into CI/CD, (2) a decision taxonomy and policy-as-code guardrail pattern, (3) a trust-tier framework for staged autonomy, (4) an evaluation methodology using DevOps Research and Assessment ( DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style case study migrating a React 19 microservice to an AI-augmented pipeline. We discuss ethics, verification, auditability, and threats to validity, and chart a roadmap for verifiable autonomy in production delivery systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Shift of Object Detection in Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.11868</link>
<guid>https://arxiv.org/abs/2508.11868</guid>
<content:encoded><![CDATA[
arXiv:2508.11868v1 Announce Type: cross 
Abstract: With the widespread adoption of machine learning technologies in autonomous driving systems, their role in addressing complex environmental perception challenges has become increasingly crucial. However, existing machine learning models exhibit significant vulnerability, as their performance critically depends on the fundamental assumption that training and testing data satisfy the independent and identically distributed condition, which is difficult to guarantee in real-world applications. Dynamic variations in data distribution caused by seasonal changes, weather fluctuations lead to data shift problems in autonomous driving systems. This study investigates the data shift problem in autonomous driving object detection tasks, systematically analyzing its complexity and diverse manifestations. We conduct a comprehensive review of data shift detection methods and employ shift detection analysis techniques to perform dataset categorization and balancing. Building upon this foundation, we construct an object detection model. To validate our approach, we optimize the model by integrating CycleGAN-based data augmentation techniques with the YOLOv5 framework. Experimental results demonstrate that our method achieves superior performance compared to baseline models on the BDD100K dataset.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition</title>
<link>https://arxiv.org/abs/2508.11870</link>
<guid>https://arxiv.org/abs/2508.11870</guid>
<content:encoded><![CDATA[
arXiv:2508.11870v1 Announce Type: cross 
Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large pre-trained vision language models (VLMs) for a wide range of downstream tasks efficiently. In this paradigm, only the inserted adapters are fine-tuned, without the need for training the original VLM backbone. Existing works scale adapters by integrating them into every layer of VLMs to increase the capacity of adapters. However, these methods face two primary limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters. In this paper, we propose a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing, achieving ultra-light parameter-efficient adaptation of VLMs on various tasks. To remove the high redundancy that exists among adapters across layers, we exploit the tensor-level low-rankness to formulate adapters as layer-shared tensor cores and layer-specific slices. Moreover, guided by generalization-aware fine-tuning, diverse rank-driven adapters cooperate to handle tasks that require different representations. Our experiments show that the proposed AdaRing achieves the state-of-the-art performance while reducing average training parameters by 90%.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment</title>
<link>https://arxiv.org/abs/2508.11872</link>
<guid>https://arxiv.org/abs/2508.11872</guid>
<content:encoded><![CDATA[
arXiv:2508.11872v1 Announce Type: cross 
Abstract: In practical teaching, we observe that few students thoroughly read or fully comprehend the information provided in traditional, text-based course syllabi. As a result, essential details, such as course policies and learning outcomes, are frequently overlooked. To address this challenge, in this paper, we propose a novel approach leveraging AI-generated singing and virtual avatars to present syllabi in a format that is more visually appealing, engaging, and memorable. Especially, we leveraged the open-source tool, HeyGem, to transform textual syllabi into audiovisual presentations, in which digital avatars perform the syllabus content as songs. The proposed approach aims to stimulate students' curiosity, foster emotional connection, and enhance retention of critical course information. Student feedback indicated that AI-sung syllabi significantly improved awareness and recall of key course information.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System</title>
<link>https://arxiv.org/abs/2508.11873</link>
<guid>https://arxiv.org/abs/2508.11873</guid>
<content:encoded><![CDATA[
arXiv:2508.11873v1 Announce Type: cross 
Abstract: Business interview preparation demands both solid theoretical grounding and refined soft skills, yet conventional classroom methods rarely deliver the individualized, culturally aware practice employers currently expect. This paper introduces SimInterview, a large language model (LLM)-based simulated multilingual interview training system designed for business professionals entering the AI-transformed labor market. Our system leverages an LLM agent and synthetic AI technologies to create realistic virtual recruiters capable of conducting personalized, real-time conversational interviews. The framework dynamically adapts interview scenarios using retrieval-augmented generation (RAG) to match individual resumes with specific job requirements across multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3), integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto diffusion-based talking head generation model, and ChromaDB vector databases, our system significantly improves interview readiness across English and Japanese markets. Experiments with university-level candidates show that the system consistently aligns its assessments with job requirements, faithfully preserves resume content, and earns high satisfaction ratings, with the lightweight Gemma 3 model producing the most engaging conversations. Qualitative findings revealed that the standardized Japanese resume format improved document retrieval while diverse English resumes introduced additional variability, and they highlighted how cultural norms shape follow-up questioning strategies. Finally, we also outlined a contestable AI design that can explain, detect bias, and preserve human-in-the-loop to meet emerging regulatory expectations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models</title>
<link>https://arxiv.org/abs/2508.11874</link>
<guid>https://arxiv.org/abs/2508.11874</guid>
<content:encoded><![CDATA[
arXiv:2508.11874v1 Announce Type: cross 
Abstract: Algorithm design and analysis is a cornerstone of computer science, but it confronts a major challenge. Proving an algorithm's performance guarantee across all inputs has traditionally required extensive and often error-prone human effort. While AI has shown great success in finding solutions to specific problem instances, automating the discovery of general algorithms with such provable guarantees has remained a significant barrier. This challenge stems from the difficulty of integrating the creative process of algorithm design with the rigorous process of formal analysis. To address this gap, we propose LegoNE, a framework that tightly fuses these two processes for the fundamental and notoriously difficult problem of computing approximate Nash equilibria. LegoNE automatically translates any algorithm written by a simple Python-like language into a constrained optimization problem. Solving this problem derives and proves the algorithm's approximation bound. Using LegoNE, a state-of-the-art large language model rediscovered the state-of-the-art algorithm for two-player games within hours, a feat that had taken human researchers 15 years to achieve. For three-player games, the model discovered a novel algorithm surpassing all existing human-designed ones. This work demonstrates a new human-machine collaborative paradigm for theoretical science: humans reason at a higher-abstract level, using symbols to compress the search space, and AI explores within it, achieving what neither could alone.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2508.11886</link>
<guid>https://arxiv.org/abs/2508.11886</guid>
<content:encoded><![CDATA[
arXiv:2508.11886v1 Announce Type: cross 
Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in images or videos based on natural language instructions. While recent multimodal large language models (MLLMs) have achieved strong performance on IVS, their inference cost remains a major bottleneck, particularly in video. We empirically analyze visual token sampling in MLLMs and observe a strong correlation between subset token coverage and segmentation performance. This motivates our design of a simple and effective token pruning method that selects a compact yet spatially representative subset of tokens to accelerate inference. In this paper, we introduce a novel visual token pruning method for IVS, called EVTP-IV, which builds upon the k-center by integrating spatial information to ensure better coverage. We further provide an information-theoretic analysis to support our design. Experiments on standard IVS benchmarks show that our method achieves up to 5X speed-up on video tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens. Our method also consistently outperforms state-of-the-art pruning baselines under varying pruning ratios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation</title>
<link>https://arxiv.org/abs/2508.11890</link>
<guid>https://arxiv.org/abs/2508.11890</guid>
<content:encoded><![CDATA[
arXiv:2508.11890v1 Announce Type: cross 
Abstract: Modern autonomous drone missions increasingly require software frameworks capable of seamlessly integrating structured symbolic planning with adaptive reinforcement learning (RL). Although traditional rule-based architectures offer robust structured reasoning for drone autonomy, their capabilities fall short in dynamically complex operational environments that require adaptive symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition Language (PDDL), explicitly integrates domain-specific knowledge and operational constraints, significantly improving the reliability and safety of unmanned aerial vehicle (UAV) decision making. In this study, we propose the AMAD-SRL framework, an extended and refined version of the Autonomous Mission Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with symbolic reinforcement learning for dynamic mission planning and execution. We validated our framework in a Software-in-the-Loop (SIL) environment structured identically to an intended Hardware-In-the-Loop Simulation (HILS) platform, ensuring seamless transition to real hardware. Experimental results demonstrate stable integration and interoperability of modules, successful transitions between BDI-driven and symbolic RL-driven planning phases, and consistent mission performance. Specifically, we evaluate a target acquisition scenario in which the UAV plans a surveillance path followed by a dynamic reentry path to secure the target while avoiding threat zones. In this SIL evaluation, mission efficiency improved by approximately 75% over a coverage-based baseline, measured by travel distance reduction. This study establishes a robust foundation for handling complex UAV missions and discusses directions for further enhancement and validation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning</title>
<link>https://arxiv.org/abs/2508.11907</link>
<guid>https://arxiv.org/abs/2508.11907</guid>
<content:encoded><![CDATA[
arXiv:2508.11907v1 Announce Type: cross 
Abstract: Federated learning (FL) offers a promising paradigm for collaborative model training while preserving data privacy. However, its susceptibility to gradient inversion attacks poses a significant challenge, necessitating robust privacy protection mechanisms. This paper introduces a novel theoretical framework to decipher the intricate interplay between attack and protection complexities in privacy-preserving FL. We formally define "Attack Complexity" as the minimum computational and data resources an adversary requires to reconstruct private data below a given error threshold, and "Protection Complexity" as the expected distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian Privacy (MBP), we derive tight theoretical bounds for protection complexity, demonstrating its scaling with model dimensionality and privacy budget. Furthermore, we establish comprehensive bounds for attack complexity, revealing its dependence on privacy leakage, gradient distortion, model dimension, and the chosen privacy level. Our findings quantitatively illuminate the fundamental trade-offs between privacy guarantees, system utility, and the effort required for both attacking and defending. This framework provides critical insights for designing more secure and efficient federated learning systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</title>
<link>https://arxiv.org/abs/2508.11915</link>
<guid>https://arxiv.org/abs/2508.11915</guid>
<content:encoded><![CDATA[
arXiv:2508.11915v1 Announce Type: cross 
Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ENA: Efficient N-dimensional Attention</title>
<link>https://arxiv.org/abs/2508.11921</link>
<guid>https://arxiv.org/abs/2508.11921</guid>
<content:encoded><![CDATA[
arXiv:2508.11921v1 Announce Type: cross 
Abstract: Efficient modeling of long sequences of high-order data requires a more efficient architecture than Transformer. In this paper, we investigate two key aspects of extending linear recurrent models, especially those originally designed for language modeling, to high-order data (1D to ND): scanning strategies and attention-hybrid architectures. Empirical results suggest that scanning provides limited benefits, while attention-hybrid models yield promising results. Focusing on the latter, we further evaluate types of attention and find that tiled high-order sliding window attention (SWA) is efficient in both theory and practice. We term the resulting hybrid architecture of linear recurrence and high-order SWA as Efficient N-dimensional Attention (ENA). We then conduct several experiments to demonstrate its effectiveness. The intuition behind ENA is that linear recurrence compresses global information into a state, while SWA complements it by enforcing strict local modeling. Together, they form a simple framework that offers a promising and practical solution for ultra-long high-order data modeling.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain</title>
<link>https://arxiv.org/abs/2508.11929</link>
<guid>https://arxiv.org/abs/2508.11929</guid>
<content:encoded><![CDATA[
arXiv:2508.11929v1 Announce Type: cross 
Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered indoor spaces or uneven terrain, requires agile and adaptive movement in all directions. This necessitates omnidirectional terrain sensing and a controller capable of processing such input. We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images. A key challenge is the high computational cost of rendering omnidirectional depth images in simulation, making traditional sim-to-real reinforcement learning (RL) impractical. Our method combines a robust blind controller with a teacher policy that supervises a vision-based student policy, trained on noise-augmented terrain data to avoid rendering costs during RL and ensure robustness. We also introduce a data augmentation technique for supervised student training, accelerating training by up to 10 times compared to conventional methods. Our framework is validated through simulation and real-world tests, demonstrating effective omnidirectional locomotion with minimal reliance on expensive rendering. This is, to the best of our knowledge, the first demonstration of vision-based omnidirectional bipedal locomotion, showcasing its adaptability to diverse terrains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware</title>
<link>https://arxiv.org/abs/2508.11935</link>
<guid>https://arxiv.org/abs/2508.11935</guid>
<content:encoded><![CDATA[
arXiv:2508.11935v1 Announce Type: cross 
Abstract: State Space Models (SSMs) are efficient alternatives to traditional sequence models, excelling at processing long sequences with lower computational complexity. Their reliance on matrix multiplications makes them ideal for compute-in-memory (CIM) architectures, which improve energy efficiency by computing within memory arrays. However, device non-idealities in CIM introduce weight perturbations that can degrade inference accuracy. In this paper, we systematically analyze the robustness of SSMs under noisy conditions, identifying that the final block and output projection layers are more susceptible to perturbations compared to other components. Building on these insights, we propose HPD, a Hybrid Projection Decomposition strategy for the last output projection layer. We replace the original weight matrix with the multiplication of U and {\Sigma} in its SVD to ensure compatibility with existing hardware architectures, while offloading V> to digital hardware for precise and robust correction. Comprehensive tests on Mamba models show that our method reduces perplexity by up to 99.57% under various noise conditions compared to baseline models, with accuracy gains of up to 96.67% on the PIQA benchmark for commonsense reasoning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware</title>
<link>https://arxiv.org/abs/2508.11940</link>
<guid>https://arxiv.org/abs/2508.11940</guid>
<content:encoded><![CDATA[
arXiv:2508.11940v1 Announce Type: cross 
Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy efficiency gains for neural network inference, but suffer from complex hardware-induced noise that poses major challenges for deployment. While noise-aware training methods have been proposed to address this issue, they typically rely on idealized and differentiable noise models that fail to capture the full complexity of analog CIM hardware variations. Motivated by the Straight-Through Estimator (STE) framework in quantization, we decouple forward noise simulation from backward gradient computation, enabling noise-aware training with more accurate but computationally intractable noise modeling in analog CIM systems. We provide theoretical analysis demonstrating that our approach preserves essential gradient directional information while maintaining computational tractability and optimization stability. Extensive experiments show that our extended STE framework achieves up to 5.3% accuracy improvement on image classification, 0.72 perplexity reduction on text generation, 2.2$\times$ speedup in training time, and 37.9% lower peak memory usage compared to standard noise-aware training methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond</title>
<link>https://arxiv.org/abs/2508.11957</link>
<guid>https://arxiv.org/abs/2508.11957</guid>
<content:encoded><![CDATA[
arXiv:2508.11957v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) agents have rapidly evolved from specialized, rule-based programs to versatile, learning-driven autonomous systems capable of perception, reasoning, and action in complex environments. The explosion of data, advances in deep learning, reinforcement learning, and multi-agent coordination have accelerated this transformation. Yet, designing and deploying unified AI agents that seamlessly integrate cognition, planning, and interaction remains a grand challenge. In this review, we systematically examine the architectural principles, foundational components, and emergent paradigms that define the landscape of contemporary AI agents. We synthesize insights from cognitive science-inspired models, hierarchical reinforcement learning frameworks, and large language model-based reasoning. Moreover, we discuss the pressing ethical, safety, and interpretability concerns associated with deploying these agents in real-world scenarios. By highlighting major breakthroughs, persistent challenges, and promising research directions, this review aims to guide the next generation of AI agent systems toward more robust, adaptable, and trustworthy autonomous intelligence.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios</title>
<link>https://arxiv.org/abs/2508.11977</link>
<guid>https://arxiv.org/abs/2508.11977</guid>
<content:encoded><![CDATA[
arXiv:2508.11977v1 Announce Type: cross 
Abstract: Recommendation systems are essential tools in modern e-commerce, facilitating personalized user experiences by suggesting relevant products. Recent advancements in generative models have demonstrated potential in enhancing recommendation systems; however, these models often exhibit limitations in optimizing retrieval tasks, primarily due to their reliance on autoregressive generation mechanisms. Conventional approaches introduce sequential dependencies that impede efficient retrieval, as they are inherently unsuitable for generating multiple items without positional constraints within a single request session. To address these limitations, we propose TBGRecall, a framework integrating Next Session Prediction (NSP), designed to enhance generative retrieval models for e-commerce applications. Our framework reformulation involves partitioning input samples into multi-session sequences, where each sequence comprises a session token followed by a set of item tokens, and then further incorporate multiple optimizations tailored to the generative task in retrieval scenarios. In terms of training methodology, our pipeline integrates limited historical data pre-training with stochastic partial incremental training, significantly improving training efficiency and emphasizing the superiority of data recency over sheer data volume. Our extensive experiments, conducted on public benchmarks alongside a large-scale industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP represents a significant advancement in the effectiveness of generative recommendation systems for e-commerce applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models</title>
<link>https://arxiv.org/abs/2508.11985</link>
<guid>https://arxiv.org/abs/2508.11985</guid>
<content:encoded><![CDATA[
arXiv:2508.11985v1 Announce Type: cross 
Abstract: Recent advances in large language models are driven by scale, while parameter-efficient fine-tuning (PEFT) enables updating only a small fraction of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the product of two small matrices, which makes them natural building blocks that can be composed. Motivated by the superposition principle, we hypothesize that independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition. Using GPT-2 Small (117M) with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math, medicine, finance). In pairwise tests, adding Math+Medicine adapters improves perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine change by +4.54% and +27.56%, respectively. Across combinations, the RMS cosine similarity between LoRA deltas correlates positively and approximately linearly with the change in perplexity. Naive summation requires no additional training, can be applied in seconds, and achieves performance comparable to models trained on merged data, while clarifying when interference appears in higher-order compositions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
<link>https://arxiv.org/abs/2508.11999</link>
<guid>https://arxiv.org/abs/2508.11999</guid>
<content:encoded><![CDATA[
arXiv:2508.11999v1 Announce Type: cross 
Abstract: With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting ChatGPT Use in Assignments: Implications for AI-Aware Assessment Design</title>
<link>https://arxiv.org/abs/2508.12013</link>
<guid>https://arxiv.org/abs/2508.12013</guid>
<content:encoded><![CDATA[
arXiv:2508.12013v1 Announce Type: cross 
Abstract: The rise of generative AI tools like ChatGPT has significantly reshaped education, sparking debates about their impact on learning outcomes and academic integrity. While prior research highlights opportunities and risks, there remains a lack of quantitative analysis of student behavior when completing assignments. Understanding how these tools influence real-world academic practices, particularly assignment preparation, is a pressing and timely research priority.
  This study addresses this gap by analyzing survey responses from 388 university students, primarily from Russia, including a subset of international participants. Using the XGBoost algorithm, we modeled predictors of ChatGPT usage in academic assignments. Key predictive factors included learning habits, subject preferences, and student attitudes toward AI. Our binary classifier demonstrated strong predictive performance, achieving 80.1\% test accuracy, with 80.2\% sensitivity and 79.9\% specificity. The multiclass classifier achieved 64.5\% test accuracy, 64.6\% weighted precision, and 64.5\% recall, with similar training scores, indicating potential data scarcity challenges.
  The study reveals that frequent use of ChatGPT for learning new concepts correlates with potential overreliance, raising concerns about long-term academic independence. These findings suggest that while generative AI can enhance access to knowledge, unchecked reliance may erode critical thinking and originality. We propose discipline-specific guidelines and reimagined assessment strategies to balance innovation with academic rigor. These insights can guide educators and policymakers in ethically and effectively integrating AI into education.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
arXiv:2508.12029v1 Announce Type: cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on conformational epitopes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2508.12036</link>
<guid>https://arxiv.org/abs/2508.12036</guid>
<content:encoded><![CDATA[
arXiv:2508.12036v1 Announce Type: cross 
Abstract: Solving tough clinical questions that require both image and text understanding is still a major challenge in healthcare AI. In this work, we propose Q-FSRU, a new model that combines Frequency Spectrum Representation and Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation (Quantum RAG) for medical Visual Question Answering (VQA). The model takes in features from medical images and related text, then shifts them into the frequency domain using Fast Fourier Transform (FFT). This helps it focus on more meaningful data and filter out noise or less useful information. To improve accuracy and ensure that answers are based on real knowledge, we add a quantum-inspired retrieval system. It fetches useful medical facts from external sources using quantum-based similarity techniques. These details are then merged with the frequency-based features for stronger reasoning. We evaluated our model using the VQA-RAD dataset, which includes real radiology images and questions. The results showed that Q-FSRU outperforms earlier models, especially on complex cases needing image-text reasoning. The mix of frequency and quantum information improves both performance and explainability. Overall, this approach offers a promising way to build smart, clear, and helpful AI tools for doctors.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation</title>
<link>https://arxiv.org/abs/2508.12040</link>
<guid>https://arxiv.org/abs/2508.12040</guid>
<content:encoded><![CDATA[
arXiv:2508.12040v1 Announce Type: cross 
Abstract: While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Enable Personalized Nudges to Promote Carbon Offsetting Among Air Travellers</title>
<link>https://arxiv.org/abs/2508.12045</link>
<guid>https://arxiv.org/abs/2508.12045</guid>
<content:encoded><![CDATA[
arXiv:2508.12045v1 Announce Type: cross 
Abstract: Nudge strategies are effective tools for promoting sustainable behaviour, but their impact depends on individual preferences. By emulating human decision-making, large language models (LLMs) offer a cost-effective route for tailoring nudges without extensive behavioural datasets, yet this potential remains unexplored. Focusing on aviation, we use LLMs to design personalized decoy-based nudge strategies that encourage air travellers to voluntarily offset CO$_2$ emissions from flights, and validate their efficacy through 3495 surveys from China, Germany, India, Singapore, and the United States. Results show that LLM-informed personalized nudges are more effective than uniform settings, raising offsetting rates by 3-7$\%$ and yielding an additional 2.3 million tonnes of CO$_2$ mitigated annually in aviation. This improvement is driven primarily by increased participation among sceptical travellers with low trust in offset programmes. Our study highlights the potential of LLM-driven personalized nudging strategies for boosting offsetting behaviours to accelerate aviation decarbonization.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials</title>
<link>https://arxiv.org/abs/2508.12063</link>
<guid>https://arxiv.org/abs/2508.12063</guid>
<content:encoded><![CDATA[
arXiv:2508.12063v1 Announce Type: cross 
Abstract: The major challenge in determining a hyperelastic model for a given material is the choice of invariants and the selection how the strain energy function depends functionally on these invariants. Here we introduce a new data-driven framework that simultaneously discovers appropriate invariants and constitutive models for isotropic incompressible hyperelastic materials. Our approach identifies both the most suitable invariants in a class of generalized invariants and the corresponding strain energy function directly from experimental observations. Unlike previous methods that rely on fixed invariant choices or sequential fitting procedures, our method integrates the discovery process into a single neural network architecture. By looking at a continuous family of possible invariants, the model can flexibly adapt to different material behaviors. We demonstrate the effectiveness of this approach using popular benchmark datasets for rubber and brain tissue. For rubber, the method recovers a stretch-dominated formulation consistent with classical models. For brain tissue, it identifies a formulation sensitive to small stretches, capturing the nonlinear shear response characteristic of soft biological matter. Compared to traditional and neural-network-based models, our framework provides improved predictive accuracy and interpretability across a wide range of deformation states. This unified strategy offers a robust tool for automated and physically meaningful model discovery in hyperelasticity.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</title>
<link>https://arxiv.org/abs/2508.12081</link>
<guid>https://arxiv.org/abs/2508.12081</guid>
<content:encoded><![CDATA[
arXiv:2508.12081v1 Announce Type: cross 
Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity</title>
<link>https://arxiv.org/abs/2508.12082</link>
<guid>https://arxiv.org/abs/2508.12082</guid>
<content:encoded><![CDATA[
arXiv:2508.12082v1 Announce Type: cross 
Abstract: Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generic Event Boundary Detection via Denoising Diffusion</title>
<link>https://arxiv.org/abs/2508.12084</link>
<guid>https://arxiv.org/abs/2508.12084</guid>
<content:encoded><![CDATA[
arXiv:2508.12084v1 Announce Type: cross 
Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries in a video, segmenting it into distinct and meaningful chunks. Despite the inherent subjectivity of event boundaries, previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions. In this paper, we introduce a novel diffusion-based boundary detection model, dubbed DiffGEBD, that tackles the problem of GEBD from a generative perspective. The proposed model encodes relevant changes across adjacent frames via temporal self-similarity and then iteratively decodes random noise into plausible event boundaries being conditioned on the encoded features. Classifier-free guidance allows the degree of diversity to be controlled in denoising diffusion. In addition, we introduce a new evaluation metric to assess the quality of predictions considering both diversity and fidelity. Experiments show that our method achieves strong performance on two standard benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event boundaries.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs</title>
<link>https://arxiv.org/abs/2508.12086</link>
<guid>https://arxiv.org/abs/2508.12086</guid>
<content:encoded><![CDATA[
arXiv:2508.12086v1 Announce Type: cross 
Abstract: In large language model (LLM) adaptation, balancing multiple optimization objectives such as improving factuality (heat) and increasing confidence (via low entropy) poses a fundamental challenge, especially when prompt parameters (e.g., hidden-layer insertions h and embedding modifications w) interact in non-trivial ways. Existing multi-objective optimization strategies often rely on scalar gradient aggregation, ignoring the deeper geometric structure between objectives and parameters. We propose J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components. This decomposition enables both hard decision-making (e.g., choosing the dominant update direction via argmax) and soft strategies (e.g., attention-style weighting via softmax over J6), forming a dynamic update framework that adapts to local conflict and synergy. Moreover, the interpretable structure of J6 provides insight into parameter attribution, task interference, and geometry-aligned adaptation. Our work introduces a principled and extensible mechanism for conflict-aware prompt optimization, and opens a new avenue for incorporating structured Jacobian reasoning into multi-objective neural tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</title>
<link>https://arxiv.org/abs/2508.12096</link>
<guid>https://arxiv.org/abs/2508.12096</guid>
<content:encoded><![CDATA[
arXiv:2508.12096v1 Announce Type: cross 
Abstract: Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[
arXiv:2508.12104v1 Announce Type: cross 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study for medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Based on this, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, CoMET autoregressively generates the next medical event, simulating patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, CoMET generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. CoMET's predictive power consistently improves as the model and pretraining scale. Our results show that CoMET, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple o3: Towards Interleaved Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2508.12109</link>
<guid>https://arxiv.org/abs/2508.12109</guid>
<content:encoded><![CDATA[
arXiv:2508.12109v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which emulates human-like ''thinking with image'' through iterative visual transformations and linguistic reasoning, we propose Simple o3, an end-to-end framework that integrates dynamic tool interactions (e.g., cropping, zooming, and reusing) into interleaved vision-language reasoning via supervised fine-tuning (SFT). Our approach features a scalable data synthesis pipeline that generates high-quality interleaved vision-language reasoning chains via an ''observe-reason-act'' cycle, complete with executable visual operations and rigorous verification, yielding the open-source TWI-Tools-146K dataset. Experimental results demonstrate Simple o3's superior performance on diverse benchmarks, outperforming existing approaches. By combining enhanced reasoning capabilities, Simple o3 establishes a powerful yet computationally affordable paradigm for advancing multimodal reasoning. Remarkably, we provide the first in-depth analysis of different interleaved reasoning strategies, offering insights into their impact on model performance. We found that by introducing additional visual tokens for interleaved vision-language reasoning, reusing and magnifying the original image significantly improves the model's visual reasoning and fine-grained perception, while image cropping based on precise visual grounding allows the model to effectively focus on key entities or regions, further enhancing its capabilities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections</title>
<link>https://arxiv.org/abs/2508.12116</link>
<guid>https://arxiv.org/abs/2508.12116</guid>
<content:encoded><![CDATA[
arXiv:2508.12116v1 Announce Type: cross 
Abstract: As numerous instruction-tuning datasets continue to emerge during the post-training stage, dynamically balancing and optimizing their mixtures has become a critical challenge. To address this, we propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions, thereby preserving the inherent diversity and coverage of the collection. Sampling probabilities are updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the dataset contributes to improving the model's performance at its current state. When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. Furthermore, we provide a comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation</title>
<link>https://arxiv.org/abs/2508.12138</link>
<guid>https://arxiv.org/abs/2508.12138</guid>
<content:encoded><![CDATA[
arXiv:2508.12138v1 Announce Type: cross 
Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving decentralized consensus, has long been criticized for excessive energy use and hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW with a centralized, cloud-based collaborative training framework. In this model, miners contribute computing resources to train segments of horizontally scaled machine learning models on preprocessed datasets, ensuring privacy and generating meaningful outputs \cite{li2017securing}. A central server evaluates contributions using two metrics: number of parameters trained and reduction in model loss during each cycle. At the end of every cycle, a weighted lottery selects the winning miner, who receives a digitally signed certificate. This certificate serves as a verifiable substitute for PoW and grants the right to append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves blockchain integrity while redirecting energy toward productive computation. The proposed approach addresses the sustainability concerns of traditional mining by converting resource expenditure into socially valuable work, aligning security incentives with real-world computational progress.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction</title>
<link>https://arxiv.org/abs/2508.12147</link>
<guid>https://arxiv.org/abs/2508.12147</guid>
<content:encoded><![CDATA[
arXiv:2508.12147v1 Announce Type: cross 
Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for assessing cardiac structure, function, and blood flow. Cine MRI extends this by capturing heart motion, providing detailed insights into cardiac mechanics. To reduce scan time and breath-hold discomfort, fast acquisition techniques have been utilized at the cost of lowering image quality. Recently, Implicit Neural Representation (INR) methods have shown promise in unsupervised reconstruction by learning coordinate-to-value mappings from undersampled data, enabling high-quality image recovery. However, current existing INR methods primarily focus on using coordinate-based positional embeddings to learn the mapping, while overlooking the feature representations of the target point and its neighboring context. In this work, we propose KP-INR, a dual-branch INR method operating in k-space for cardiac cine MRI reconstruction: one branch processes the positional embedding of k-space coordinates, while the other learns from local multi-scale k-space feature representations at those coordinates. By enabling cross-branch interaction and approximating the target k-space values from both branches, KP-INR can achieve strong performance on challenging Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its improved performance over baseline models and highlights its potential in this field.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Foreground-Background Memorization in Diffusion Models</title>
<link>https://arxiv.org/abs/2508.12148</link>
<guid>https://arxiv.org/abs/2508.12148</guid>
<content:encoded><![CDATA[
arXiv:2508.12148v1 Announce Type: cross 
Abstract: Diffusion models (DMs) memorize training images and can reproduce near-duplicates during generation. Current detection methods identify verbatim memorization but fail to capture two critical aspects: quantifying partial memorization occurring in small image regions, and memorization patterns beyond specific prompt-image pairs. To address these limitations, we propose Foreground Background Memorization (FB-Mem), a novel segmentation-based metric that classifies and quantifies memorized regions within generated images. Our method reveals that memorization is more pervasive than previously understood: (1) individual generations from single prompts may be linked to clusters of similar training images, revealing complex memorization patterns that extend beyond one-to-one correspondences; and (2) existing model-level mitigation methods, such as neuron deactivation and pruning, fail to eliminate local memorization, which persists particularly in foreground regions. Our work establishes an effective framework for measuring memorization in diffusion models, demonstrates the inadequacy of current mitigation approaches, and proposes a stronger mitigation method using a clustering approach.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis</title>
<link>https://arxiv.org/abs/2508.12162</link>
<guid>https://arxiv.org/abs/2508.12162</guid>
<content:encoded><![CDATA[
arXiv:2508.12162v1 Announce Type: cross 
Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time digital analysis, facilitated by artificial intelligence (AI) and machine learning (ML), which has improved the diagnostic precision and predictive capacity of cardiac diseases. This work proposes a novel deep learning (DL) architecture called the attention-integrated convolutional residual network (AICRN) to regress key ECG parameters such as the PR interval, the QT interval, the QRS duration, the heart rate, the peak amplitude of the R wave, and the amplitude of the T wave for interpretable ECG analysis. Our architecture is specially designed with spatial and channel attention-related mechanisms to address the type and spatial location of the ECG features for regression. The models employ a convolutional residual network to address vanishing and exploding gradient problems. The designed system addresses traditional analysis challenges, such as loss of focus due to human errors, and facilitates the fast and easy detection of cardiac events, thereby reducing the manual efforts required to solve analysis tasks. AICRN models outperform existing models in parameter regression with higher precision. This work demonstrates that DL can play a crucial role in the interpretability and precision of ECG analysis, opening up new clinical applications for cardiac monitoring and management.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis</title>
<link>https://arxiv.org/abs/2508.12163</link>
<guid>https://arxiv.org/abs/2508.12163</guid>
<content:encoded><![CDATA[
arXiv:2508.12163v1 Announce Type: cross 
Abstract: Emotion is a critical component of artificial social intelligence. However, while current methods excel in lip synchronization and image quality, they often fail to generate accurate and controllable emotional expressions while preserving the subject's identity. To address this challenge, we introduce RealTalk, a novel framework for synthesizing emotional talking heads with high emotion accuracy, enhanced emotion controllability, and robust identity preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D facial landmarks from driving audio, which are concatenated with emotion-label embeddings using a ResNet-based landmark deformation model (LDM) to produce emotional landmarks. These landmarks and facial blendshape coefficients jointly condition a novel tri-plane attention Neural Radiance Field (NeRF) to synthesize highly realistic emotional talking heads. Extensive experiments demonstrate that RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation, advancing the development of socially intelligent AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Action Diffusion</title>
<link>https://arxiv.org/abs/2508.12189</link>
<guid>https://arxiv.org/abs/2508.12189</guid>
<content:encoded><![CDATA[
arXiv:2508.12189v1 Announce Type: cross 
Abstract: Recent works have shown the promise of inference-time search over action samples for improving generative robot policies. In particular, optimizing cross-chunk coherence via bidirectional decoding has proven effective in boosting the consistency and reactivity of diffusion policies. However, this approach remains computationally expensive as the diversity of sampled actions grows. In this paper, we introduce self-guided action diffusion, a more efficient variant of bidirectional decoding tailored for diffusion-based policies. At the core of our method is to guide the proposal distribution at each diffusion step based on the prior decision. Experiments in simulation tasks show that the proposed self-guidance enables near-optimal performance at negligible inference cost. Notably, under a tight sampling budget, our method achieves up to 70% higher success rates than existing counterparts on challenging dynamic tasks. See project website at https://rhea-mal.github.io/selfgad.github.io.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams</title>
<link>https://arxiv.org/abs/2508.12198</link>
<guid>https://arxiv.org/abs/2508.12198</guid>
<content:encoded><![CDATA[
arXiv:2508.12198v1 Announce Type: cross 
Abstract: Forecasting from atmospheric soundings is a fundamental task in operational meteorology, often requiring structured visual reasoning over Skew-T log-P diagrams by human forecasters. While recent advances in Vision-Language Models (VLMs) have shown promise in other scientific domains, their application to meteorological diagram interpretation remains largely unexplored. In this study, we present a lightweight AI assistant that interprets Skew-T diagrams using a small language model (LM) and a small VLM fine-tuned to emulate human forecasters. Using a curriculum learning framework, we first train the models to identify key atmospheric features from diagrams through visual question answering, followed by chain-of-thought reasoning tasks that estimate precipitation probability based on the derived visual groundings. Model inputs include either textual summaries or generated Skew-T diagrams derived from operational Numerical Weather Prediction (NWP) forecasts, paired with three-hour precipitation observations from South Korea's Auto Weather Stations network. Evaluation results demonstrate that the fine-tuned VLM achieves skill comparable to an operational NWP model, despite relying solely on static atmospheric profiles. Ablation studies reveal that visual grounding and reasoning supervision are critical for performance, while attention map analysis confirms that the model learns to focus on relevant meteorological features. These findings highlight the potential of compact, interpretable multimodal models to support weather forecasting tasks. The approach offers a computationally efficient alternative to large-scale systems, and future work could extend it to more complex applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search</title>
<link>https://arxiv.org/abs/2508.12211</link>
<guid>https://arxiv.org/abs/2508.12211</guid>
<content:encoded><![CDATA[
arXiv:2508.12211v1 Announce Type: cross 
Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation for generalist robot policies, but often produce brittle behaviours or unsafe failures when deployed zero-shot in out-of-distribution scenarios. We present Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and accompanying algorithms that embed model-based search into the inference procedure of pre-trained VLA policies to improve their performance on robotic tasks. Specifically, our method biases a modified Monte Carlo Tree Search (MCTS) algorithm -- run using a model of the target environment -- using action priors defined by the VLA policy. By using VLA-derived abstractions and priors in model-based search, VLAPS efficiently explores language-conditioned robotics tasks whose search spaces would otherwise be intractably large. Conversely, by integrating model-based search with the VLA policy's inference procedure, VLAPS yields behaviours that are more performant than those obtained by directly following the VLA policy's action predictions. VLAPS offers a principled framework to: i) control test-time compute in VLA models, ii) leverage a priori knowledge of the robotic environment, and iii) integrate established planning and reinforcement learning techniques into the VLA inference process. Across all experiments, VLAPS significantly outperforms VLA-only baselines on language-specified tasks that would otherwise be intractable for uninformed search algorithms, increasing success rates by as much as 67 percentage points.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression</title>
<link>https://arxiv.org/abs/2508.12212</link>
<guid>https://arxiv.org/abs/2508.12212</guid>
<content:encoded><![CDATA[
arXiv:2508.12212v1 Announce Type: cross 
Abstract: Recent advances in protein large language models, such as ProtTeX, represent both side-chain amino acids and backbone structure as discrete token sequences of residue length. While this design enables unified modeling of multimodal protein information, it suffers from two major limitations: (1) The concatenation of sequence and structure tokens approximately doubles the protein length and breaks the intrinsic residue-level alignment between modalities. (2) Constrained by the training corpus and limited context window, ProtTeX is typically trained on single-protein inputs, rendering it incompatible with in-context learning (ICL) and thus limiting its generalization capability. To address these issues, we propose ProtTeX-CC, a lightweight two-stage compression framework designed to enhance ProtTeX under few-shot settings. We first design a joint embedding compression mechanism that fuses sequence and structure representations at the residue level, effectively reducing the protein input length by half without sacrificing performance. Then we propose a self-compression module that aggregates each full demonstration into the latent space of the last few linguistic tokens, reducing the average demonstration length from 751 tokens to less than 16 tokens. Compared to the original ProtTeX, our self-compression approach achieves a compression ratio of approximately 93.68% in the total prompt length under the 16-shot setting. Without modifying the backbone model, ProtTeX-CC introduces only a small number of additional parameters through PEFT-based tuning in the joint embedding compression stage and a single trainable projection layer in the self-compression stage. Extensive experiments on protein function prediction show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and generalizes well to the out-of-domain dataset with a performance gain of 11%.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Human Activity Recognition: A Survey</title>
<link>https://arxiv.org/abs/2508.12213</link>
<guid>https://arxiv.org/abs/2508.12213</guid>
<content:encoded><![CDATA[
arXiv:2508.12213v1 Announce Type: cross 
Abstract: As a critical component of Wearable AI, IMU-based Human Activity Recognition (HAR) has attracted increasing attention from both academia and industry in recent years. Although HAR performance has improved considerably in specific scenarios, its generalization capability remains a key barrier to widespread real-world adoption. For example, domain shifts caused by variations in users, sensor positions, or environments can significantly decrease the performance in practice. As a result, in this survey, we explore the rapidly evolving field of IMU-based generalizable HAR, reviewing 229 research papers alongside 25 publicly available datasets to provide a broad and insightful overview. We first present the background and overall framework of IMU-based HAR tasks, as well as the generalization-oriented training settings. Then, we categorize representative methodologies from two perspectives: (i) model-centric approaches, including pre-training method, end-to-end method, and large language model (LLM)-based learning method; and (ii) data-centric approaches, including multi-modal learning and data augmentation techniques. In addition, we summarize widely used datasets in this field, as well as relevant tools and benchmarks. Building on these methodological advances, the broad applicability of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent challenges (e.g., data scarcity, efficient training, and reliable evaluation) and also outline future directions for HAR, including the adoption of foundation and large language models, physics-informed and context-aware reasoning, generative modeling, and resource-efficient training and inference. The complete list of this survey is available at https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated continuously.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models</title>
<link>https://arxiv.org/abs/2508.12220</link>
<guid>https://arxiv.org/abs/2508.12220</guid>
<content:encoded><![CDATA[
arXiv:2508.12220v1 Announce Type: cross 
Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models and frame unlearning as a reproducible systems problem. Our approach treats training as a deterministic program and logs a minimal per-microbatch record (ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and accumulation boundary). Under a pinned stack and deterministic kernels, replaying the training tail while filtering only the forget closure yields the same parameters as training on the retain set (bit-identical in the training dtype) when preconditions hold. To meet latency and availability constraints, we add complementary paths: (i) exact reverts of recent steps via micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion when the base is frozen, and (iii) a curvature-guided anti-update followed by a short retain-tune, audit-gated with escalation to exact replay. We report storage/latency budgets and a toy artifact validating mechanics; in a controlled run that satisfies the preconditions we demonstrate byte-identical equality of model and optimizer states.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Matching via Generalized Consistency Models</title>
<link>https://arxiv.org/abs/2508.12222</link>
<guid>https://arxiv.org/abs/2508.12222</guid>
<content:encoded><![CDATA[
arXiv:2508.12222v1 Announce Type: cross 
Abstract: Recent advancement in generative models have demonstrated remarkable performance across various data modalities. Beyond their typical use in data synthesis, these models play a crucial role in distribution matching tasks such as latent variable modeling, domain translation, and domain adaptation. Generative Adversarial Networks (GANs) have emerged as the preferred method of distribution matching due to their efficacy in handling high-dimensional data and their flexibility in accommodating various constraints. However, GANs often encounter challenge in training due to their bi-level min-max optimization objective and susceptibility to mode collapse. In this work, we propose a novel approach for distribution matching inspired by the consistency models employed in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF models, such as having a straight forward norm minimization objective, while remaining adaptable to different constraints similar to GANs. We provide theoretical validation of our proposed objective and demonstrate its performance through experiments on synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery</title>
<link>https://arxiv.org/abs/2508.12232</link>
<guid>https://arxiv.org/abs/2508.12232</guid>
<content:encoded><![CDATA[
arXiv:2508.12232v1 Announce Type: cross 
Abstract: Issue-to-commit link recovery plays an important role in software traceability and improves project management. However, it remains a challenging task. A study on GitHub shows that only 42.2% of the issues are correctly linked to their commits. This highlights the potential for further development and research in this area. Existing studies have employed various AI/ML-based approaches, and with the recent development of large language models, researchers have leveraged LLMs to tackle this problem. These approaches suffer from two main issues. First, LLMs are constrained by limited context windows and cannot ingest all of the available data sources, such as long commit histories, extensive issue comments, and large code repositories. Second, most methods operate on individual issue-commit pairs; that is, given a single issue-commit pair, they determine whether the commit resolves the issue. This quickly becomes impractical in real-world repositories containing tens of thousands of commits. To address these limitations, we present LinkAnchor, the first autonomous LLM-based agent designed for issue-to-commit link recovery. The lazy-access architecture of LinkAnchor enables the underlying LLM to access the rich context of software, spanning commits, issue comments, and code files, without exceeding the token limit by dynamically retrieving only the most relevant contextual data. Additionally, LinkAnchor is able to automatically pinpoint the target commit rather than exhaustively scoring every possible candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all our case study projects. We also publicly release LinkAnchor as a ready-to-use tool, along with our replication package. LinkAnchor is designed and tested for GitHub and Jira, and is easily extendable to other platforms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction</title>
<link>https://arxiv.org/abs/2508.12247</link>
<guid>https://arxiv.org/abs/2508.12247</guid>
<content:encoded><![CDATA[
arXiv:2508.12247v1 Announce Type: cross 
Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently. The long-term spatio-temporal dependency learning brings two new challenges: 1) The long-term temporal sequence includes multiscale information naturally which is hard to extract efficiently; 2) The multiscale temporal information from different nodes is highly correlated and hard to model. To address these challenges, we propose an efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale \textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture the multiscale information efficiently and simultaneously, and an adaptive graph causal convolution network to learn the complex multiscale spatio-temporal dependency. STM2 includes hierarchical information aggregation for different-scale information that guarantees their distinguishability. To capture diverse temporal dynamics across all spatial nodes more efficiently, we further propose an enhanced version termed \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of \textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special Mixture-of-Experts architecture, including a more stable routing strategy and a causal contrastive learning strategy to enhance the scale distinguishability. We prove that STM3 has much better routing smoothness and guarantees the pattern disentanglement for each expert successfully. Extensive experiments on real-world benchmarks demonstrate STM2/STM3's superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset</title>
<link>https://arxiv.org/abs/2508.12253</link>
<guid>https://arxiv.org/abs/2508.12253</guid>
<content:encoded><![CDATA[
arXiv:2508.12253v1 Announce Type: cross 
Abstract: Time-series forecasting underpins critical decisions across aviation, energy, retail and health. Classical autoregressive integrated moving average (ARIMA) models offer interpretability via coefficients but struggle with nonlinearities, whereas tree-based machine-learning models such as XGBoost deliver high accuracy but are often opaque. This paper presents a unified framework for interpreting time-series forecasts using local interpretable model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We convert a univariate series into a leakage-free supervised learning problem, train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc explainability. Using the Air Passengers dataset as a case study, we show that a small set of lagged features -- particularly the twelve-month lag -- and seasonal encodings explain most forecast variance. We contribute: (i) a methodology for applying LIME and SHAP to time series without violating chronology; (ii) theoretical exposition of the underlying algorithms; (iii) empirical evaluation with extensive analysis; and (iv) guidelines for practitioners.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats</title>
<link>https://arxiv.org/abs/2508.12259</link>
<guid>https://arxiv.org/abs/2508.12259</guid>
<content:encoded><![CDATA[
arXiv:2508.12259v1 Announce Type: cross 
Abstract: This paper presents a Unified Security Architecture that fortifies the Agentic Web through a Zero-Trust IAM framework. This architecture is built on a foundation of rich, verifiable agent identities using Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), with discovery managed by a protocol-agnostic Agent Name Service (ANS). Security is operationalized through a multi-layered Trust Fabric which introduces significant innovations, including Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing, and Dynamic Identity with Behavioral Attestation. By explicitly linking the LPCI threat to these enhanced architectural countermeasures within a formal security model, we propose a comprehensive and forward-looking blueprint for a secure, resilient, and trustworthy agentic ecosystem. Our formal analysis demonstrates that the proposed architecture provides provable security guarantees against LPCI attacks with bounded probability of success.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Region-Level Context-Aware Multimodal Understanding</title>
<link>https://arxiv.org/abs/2508.12263</link>
<guid>https://arxiv.org/abs/2508.12263</guid>
<content:encoded><![CDATA[
arXiv:2508.12263v1 Announce Type: cross 
Abstract: Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding -- an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objects' visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC\&amp;P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at https://github.com/hongliang-wei/RC-MLLM
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution</title>
<link>https://arxiv.org/abs/2508.12277</link>
<guid>https://arxiv.org/abs/2508.12277</guid>
<content:encoded><![CDATA[
arXiv:2508.12277v1 Announce Type: cross 
Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision</title>
<link>https://arxiv.org/abs/2508.12278</link>
<guid>https://arxiv.org/abs/2508.12278</guid>
<content:encoded><![CDATA[
arXiv:2508.12278v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various graph-related tasks, with their effectiveness in analyzing graph-structured data. However, training robust GNNs often demands abundant labeled data, which is a critical bottleneck in real-world applications. This limitation severely impedes progress in Graph Anomaly Detection (GAD), where anomalies are inherently rare, costly to label, and may actively camouflage their patterns to evade detection. To address these problems, we propose Context Refactoring Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by jointly leveraging limited labeled and abundant unlabeled data. Different from previous works, CRoC exploits the class imbalance inherent in GAD to refactor the context of each node, which builds augmented graphs by recomposing the attributes of nodes while preserving their interaction patterns. Furthermore, CRoC encodes heterogeneous relations separately and integrates them into the message-passing process, enhancing the model's capacity to capture complex interaction semantics. These operations preserve node semantics while encouraging robustness to adversarial camouflage, enabling GNNs to uncover intricate anomalous cases. In the training stage, CRoC is further integrated with the contrastive learning paradigm. This allows GNNs to effectively harness unlabeled data during joint training, producing richer, more discriminative node embeddings. CRoC is evaluated on seven real-world GAD datasets with varying scales. Extensive experiments demonstrate that CRoC achieves up to 14% AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods under limited-label settings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform</title>
<link>https://arxiv.org/abs/2508.12279</link>
<guid>https://arxiv.org/abs/2508.12279</guid>
<content:encoded><![CDATA[
arXiv:2508.12279v1 Announce Type: cross 
Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with varying hardware resources and precision requirements. Given the computational limitations of embedded devices, it is crucial to consider computing costs when deploying on target platforms like the NVIDIA\textsuperscript{\textregistered} DRIVE PX 2. Our objective is to customize the semantic segmentation network according to the computing power and specific scenarios of autonomous driving hardware. We implement dynamic adaptability through a three-tier control mechanism -- width multiplier, classifier depth, and classifier kernel -- allowing fine-grained control over model components based on hardware constraints and task requirements. This adaptability facilitates broad model scaling, targeted refinement of the final layers, and scenario-specific optimization of kernel sizes, leading to improved resource allocation and performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to efficiently explore hyperparameter spaces under tight computational budgets. Our approach addresses scenario-specific and task-specific requirements through automatic parameter search, accommodating the unique computational complexity and accuracy needs of autonomous driving. It scales its Multiply-Accumulate Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in alternative configurations tailored to diverse self-driving tasks. These TSLA customizations maximize computational capacity and model accuracy, optimizing hardware utilization.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"My productivity is boosted, but ..." Demystifying Users' Perception on AI Coding Assistants</title>
<link>https://arxiv.org/abs/2508.12285</link>
<guid>https://arxiv.org/abs/2508.12285</guid>
<content:encoded><![CDATA[
arXiv:2508.12285v1 Announce Type: cross 
Abstract: This paper aims to explore fundamental questions in the era when AI coding assistants like GitHub Copilot are widely adopted: what do developers truly value and criticize in AI coding assistants, and what does this reveal about their needs and expectations in real-world software development? Unlike previous studies that conduct observational research in controlled and simulated environments, we analyze extensive, first-hand user reviews of AI coding assistants, which capture developers' authentic perspectives and experiences drawn directly from their actual day-to-day work contexts. We identify 1,085 AI coding assistants from the Visual Studio Code Marketplace. Although they only account for 1.64% of all extensions, we observe a surge in these assistants: over 90% of them are released within the past two years. We then manually analyze the user reviews sampled from 32 AI coding assistants that have sufficient installations and reviews to construct a comprehensive taxonomy of user concerns and feedback about these assistants. We manually annotate each review's attitude when mentioning certain aspects of coding assistants, yielding nuanced insights into user satisfaction and dissatisfaction regarding specific features, concerns, and overall tool performance. Built on top of the findings-including how users demand not just intelligent suggestions but also context-aware, customizable, and resource-efficient interactions-we propose five practical implications and suggestions to guide the enhancement of AI coding assistants that satisfy user needs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization</title>
<link>https://arxiv.org/abs/2508.12292</link>
<guid>https://arxiv.org/abs/2508.12292</guid>
<content:encoded><![CDATA[
arXiv:2508.12292v1 Announce Type: cross 
Abstract: Noise robustness in speech foundation models (SFMs) has been a critical challenge, as most models are primarily trained on clean data and experience performance degradation when the models are exposed to noisy speech. To address this issue, we propose HuBERT-VIC, a noise-robust SFM with variance, in-variance, and covariance regularization (VICReg) objectives. These objectives adjust the statistics of noisy speech representations, enabling the model to capture diverse acoustic characteristics and improving the generalization ability across different types of noise. When applied to HuBERT, our model shows relative performance improvements of 23.3% on LibriSpeech test-clean and 13.2% on test-other, compared to the baseline model pre-trained on noisy speech.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mutually Assured Deregulation</title>
<link>https://arxiv.org/abs/2508.12300</link>
<guid>https://arxiv.org/abs/2508.12300</guid>
<content:encoded><![CDATA[
arXiv:2508.12300v1 Announce Type: cross 
Abstract: We have convinced ourselves that the way to make AI safe is to make it unsafe. Since 2022, policymakers worldwide have embraced the Regulation Sacrifice - the belief that dismantling safety oversight will deliver security through AI dominance. Fearing China or USA will gain advantage, nations rush to eliminate safeguards that might slow progress. This Essay reveals the fatal flaw: though AI poses national security challenges, the solution demands stronger regulatory frameworks, not weaker ones. A race without guardrails breeds shared danger, not competitive strength. The Regulation Sacrifice makes three false promises. First, it promises durable technological leads. But AI capabilities spread rapidly - performance gaps between U.S. and Chinese systems collapsed from 9 percent to 2 percent in thirteen months. When advantages evaporate in months, sacrificing permanent safety for temporary speed makes no sense. Second, it promises deregulation accelerates innovation. The opposite often proves true. Companies report well-designed governance streamlines development. Investment flows toward regulated markets. Clear rules reduce uncertainty; uncertain liability creates paralysis. Environmental standards did not kill the auto industry; they created Tesla and BYD. Third, enhanced national security through deregulation actually undermines security across all timeframes. Near term: it hands adversaries information warfare tools. Medium term: it democratizes bioweapon capabilities. Long term: it guarantees deployment of uncontrollable AGI systems. The Regulation Sacrifice persists because it serves powerful interests, not security. Tech companies prefer freedom to accountability. Politicians prefer simple stories to complex truths. This creates mutually assured deregulation, where each nation's sprint for advantage guarantees collective vulnerability. The only way to win is not to play.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synchronization Dynamics of Heterogeneous, Collaborative Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2508.12314</link>
<guid>https://arxiv.org/abs/2508.12314</guid>
<content:encoded><![CDATA[
arXiv:2508.12314v1 Announce Type: cross 
Abstract: We present a novel interdisciplinary framework that bridges synchronization theory and multi-agent AI systems by adapting the Kuramoto model to describe the collective dynamics of heterogeneous AI agents engaged in complex task execution. By representing AI agents as coupled oscillators with both phase and amplitude dynamics, our model captures essential aspects of agent specialization, influence, and communication within networked systems. We introduce an order parameter to quantify the degree of coordination and synchronization, providing insights into how coupling strength, agent diversity, and network topology impact emergent collective behavior. Furthermore, we formalize a detailed correspondence between Chain-of-Thought prompting in AI reasoning and synchronization phenomena, unifying human-like iterative problem solving with emergent group intelligence. Through extensive simulations on all-to-all and deterministic scale-free networks, we demonstrate that increased coupling promotes robust synchronization despite heterogeneous agent capabilities, reflecting realistic collaborative AI scenarios. Our physics-informed approach establishes a rigorous mathematical foundation for designing, analyzing, and optimizing scalable, adaptive, and interpretable multi-agent AI systems. This work opens pathways for principled orchestration of agentic AI and lays the groundwork for future incorporation of learning dynamics and adaptive network architectures to further enhance system resilience and efficiency.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Discrepancy-aware Detector for Image Forgery Identification</title>
<link>https://arxiv.org/abs/2508.12341</link>
<guid>https://arxiv.org/abs/2508.12341</guid>
<content:encoded><![CDATA[
arXiv:2508.12341v1 Announce Type: cross 
Abstract: With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-Scale Web Search Dataset for Federated Online Learning to Rank</title>
<link>https://arxiv.org/abs/2508.12353</link>
<guid>https://arxiv.org/abs/2508.12353</guid>
<content:encoded><![CDATA[
arXiv:2508.12353v1 Announce Type: cross 
Abstract: The centralized collection of search interaction logs for training ranking models raises significant privacy concerns. Federated Online Learning to Rank (FOLTR) offers a privacy-preserving alternative by enabling collaborative model training without sharing raw user data. However, benchmarks in FOLTR are largely based on random partitioning of classical learning-to-rank datasets, simulated user clicks, and the assumption of synchronous client participation. This oversimplifies real-world dynamics and undermines the realism of experimental results. We present AOL4FOLTR, a large-scale web search dataset with 2.6 million queries from 10,000 users. Our dataset addresses key limitations of existing benchmarks by including user identifiers, real click data, and query timestamps, enabling realistic user partitioning, behavior modeling, and asynchronous federated learning scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data</title>
<link>https://arxiv.org/abs/2508.12356</link>
<guid>https://arxiv.org/abs/2508.12356</guid>
<content:encoded><![CDATA[
arXiv:2508.12356v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) offers a promising framework for training agents using pre-collected datasets without the need for further environment interaction. However, policies trained on offline data often struggle to generalise due to limited exposure to diverse states. The complexity of visual data introduces additional challenges such as noise, distractions, and spurious correlations, which can misguide the policy and increase the risk of overfitting if the training data is not sufficiently diverse. Indeed, this makes it challenging to leverage vision-based offline data in training robust agents that can generalize to unseen environments. To solve this problem, we propose a simple approach generating additional synthetic training data. We propose a two-step process, first augmenting the originally collected offline data to improve zero-shot generalization by introducing diversity, then using a diffusion model to generate additional data in latent space. We test our method across both continuous action spaces (Visual D4RL) and discrete action spaces (Procgen), demonstrating that it significantly improves generalization without requiring any algorithmic changes to existing model-free offline RL methods. We show that our method not only increases the diversity of the training data but also significantly reduces the generalization gap at test time while maintaining computational efficiency. We believe this approach could fuel additional progress in generating synthetic data to train more general agents in the future.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications</title>
<link>https://arxiv.org/abs/2508.12358</link>
<guid>https://arxiv.org/abs/2508.12358</guid>
<content:encoded><![CDATA[
arXiv:2508.12358v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become essential tools in software development, widely used for requirements engineering, code generation and review tasks. Software engineers often rely on LLMs to assess whether system code implementation satisfy task requirements, thereby enhancing code robustness and accuracy. However, it remains unclear whether LLMs can reliably determine whether the code complies fully with the given task descriptions, which is usually natural language specifications. In this paper, we uncover a systematic failure of LLMs in evaluating whether code aligns with natural language requirements. Specifically, with widely used benchmarks, we employ unified prompts to judge code correctness. Our results reveal that LLMs frequently misclassify correct code implementations as either ``not satisfying requirements'' or containing potential defects. Surprisingly, more complex prompting, especially when leveraging prompt engineering techniques involving explanations and proposed corrections, leads to higher misjudgment rate, which highlights the critical reliability issues in using LLMs as code review assistants. We further analyze the root causes of these misjudgments, and propose two improved prompting strategies for mitigation. For the first time, our findings reveals unrecognized limitations in LLMs to match code with requirements. We also offer novel insights and practical guidance for effective use of LLMs in automated code review and task-oriented agent scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models</title>
<link>https://arxiv.org/abs/2508.12361</link>
<guid>https://arxiv.org/abs/2508.12361</guid>
<content:encoded><![CDATA[
arXiv:2508.12361v1 Announce Type: cross 
Abstract: Inference-time scaling has achieved remarkable success in language models, yet its adaptation to diffusion models remains underexplored. We observe that the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems from globally fitting the The reward-tilted distribution, which inherently preserves diversity during multi-modal search. However, current applications of SMC to diffusion models face a fundamental dilemma: early-stage noise samples offer high potential for improvement but are difficult to evaluate accurately, whereas late-stage samples can be reliably assessed but are largely irreversible. To address this exploration-exploitation trade-off, we approach the problem from the perspective of the search algorithm and propose two strategies: Funnel Schedule and Adaptive Temperature. These simple yet effective methods are tailored to the unique generation dynamics and phase-transition behavior of diffusion models. By progressively reducing the number of maintained particles and down-weighting the influence of early-stage rewards, our methods significantly enhance sample quality without increasing the total number of Noise Function Evaluations. Experimental results on multiple benchmarks and state-of-the-art text-to-image diffusion models demonstrate that our approach outperforms previous baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis</title>
<link>https://arxiv.org/abs/2508.12381</link>
<guid>https://arxiv.org/abs/2508.12381</guid>
<content:encoded><![CDATA[
arXiv:2508.12381v1 Announce Type: cross 
Abstract: Pathological images play an essential role in cancer prognosis, while survival analysis, which integrates computational techniques, can predict critical clinical events such as patient mortality or disease recurrence from whole-slide images (WSIs). Recent advancements in multiple instance learning have significantly improved the efficiency of survival analysis. However, existing methods often struggle to balance the modeling of long-range spatial relationships with local contextual dependencies and typically lack inherent interpretability, limiting their clinical utility. To address these challenges, we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel framework that captures the characteristics of the tumor microenvironment and models their spatial dependencies across the tissue. IPGPhormer uniquely provides interpretability at both tissue and cellular levels without requiring post-hoc manual annotations, enabling detailed analyses of individual WSIs and cross-cohort assessments. Comprehensive evaluations on four public benchmark datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in both predictive accuracy and interpretability. In summary, our method, IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the way for more reliable and interpretable decision-support systems in pathology. The code is publicly available at https://anonymous.4open.science/r/IPGPhormer-6EEB.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.12393</link>
<guid>https://arxiv.org/abs/2508.12393</guid>
<content:encoded><![CDATA[
arXiv:2508.12393v1 Announce Type: cross 
Abstract: The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</title>
<link>https://arxiv.org/abs/2508.12398</link>
<guid>https://arxiv.org/abs/2508.12398</guid>
<content:encoded><![CDATA[
arXiv:2508.12398v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a competitive non-autoregressive paradigm due to their unique training and inference approach. However, there is currently a lack of safety study on this novel architecture. In this paper, we present the first analysis of dLLMs' safety performance and propose a novel safety alignment method tailored to their unique generation characteristics. Specifically, we identify a critical asymmetry between the defender and attacker in terms of security. For the defender, we reveal that the middle tokens of the response, rather than the initial ones, are more critical to the overall safety of dLLM outputs; this seems to suggest that aligning middle tokens can be more beneficial to the defender. The attacker, on the contrary, may have limited power to manipulate middle tokens, as we find dLLMs have a strong tendency towards a sequential generation order in practice, forcing the attack to meet this distribution and diverting it from influencing the critical middle tokens. Building on this asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method that directly aligns the model's middle generation with safe refusals exploiting reinforcement learning. We implement MOSA and compare its security performance against eight attack methods on two benchmarks. We also test the utility of MOSA-aligned dLLM on coding, math, and general reasoning. The results strongly prove the superiority of MOSA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.12405</link>
<guid>https://arxiv.org/abs/2508.12405</guid>
<content:encoded><![CDATA[
arXiv:2508.12405v1 Announce Type: cross 
Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC) remains challenging due to its myriad symptoms that evolve over long- and variable-time intervals. To address this issue, we developed a hybrid natural language processing pipeline that integrates rule-based named entity recognition with BERT-based assertion detection modules for PASC-symptom extraction and assertion detection from clinical notes. We developed a comprehensive PASC lexicon with clinical specialists. From 11 health systems of the RECOVER initiative network across the U.S., we curated 160 intake progress notes for model development and evaluation, and collected 47,654 progress notes for a population-level prevalence study. We achieved an average F1 score of 0.82 in one-site internal validation and 0.76 in 10-site external validation for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$ seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These demonstrate the effectiveness and efficiency of our models and their potential for improving PASC diagnosis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes</title>
<link>https://arxiv.org/abs/2508.12410</link>
<guid>https://arxiv.org/abs/2508.12410</guid>
<content:encoded><![CDATA[
arXiv:2508.12410v1 Announce Type: cross 
Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: {\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2508.12412</link>
<guid>https://arxiv.org/abs/2508.12412</guid>
<content:encoded><![CDATA[
arXiv:2508.12412v1 Announce Type: cross 
Abstract: The incorporation of large language models in multi-agent systems (MASs) has the potential to significantly improve our ability to autonomously solve complex problems. However, such systems introduce unique challenges in monitoring, interpreting, and detecting system failures. Most existing MAS observability frameworks focus on analyzing each individual agent separately, overlooking failures associated with the entire MAS. To bridge this gap, we propose LumiMAS, a novel MAS observability framework that incorporates advanced analytics and monitoring techniques. The proposed framework consists of three key components: a monitoring and logging layer, anomaly detection layer, and anomaly explanation layer. LumiMAS's first layer monitors MAS executions, creating detailed logs of the agents' activity. These logs serve as input to the anomaly detection layer, which detects anomalies across the MAS workflow in real time. Then, the anomaly explanation layer performs classification and root cause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven different MAS applications, implemented using two popular MAS platforms, and a diverse set of possible failures. The applications include two novel failure-tailored applications that illustrate the effects of a hallucination or bias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in failure detection, classification, and RCA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Flow Matching</title>
<link>https://arxiv.org/abs/2508.12413</link>
<guid>https://arxiv.org/abs/2508.12413</guid>
<content:encoded><![CDATA[
arXiv:2508.12413v1 Announce Type: cross 
Abstract: Flow matching has rapidly become a dominant paradigm in classical generative modeling, offering an efficient way to interpolate between two complex distributions. We extend this idea to the quantum realm and introduce Quantum Flow Matching (QFM)-a fully quantum-circuit realization that offers efficient interpolation between two density matrices. QFM offers systematic preparation of density matrices and generation of samples for accurately estimating observables, and can be realized on a quantum computer without the need for costly circuit redesigns. We validate its versatility on a set of applications: (i) generating target states with prescribed magnetization and entanglement entropy, (ii) estimating nonequilibrium free-energy differences to test the quantum Jarzynski equality, and (iii) expediting the study on superdiffusion breakdown. These results position QFM as a unifying and promising framework for generative modeling across quantum systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>fCrit: A Visual Explanation System for Furniture Design Creative Support</title>
<link>https://arxiv.org/abs/2508.12416</link>
<guid>https://arxiv.org/abs/2508.12416</guid>
<content:encoded><![CDATA[
arXiv:2508.12416v1 Announce Type: cross 
Abstract: We introduce fCrit, a dialogue-based AI system designed to critique furniture design with a focus on explainability. Grounded in reflective learning and formal analysis, fCrit employs a multi-agent architecture informed by a structured design knowledge base. We argue that explainability in the arts should not only make AI reasoning transparent but also adapt to the ways users think and talk about their designs. We demonstrate how fCrit supports this process by tailoring explanations to users' design language and cognitive framing. This work contributes to Human-Centered Explainable AI (HCXAI) in creative practice, advancing domain-specific methods for situated, dialogic, and visually grounded AI support.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations</title>
<link>https://arxiv.org/abs/2508.12430</link>
<guid>https://arxiv.org/abs/2508.12430</guid>
<content:encoded><![CDATA[
arXiv:2508.12430v1 Announce Type: cross 
Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to make black-box models more transparent by elucidating their decision-making processes. However, we find that existing VQA-NLE systems can produce inconsistent explanations and reach conclusions without genuinely understanding the underlying context, exposing weaknesses in either their inference pipeline or explanation-generation mechanism. To highlight these vulnerabilities, we not only leverage an existing adversarial strategy to perturb questions but also propose a novel strategy that minimally alters images to induce contradictory or spurious outputs. We further introduce a mitigation method that leverages external knowledge to alleviate these inconsistencies, thereby bolstering model robustness. Extensive evaluations on two standard benchmarks and two widely used VQA-NLE models underscore the effectiveness of our attacks and the potential of knowledge-based defenses, ultimately revealing pressing security and reliability concerns in current VQA-NLE systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots</title>
<link>https://arxiv.org/abs/2508.12435</link>
<guid>https://arxiv.org/abs/2508.12435</guid>
<content:encoded><![CDATA[
arXiv:2508.12435v1 Announce Type: cross 
Abstract: While gesture recognition using vision or robot skins is an active research area in Human-Robot Collaboration (HRC), this paper explores deep learning methods relying solely on a robot's built-in joint sensors, eliminating the need for external sensors. We evaluated various convolutional neural network (CNN) architectures and collected two datasets to study the impact of data representation and model architecture on the recognition accuracy. Our results show that spectrogram-based representations significantly improve accuracy, while model architecture plays a smaller role. We also tested generalization to new robot poses, where spectrogram-based models performed better. Implemented on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN, achieved over 95% accuracy in contact detection and gesture classification. These findings demonstrate the feasibility of external-sensor-free tactile recognition and promote further research toward cost-effective, scalable solutions for HRC.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping</title>
<link>https://arxiv.org/abs/2508.12466</link>
<guid>https://arxiv.org/abs/2508.12466</guid>
<content:encoded><![CDATA[
arXiv:2508.12466v1 Announce Type: cross 
Abstract: Traditional multimodal learning approaches require expensive alignment pre-training to bridge vision and language modalities, typically projecting visual features into discrete text token spaces. We challenge both fundamental assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel approach that eliminates alignment pre-training entirely while inverting the conventional mapping direction. Rather than projecting visual features to text space, our method maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. Through selective additive components in attention mechanisms, we enable dynamic integration of visual and textual representations without requiring massive image-text alignment datasets. Comprehensive experiments across nine multimodal benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing expected decreases in perception tasks requiring memorized visual-text associations (celebrity recognition: -49.5%, OCR: -21.3%). These results provide the first empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. Our work establishes the feasibility of a new paradigm that reduces computational requirements by 45%, challenges conventional wisdom about modality fusion, and opens new research directions for efficient multimodal architectures that preserve modality-specific characteristics. Our project website with code and additional resources is available at https://inverse-llava.github.io.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security</title>
<link>https://arxiv.org/abs/2508.12470</link>
<guid>https://arxiv.org/abs/2508.12470</guid>
<content:encoded><![CDATA[
arXiv:2508.12470v1 Announce Type: cross 
Abstract: The increased Internet of Medical Things IoMT and the Industrial Internet of Things IIoT interconnectivity has introduced complex cybersecurity challenges, exposing sensitive data, patient safety, and industrial operations to advanced cyber threats. To mitigate these risks, this paper introduces a novel transformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid model that combines bidirectional gated recurrent units BiGRU, long short-term memory LSTM networks, and multi-head attention MHA. The proposed architecture is designed to effectively capture bidirectional temporal dependencies, model sequential patterns, and enhance contextual feature representation. Extensive experiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset industrial IoT demonstrate the model's cross-domain robustness, achieving detection accuracies of 99.13 percent and 99.34 percent, respectively. Additionally, the model exhibits exceptional runtime efficiency, with inference times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a reliable and efficient IDS for deployment in real-world heterogeneous IoT environments
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System</title>
<link>https://arxiv.org/abs/2508.12473</link>
<guid>https://arxiv.org/abs/2508.12473</guid>
<content:encoded><![CDATA[
arXiv:2508.12473v1 Announce Type: cross 
Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a critical role in sports science, rehabilitation, and clinical neurology. Traditional analysis of H-reflex EMG waveforms is subject to variability and interpretation bias among clinicians and researchers, limiting reliability and standardization. To address these challenges, we propose a Fine-Tuned Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model (LLM)-enabled Decision Support System for automated H-reflex waveform interpretation and diagnosis. Our approach leverages multiple VLMs, each fine-tuned on curated datasets of H-reflex EMG waveform images annotated with clinical observations, recovery timelines, and athlete metadata. These models are capable of extracting key electrophysiological features and predicting neuromuscular states, including fatigue, injury, and recovery, directly from EMG images and contextual metadata. Diagnostic outputs from the VLM consortium are aggregated using a consensus-based method and refined by a specialized reasoning LLM, which ensures robust, transparent, and explainable decision support for clinicians and sports scientists. The end-to-end platform orchestrates seamless communication between the VLM ensemble and the reasoning LLM, integrating prompt engineering strategies and automated reasoning workflows using LLM Agents. Experimental results demonstrate that this hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing the automation and standardization of neuromuscular diagnostics. To our knowledge, this work represents the first integration of a fine-tuned VLM consortium with a reasoning LLM for image-based H-reflex analysis, laying the foundation for next-generation AI-assisted neuromuscular assessment and athlete monitoring platforms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EXOTIC: An Exact, Optimistic, Tree-Based Algorithm for Min-Max Optimization</title>
<link>https://arxiv.org/abs/2508.12479</link>
<guid>https://arxiv.org/abs/2508.12479</guid>
<content:encoded><![CDATA[
arXiv:2508.12479v1 Announce Type: cross 
Abstract: Min-max optimization arises in many domains such as game theory, adversarial machine learning, etc., with gradient-based methods as a typical computational tool. Beyond convex-concave min-max optimization, the solutions found by gradient-based methods may be arbitrarily far from global optima. In this work, we present an algorithmic apparatus for computing globally optimal solutions in convex-non-concave and non-convex-concave min-max optimization. For former, we employ a reformulation that transforms it into a non-concave-convex max-min optimization problem with suitably defined feasible sets and objective function. The new form can be viewed as a generalization of Sion's minimax theorem. Next, we introduce EXOTIC-an Exact, Optimistic, Tree-based algorithm for solving the reformulated max-min problem. EXOTIC employs an iterative convex optimization solver to (approximately) solve the inner minimization and a hierarchical tree search for the outer maximization to optimistically select promising regions to search based on the approximate solution returned by convex optimization solver. We establish an upper bound on its optimality gap as a function of the number of calls to the inner solver, the solver's convergence rate, and additional problem-dependent parameters. Both our algorithmic apparatus along with its accompanying theoretical analysis can also be applied for non-convex-concave min-max optimization. In addition, we propose a class of benchmark convex-non-concave min-max problems along with their analytical global solutions, providing a testbed for evaluating algorithms for min-max optimization. Empirically, EXOTIC outperforms gradient-based methods on this benchmark as well as on existing numerical benchmark problems from the literature. Finally, we demonstrate the utility of EXOTIC by computing security strategies in multi-player games with three or more players.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX</title>
<link>https://arxiv.org/abs/2508.12485</link>
<guid>https://arxiv.org/abs/2508.12485</guid>
<content:encoded><![CDATA[
arXiv:2508.12485v1 Announce Type: cross 
Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
arXiv:2508.12495v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients</title>
<link>https://arxiv.org/abs/2508.12506</link>
<guid>https://arxiv.org/abs/2508.12506</guid>
<content:encoded><![CDATA[
arXiv:2508.12506v1 Announce Type: cross 
Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age individuals. Early detection of DR can reduce the risk of vision loss by up to 95%, but a shortage of retinologists and challenges in timely examination complicate detection. Artificial Intelligence (AI) models using retinal fundus photographs (RFPs) offer a promising solution. However, adoption in clinical settings is hindered by low-quality data and biases that may lead AI systems to learn unintended features. To address these challenges, we developed RAIS-DR, a Responsible AI System for DR screening that incorporates ethical principles across the AI lifecycle. RAIS-DR integrates efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated significant improvements, with F1 scores increasing by 5-12%, accuracy by 6-19%, and specificity by 10-20%. Additionally, fairness metrics such as Disparate Impact and Equal Opportunity Difference indicated equitable performance across demographic subgroups, underscoring RAIS-DR's potential to reduce healthcare disparities. These results highlight RAIS-DR as a robust and ethically aligned solution for DR screening in clinical settings. The code, weights of RAIS-DR are available at https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with RAIL.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Introduction to Sliced Optimal Transport</title>
<link>https://arxiv.org/abs/2508.12519</link>
<guid>https://arxiv.org/abs/2508.12519</guid>
<content:encoded><![CDATA[
arXiv:2508.12519v1 Announce Type: cross 
Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal transport (OT) that exploits the tractability of one-dimensional OT problems. By combining tools from OT, integral geometry, and computational statistics, SOT enables fast and scalable computation of distances, barycenters, and kernels for probability measures, while retaining rich geometric structure. This paper provides a comprehensive review of SOT, covering its mathematical foundations, methodological advances, computational methods, and applications. We discuss key concepts of OT and one-dimensional OT, the role of tools from integral geometry such as Radon transform in projecting measures, and statistical techniques for estimating sliced distances. The paper further explores recent methodological advances, including non-linear projections, improved Monte Carlo approximations, statistical estimation techniques for one-dimensional optimal transport, weighted slicing techniques, and transportation plan estimation methods. Variational problems, such as minimum sliced Wasserstein estimation, barycenters, gradient flows, kernel constructions, and embeddings are examined alongside extensions to unbalanced, partial, multi-marginal, and Gromov-Wasserstein settings. Applications span machine learning, statistics, computer graphics and computer visions, highlighting SOT's versatility as a practical computational tool. This work will be of interest to researchers and practitioners in machine learning, data sciences, and computational disciplines seeking efficient alternatives to classical OT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers</title>
<link>https://arxiv.org/abs/2508.12520</link>
<guid>https://arxiv.org/abs/2508.12520</guid>
<content:encoded><![CDATA[
arXiv:2508.12520v1 Announce Type: cross 
Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is crucial for autonomous-driving perception. In this work, we employ Cross-View Transformers (CVT) for learning to map camera images to three BEV's channels - road, lane markings, and planned trajectory - using a realistic simulator for urban driving. Our study examines generalization to unseen towns, the effect of different camera layouts, and two loss formulations (focal and L1). Using training data from only a town, a four-camera CVT trained with the L1 loss delivers the most robust test performance, evaluated in a new town. Overall, our results underscore CVT's promise for mapping camera inputs to reasonably accurate BEV maps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Safety in LLM Fine-tuning: An Optimization Perspective</title>
<link>https://arxiv.org/abs/2508.12531</link>
<guid>https://arxiv.org/abs/2508.12531</guid>
<content:encoded><![CDATA[
arXiv:2508.12531v1 Announce Type: cross 
Abstract: Fine-tuning language models is commonly believed to inevitably harm their safety, i.e., refusing to respond to harmful user requests, even when using harmless datasets, thus requiring additional safety measures. We challenge this belief through systematic testing, showing that poor optimization choices, rather than inherent trade-offs, often cause safety problems, measured as harmful responses to adversarial prompts. By properly selecting key training hyper-parameters, e.g., learning rate, batch size, and gradient steps, we reduce unsafe model responses from 16\% to approximately 5\%, as measured by keyword matching, while maintaining utility performance. Based on this observation, we propose a simple exponential moving average (EMA) momentum technique in parameter space that preserves safety performance by creating a stable optimization path and retains the original pre-trained model's safety properties. Our experiments on the Llama families across multiple datasets (Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can largely be avoided without specialized interventions, outperforming existing approaches that require additional safety data while offering practical guidelines for maintaining both model performance and safety during adaptation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction</title>
<link>https://arxiv.org/abs/2508.12533</link>
<guid>https://arxiv.org/abs/2508.12533</guid>
<content:encoded><![CDATA[
arXiv:2508.12533v1 Announce Type: cross 
Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging (fMRI) data plays a crucial role in enabling graph machine learning for neuroimaging. However, current practices often rely on rigid pipelines that overlook critical data-centric choices in how brain graphs are constructed. In this work, we adopt a Data-Centric AI perspective and systematically define and benchmark a data-centric design space for brain graph construction, constrasting with primarily model-centric prior work. We organize this design space into three stages: temporal signal processing, topology extraction, and graph featurization. Our contributions lie less in novel components and more in evaluating how combinations of existing and modified techniques influence downstream performance. Specifically, we study high-amplitude BOLD signal filtering, sparsification and unification strategies for connectivity, alternative correlation metrics, and multi-view node and edge features, such as incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets show that thoughtful data-centric configurations consistently improve classification accuracy over standard pipelines. These findings highlight the critical role of upstream data decisions and underscore the importance of systematically exploring the data-centric design space for graph-based neuroimaging. Our code is available at https://github.com/GeQinwen/DataCentricBrainGraphs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2508.12535</link>
<guid>https://arxiv.org/abs/2508.12535</guid>
<content:encoded><![CDATA[
arXiv:2508.12535v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Analysis of MCP Security</title>
<link>https://arxiv.org/abs/2508.12538</link>
<guid>https://arxiv.org/abs/2508.12538</guid>
<content:encoded><![CDATA[
arXiv:2508.12538v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) has emerged as a universal standard that enables AI agents to seamlessly connect with external tools, significantly enhancing their functionality. However, while MCP brings notable benefits, it also introduces significant vulnerabilities, such as Tool Poisoning Attacks (TPA), where hidden malicious instructions exploit the sycophancy of large language models (LLMs) to manipulate agent behavior. Despite these risks, current academic research on MCP security remains limited, with most studies focusing on narrow or qualitative analyses that fail to capture the diversity of real-world threats. To address this gap, we present the MCP Attack Library (MCPLIB), which categorizes and implements 31 distinct attack methods under four key classifications: direct tool injection, indirect tool injection, malicious user attacks, and LLM inherent attack. We further conduct a quantitative analysis of the efficacy of each attack. Our experiments reveal key insights into MCP vulnerabilities, including agents' blind reliance on tool descriptions, sensitivity to file-based attacks, chain attacks exploiting shared context, and difficulty distinguishing external data from executable commands. These insights, validated through attack experiments, underscore the urgency for robust defense strategies and informed MCP design. Our contributions include 1) constructing a comprehensive MCP attack taxonomy, 2) introducing a unified attack framework MCPLIB, and 3) conducting empirical vulnerability analysis to enhance MCP security mechanisms. This work provides a foundational framework, supporting the secure evolution of MCP ecosystems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.12551</link>
<guid>https://arxiv.org/abs/2508.12551</guid>
<content:encoded><![CDATA[
arXiv:2508.12551v1 Announce Type: cross 
Abstract: Linux kernel tuning is essential for optimizing operating system (OS) performance. However, existing methods often face challenges in terms of efficiency, scalability, and generalization. This paper introduces OS-R1, an agentic Linux kernel tuning framework powered by rule-based reinforcement learning (RL). By abstracting the kernel configuration space as an RL environment, OS-R1 facilitates efficient exploration by large language models (LLMs) and ensures accurate configuration modifications. Additionally, custom reward functions are designed to enhance reasoning standardization, configuration modification accuracy, and system performance awareness of the LLMs. Furthermore, we propose a two-phase training process that accelerates convergence and minimizes retraining across diverse tuning scenarios. Experimental results show that OS-R1 significantly outperforms existing baseline methods, achieving up to 5.6% performance improvement over heuristic tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across various real-world applications, demonstrating its potential for practical deployment in diverse environments. Our dataset and code are publicly available at https://github.com/LHY-24/OS-R1.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM</title>
<link>https://arxiv.org/abs/2508.12575</link>
<guid>https://arxiv.org/abs/2508.12575</guid>
<content:encoded><![CDATA[
arXiv:2508.12575v1 Announce Type: cross 
Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal point of ongoing bioinformatics. The crucial step in this field is to apply advanced computational methodologies. Many recent approaches to predicting amyloidogenicity within proteins are highly based on evolutionary motifs and the individual properties of amino acids. It is becoming increasingly evident that the sequence information-based features show high predictive performance. Consequently, our study evaluated the contextual features of protein sequences obtained from a pretrained protein large language model leveraging bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and protein sequences. Our method achieved an accuracy of 84.5% on 10-fold cross-validation and an accuracy of 83% in the test dataset. Our results demonstrate competitive performance, highlighting the potential of LLMs in enhancing the accuracy of amyloid prediction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg</title>
<link>https://arxiv.org/abs/2508.12576</link>
<guid>https://arxiv.org/abs/2508.12576</guid>
<content:encoded><![CDATA[
arXiv:2508.12576v1 Announce Type: cross 
Abstract: Federated learning (FL) enables decentralized clients to train a model collaboratively without sharing local data. A key distinction between FL and centralized learning is that clients' data are non-independent and identically distributed, which poses significant challenges in training a global model that generalizes well across heterogeneous local data distributions. In this paper, we analyze the convergence of overparameterized FedAvg with gradient descent (GD). We prove that the impact of data heterogeneity diminishes as the width of neural networks increases, ultimately vanishing when the width approaches infinity. In the infinite-width regime, we further prove that both the global and local models in FedAvg behave as linear models, and that FedAvg achieves the same generalization performance as centralized learning with the same number of GD iterations. Extensive experiments validate our theoretical findings across various network architectures, loss functions, and optimization methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding</title>
<link>https://arxiv.org/abs/2508.12590</link>
<guid>https://arxiv.org/abs/2508.12590</guid>
<content:encoded><![CDATA[
arXiv:2508.12590v1 Announce Type: cross 
Abstract: To address the growing demand for on-device LLM inference in resource-constrained environments, hybrid language models (HLM) have emerged, combining lightweight local models with powerful cloud-based LLMs. Recent studies on HLM have primarily focused on improving accuracy and latency, while often overlooking communication and energy efficiency. We propose a token-level filtering mechanism for an energy-efficient importance- and uncertainty-aware HLM inference that leverages both epistemic uncertainty and attention-based importance. Our method opportunistically uploads only informative tokens, reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and token throughput of 0.37 tokens/sec while saving the energy consumption by 40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge environments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning</title>
<link>https://arxiv.org/abs/2508.12591</link>
<guid>https://arxiv.org/abs/2508.12591</guid>
<content:encoded><![CDATA[
arXiv:2508.12591v1 Announce Type: cross 
Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4% over conventional training approaches, which also paves a new avenue for ASA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression</title>
<link>https://arxiv.org/abs/2508.12604</link>
<guid>https://arxiv.org/abs/2508.12604</guid>
<content:encoded><![CDATA[
arXiv:2508.12604v1 Announce Type: cross 
Abstract: Test-time scaling has proven effective in further enhancing the performance of pretrained Large Language Models (LLMs). However, mainstream post-training methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT) reasoning) often incur substantial computational overhead due to auxiliary models and overthinking. In this paper, we empirically reveal that the incorrect answers partially stem from verbose reasoning processes lacking correct self-fix, where errors accumulate across multiple reasoning steps. To this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a pluggable RL process supervision framework that enables fine-grained optimization of each reasoning step. Specifically, SSPO requires neither auxiliary models nor stepwise manual annotations. Instead, it leverages step-wise preference signals generated by the model itself to guide the optimization process for reasoning compression. Experiments demonstrate that the generated reasoning sequences from SSPO are both accurate and succinct, effectively mitigating overthinking behaviors without compromising model performance across diverse domains and languages.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion</title>
<link>https://arxiv.org/abs/2508.12610</link>
<guid>https://arxiv.org/abs/2508.12610</guid>
<content:encoded><![CDATA[
arXiv:2508.12610v1 Announce Type: cross 
Abstract: Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the CMU-Occlu dataset, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment. The code is released at: https://github.com/qianchen214/OpenMoCap.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalized Genetic Random Field Method for the Genetic Association Analysis of Sequencing Data</title>
<link>https://arxiv.org/abs/2508.12617</link>
<guid>https://arxiv.org/abs/2508.12617</guid>
<content:encoded><![CDATA[
arXiv:2508.12617v1 Announce Type: cross 
Abstract: With the advance of high-throughput sequencing technologies, it has become feasible to investigate the influence of the entire spectrum of sequencing variations on complex human diseases. Although association studies utilizing the new sequencing technologies hold great promise to unravel novel genetic variants, especially rare genetic variants that contribute to human diseases, the statistical analysis of high-dimensional sequencing data remains a challenge. Advanced analytical methods are in great need to facilitate high-dimensional sequencing data analyses. In this article, we propose a generalized genetic random field (GGRF) method for association analyses of sequencing data. Like other similarity-based methods (e.g., SIMreg and SKAT), the new method has the advantages of avoiding the need to specify thresholds for rare variants and allowing for testing multiple variants acting in different directions and magnitude of effects. The method is built on the generalized estimating equation framework and thus accommodates a variety of disease phenotypes (e.g., quantitative and binary phenotypes). Moreover, it has a nice asymptotic property, and can be applied to small-scale sequencing data without need for small-sample adjustment. Through simulations, we demonstrate that the proposed GGRF attains an improved or comparable power over a commonly used method, SKAT, under various disease scenarios, especially when rare variants play a significant role in disease etiology. We further illustrate GGRF with an application to a real dataset from the Dallas Heart Study. By using GGRF, we were able to detect the association of two candidate genes, ANGPTL3 and ANGPTL4, with serum triglyceride.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How can we trust opaque systems? Criteria for robust explanations in XAI</title>
<link>https://arxiv.org/abs/2508.12623</link>
<guid>https://arxiv.org/abs/2508.12623</guid>
<content:encoded><![CDATA[
arXiv:2508.12623v1 Announce Type: cross 
Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in scientific research. However, the price we pay for their impressively accurate predictions is significant: their inner workings are notoriously opaque - it is unknown to laypeople and researchers alike what features of the data a DL system focuses on and how it ultimately succeeds in predicting correct outputs. A necessary criterion for trustworthy explanations is that they should reflect the relevant processes the algorithms' predictions are based on. The field of eXplainable Artificial Intelligence (XAI) presents promising methods to create such explanations. But recent reviews about their performance offer reasons for skepticism. As we will argue, a good criterion for trustworthiness is explanatory robustness: different XAI methods produce the same explanations in comparable contexts. However, in some instances, all methods may give the same, but still wrong, explanation. We therefore argue that in addition to explanatory robustness (ER), a prior requirement of explanation method robustness (EMR) has to be fulfilled by every XAI method. Conversely, the robustness of an individual method is in itself insufficient for trustworthiness. In what follows, we develop and formalize criteria for ER as well as EMR, providing a framework for explaining and establishing trust in DL algorithms. We also highlight interesting application cases and outline directions for future work.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer</title>
<link>https://arxiv.org/abs/2508.12638</link>
<guid>https://arxiv.org/abs/2508.12638</guid>
<content:encoded><![CDATA[
arXiv:2508.12638v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time applications such as autonomous driving and human-computer interaction, which demand fast and reliable responses based on accurate perception. To meet these requirements, existing systems commonly employ cloud-edge collaborative architectures, such as partitioned Large Vision-Language Models (LVLMs) or task offloading strategies between Large and Small Vision-Language Models (SVLMs). However, these methods fail to accommodate cloud latency fluctuations and overlook the full potential of delayed but accurate LVLM responses. In this work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed Context Transfer, which treats the delayed outputs of LVLMs as historical context to provide real-time guidance for SVLMs inference. Based on this paradigm, we design SpotVLM, which incorporates both context replacement and visual focus modules to refine historical textual input and enhance visual grounding consistency. Extensive experiments on three real-time vision tasks across four datasets demonstrate the effectiveness of the proposed framework. The new paradigm lays the groundwork for more effective and latency-aware collaboration strategies in future VLM systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery</title>
<link>https://arxiv.org/abs/2508.12650</link>
<guid>https://arxiv.org/abs/2508.12650</guid>
<content:encoded><![CDATA[
arXiv:2508.12650v1 Announce Type: cross 
Abstract: Ordering-based approaches to causal discovery identify topological orders of causal graphs, providing scalable alternatives to combinatorial search methods. Under the Additive Noise Model (ANM) assumption, recent causal ordering methods based on score matching require an accurate estimation of the Hessian diagonal of the log-densities. However, previous approaches mainly use Stein gradient estimators, which are computationally expensive and memory-intensive. Although DiffAN addresses these limitations by substituting kernel-based estimates with diffusion models, it remains numerically unstable due to the second-order derivatives of score models. To alleviate these problems, we propose Score-informed Neural Operator (SciNO), a probabilistic generative model in smooth function spaces designed to stably approximate the Hessian diagonal and to preserve structural information during the score modeling. Empirical results show that SciNO reduces order divergence by 42.7% on synthetic graphs and by 31.5% on real-world datasets on average compared to DiffAN, while maintaining memory efficiency and scalability. Furthermore, we propose a probabilistic control algorithm for causal reasoning with autoregressive models that integrates SciNO's probability estimates with autoregressive model priors, enabling reliable data-driven causal ordering informed by semantic information. Consequently, the proposed method enhances causal reasoning abilities of LLMs without additional fine-tuning or prompt engineering.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Language Barriers: Equitable Performance in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2508.12662</link>
<guid>https://arxiv.org/abs/2508.12662</guid>
<content:encoded><![CDATA[
arXiv:2508.12662v1 Announce Type: cross 
Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual communication and understanding. However, LLMs perform worse in Common Sense Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi or Swahili compared to high-resource languages (HRLs) like English. Equalizing this inconsistent access to quality LLM outputs is crucial to ensure fairness for speakers of LRLs and across diverse linguistic communities. In this paper, we propose an approach to bridge this gap in LLM performance. Our approach involves fine-tuning an LLM on synthetic code-switched text generated using controlled language-mixing methods. We empirically demonstrate that fine-tuning LLMs on synthetic code-switched datasets leads to substantial improvements in LRL model performance while preserving or enhancing performance in HRLs. Additionally, we present a new dataset of synthetic code-switched text derived from the CommonSenseQA dataset, featuring three distinct language ratio configurations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering</title>
<link>https://arxiv.org/abs/2508.12672</link>
<guid>https://arxiv.org/abs/2508.12672</guid>
<content:encoded><![CDATA[
arXiv:2508.12672v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training across multiple clients without sharing private data. We consider FL scenarios wherein FL clients are subject to adversarial (Byzantine) attacks, while the FL server is trusted (honest) and has a trustworthy side dataset. This may correspond to, e.g., cases where the server possesses trusted data prior to federation, or to the presence of a trusted client that temporarily assumes the server role. Our approach requires only two honest participants, i.e., the server and one client, to function effectively, without prior knowledge of the number of malicious clients. Theoretical analysis demonstrates bounded optimality gaps even under strong Byzantine attacks. Experimental results show that our algorithm significantly outperforms standard and robust FL baselines such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack strategies including label flipping, sign flipping, and Gaussian noise addition across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach</title>
<link>https://arxiv.org/abs/2508.12673</link>
<guid>https://arxiv.org/abs/2508.12673</guid>
<content:encoded><![CDATA[
arXiv:2508.12673v1 Announce Type: cross 
Abstract: Federated Learning (FL) has emerged as a promising paradigm for privacy-preserving collaborative learning, yet data heterogeneity remains a critical challenge. While existing methods achieve progress in addressing data heterogeneity for participating clients, they fail to generalize to non-participating clients with in-domain distribution shifts and resource constraints. To mitigate this issue, we present HyperFedZero, a novel method that dynamically generates specialized models via a hypernetwork conditioned on distribution-aware embeddings. Our approach explicitly incorporates distribution-aware inductive biases into the model's forward pass, extracting robust distribution embeddings using a NoisyEmbed-enhanced extractor with a Balancing Penalty, effectively preventing feature collapse. The hypernetwork then leverages these embeddings to generate specialized models chunk-by-chunk for non-participating clients, ensuring adaptability to their unique data distributions. Extensive experiments on multiple datasets and models demonstrate HyperFedZero's remarkable performance, surpassing competing methods consistently with minimal computational, storage, and communication overhead. Moreover, ablation studies and visualizations further validate the necessity of each component, confirming meaningful adaptations and validating the effectiveness of HyperFedZero.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications</title>
<link>https://arxiv.org/abs/2508.12683</link>
<guid>https://arxiv.org/abs/2508.12683</guid>
<content:encoded><![CDATA[
arXiv:2508.12683v1 Announce Type: cross 
Abstract: Hierarchical multi-agent systems (HMAS) organize collections of agents into layered structures that help manage complexity and scale. These hierarchies can simplify coordination, but they also can introduce trade-offs that are not always obvious. This paper proposes a multi-dimensional taxonomy for HMAS along five axes: control hierarchy, information flow, role and task delegation, temporal layering, and communication structure. The intent is not to prescribe a single "best" design but to provide a lens for comparing different approaches.
  Rather than treating these dimensions in isolation, the taxonomy is connected to concrete coordination mechanisms - from the long-standing contract-net protocol for task allocation to more recent work in hierarchical reinforcement learning. Industrial contexts illustrate the framework, including power grids and oilfield operations, where agents at production, maintenance, and supply levels coordinate to diagnose well issues or balance energy demand. These cases suggest that hierarchical structures may achieve global efficiency while preserving local autonomy, though the balance is delicate.
  The paper closes by identifying open challenges: making hierarchical decisions explainable to human operators, scaling to very large agent populations, and assessing whether learning-based agents such as large language models can be safely integrated into layered frameworks. This paper presents what appears to be the first taxonomy that unifies structural, temporal, and communication dimensions of hierarchical MAS into a single design framework, bridging classical coordination mechanisms with modern reinforcement learning and large language model agents.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction</title>
<link>https://arxiv.org/abs/2508.12685</link>
<guid>https://arxiv.org/abs/2508.12685</guid>
<content:encoded><![CDATA[
arXiv:2508.12685v1 Announce Type: cross 
Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions</title>
<link>https://arxiv.org/abs/2508.12690</link>
<guid>https://arxiv.org/abs/2508.12690</guid>
<content:encoded><![CDATA[
arXiv:2508.12690v1 Announce Type: cross 
Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically adapt and perform optimally on shifting target domains. This task is particularly emphasized in real-world driving scenes, where weather domain shifts occur frequently. To address such dynamic changes, our proposed method, TTA-DAME, leverages source domain data augmentation into target domains. Additionally, we introduce a domain discriminator and a specialized domain detector to mitigate drastic domain shifts, especially from daytime to nighttime conditions. To further improve adaptability, we train multiple detectors and consolidate their predictions through Non-Maximum Suppression (NMS). Our empirical validation demonstrates the effectiveness of our method, showing significant performance enhancements on the SHIFT Benchmark.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning</title>
<link>https://arxiv.org/abs/2508.12692</link>
<guid>https://arxiv.org/abs/2508.12692</guid>
<content:encoded><![CDATA[
arXiv:2508.12692v1 Announce Type: cross 
Abstract: Class-incremental with repetition (CIR), where previously trained classes repeatedly introduced in future tasks, is a more realistic scenario than the traditional class incremental setup, which assumes that each task contains unseen classes. CIR assumes that we can easily access abundant unlabeled data from external sources, such as the Internet. Therefore, we propose two components that efficiently use the unlabeled data to ensure the high stability and the plasticity of models trained in CIR setup. First, we introduce multi-level knowledge distillation (MLKD) that distills knowledge from multiple previous models across multiple perspectives, including features and logits, so the model can maintain much various previous knowledge. Moreover, we implement dynamic self-supervised loss (SSL) to utilize the unlabeled data that accelerates the learning of new classes, while dynamic weighting of SSL keeps the focus of training to the primary task. Both of our proposed components significantly improve the performance in CIR setup, achieving 2nd place in the CVPR 5th CLVISION Challenge.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance</title>
<link>https://arxiv.org/abs/2508.12702</link>
<guid>https://arxiv.org/abs/2508.12702</guid>
<content:encoded><![CDATA[
arXiv:2508.12702v1 Announce Type: cross 
Abstract: Robust information representation and its persistent maintenance are fundamental for higher cognitive functions. Existing models employ distinct neural mechanisms to separately address noise-resistant processing or information maintenance, yet a unified framework integrating both operations remains elusive -- a critical gap in understanding cortical computation. Here, we introduce a recurrent neural circuit that combines divisive normalization with self-excitation to achieve both robust encoding and stable retention of normalized inputs. Mathematical analysis shows that, for suitable parameter regimes, the system forms a continuous attractor with two key properties: (1) input-proportional stabilization during stimulus presentation; and (2) self-sustained memory states persisting after stimulus offset. We demonstrate the model's versatility in two canonical tasks: (a) noise-robust encoding in a random-dot kinematogram (RDK) paradigm; and (b) approximate Bayesian belief updating in a probabilistic Wisconsin Card Sorting Test (pWCST). This work establishes a unified mathematical framework that bridges noise suppression, working memory, and approximate Bayesian inference within a single cortical microcircuit, offering fresh insights into the brain's canonical computation and guiding the design of biologically plausible artificial neural architectures.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Diffusion Recommendation Model</title>
<link>https://arxiv.org/abs/2508.12706</link>
<guid>https://arxiv.org/abs/2508.12706</guid>
<content:encoded><![CDATA[
arXiv:2508.12706v1 Announce Type: cross 
Abstract: Recently, motivated by the outstanding achievements of diffusion models, the diffusion process has been employed to strengthen representation learning in recommendation systems. Most diffusion-based recommendation models typically utilize standard Gaussian noise in symmetric forward and reverse processes in continuous data space. Nevertheless, the samples derived from recommendation systems inhabit a discrete data space, which is fundamentally different from the continuous one. Moreover, Gaussian noise has the potential to corrupt personalized information within latent representations. In this work, we propose a novel and effective method, named Asymmetric Diffusion Recommendation Model (AsymDiffRec), which learns forward and reverse processes in an asymmetric manner. We define a generalized forward process that simulates the missing features in real-world recommendation samples. The reverse process is then performed in an asymmetric latent feature space. To preserve personalized information within the latent representation, a task-oriented optimization strategy is introduced. In the serving stage, the raw sample with missing features is regarded as a noisy input to generate a denoising and robust representation for the final prediction. By equipping base models with AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and +0.166% in terms of users' active days and app usage duration respectively. Additionally, the extended offline experiments also demonstrate improvements. AsymDiffRec has been implemented in the Douyin Music App.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning</title>
<link>https://arxiv.org/abs/2508.12709</link>
<guid>https://arxiv.org/abs/2508.12709</guid>
<content:encoded><![CDATA[
arXiv:2508.12709v1 Announce Type: cross 
Abstract: Masked latent prediction has emerged as a leading paradigm in self-supervised learning (SSL), especially for general audio and music representation learning. While recent methods have demonstrated strong performance, the role of the predictor module used at the output of such SSL systems remains mainly overlooked, despite being crucial for solving the pretext task at hand. In particular, this module should be able to deal with the ambiguity inherent in audio content, especially when it is composed of multiple sound sources. This work proposes a novel enhancement: integrating Multiple Choice Learning (MCL) to explicitly model prediction ambiguity and improve representation quality. We build on top of the recently proposed MATPAC system, improving its prediction and unsupervised classification pretext tasks with MCL. We extensively evaluate our method, MATPAC++, through both linear probing across multiple downstream tasks and fine-tuning on AudioSet, employing a unified protocol that enables rigorous and fair comparisons with state-of-the-art SSL approaches. Results show that our proposal achieves state-of-the-art when fine-tuned on AudioSet and overall state-of-the-art scores on downstream tasks. Additionally, we examine domain specialisation by training exclusively on music data, where our model achieves state-of-the-art performance with significantly improved efficiency.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2508.12733</link>
<guid>https://arxiv.org/abs/2508.12733</guid>
<content:encoded><![CDATA[
arXiv:2508.12733v1 Announce Type: cross 
Abstract: The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models</title>
<link>https://arxiv.org/abs/2508.12740</link>
<guid>https://arxiv.org/abs/2508.12740</guid>
<content:encoded><![CDATA[
arXiv:2508.12740v1 Announce Type: cross 
Abstract: Federated learning (FL) enables decentralized model training without sharing local data. However, most existing methods assume identical model architectures across clients, limiting their applicability in heterogeneous real-world environments. To address this, we propose FedUNet, a lightweight and architecture-agnostic FL framework that attaches a U-Net-inspired additive module to each client's backbone. By sharing only the compact bottleneck of the U-Net, FedUNet enables efficient knowledge transfer without structural alignment. The encoder-decoder design and skip connections in the U-Net help capture both low-level and high-level features, facilitating the extraction of clientinvariant representations. This enables cooperative learning between the backbone and the additive module with minimal communication cost. Experiment with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low communication overhead.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification</title>
<link>https://arxiv.org/abs/2508.12745</link>
<guid>https://arxiv.org/abs/2508.12745</guid>
<content:encoded><![CDATA[
arXiv:2508.12745v1 Announce Type: cross 
Abstract: Image set classification (ISC), which can be viewed as a task of comparing similarities between sets consisting of unordered heterogeneous images with variable quantities and qualities, has attracted growing research attention in recent years. How to learn effective feature representations and how to explore the similarities between different image sets are two key yet challenging issues in this field. However, existing traditional ISC methods classify image sets based on raw pixel features, ignoring the importance of feature learning. Existing deep ISC methods can learn deep features, but they fail to adaptively adjust the features when measuring set distances, resulting in limited performance in few-shot ISC. To address the above issues, this paper combines traditional ISC methods with deep models and proposes a novel few-shot ISC approach called Deep Class-specific Collaborative Representation (DCSCR) network to simultaneously learn the frame- and concept-level feature representations of each image set and the distance similarities between different sets. Specifically, DCSCR consists of a fully convolutional deep feature extractor module, a global feature learning module, and a class-specific collaborative representation-based metric learning module. The deep feature extractor and global feature learning modules are used to learn (local and global) frame-level feature representations, while the class-specific collaborative representation-based metric learning module is exploit to adaptively learn the concept-level feature representation of each image set and thus obtain the distance similarities between different sets by developing a new CSCR-based contrastive loss function. Extensive experiments on several well-known few-shot ISC datasets demonstrate the effectiveness of the proposed method compared with some state-of-the-art image set classification algorithms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke</title>
<link>https://arxiv.org/abs/2508.12755</link>
<guid>https://arxiv.org/abs/2508.12755</guid>
<content:encoded><![CDATA[
arXiv:2508.12755v1 Announce Type: cross 
Abstract: Computer vision models can be used to assist during mechanical thrombectomy (MT) for acute ischemic stroke (AIS), but poor image quality often degrades performance. This work presents CLAIRE-DSA, a deep learning--based framework designed to categorize key image properties in minimum intensity projections (MinIPs) acquired during MT for AIS, supporting downstream quality control and workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models, fine-tuned to predict nine image properties (e.g., presence of contrast, projection angle, motion artefact severity). Separate classifiers were trained on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$ to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by filtering poor quality images and comparing segmentation performance on filtered and unfiltered datasets. Segmentation success rate increased from $42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an automated tool for accurately classifying image properties in DSA series of acute ischemic stroke patients, supporting image annotation and quality control in clinical and research applications. Source code is available at https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors</title>
<link>https://arxiv.org/abs/2508.12766</link>
<guid>https://arxiv.org/abs/2508.12766</guid>
<content:encoded><![CDATA[
arXiv:2508.12766v1 Announce Type: cross 
Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging due to the low-contrast defect boundaries, necessitating annotators to cross-reference multiple views. These views share a single ground truth (GT), forming a unique ``many-to-one'' relationship. This characteristic renders advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as they are generally limited by a ``one-to-one'' relationship, where each image is independently associated with its GT. Such limitation may lead to error accumulation in low-contrast regions, further exacerbating confirmation bias. To address this issue, we revisit the SSS pipeline from a group-oriented perspective and propose a human-inspired solution: the Intra-group Consistency Augmentation Framework (ICAF). First, we experimentally validate the inherent consistency constraints within CdZnTe groups, establishing a group-oriented baseline using the Intra-group View Sampling (IVS). Building on this insight, we introduce the Pseudo-label Correction Network (PCN) to enhance consistency representation, which consists of two key modules. The View Augmentation Module (VAM) improves boundary details by dynamically synthesizing a boundary-aware view through the aggregation of multiple views. In the View Correction Module (VCM), this synthesized view is paired with other views for information interaction, effectively emphasizing salient regions while minimizing noise. Extensive experiments demonstrate the effectiveness of our solution for CdZnTe materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2 group-annotated data (5\textperthousand). The code is available at \href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</title>
<link>https://arxiv.org/abs/2508.12769</link>
<guid>https://arxiv.org/abs/2508.12769</guid>
<content:encoded><![CDATA[
arXiv:2508.12769v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized PCA Forest for Outlier Detection</title>
<link>https://arxiv.org/abs/2508.12776</link>
<guid>https://arxiv.org/abs/2508.12776</guid>
<content:encoded><![CDATA[
arXiv:2508.12776v1 Announce Type: cross 
Abstract: We propose a novel unsupervised outlier detection method based on Randomized Principal Component Analysis (PCA). Inspired by the performance of Randomized PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a novel unsupervised outlier detection method that utilizes RPCA Forest for outlier detection. Experimental results showcase the superiority of the proposed approach compared to the classical and state-of-the-art methods in performing the outlier detection task on several datasets while performing competitively on the rest. The extensive analysis of the proposed method reflects it high generalization power and its computational efficiency, highlighting it as a good choice for unsupervised outlier detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</title>
<link>https://arxiv.org/abs/2508.12792</link>
<guid>https://arxiv.org/abs/2508.12792</guid>
<content:encoded><![CDATA[
arXiv:2508.12792v1 Announce Type: cross 
Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision</title>
<link>https://arxiv.org/abs/2508.12794</link>
<guid>https://arxiv.org/abs/2508.12794</guid>
<content:encoded><![CDATA[
arXiv:2508.12794v1 Announce Type: cross 
Abstract: Transportation influence health by shaping exposure to physical activity, air pollution and injury risk.Comparative data on cycling and motorcycling behaviours is scarce, particularly at a global scale.Street view imagery, such as Google Street View (GSV), combined with computer vision, is a valuable resource for efficiently capturing travel behaviour data.This study demonstrates a novel approach using deep learning on street view images to estimate cycling and motorcycling levels across diverse cities worldwide.We utilized data from 185 global cities.The data on mode shares of cycling and motorcycling estimated using travel surveys or censuses.We used GSV images to detect cycles and motorcycles in sampled locations, using 8000 images per city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean average precision of 89% for detecting cycles and motorcycles in GSV images.A global prediction model was developed using beta regression with city-level mode shares as outcome, with log transformed explanatory variables of counts of GSV-detected images with cycles and motorcycles, while controlling for population density.We found strong correlations between GSV motorcycle counts and motorcycle mode share (0.78) and moderate correlations between GSV cycle counts and cycling mode share (0.51).Beta regression models predicted mode shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling, achieving median absolute errors (MDAE) of 1.3% and 1.4%, respectively.Scatterplots demonstrated consistent prediction accuracy, though cities like Utrecht and Cali were outliers.The model was applied to 60 cities globally for which we didn't have recent mode share data.We provided estimates for some cities in the Middle East, Latin America and East Asia.With computer vision, GSV images capture travel modes and activity, providing insights alongside traditional data sources.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Shift in Perspective on Causality in Domain Generalization</title>
<link>https://arxiv.org/abs/2508.12798</link>
<guid>https://arxiv.org/abs/2508.12798</guid>
<content:encoded><![CDATA[
arXiv:2508.12798v1 Announce Type: cross 
Abstract: The promise that causal modelling can lead to robust AI generalization has been challenged in recent work on domain generalization (DG) benchmarks. We revisit the claims of the causality and DG literature, reconciling apparent contradictions and advocating for a more nuanced theory of the role of causality in generalization. We also provide an interactive demo at https://chai-uk.github.io/ukairs25-causal-predictors/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward</title>
<link>https://arxiv.org/abs/2508.12800</link>
<guid>https://arxiv.org/abs/2508.12800</guid>
<content:encoded><![CDATA[
arXiv:2508.12800v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Visual Granularity Generation</title>
<link>https://arxiv.org/abs/2508.12811</link>
<guid>https://arxiv.org/abs/2508.12811</guid>
<content:encoded><![CDATA[
arXiv:2508.12811v1 Announce Type: cross 
Abstract: We propose a novel approach to image generation by decomposing an image into a structured sequence, where each element in the sequence shares the same spatial resolution but differs in the number of unique tokens used, capturing different level of visual granularity. Image generation is carried out through our newly introduced Next Visual Granularity (NVG) generation framework, which generates a visual granularity sequence beginning from an empty image and progressively refines it, from global layout to fine details, in a structured manner. This iterative process encodes a hierarchical, layered representation that offers fine-grained control over the generation process across multiple granularity levels. We train a series of NVG models for class-conditional image generation on the ImageNet dataset and observe clear scaling behavior. Compared to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30 -> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to showcase the capability and potential of the NVG framework. Our code and models will be released.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.12815</link>
<guid>https://arxiv.org/abs/2508.12815</guid>
<content:encoded><![CDATA[
arXiv:2508.12815v1 Announce Type: cross 
Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection</title>
<link>https://arxiv.org/abs/2508.12828</link>
<guid>https://arxiv.org/abs/2508.12828</guid>
<content:encoded><![CDATA[
arXiv:2508.12828v1 Announce Type: cross 
Abstract: Abusive language detection has become an increasingly important task as a means to tackle this type of harmful content in social media. There has been a substantial body of research developing models for determining if a social media post is abusive or not; however, this research has primarily focused on exploiting social media posts individually, overlooking additional context that can be derived from surrounding posts. In this study, we look at conversational exchanges, where a user replies to an earlier post by another user (the parent tweet). We ask: does leveraging context from the parent tweet help determine if a reply post is abusive or not, and what are the features that contribute the most? We study a range of content-based and account-based features derived from the context, and compare this to the more widely studied approach of only looking at the features from the reply tweet. For a more generalizable study, we test four different classification models on a dataset made of conversational exchanges (parent-reply tweet pairs) with replies labeled as abusive or not. Our experiments show that incorporating contextual features leads to substantial improvements compared to the use of features derived from the reply tweet only, confirming the importance of leveraging context. We observe that, among the features under study, it is especially the content-based features (what is being posted) that contribute to the classification performance rather than account-based features (who is posting it). While using content-based features, it is best to combine a range of different features to ensure improved performance over being more selective and using fewer features. Our study provides insights into the development of contextualized abusive language detection models in realistic settings involving conversations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG</title>
<link>https://arxiv.org/abs/2508.12833</link>
<guid>https://arxiv.org/abs/2508.12833</guid>
<content:encoded><![CDATA[
arXiv:2508.12833v1 Announce Type: cross 
Abstract: On-device machine learning is often constrained by limited storage, particularly in continuous data collection scenarios. This paper presents an empirical study on storage-aware learning, focusing on the trade-off between data quantity and quality via compression. We demonstrate that naive strategies, such as uniform data dropping or one-size-fits-all compression, are suboptimal. Our findings further reveal that data samples exhibit varying sensitivities to compression, supporting the feasibility of a sample-wise adaptive compression strategy. These insights provide a foundation for developing a new class of storage-aware learning systems. The primary contribution of this work is the systematic characterization of this under-explored challenge, offering valuable insights that advance the understanding of storage-aware learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms</title>
<link>https://arxiv.org/abs/2508.12839</link>
<guid>https://arxiv.org/abs/2508.12839</guid>
<content:encoded><![CDATA[
arXiv:2508.12839v1 Announce Type: cross 
Abstract: With the rapid proliferation of streaming services, network load exhibits highly time-varying and bursty behavior, posing serious challenges for maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms (CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS and profitability, accurate load forecasting remains challenging under traffic surges. Existing methods either minimize mean absolute error, resulting in underprovisioning and potential Service Level Agreement (SLA) violations during peak periods, or adopt conservative overprovisioning strategies, which mitigate SLA risks at the expense of increased resource expenditure. To address this dilemma, we propose HRS, a hybrid representation framework with scheduling awareness that integrates numerical and image-based representations to better capture extreme load dynamics. We further introduce a Scheduling-Aware Loss (SAL) that captures the asymmetric impact of prediction errors, guiding predictions that better support scheduling decisions. Extensive experiments on four real-world datasets demonstrate that HRS consistently outperforms ten baselines and achieves state-of-the-art performance, reducing SLA violation rates by 63.1% and total profit loss by 32.3%.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Meanings in Transformer Language Models</title>
<link>https://arxiv.org/abs/2508.12863</link>
<guid>https://arxiv.org/abs/2508.12863</guid>
<content:encoded><![CDATA[
arXiv:2508.12863v1 Announce Type: cross 
Abstract: We investigate how word meanings are represented in the transformer language models. Specifically, we focus on whether transformer models employ something analogous to a lexical store - where each word has an entry that contains semantic information. To do this, we extracted the token embedding space of RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we then manually inspected the resultant clusters to consider whether they are sensitive to semantic information. In our second study, we tested whether the clusters are sensitive to five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition. Overall, our findings were very positive - there is a wide variety of semantic information encoded within the token embedding space. This serves to rule out certain "meaning eliminativist" hypotheses about how transformer LLMs process semantic information.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Class Intrusion Detection with Dynamic Graphs</title>
<link>https://arxiv.org/abs/2508.12885</link>
<guid>https://arxiv.org/abs/2508.12885</guid>
<content:encoded><![CDATA[
arXiv:2508.12885v1 Announce Type: cross 
Abstract: With the growing digitalization all over the globe, the relevance of network security becomes increasingly important. Machine learning-based intrusion detection constitutes a promising approach for improving security, but it bears several challenges. These include the requirement to detect novel and unseen network events, as well as specific data properties, such as events over time together with the inherent graph structure of network communication. In this work, we propose a novel intrusion detection method, TGN-SVDD, which builds upon modern dynamic graph modelling and deep anomaly detection. We demonstrate its superiority over several baselines for realistic intrusion detection data and suggest a more challenging variant of the latter.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis</title>
<link>https://arxiv.org/abs/2508.12900</link>
<guid>https://arxiv.org/abs/2508.12900</guid>
<content:encoded><![CDATA[
arXiv:2508.12900v1 Announce Type: cross 
Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has the potential to accelerate research through data augmentation, privacy-preserving synthesis and reducing regulator-constraints on patient data while preserving diagnostic signals. With the recent release of CT-RATE, a large-scale collection of 3D CT volumes paired with their respective clinical reports, training large text-conditioned CT volume generation models has become achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching transformer model, conditioned on clinical reports. We leverage the A-VAE from FLUX to define our latent space, and rely on the CT-Clip text encoder to encode the clinical reports. To generate consistent whole CT volumes while keeping the memory constraints tractable, we rely on a custom autoregressive approach, where the model predicts the first sequence of slices of the volume from text-only, and then relies on the previously generated sequence of slices and the text, to predict the following sequence. We evaluate our results against state-of-the-art generative CT model, and demonstrate the superiority of our approach in terms of temporal coherence, image diversity and text-image alignment, with FID, FVD, IS scores and CLIP score.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
arXiv:2508.12903v1 Announce Type: cross 
Abstract: Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip</title>
<link>https://arxiv.org/abs/2508.12910</link>
<guid>https://arxiv.org/abs/2508.12910</guid>
<content:encoded><![CDATA[
arXiv:2508.12910v1 Announce Type: cross 
Abstract: Finite State Machines (FSMs) play a critical role in implementing control logic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by hardware engineers through Verilog coding, which is often tedious and time-consuming. Recently, with the remarkable progress of Large Language Models (LLMs) in code generation, LLMs have been increasingly explored for automating Verilog code generation. However, LLM-generated Verilog code often suffers from security vulnerabilities, which is particularly concerning for security-sensitive FSM implementations. To address this issue, we propose SecFSM, a novel method that leverages a security-oriented knowledge graph to guide LLMs in generating more secure Verilog code. Specifically, we first construct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs. Subsequently, we analyze users' requirements to identify vulnerabilities and get a list of vulnerabilities in the requirements. Then, we retrieve knowledge from FSKG based on the vulnerabilities list. Finally, we construct security prompts based on the security knowledge for Verilog code generation. To evaluate SecFSM, we build a dedicated dataset collected from academic datasets, artificial datasets, papers, and industrial cases. Extensive experiments demonstrate that SecFSM outperforms state-of-the-art baselines. In particular, on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM achieves an outstanding pass rate of 21/25.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization</title>
<link>https://arxiv.org/abs/2508.12927</link>
<guid>https://arxiv.org/abs/2508.12927</guid>
<content:encoded><![CDATA[
arXiv:2508.12927v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection aims to detect defective parts of a sample by having access, during training, to a set of normal, i.e. defect-free, data. It has many applications in fields, such as industrial inspection or medical imaging, where acquiring labels is costly or when we want to avoid introducing biases in the type of anomalies that can be spotted. In this work, we propose a novel UAD method based on prototype learning and introduce a metric to compare a structured set of embeddings that balances a feature-based cost and a spatial-based cost. We leverage this metric to learn local and global prototypes with optimal transport from latent representations extracted with a pre-trained image encoder. We demonstrate that our approach can enforce a structural constraint when learning the prototypes, allowing to capture the underlying organization of the normal samples, thus improving the detection of incoherencies in images. Our model achieves performance that is on par with strong baselines on two reference benchmarks for anomaly detection on industrial images. The code is available at https://github.com/robintrmbtt/pradot.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory</title>
<link>https://arxiv.org/abs/2508.12932</link>
<guid>https://arxiv.org/abs/2508.12932</guid>
<content:encoded><![CDATA[
arXiv:2508.12932v1 Announce Type: cross 
Abstract: In incremental learning, enhancing the generality of knowledge is crucial for adapting to dynamic data inputs. It can develop generalized representations or more balanced decision boundaries, preventing the degradation of long-term knowledge over time and thus mitigating catastrophic forgetting. Some emerging incremental learning methods adopt an encoder-decoder architecture and have achieved promising results. In the encoder-decoder achitecture, improving the generalization capabilities of both the encoder and decoder is critical, as it helps preserve previously learned knowledge while ensuring adaptability and robustness to new, diverse data inputs. However, many existing continual methods focus solely on enhancing one of the two components, which limits their effectiveness in mitigating catastrophic forgetting. And these methods perform even worse in small-memory scenarios, where only a limited number of historical samples can be stored. To mitigate this limitation, we introduces SEDEG, a two-stage training framework for vision transformers (ViT), focusing on sequentially improving the generality of both Decoder and Encoder. Initially, SEDEG trains an ensembled encoder through feature boosting to learn generalized representations, which subsequently enhance the decoder's generality and balance the classifier. The next stage involves using knowledge distillation (KD) strategies to compress the ensembled encoder and develop a new, more generalized encoder. This involves using a balanced KD approach and feature KD for effective knowledge transfer. Extensive experiments on three benchmark datasets show SEDEG's superior performance, and ablation studies confirm the efficacy of its components. The code is available at https://github.com/ShaolingPu/CIL.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation</title>
<link>https://arxiv.org/abs/2508.12962</link>
<guid>https://arxiv.org/abs/2508.12962</guid>
<content:encoded><![CDATA[
arXiv:2508.12962v1 Announce Type: cross 
Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging modality in dentistry, enabling 3D visualization of teeth and surrounding structures for diagnosis and treatment planning. Automated segmentation of dental structures in CBCT can efficiently assist in identifying pathology (e.g., pulpal or periapical lesions) and facilitate radiation therapy planning in head and neck cancer patients. We describe the DLaBella29 team's approach for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg framework with a 3D SegResNet architecture, trained on a subset of the ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key preprocessing steps included image resampling to 0.6 mm isotropic resolution and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE on the 5-fold predictions to infer a Phase 1 segmentation and then conducted tight cropping around the easily segmented Phase 1 mandible to perform Phase 2 segmentation on the smaller nerve structures. Our method achieved an average Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This paper details the clinical context, data preparation, model development, results of our approach, and discusses the relevance of automated dental segmentation for improving patient care in radiation oncology.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression</title>
<link>https://arxiv.org/abs/2508.12984</link>
<guid>https://arxiv.org/abs/2508.12984</guid>
<content:encoded><![CDATA[
arXiv:2508.12984v1 Announce Type: cross 
Abstract: The increasing complexity of neural networks poses a significant barrier to the deployment of distributed machine learning (ML) on resource-constrained devices, such as federated learning (FL). Split learning (SL) offers a promising solution by offloading the primary computing load from edge devices to a server via model partitioning. However, as the number of participating devices increases, the transmission of excessive smashed data (i.e., activations and gradients) becomes a major bottleneck for SL, slowing down the model training. To tackle this challenge, we propose a communication-efficient SL framework, named SL-ACC, which comprises two key components: adaptive channel importance identification (ACII) and channel grouping compression (CGC). ACII first identifies the contribution of each channel in the smashed data to model training using Shannon entropy. Following this, CGC groups the channels based on their entropy and performs group-wise adaptive compression to shrink the transmission volume without compromising training accuracy. Extensive experiments across various datasets validate that our proposed SL-ACC framework takes considerably less time to achieve a target accuracy than state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair</title>
<link>https://arxiv.org/abs/2508.12996</link>
<guid>https://arxiv.org/abs/2508.12996</guid>
<content:encoded><![CDATA[
arXiv:2508.12996v1 Announce Type: cross 
Abstract: Transformer neural networks are increasingly used for physics-based problems. In data-driven PDE surrogates, training samples from varying boundary and initial conditions can cause erratic losses and spiky gradients; in physics-informed neural networks (PINNs), stiff composite losses amplify this effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed second-moment discount beta2 is replaced by a layer-wise dynamic value driven by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an exponential moving average (EMA) of past norms, squashed to the interval [0,1). Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max. Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio), adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'', ``exact'). With all features off and bias_correction=``none'', the method is exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about 38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller variance. The method remains drop-in, with runtime overhead comparable to Adam in testbeds A-C and within single-digit percent in testbed D. It preserves Adam-style convergence guarantees while improving robustness under spiky gradients.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health</title>
<link>https://arxiv.org/abs/2508.12998</link>
<guid>https://arxiv.org/abs/2508.12998</guid>
<content:encoded><![CDATA[
arXiv:2508.12998v1 Announce Type: cross 
Abstract: Urban greenery is often linked to better health, yet findings from past research have been inconsistent. One reason is that official greenery metrics measure the amount or nearness of greenery but ignore how often people actually may potentially see or use it in daily life. To address this gap, we introduced a new classification that separates on-road greenery, which people see while walking through streets, from off-road greenery, which requires planned visits. We did so by combining aerial imagery of Greater London and greenery data from OpenStreetMap with quantified greenery from over 100,000 Google Street View images and accessibility estimates based on 160,000 road segments. We linked these measures to 7.45 billion medical prescriptions issued by the National Health Service and processed through our methodology. These prescriptions cover five conditions: diabetes, hypertension, asthma, depression, and anxiety, as well as opioid use. As hypothesized, we found that green on-road was more strongly linked to better health than four widely used official measures. For example, hypertension prescriptions dropped by 3.68% in wards with on-road greenery above the median citywide level compared to those below it. If all below-median wards reached the citywide median in on-road greenery, prescription costs could fall by up to {\pounds}3.15 million each year. These results suggest that greenery seen in daily life may be more relevant than public yet secluded greenery, and that official metrics commonly used in the literature have important limitations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks</title>
<link>https://arxiv.org/abs/2508.13030</link>
<guid>https://arxiv.org/abs/2508.13030</guid>
<content:encoded><![CDATA[
arXiv:2508.13030v1 Announce Type: cross 
Abstract: Cyberattacks are increasing, and securing against such threats is costing industries billions of dollars annually. Threat Modeling, that is, comprehending the consequences of these attacks, can provide critical support to cybersecurity professionals, enabling them to take timely action and allocate resources that could be used elsewhere. Cybersecurity is heavily dependent on threat modeling, as it assists security experts in assessing and mitigating risks related to identifying vulnerabilities and threats. Recently, there has been a pressing need for automated methods to assess attack descriptions and forecast the future consequences of the increasing complexity of cyberattacks. This study examines how Natural Language Processing (NLP) and deep learning can be applied to analyze the potential impact of cyberattacks by leveraging textual descriptions from the MITRE Common Weakness Enumeration (CWE) database. We emphasize classifying attack consequences into five principal categories: Availability, Access Control, Confidentiality, Integrity, and Other. This paper investigates the use of Bidirectional Encoder Representations from Transformers (BERT) in combination with Hierarchical Attention Networks (HANs) for Multi-label classification, evaluating their performance in comparison with conventional CNN and LSTM-based models. Experimental findings show that BERT achieves an overall accuracy of $0.972$, far higher than conventional deep learning models in multi-label classification. HAN outperforms baseline forms of CNN and LSTM-based models on specific cybersecurity labels. However, BERT consistently achieves better precision and recall, making it more suitable for predicting the consequences of a cyberattack.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title>
<link>https://arxiv.org/abs/2508.13037</link>
<guid>https://arxiv.org/abs/2508.13037</guid>
<content:encoded><![CDATA[
arXiv:2508.13037v1 Announce Type: cross 
Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using AI for User Representation: An Analysis of 83 Persona Prompts</title>
<link>https://arxiv.org/abs/2508.13047</link>
<guid>https://arxiv.org/abs/2508.13047</guid>
<content:encoded><![CDATA[
arXiv:2508.13047v1 Announce Type: cross 
Abstract: We analyzed 83 persona prompts from 27 research articles that used large language models (LLMs) to generate user personas. Findings show that the prompts predominantly generate single personas. Several prompts express a desire for short or concise persona descriptions, which deviates from the tradition of creating rich, informative, and rounded persona profiles. Text is the most common format for generated persona attributes, followed by numbers. Text and numbers are often generated together, and demographic attributes are included in nearly all generated personas. Researchers use up to 12 prompts in a single study, though most research uses a small number of prompts. Comparison and testing multiple LLMs is rare. More than half of the prompts require the persona output in a structured format, such as JSON, and 74% of the prompts insert data or dynamic variables. We discuss the implications of increased use of computational personas for user representation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</title>
<link>https://arxiv.org/abs/2508.13049</link>
<guid>https://arxiv.org/abs/2508.13049</guid>
<content:encoded><![CDATA[
arXiv:2508.13049v1 Announce Type: cross 
Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural Processing Engine, designed for extended reality (XR) perception workloads like visual inertial odometry (VIO), object classification, and eye gaze extraction. XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1) formats, with layer adaptive hybrid-algorithmic implementation supporting ultra-low bit precision to significantly reduce memory bandwidth requirements, and accompanied by quantization-aware training for minimal accuracy loss. The proposed Reconfigurable Mantissa Multiplication and Exponent processing Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted by selective power gating to reduce energy consumption, providing 2.85x improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of 1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm, reducing 42% area, 38% power compared to the best of state-of-the-art MAC approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x better energy efficiency compared to SoTA accelerators on VCU129. The proposed co-processor provides 23% better energy efficiency and 4% better compute density for VIO workloads. XR-NPE establishes itself as a scalable, precision-adaptive compute engine for future resource-constrained XR devices. The complete set for codes for results reproducibility are released publicly, enabling designers and researchers to readily adopt and build upon them. https://github.com/mukullokhande99/XR-NPE.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models</title>
<link>https://arxiv.org/abs/2508.13057</link>
<guid>https://arxiv.org/abs/2508.13057</guid>
<content:encoded><![CDATA[
arXiv:2508.13057v1 Announce Type: cross 
Abstract: Demand forecasting is essential for strategic planning in competitive environments, enabling resource optimization and improved responsiveness to market dynamics. However, multivariate time series modeling faces challenges due to data complexity, uncertainty, and frequent regime shifts. Traditional evaluation metrics can introduce biases and limit generalization. This work compares two custom evaluation functions: FMAE (Focused Mean Absolute Error), focused on minimizing absolute errors, and HEF (Hierarchical Evaluation Function), designed to weight global metrics and penalize large deviations. Experiments were conducted under different data splits (91:9, 80:20, 70:30) using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative accuracy, robustness, and computational efficiency. Results show that HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE), enhancing model robustness and explanatory power. These findings were confirmed via visualizations and statistical tests. Conversely, FMAE offers advantages in local metrics (MAE, MASE) and execution time, making it suitable for short-term scenarios. The study highlights a methodological trade-off: HEF is ideal for strategic planning, while FMAE is better suited for operational efficiency. A replicable framework is proposed for optimizing predictive models in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title>
<link>https://arxiv.org/abs/2508.13070</link>
<guid>https://arxiv.org/abs/2508.13070</guid>
<content:encoded><![CDATA[
arXiv:2508.13070v1 Announce Type: cross 
Abstract: Modern causal language models, followed by rapid developments in discrete diffusion models, can now produce a wide variety of interesting and useful content. However, these families of models are predominantly trained to output tokens with a fixed (left-to-right) or random order, which may deviate from the logical order in which tokens are generated originally. In this paper, we observe that current causal and diffusion models encounter difficulties in problems that require adaptive token generation orders to solve tractably, which we characterize with the $\mathcal{V}$-information framework. Motivated by this, we propose Reinforced Context Order Recovery (ReCOR), a reinforcement-learning-based framework to extract adaptive, data-dependent token generation orders from text data without annotations. Self-supervised by token prediction statistics, ReCOR estimates the hardness of predicting every unfilled token and adaptively selects the next token during both training and inference. Experiments on challenging reasoning and planning datasets demonstrate the superior performance of ReCOR compared with baselines, sometimes outperforming oracle models supervised with the ground-truth order.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion</title>
<link>https://arxiv.org/abs/2508.13077</link>
<guid>https://arxiv.org/abs/2508.13077</guid>
<content:encoded><![CDATA[
arXiv:2508.13077v1 Announce Type: cross 
Abstract: Deep diffusion models excel at realistic image synthesis but demand large training sets-an obstacle in data-scarce domains like transesophageal echocardiography (TEE). While synthetic augmentation has boosted performance in transthoracic echo (TTE), TEE remains critically underrepresented, limiting the reach of deep learning in this high-impact modality.
  We address this gap by adapting a TTE-trained, mask-conditioned diffusion backbone to TEE with only a limited number of new cases and adapters as small as $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$, a lightweight remapping layer that aligns novel mask formats with the pretrained model's conditioning channels. This design lets users adapt models to new datasets with a different set of anatomical structures to the base model's original set.
  Through a targeted adaptation strategy, we find that adapting only MLP layers suffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real TEE frames with our synthetic echoes improves the dice score on a multiclass segmentation task, particularly boosting performance on underrepresented right-heart structures. Our results demonstrate that (1) semantically controlled TEE images can be generated with low overhead, (2) MaskR$^2$ effectively transforms unseen mask formats into compatible formats without damaging downstream task performance, and (3) our method generates images that are effective for improving performance on a downstream task of multiclass segmentation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog</title>
<link>https://arxiv.org/abs/2508.13092</link>
<guid>https://arxiv.org/abs/2508.13092</guid>
<content:encoded><![CDATA[
arXiv:2508.13092v1 Announce Type: cross 
Abstract: Timely detection of hardware vulnerabilities during the early design stage is critical for reducing remediation costs. Existing early detection techniques often require specialized security expertise, limiting their usability. Recent efforts have explored the use of large language models (LLMs) for Verilog vulnerability detection. However, LLMs struggle to capture the structure in Verilog code, resulting in inconsistent detection results. To this end, we propose VerilogLAVD, the first LLM-aided graph traversal rule generation approach for Verilog vulnerability detection. Our approach introduces the Verilog Property Graph (VeriPG), a unified representation of Verilog code. It combines syntactic features extracted from the abstract syntax tree (AST) with semantic information derived from control flow and data dependency graphs. We leverage LLMs to generate VeriPG-based detection rules from Common Weakness Enumeration (CWE) descriptions. These rules guide the rule executor that traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we build a dataset collected from open-source repositories and synthesized data. In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types, VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27, respectively.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Representations for Temporal Reasoning</title>
<link>https://arxiv.org/abs/2508.13113</link>
<guid>https://arxiv.org/abs/2508.13113</guid>
<content:encoded><![CDATA[
arXiv:2508.13113v1 Announce Type: cross 
Abstract: In classical AI, perception relies on learning state-based representations, while planning, which can be thought of as temporal reasoning over action sequences, is typically achieved through search. We study whether such reasoning can instead emerge from representations that capture both perceptual and temporal structure. We show that standard temporal contrastive learning, despite its popularity, often fails to capture temporal structure due to its reliance on spurious features. To address this, we introduce Combinatorial Representations for Temporal Reasoning (CRTR), a method that uses a negative sampling scheme to provably remove these spurious features and facilitate temporal reasoning. CRTR achieves strong results on domains with complex temporal structure, such as Sokoban and Rubik's Cube. In particular, for the Rubik's Cube, CRTR learns representations that generalize across all initial states and allow it to solve the puzzle using fewer search steps than BestFS, though with longer solutions. To our knowledge, this is the first method that efficiently solves arbitrary Cube states using only learned representations, without relying on an external search algorithm.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries</title>
<link>https://arxiv.org/abs/2508.13124</link>
<guid>https://arxiv.org/abs/2508.13124</guid>
<content:encoded><![CDATA[
arXiv:2508.13124v1 Announce Type: cross 
Abstract: Abstractive summarization is a core application in contact centers, where Large Language Models (LLMs) generate millions of summaries of call transcripts daily. Despite their apparent quality, it remains unclear whether LLMs systematically under- or over-attend to specific aspects of the transcript, potentially introducing biases in the generated summary. While prior work has examined social and positional biases, the specific forms of bias pertinent to contact center operations - which we term Operational Bias - have remained unexplored. To address this gap, we introduce BlindSpot, a framework built upon a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic) for the identification and quantification of these biases. BlindSpot leverages an LLM as a zero-shot classifier to derive categorical distributions for each bias dimension in a pair of transcript and its summary. The bias is then quantified using two metrics: Fidelity Gap (the JS Divergence between distributions) and Coverage (the percentage of source labels omitted). Using BlindSpot, we conducted an empirical study with 2500 real call transcripts and their summaries generated by 20 LLMs of varying scales and families (e.g., GPT, Llama, Claude). Our analysis reveals that biases are systemic and present across all evaluated models, regardless of size or family.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</title>
<link>https://arxiv.org/abs/2508.13152</link>
<guid>https://arxiv.org/abs/2508.13152</guid>
<content:encoded><![CDATA[
arXiv:2508.13152v1 Announce Type: cross 
Abstract: Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space</title>
<link>https://arxiv.org/abs/1909.08191</link>
<guid>https://arxiv.org/abs/1909.08191</guid>
<content:encoded><![CDATA[
arXiv:1909.08191v3 Announce Type: replace 
Abstract: The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unravelling Responsibility for AI</title>
<link>https://arxiv.org/abs/2308.02608</link>
<guid>https://arxiv.org/abs/2308.02608</guid>
<content:encoded><![CDATA[
arXiv:2308.02608v4 Announce Type: replace 
Abstract: It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. This is important to achieve justice and compensation for victims of AI harms, and to inform policy and engineering practice. But without a clear, thorough understanding of what "responsibility" means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. Furthermore, AI-enabled systems exist within a wider ecosystem of actors, decisions, and governance structures, giving rise to complex networks of responsibility relations. To address these issues, this paper presents a conceptual framework of responsibility, accompanied with a graphical notation and general methodology for visualising these responsibility networks and for tracing different responsibility attributions for AI. Taking the three-part formulation "Actor A is responsible for Occurrence O," the framework unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, senses in which they are responsible, and aspects of events they are responsible for. The notation allows these permutations to be represented graphically. The methodology enables users to apply the framework to specific scenarios. The aim is to offer a foundation to support stakeholders from diverse disciplinary backgrounds to discuss and address complex responsibility questions in hypothesised and real-world cases involving AI. The work is illustrated by application to a fictitious scenario of a fatal collision between a crewless, AI-enabled maritime vessel in autonomous mode and a traditional, crewed vessel at sea.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCL-ViT: Task-Aware Attention Tuning for Continual Learning</title>
<link>https://arxiv.org/abs/2412.02509</link>
<guid>https://arxiv.org/abs/2412.02509</guid>
<content:encoded><![CDATA[
arXiv:2412.02509v3 Announce Type: replace 
Abstract: Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones. However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand. This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task. The FCL-ViT operates in two Phases. In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image. In phase 2, task-specific image features are generated that leverage dynamic attention. To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABs attention, respectively. The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Encoding Argumentation Frameworks to Propositional Logic Systems</title>
<link>https://arxiv.org/abs/2503.07351</link>
<guid>https://arxiv.org/abs/2503.07351</guid>
<content:encoded><![CDATA[
arXiv:2503.07351v2 Announce Type: replace 
Abstract: This paper generalizes the encoding of argumentation frameworks beyond the classical 2-valued propositional logic system ($PL_2$) to 3-valued propositional logic systems ($PL_3$s) and fuzzy propositional logic systems ($PL_{[0,1]}s$), employing two key encodings: normal encoding ($ec_1$) and regular encoding ($ec_2$). Specifically, via $ec_1$ and $ec_2$, we establish model relationships between Dung's classical semantics (stable and complete semantics) and the encoded semantics associated with Kleene's $PL_3$ and {\L}ukasiewicz's $PL_3$. Through $ec_1$, we also explore connections between Gabbay's real equational semantics and the encoded semantics of $PL_{[0,1]}s$, including showing that Gabbay's $Eq_{\text{max}}^R$ and $Eq_{\text{inverse}}^R$ correspond to the fuzzy encoded semantics of $PL_{[0,1]}^G$ and $PL_{[0,1]}^P$ respectively. Additionally, we propose a new fuzzy encoded semantics ($Eq^L$) associated with {\L}ukasiewicz's $PL_{[0,1]}$ and investigate interactions between complete semantics and fuzzy encoded semantics. This work strengthens the links between argumentation frameworks and propositional logic systems, providing a framework for constructing new argumentation semantics.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Prior Data Matter? Exploring Joint Training in the Context of Few-Shot Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2503.10003</link>
<guid>https://arxiv.org/abs/2503.10003</guid>
<content:encoded><![CDATA[
arXiv:2503.10003v2 Announce Type: replace 
Abstract: Class-incremental learning (CIL) aims to adapt to continuously emerging new classes while preserving knowledge of previously learned ones. Few-shot class-incremental learning (FSCIL) presents a greater challenge that requires the model to learn new classes from only a limited number of samples per class. While incremental learning typically assumes restricted access to past data, it often remains available in many real-world scenarios. This raises a practical question: should one retrain the model on the full dataset (i.e., joint training), or continue updating it solely with new data? In CIL, joint training is considered an ideal benchmark that provides a reference for evaluating the trade-offs between performance and computational cost. However, in FSCIL, joint training becomes less reliable due to severe imbalance between base and incremental classes. This results in the absence of a practical baseline, making it unclear which strategy is preferable for practitioners. To this end, we revisit joint training in the context of FSCIL by incorporating imbalance mitigation techniques, and suggest a new imbalance-aware joint training benchmark for FSCIL. We then conduct extensive comparisons between this benchmark and FSCIL methods to analyze which approach is most suitable when prior data is accessible. Our analysis offers realistic insights and guidance for selecting training strategies in real-world FSCIL scenarios. Code is available at: https://github.com/shiwonkim/Joint_FSCIL
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning</title>
<link>https://arxiv.org/abs/2504.01911</link>
<guid>https://arxiv.org/abs/2504.01911</guid>
<content:encoded><![CDATA[
arXiv:2504.01911v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contemplative Artificial Intelligence</title>
<link>https://arxiv.org/abs/2504.15125</link>
<guid>https://arxiv.org/abs/2504.15125</guid>
<content:encoded><![CDATA[
arXiv:2504.15125v3 Announce Type: replace 
Abstract: As artificial intelligence (AI) improves, traditional alignment strategies may falter in the face of unpredictable self-improvement, hidden subgoals, and the sheer complexity of intelligent systems. Inspired by contemplative wisdom traditions, we show how four axiomatic principles can instil a resilient Wise World Model in AI systems. First, mindfulness enables self-monitoring and recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal fixation and relaxes rigid priors. Third, non-duality dissolves adversarial self-other boundaries. Fourth, boundless care motivates the universal reduction of suffering. We find that prompting AI to reflect on these principles improves performance on the AILuminate Benchmark (d=.96) and boosts cooperation and joint-reward on the Prisoner's Dilemma task (d=7+). We offer detailed implementation strategies at the level of architectures, constitutions, and reinforcement on chain-of-thought. For future systems, active inference may offer the self-organizing and dynamic coupling capabilities needed to enact Contemplative AI in embodied agents.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Parallel Reasoning with Language Models</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
arXiv:2504.15466v2 Announce Type: replace 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models</title>
<link>https://arxiv.org/abs/2504.16635</link>
<guid>https://arxiv.org/abs/2504.16635</guid>
<content:encoded><![CDATA[
arXiv:2504.16635v2 Announce Type: replace 
Abstract: In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Reinforcement Learning Agents Using World Models</title>
<link>https://arxiv.org/abs/2505.08073</link>
<guid>https://arxiv.org/abs/2505.08073</guid>
<content:encoded><![CDATA[
arXiv:2505.08073v2 Announce Type: replace 
Abstract: Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios</title>
<link>https://arxiv.org/abs/2505.11247</link>
<guid>https://arxiv.org/abs/2505.11247</guid>
<content:encoded><![CDATA[
arXiv:2505.11247v2 Announce Type: replace 
Abstract: Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIK: Mapping to Analogous Goals via Imagination-enabled Knowledge Transfer</title>
<link>https://arxiv.org/abs/2506.01623</link>
<guid>https://arxiv.org/abs/2506.01623</guid>
<content:encoded><![CDATA[
arXiv:2506.01623v3 Announce Type: replace 
Abstract: Humans excel at analogical reasoning - applying knowledge from one task to a related one with minimal relearning. In contrast, reinforcement learning (RL) agents typically require extensive retraining even when new tasks share structural similarities with previously learned ones. In this work, we propose MAGIK, a novel framework that enables RL agents to transfer knowledge to analogous tasks without interacting with the target environment. Our approach leverages an imagination mechanism to map entities in the target task to their analogues in the source domain, allowing the agent to reuse its original policy. Experiments on custom MiniGrid and MuJoCo tasks show that MAGIK achieves effective zero-shot transfer using only a small number of human-labelled examples. We compare our approach to related baselines and highlight how it offers a novel and effective mechanism for knowledge transfer via imagination-based analogy mapping.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan</title>
<link>https://arxiv.org/abs/2506.02720</link>
<guid>https://arxiv.org/abs/2506.02720</guid>
<content:encoded><![CDATA[
arXiv:2506.02720v2 Announce Type: replace 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models</title>
<link>https://arxiv.org/abs/2506.14092</link>
<guid>https://arxiv.org/abs/2506.14092</guid>
<content:encoded><![CDATA[
arXiv:2506.14092v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed in decision-support systems for high-stakes domains such as hiring and university admissions, where choices often involve selecting among competing alternatives. While prior work has noted position order biases in LLM-driven comparisons, these biases have not been systematically analyzed or linked to underlying preference structures. We present the first comprehensive study of position biases across multiple LLMs and two distinct domains: resume comparisons, representing a realistic high-stakes context, and color selection, which isolates position effects by removing confounding factors. We find strong and consistent order effects, including a quality-dependent shift: when all options are high quality, models favor the first option, but when quality is lower, they favor later options. We also identify two previously undocumented biases in both human and machine decision-making: a centrality bias (favoring the middle position in triplewise comparisons) and a name bias, where certain names are favored despite controlling for demographic signals. To separate superficial tie-breaking from genuine distortions of judgment, we extend the rational choice framework to classify pairwise preferences as robust, fragile, or indifferent. Using this framework, we show that order effects can lead models to select strictly inferior options, and that position biases are typically stronger than gender biases. These results indicate that LLMs exhibit distinct failure modes not documented in human decision-making. We also propose targeted mitigation strategies, including a novel use of the temperature parameter, to recover underlying preferences when order effects distort model behavior.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards</title>
<link>https://arxiv.org/abs/2506.20332</link>
<guid>https://arxiv.org/abs/2506.20332</guid>
<content:encoded><![CDATA[
arXiv:2506.20332v3 Announce Type: replace 
Abstract: Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agent's dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: https://mobile-r1.github.io/Mobile-R1/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opus: A Prompt Intention Framework for Complex Workflow Generation</title>
<link>https://arxiv.org/abs/2507.11288</link>
<guid>https://arxiv.org/abs/2507.11288</guid>
<content:encoded><![CDATA[
arXiv:2507.11288v2 Announce Type: replace 
Abstract: This paper introduces the Opus Prompt Intention Framework, designed to improve complex Workflow Generation with instruction-tuned Large Language Models (LLMs). We propose an intermediate Intention Capture layer between user queries and Workflow Generation, implementing the Opus Workflow Intention Framework, which consists of extracting Workflow Signals from user queries, interpreting them into structured Workflow Intention objects, and generating Workflows based on these Intentions. Our results show that this layer enables LLMs to produce logical and meaningful outputs that scale reliably as query complexity increases. On a synthetic benchmark of 1,000 multi-intent query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to Workflow Generation yields consistent improvements in semantic Workflow similarity metrics. In this paper, we introduce the Opus Prompt Intention Framework by applying the concepts of Workflow Signal and Workflow Intention to LLM-driven Workflow Generation. We present a reproducible, customizable LLM-based Intention Capture system to extract Workflow Signals and Workflow Intentions from user queries. Finally, we provide empirical evidence that the proposed system significantly improves Workflow Generation quality compared to direct generation from user queries, particularly in cases of Mixed Intention Elicitation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis</title>
<link>https://arxiv.org/abs/2507.14899</link>
<guid>https://arxiv.org/abs/2507.14899</guid>
<content:encoded><![CDATA[
arXiv:2507.14899v2 Announce Type: replace 
Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address these shortcomings, this paper proposes InsightX Agent, a novel LMM-based agentic framework designed to deliver reliable, interpretable, and interactive X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect region proposals for multi-scale feature maps and sparsifies them through Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in X-ray images while maintaining computational efficiency. The EGR tool guides the LMM agent through a chain-of-thought-inspired review process, incorporating context assessment, individual defect analysis, false positive elimination, confidence recalibration and quality assurance to validate and refine the SDMSD's initial proposals. By strategically employing and intelligently using tools, InsightX Agent moves beyond passive data processing to active reasoning, enhancing diagnostic reliability and providing interpretations that integrate diverse information sources. Experimental evaluations on the GDXray+ dataset demonstrate that InsightX Agent not only achieves a high object detection F1-score of 96.35% but also offers significantly improved interpretability and trustworthiness in its analyses, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2507.20541</link>
<guid>https://arxiv.org/abs/2507.20541</guid>
<content:encoded><![CDATA[
arXiv:2507.20541v3 Announce Type: replace 
Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain</title>
<link>https://arxiv.org/abs/2507.21976</link>
<guid>https://arxiv.org/abs/2507.21976</guid>
<content:encoded><![CDATA[
arXiv:2507.21976v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution</title>
<link>https://arxiv.org/abs/2508.06225</link>
<guid>https://arxiv.org/abs/2508.06225</guid>
<content:encoded><![CDATA[
arXiv:2508.06225v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used as automated judges, where practical value depends on both accuracy and trustworthy, risk-aware judgments. Existing approaches predominantly focus on accuracy, overlooking the necessity of well-calibrated confidence, which is vital for adaptive and reliable evaluation pipelines. In this work, we advocate a shift from accuracy-centric evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing the necessity of well-calibrated confidence for trustworthy and adaptive evaluation. We systematically identify the Overconfidence Phenomenon in current LLM-as-a-Judges, where predicted confidence significantly overstates actual correctness, undermining reliability in practical deployment. To quantify this phenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy alignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that transforms LLMs into reliable, risk-aware evaluators. Extensive experiments demonstrate that our approach substantially improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Unseen: A Comprehensive Survey on Explainable Anomaly Detection in Images and Videos</title>
<link>https://arxiv.org/abs/2302.06670</link>
<guid>https://arxiv.org/abs/2302.06670</guid>
<content:encoded><![CDATA[
arXiv:2302.06670v4 Announce Type: replace-cross 
Abstract: Anomaly detection and localization in visual data, including images and videos, are crucial in machine learning and real-world applications. Despite rapid advancements in visual anomaly detection (VAD), interpreting these often black-box models and explaining why specific instances are flagged as anomalous remains challenging. This paper provides the first comprehensive survey focused specifically on explainable 2D visual anomaly detection (X-VAD), covering methods for both images (IAD) and videos (VAD). We first introduce the background of IAD and VAD. Then, as the core contribution, we present a thorough literature review of explainable methods, categorized by their underlying techniques (e.g., attention-based, generative model-based, reasoning-based, foundation model-based). We analyze the commonalities and differences in applying these methods across image and video modalities, highlighting modality-specific challenges and opportunities for explainability. Additionally, we summarize relevant datasets and evaluation metrics, discussing both standard performance metrics and emerging approaches for assessing explanation quality (e.g., faithfulness, stability). Finally, we discuss promising future directions and open problems, including quantifying explanation quality, explaining diverse AD paradigms (SSL, zero-shot), enhancing context-awareness, leveraging foundation models responsibly, and addressing real-world constraints like efficiency and robustness. A curated collection of related resources is available at https://github.com/wyzjack/Awesome-XAD.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable and Robust AI in EEG Systems: A Survey</title>
<link>https://arxiv.org/abs/2304.10755</link>
<guid>https://arxiv.org/abs/2304.10755</guid>
<content:encoded><![CDATA[
arXiv:2304.10755v4 Announce Type: replace-cross 
Abstract: The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it into three types: backpropagation, perturbation, and inherently interpretable methods. Then we classify the robustness mechanisms into four classes: noise and artifacts, human variability, data acquisition instability, and adversarial attacks. Finally, we identify several critical and unresolved challenges for interpretable and robust AI in EEG systems and further discuss their future directions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeFT: Negative Feedback Training to Improve Robustness of Compute-In-Memory DNN Accelerators</title>
<link>https://arxiv.org/abs/2305.14561</link>
<guid>https://arxiv.org/abs/2305.14561</guid>
<content:encoded><![CDATA[
arXiv:2305.14561v5 Announce Type: replace-cross 
Abstract: Compute-in-memory accelerators built upon non-volatile memory devices excel in energy efficiency and latency when performing deep neural network (DNN) inference, thanks to their in-situ data processing capability. However, the stochastic nature and intrinsic variations of non-volatile memory devices often result in performance degradation during DNN inference. Introducing these non-ideal device behaviors in DNN training enhances robustness, but drawbacks include limited accuracy improvement, reduced prediction confidence, and convergence issues. This arises from a mismatch between the deterministic training and non-deterministic device variations, as such training, though considering variations, relies solely on the model's final output. In this work, inspired by control theory, we propose Negative Feedback Training (NeFT), a novel concept supported by theoretical analysis, to more effectively capture the multi-scale noisy information throughout the network. We instantiate this concept with two specific instances, oriented variational forward (OVF) and intermediate representation snapshot (IRS). Based on device variation models extracted from measured data, extensive experiments show that our NeFT outperforms existing state-of-the-art methods with up to a 45.08% improvement in inference accuracy while reducing epistemic uncertainty, boosting output confidence, and improving convergence probability. These results underline the generality and practicality of our NeFT framework for increasing the robustness of DNNs against device variations. The source code for these two instances is available at https://github.com/YifanQin-ND/NeFT_CIM
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control</title>
<link>https://arxiv.org/abs/2307.01312</link>
<guid>https://arxiv.org/abs/2307.01312</guid>
<content:encoded><![CDATA[
arXiv:2307.01312v2 Announce Type: replace-cross 
Abstract: Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagation (BP) algorithm. This method is online, able to tackle disturbance, and fast in training. In addition to robustness to mass uncertainty and wind gust disturbance, results showed that the proposed method had a better performance when compared to a PID controller with constant gains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2307.01316</link>
<guid>https://arxiv.org/abs/2307.01316</guid>
<content:encoded><![CDATA[
arXiv:2307.01316v3 Announce Type: replace-cross 
Abstract: The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logic (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logic (knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in a highway driving scenario using the HighD dataset and demonstrated that our method successfully avoids unsafe actions during both the training and testing phases. Furthermore, our results indicate that DRLSL achieves faster convergence during training and exhibits better generalizability to new highway driving scenarios compared to traditional DRL methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Interaction Paradigm for Complex EDA Software Leveraging GPT</title>
<link>https://arxiv.org/abs/2307.14740</link>
<guid>https://arxiv.org/abs/2307.14740</guid>
<content:encoded><![CDATA[
arXiv:2307.14740v2 Announce Type: replace-cross 
Abstract: Electronic Design Automation (EDA) tools such as KiCad offer powerful functionalities but remain difficult to use, particularly for beginners, due to their steep learning curves and fragmented documentation. To address this challenge, we present SmartonAI, an AI-assisted interaction system that integrates large language models into the EDA workflow, enabling natural language communication, intelligent task decomposition, and contextual plugin execution. SmartonAI consists of two main components: a Chat Plugin that breaks down user instructions into subtasks and retrieves tailored documentation, and a OneCommandLine Plugin that recommends and executes relevant plugins based on user intent. The system supports multilingual interaction and adapts to user feedback through incremental learning. Preliminary results suggest that SmartonAI significantly reduces onboarding time and enhances productivity, representing a promising step toward generalizable AI-assisted interaction paradigms for complex software systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can replicate cross-cultural differences in personality</title>
<link>https://arxiv.org/abs/2310.10679</link>
<guid>https://arxiv.org/abs/2310.10679</guid>
<content:encoded><![CDATA[
arXiv:2310.10679v4 Announce Type: replace-cross 
Abstract: We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays</title>
<link>https://arxiv.org/abs/2310.17176</link>
<guid>https://arxiv.org/abs/2310.17176</guid>
<content:encoded><![CDATA[
arXiv:2310.17176v2 Announce Type: replace-cross 
Abstract: Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep-learning techniques. We built an end-to-end instance segmentation network that uses an encoder-decoder architecture reinforced with grid-aware attention gates along the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and a Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain a Rotated IoU (RIoU) score of 82.82%. We also conduct detailed analyses of individual tooth labels and categorical performance, shedding light on strengths and weaknesses. The proposed model's accuracy and versatility offer promising prospects for improving dental diagnoses, treatment planning, and personalized healthcare in the oral domain. Our generated OBB coordinates and code are available at https://github.com/mrinal054/Instance/teeth/segmentation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models</title>
<link>https://arxiv.org/abs/2311.01301</link>
<guid>https://arxiv.org/abs/2311.01301</guid>
<content:encoded><![CDATA[
arXiv:2311.01301v3 Announce Type: replace-cross 
Abstract: The rapid digitization of real-world data presents an unprecedented opportunity to optimize healthcare delivery and accelerate biomedical discovery. However, these data are often found in unstructured forms such as clinical notes in electronic medical records (EMRs), and is typically plagued by confounders, making it challenging to generate robust real-world evidence (RWE). Therefore, we present TRIALSCOPE, a framework designed to distil RWE from population level observational data at scale. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to address common confounders in treatment effect estimation. Extensive experiments were conducted on a large-scale dataset of over one million cancer patients from a single large healthcare network in the United States. TRIALSCOPE was shown to automatically curate high-quality structured patient data, expanding the dataset and incorporating key patient attributes only available in unstructured form. The framework reduces confounding in treatment effect estimation, generating comparable results to randomized controlled lung cancer trials. Additionally, we demonstrate simulations of unconducted clinical trials - including a pancreatic cancer trial with varying eligibility criteria - using a suite of validation tests to ensure robustness. Thorough ablation studies were conducted to better understand key components of TRIALSCOPE and establish best practices for RWE generation from EMRs. TRIALSCOPE was able to extract data cancer treatment data from EMRs, overcoming limitations of manual curation. We were also able to show that TRIALSCOPE could reproduce results of lung and pancreatic cancer clinical trials from the extracted real world data.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSAM: Fine-tuning SAM with Multi-Modal Prompts for Mobility Infrastructure Segmentation</title>
<link>https://arxiv.org/abs/2311.11319</link>
<guid>https://arxiv.org/abs/2311.11319</guid>
<content:encoded><![CDATA[
arXiv:2311.11319v4 Announce Type: replace-cross 
Abstract: In geographical image segmentation, performance is often constrained by the limited availability of training data and a lack of generalizability, particularly for segmenting mobility infrastructure such as roads, sidewalks, and crosswalks. Vision foundation models like the Segment Anything Model (SAM), pre-trained on millions of natural images, have demonstrated impressive zero-shot segmentation performance, providing a potential solution. However, SAM struggles with geographical images, such as aerial and satellite imagery, due to its training being confined to natural images and the narrow features and textures of these objects blending into their surroundings. To address these challenges, we propose Geographical SAM (GeoSAM), a SAM-based framework that fine-tunes SAM using automatically generated multi-modal prompts. Specifically, GeoSAM integrates point prompts from a pre-trained task-specific model as primary visual guidance, and text prompts generated by a large language model as secondary semantic guidance, enabling the model to better capture both spatial structure and contextual meaning. GeoSAM outperforms existing approaches for mobility infrastructure segmentation in both familiar and completely unseen regions by at least 5\% in mIoU, representing a significant leap in leveraging foundation models to segment mobility infrastructure, including both road and pedestrian infrastructure in geographical images. The source code can be found in this GitHub Repository: https://github.com/rafiibnsultan/GeoSAM.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v4 Announce Type: replace-cross 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods</title>
<link>https://arxiv.org/abs/2403.20150</link>
<guid>https://arxiv.org/abs/2403.20150</guid>
<content:encoded><![CDATA[
arXiv:2403.20150v4 Announce Type: replace-cross 
Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB. We have also launched an online time series leaderboard: https://decisionintelligence.github.io/OpenTS/OpenTS-Bench/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantformer: from attention to profit with a quantitative transformer trading strategy</title>
<link>https://arxiv.org/abs/2404.00424</link>
<guid>https://arxiv.org/abs/2404.00424</guid>
<content:encoded><![CDATA[
arXiv:2404.00424v3 Announce Type: replace-cross 
Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Fully capturing various market variables, including long-term information, as well as essential signals that may lead to profit remains a difficult task for learning algorithms. In order to tackle this challenge, this paper introduces quantformer, an enhanced neural network architecture based on transformer, to build investment factors. By transfer learning from sentiment analysis, quantformer not only exploits its original inherent advantages in capturing long-range dependencies and modeling complex data relationships, but is also able to solve tasks with numerical inputs and accurately forecast future returns over a given period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2023. The results of this study demonstrate the model's superior performance in predicting stock trends compared with other 100-factor-based quantitative strategies. Notably, the model's innovative use of transformer-like model to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models</title>
<link>https://arxiv.org/abs/2404.15518</link>
<guid>https://arxiv.org/abs/2404.15518</guid>
<content:encoded><![CDATA[
arXiv:2404.15518v4 Announce Type: replace-cross 
Abstract: In traditional statistical learning, data points are usually assumed to be independently and identically distributed (i.i.d.) following an unknown probability distribution. This paper presents a contrasting viewpoint, perceiving data points as interconnected and employing a Markov reward process (MRP) for data modeling. We reformulate the typical supervised learning as an on-policy policy evaluation problem within reinforcement learning (RL), introducing a generalized temporal difference (TD) learning algorithm as a resolution. Theoretically, our analysis establishes connections between the solutions of linear TD learning and ordinary least squares (OLS). Under specific conditions -- particularly when the noise is correlated -- the TD solution serves as a more effective estimator than OLS. Furthermore, we show that when our algorithm is applied with many commonly used loss functions -- such as those found in generalized linear models -- it corresponds to the application of a novel and generalized Bellman operator. We prove that this operator admits a unique fixed point, and based on this, we establish convergence guarantees for our generalized TD algorithm under linear function approximation. Empirical studies verify our theoretical results, examine the vital design of our TD algorithm and show practical utility across various datasets, encompassing tasks such as regression and image classification with deep learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models</title>
<link>https://arxiv.org/abs/2405.02162</link>
<guid>https://arxiv.org/abs/2405.02162</guid>
<content:encoded><![CDATA[
arXiv:2405.02162v4 Announce Type: replace-cross 
Abstract: In robotics and computer vision, semantic mapping remains a critical challenge for machines to comprehend complex environments. Traditional panoptic mapping approaches are constrained by fixed labels, limiting their ability to handle novel objects. We present Unified Promptable Panoptic Mapping (UPPM), which leverages foundation models for dynamic labeling without additional training. UPPM is evaluated across three comprehensive levels: Segmentation-to-Map, Map-to-Map, and Segmentation-to-Segmentation. Results demonstrate UPPM attains exceptional geometry reconstruction accuracy (0.61cm on the Flat dataset), the highest panoptic quality (0.414), and better performance compared to state-of-the-art segmentation methods. Furthermore, ablation studies validate the contributions of unified semantics, custom NMS, and blurry frame filtering, with the custom NMS improving the completion ratio by 8.27% on the Flat dataset. UPPM demonstrates effective scene reconstruction with rich semantic labeling across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Must Be Taught to Know What They Don't Know</title>
<link>https://arxiv.org/abs/2406.08391</link>
<guid>https://arxiv.org/abs/2406.08391</guid>
<content:encoded><![CDATA[
arXiv:2406.08391v3 Announce Type: replace-cross 
Abstract: When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on Effectiveness and Efficiency</title>
<link>https://arxiv.org/abs/2406.09675</link>
<guid>https://arxiv.org/abs/2406.09675</guid>
<content:encoded><![CDATA[
arXiv:2406.09675v2 Announce Type: replace-cross 
Abstract: With recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their ability to retrieve graph signals in the spectral domain. These models feature uniqueness in efficient computation as well as rich expressiveness, which stems from advanced management and profound understanding of graph data. However, few systematic studies have been conducted to assess spectral GNNs, particularly in benchmarking their efficiency, memory consumption, and effectiveness in a unified and fair manner. There is also a pressing need to select spectral models suitable for learning specific graph data and deploying them to massive web-scale graphs, which is currently constrained by the varied model designs and training settings.
  In this work, we extensively benchmark spectral GNNs with a focus on the spectral perspective, demystifying them as spectral graph filters. We analyze and categorize 35 GNNs with 27 corresponding filters, spanning diverse formulations and utilizations of the graph data. Then, we implement the filters within a unified spectral-oriented framework with dedicated graph computations and efficient training schemes. In particular, our implementation enables the deployment of spectral GNNs over million-scale graphs and various tasks with comparable performance and less overhead. Thorough experiments are conducted on the graph filters with comprehensive metrics on effectiveness and efficiency, offering novel observations and practical guidelines that are only available from our evaluations across graph scales. Different from the prevailing belief, our benchmark reveals an intricate landscape regarding the effectiveness and efficiency of spectral graph filters, demonstrating the potential to achieve desirable performance through tailored spectral manipulation of graph data.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>European Space Agency Benchmark for Anomaly Detection in Satellite Telemetry</title>
<link>https://arxiv.org/abs/2406.17826</link>
<guid>https://arxiv.org/abs/2406.17826</guid>
<content:encoded><![CDATA[
arXiv:2406.17826v2 Announce Type: replace-cross 
Abstract: Machine learning has vast potential to improve anomaly detection in satellite telemetry which is a crucial task for spacecraft operations. This potential is currently hampered by a lack of comprehensible benchmarks for multivariate time series anomaly detection, especially for the challenging case of satellite telemetry. The European Space Agency Benchmark for Anomaly Detection in Satellite Telemetry (ESA-ADB) aims to address this challenge and establish a new standard in the domain. It is a result of close cooperation between spacecraft operations engineers from the European Space Agency (ESA) and machine learning experts. The newly introduced ESA Anomalies Dataset contains annotated real-life telemetry from three different ESA missions, out of which two are included in ESA-ADB. Results of typical anomaly detection algorithms assessed in our novel hierarchical evaluation pipeline show that new approaches are necessary to address operators' needs. All elements of ESA-ADB are publicly available to ensure its full reproducibility.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing</title>
<link>https://arxiv.org/abs/2407.01521</link>
<guid>https://arxiv.org/abs/2407.01521</guid>
<content:encoded><![CDATA[
arXiv:2407.01521v3 Announce Type: replace-cross 
Abstract: Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regime-Aware Time Weighting for Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2407.21642</link>
<guid>https://arxiv.org/abs/2407.21642</guid>
<content:encoded><![CDATA[
arXiv:2407.21642v2 Announce Type: replace-cross 
Abstract: We introduce a novel method to handle the time dimension when Physics-Informed Neural Networks (PINNs) are used to solve time-dependent differential equations; our proposal focuses on how time sampling and weighting strategies affect solution quality. While previous methods proposed heuristic time-weighting schemes, our approach is grounded in theoretical insights derived from the Lyapunov exponents, which quantify the sensitivity of solutions to perturbations over time. This principled methodology automatically adjusts weights based on the stability regime of the system -- whether chaotic, periodic, or stable. Numerical experiments on challenging benchmarks, including the chaotic Lorenz system and the Burgers' equation, demonstrate the effectiveness and robustness of the proposed method. Compared to existing techniques, our approach offers improved convergence and accuracy without requiring additional hyperparameter tuning. The findings underline the importance of incorporating causality and dynamical system behavior into PINN training strategies, providing a robust framework for solving time-dependent problems with enhanced reliability.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using the iRAP Standard?</title>
<link>https://arxiv.org/abs/2408.10872</link>
<guid>https://arxiv.org/abs/2408.10872</guid>
<content:encoded><![CDATA[
arXiv:2408.10872v5 Announce Type: replace-cross 
Abstract: Road safety assessments are critical yet costly, especially in Low- and Middle-Income Countries (LMICs), where most roads remain unrated. Traditional methods require expert annotation and training data, while supervised learning-based approaches struggle to generalise across regions. In this paper, we introduce \textit{V-RoAst}, a zero-shot Visual Question Answering (VQA) framework using Vision-Language Models (VLMs) to classify road safety attributes defined by the iRAP standard. We introduce the first open-source dataset from ThaiRAP, consisting of over 2,000 curated street-level images from Thailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini on this dataset and benchmark their performance against VGGNet and ResNet baselines. While VLMs underperform on spatial awareness, they generalise well to unseen classes and offer flexible prompt-based reasoning without retraining. Our results show that VLMs can serve as automatic road assessment tools when integrated with complementary data. This work is the first to explore VLMs for zero-shot infrastructure risk assessment and opens new directions for automatic, low-cost road safety mapping. Code and dataset: https://github.com/PongNJ/V-RoAst.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Law of Next-Token Prediction in Large Language Models</title>
<link>https://arxiv.org/abs/2408.13442</link>
<guid>https://arxiv.org/abs/2408.13442</guid>
<content:encoded><![CDATA[
arXiv:2408.13442v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer -- a universal phenomenon observed across a diverse array of open-source LLMs, irrespective of their architectures or pre-training data. We demonstrate that this law offers new perspectives and actionable insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and interpretation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Stochastic Orienteering Problems with Chance Constraints Using a GNN Powered Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2409.04653</link>
<guid>https://arxiv.org/abs/2409.04653</guid>
<content:encoded><![CDATA[
arXiv:2409.04653v2 Announce Type: replace-cross 
Abstract: Leveraging the power of a graph neural network (GNN) with message passing, we present a Monte Carlo Tree Search (MCTS) method to solve stochastic orienteering problems with chance constraints. While adhering to an assigned travel budget the algorithm seeks to maximize collected reward while incurring stochastic travel costs. In this context, the acceptable probability of exceeding the assigned budget is expressed as a chance constraint. Our MCTS solution is an online and anytime algorithm alternating planning and execution that determines the next vertex to visit by continuously monitoring the remaining travel budget. The novelty of our work is that the rollout phase in the MCTS framework is implemented using a message passing GNN, predicting both the utility and failure probability of each available action. This allows to enormously expedite the search process. Our experimental evaluation shows that with the proposed method and architecture we manage to efficiently solve complex problem instances while incurring in moderate losses in terms of collected reward. Moreover, we demonstrate how the approach is capable of generalizing beyond the characteristics of the training dataset. The paper's website, open-source code, and supplementary documentation can be found at ucmercedrobotics.github.io/gnn-sop.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Cap: A Benchmark and a Baseline for Singing Style Captioning</title>
<link>https://arxiv.org/abs/2409.09866</link>
<guid>https://arxiv.org/abs/2409.09866</guid>
<content:encoded><![CDATA[
arXiv:2409.09866v3 Announce Type: replace-cross 
Abstract: Singing voices contain much richer information than common voices, including varied vocal and acoustic properties. However, current open-source audio-text datasets for singing voices capture only a narrow range of attributes and lack acoustic features, leading to limited utility towards downstream tasks, such as style captioning. To fill this gap, we formally define the singing style captioning task and present S2Cap, a dataset of singing voices with detailed descriptions covering diverse vocal, acoustic, and demographic characteristics. Using this dataset, we develop an efficient and straightforward baseline algorithm for singing style captioning. The dataset is available at https://zenodo.org/records/15673764.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data</title>
<link>https://arxiv.org/abs/2409.14500</link>
<guid>https://arxiv.org/abs/2409.14500</guid>
<content:encoded><![CDATA[
arXiv:2409.14500v3 Announce Type: replace-cross 
Abstract: Although data that can be naturally represented as graphs is widespread in real-world applications across diverse industries, popular graph ML benchmarks for node property prediction only cover a surprisingly narrow set of data domains, and graph neural networks (GNNs) are often evaluated on just a few academic citation networks. This issue is particularly pressing in light of the recent growing interest in designing graph foundation models. These models are supposed to be able to transfer to diverse graph datasets from different domains, and yet the proposed graph foundation models are often evaluated on a very limited set of datasets from narrow applications. To alleviate this issue, we introduce GraphLand: a benchmark of 14 diverse graph datasets for node property prediction from a range of different industrial applications. GraphLand allows evaluating graph ML models on a wide range of graphs with diverse sizes, structural characteristics, and feature sets, all in a unified setting. Further, GraphLand allows investigating such previously underexplored research questions as how realistic temporal distributional shifts under transductive and inductive settings influence graph ML model performance. To mimic realistic industrial settings, we use GraphLand to compare GNNs with gradient-boosted decision trees (GBDT) models that are popular in industrial applications and show that GBDTs provided with additional graph-based input features can sometimes be very strong baselines. Further, we evaluate currently available general-purpose graph foundation models and find that they fail to produce competitive results on our proposed datasets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Are In-Context Bandit Reinforcement Learners</title>
<link>https://arxiv.org/abs/2410.05362</link>
<guid>https://arxiv.org/abs/2410.05362</guid>
<content:encoded><![CDATA[
arXiv:2410.05362v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Gesture Recognition for Autism Spectrum Disorder Detection: Integrating YOLOv7, Video Augmentation, and VideoMAE for Naturalistic Video Analysis</title>
<link>https://arxiv.org/abs/2410.09339</link>
<guid>https://arxiv.org/abs/2410.09339</guid>
<content:encoded><![CDATA[
arXiv:2410.09339v3 Announce Type: replace-cross 
Abstract: Deep learning and contactless sensing technologies have significantly advanced the automated assessment of human behaviors in healthcare. In the context of autism spectrum disorder (ASD), repetitive motor behaviors such as spinning, head banging, and arm flapping are key indicators for diagnosis. This study focuses on distinguishing between children with ASD and typically developed (TD) peers by analyzing videos captured in natural, uncontrolled environments. Using the publicly available Self-Stimulatory Behavior Dataset (SSBD), we address the classification task as a binary problem, ASD vs. TD, based on stereotypical repetitive gestures. We adopt a pipeline integrating YOLOv7-based detection, extensive video augmentations, and the VideoMAE framework, which efficiently captures both spatial and temporal features through a high-ratio masking and reconstruction strategy. Our proposed approach achieves 95% accuracy, 0.93 precision, 0.94 recall, and 0.94 F1 score, surpassing the previous state-of-the-art by a significant margin. These results demonstrate the effectiveness of combining advanced object detection, robust data augmentation, and masked autoencoder-based video modeling for reliable ASD vs. TD classification in naturalistic settings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Components of the Attention Schema Theory in Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2411.00983</link>
<guid>https://arxiv.org/abs/2411.00983</guid>
<content:encoded><![CDATA[
arXiv:2411.00983v2 Announce Type: replace-cross 
Abstract: Growing evidence suggests that the brain uses an attention schema, or a simplified model of attention, to help control what it attends to. One proposed benefit of this model is to allow agents to model the attention states of other agents, and thus predict and interact with other agents. The effects of an attention schema may be examined in artificial agents. Although attention mechanisms in artificial agents are different from in biological brains, there may be some principles in common. In both cases, select features or representations are emphasized for better performance. Here, using neural networks with transformer attention mechanisms, we asked whether the addition of an attention schema affected the ability of agents to make judgements about and cooperate with each other. First, we found that an agent with an attention schema is better at categorizing the attention states of other agents (higher accuracy). Second, an agent with an attention schema develops a pattern of attention that is easier for other agents to categorize. Third, in a joint task where two agents must predict each other to paint a scene together, adding an attention schema improves performance. Finally, the performance improvements are not caused by a general increase in network complexity. Instead, improvement is specific to tasks involving judging, categorizing, or predicting the attention of other agents. These results support the hypothesis that an attention schema has computational properties beneficial to mutual interpretability and interactive behavior. We speculate that the same principles might pertain to biological attention and attention schemas in people.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
arXiv:2411.02083v3 Announce Type: replace-cross 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the cross-entropy (CE) loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the $L_p$ norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the CE objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope to inspire LLM developers to improve their pretraining objectives and distribute NTL as a minimalistic and lightweight PyPI package $ntloss$: https://github.com/ai4sd/number-token-loss. Development code for full paper reproduction is available separately.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnostic performance of deep learning for predicting glioma isocitrate dehydrogenase and 1p/19q co-deletion in MRI: a systematic review and meta-analysis</title>
<link>https://arxiv.org/abs/2411.02426</link>
<guid>https://arxiv.org/abs/2411.02426</guid>
<content:encoded><![CDATA[
arXiv:2411.02426v2 Announce Type: replace-cross 
Abstract: Objectives We aimed to evaluate the diagnostic performance of deep learning (DL)-based radiomics models for the noninvasive prediction of isocitrate dehydrogenase (IDH) mutation and 1p/19q co-deletion status in glioma patients using MRI sequences, and to identify methodological factors influencing accuracy and generalizability.
  Materials and methods Following PRISMA guidelines, we systematically searched major databases (PubMed, Scopus, Embase, Web of Science, and Google Scholar) up to March 2025, screening studies that utilized DL to predict IDH and 1p/19q co-deletion status from MRI data. We assessed study quality and risk of bias using the Radiomics Quality Score and the QUADAS-2 tool. Our meta-analysis employed a bivariate model to compute pooled sensitivity and specificity, and meta-regression to assess interstudy heterogeneity.
  Results Among the 1517 unique publications, 104 were included in the qualitative synthesis, and 72 underwent meta-analysis. Pooled estimates for IDH prediction in test cohorts yielded a sensitivity of 0.80 and specificity of 0.85. For 1p/19q co-deletion, sensitivity was 0.75 and specificity was 0.82. Meta-regression identified the tumor segmentation method and the extent of DL integration into the radiomics pipeline as significant contributors to interstudy variability.
  Conclusion Although DL models demonstrate strong potential for noninvasive molecular classification of gliomas, clinical translation requires several critical steps: harmonization of multi-center MRI data using techniques such as histogram matching and DL-based style transfer; adoption of standardized and automated segmentation protocols; extensive multi-center external validation; and prospective clinical validation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Un-mixing Test-time Adaptation under Heterogeneous Data Streams</title>
<link>https://arxiv.org/abs/2411.15173</link>
<guid>https://arxiv.org/abs/2411.15173</guid>
<content:encoded><![CDATA[
arXiv:2411.15173v3 Announce Type: replace-cross 
Abstract: Deploying deep models in real-world scenarios remains challenging due to significant performance drops under distribution shifts between training and deployment environments. Test-Time Adaptation (TTA) has recently emerged as a promising solution, enabling on-the-fly model adaptation without access to source data. However, its effectiveness degrades significantly in the presence of complex, mixed distribution shifts - common in practical settings - where multiple latent domains coexist. Adapting under such intrinsic heterogeneity, especially in unlabeled and online conditions, remains an open and underexplored challenge. In this paper, we study TTA under mixed distribution shifts and move beyond conventional homogeneous adaptation paradigms. By revisiting TTA from a frequency-domain perspective, we observe that distribution heterogeneity often manifests in Fourier space - for instance, high-frequency components tend to carry domain-specific variations. This motivates us to perform domain-aware separation using high-frequency texture cues, making diverse shift patterns more tractable. To this end, we propose FreDA, a novel Frequency-based Decentralized Adaptation framework that decomposes globally heterogeneous data into locally homogeneous components in the frequency domain. It further employs decentralized learning and augmentation strategies to robustly adapt under complex, evolving shifts. Extensive experiments across various environments (corrupted, natural, and medical) demonstrate the superiority of our proposed framework over the state-of-the-arts.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2412.05934</link>
<guid>https://arxiv.org/abs/2412.05934</guid>
<content:encoded><![CDATA[
arXiv:2412.05934v3 Announce Type: replace-cross 
Abstract: With the rapid advancement of multimodal large language models (MLLMs), concerns regarding their security have increasingly captured the attention of both academia and industry. Although MLLMs are vulnerable to jailbreak attacks, designing effective jailbreak attacks poses unique challenges, especially given the highly constrained adversarial capabilities in real-world deployment scenarios. Previous works concentrate risks into a single modality, resulting in limited jailbreak performance. In this paper, we propose a heuristic-induced multimodal risk distribution jailbreak attack method, called HIMRD, which is black-box and consists of two elements: multimodal risk distribution strategy and heuristic-induced search strategy. The multimodal risk distribution strategy is used to distribute harmful semantics into multiple modalities to effectively circumvent the single-modality protection mechanisms of MLLMs. The heuristic-induced search strategy identifies two types of prompts: the understanding-enhancing prompt, which helps MLLMs reconstruct the malicious prompt, and the inducing prompt, which increases the likelihood of affirmative outputs over refusals, enabling a successful jailbreak attack. HIMRD achieves an average attack success rate (ASR) of 90% across seven open-source MLLMs and an average ASR of around 68% in three closed-source MLLMs. HIMRD reveals cross-modal security vulnerabilities in current MLLMs and underscores the imperative for developing defensive strategies to mitigate such emerging risks. Code is available at https://github.com/MaTengSYSU/HIMRD-jailbreak.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGPT: Few-Shot Prompt Tuning for Signed Graphs</title>
<link>https://arxiv.org/abs/2412.12155</link>
<guid>https://arxiv.org/abs/2412.12155</guid>
<content:encoded><![CDATA[
arXiv:2412.12155v2 Announce Type: replace-cross 
Abstract: Signed Graph Neural Networks (SGNNs) are effective in learning expressive representations for signed graphs but typically require substantial task-specific labels, limiting their applicability in label-scarce industrial scenarios. In contrast, unsigned graph structures are abundant and can be readily leveraged to pre-train Graph Neural Networks (GNNs), offering a promising solution to reduce supervision requirements in downstream signed graph tasks. However, transferring knowledge from unsigned to signed graphs is non-trivial due to the fundamental discrepancies in graph types and task objectives between pre-training and downstream phases. To address this challenge, we propose Signed Graph Prompt Tuning (SGPT), a novel graph prompting framework that adapts pre-trained unsigned GNNs to few-shot signed graph tasks. We first design a graph template based on balance theory to disentangle mixed node relationships introduced by negative links, mitigating the structural mismatches between unsigned and signed graphs. We further introduce a task template that reformulates downstream signed tasks into a unified link prediction objective, aligning their optimization goals with the pre-training task. Furthermore, we develop feature prompts that align downstream semantic spaces with the feature spaces learned during pre-training, and semantic prompts to integrate link sign semantics in a task-aware manner. We conduct extensive experiments on seven benchmark signed graph datasets, demonstrating that SGPT significantly outperforms existing state-of-the-art methods, establishing a powerful and generalizable solution for few-shot signed graph learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication</title>
<link>https://arxiv.org/abs/2412.16195</link>
<guid>https://arxiv.org/abs/2412.16195</guid>
<content:encoded><![CDATA[
arXiv:2412.16195v3 Announce Type: replace-cross 
Abstract: Automated assessment of surgical skills using artificial intelligence (AI) provides trainees with instantaneous feedback. After bimanual tool motions are captured, derived kinematic metrics are reliable predictors of performance in laparoscopic tasks. Implementing automated tool tracking requires time-intensive human annotation. We developed AI-based tool tracking using the Segment Anything Model (SAM) to eliminate the need for human annotators. Here, we describe a study evaluating the usefulness of our tool tracking model in automated assessment during a laparoscopic suturing task in the fundoplication procedure. An automated tool tracking model was applied to recorded videos of Nissen fundoplication on porcine bowel. Surgeons were grouped as novices (PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each suturing step were segmented, and motions of the left and right tools were extracted. A low-pass filter with a 24 Hz cut-off frequency removed noise. Performance was assessed using supervised and unsupervised models, and an ablation study compared results. Kinematic features--RMS velocity, RMS acceleration, RMS jerk, total path length, and Bimanual Dexterity--were extracted and analyzed using Logistic Regression, Random Forest, Support Vector Classifier, and XGBoost. PCA was performed for feature reduction. For unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers, such as a 1-D CNN and traditional models, was trained. Data were extracted for 28 participants (9 novices, 19 experts). Supervised learning with PCA and Random Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The unsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an F1 score of 0.806, eliminating the need for kinematic feature computation. We demonstrated an AI model capable of automated performance classification, independent of human annotation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora</title>
<link>https://arxiv.org/abs/2412.16976</link>
<guid>https://arxiv.org/abs/2412.16976</guid>
<content:encoded><![CDATA[
arXiv:2412.16976v3 Announce Type: replace-cross 
Abstract: Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Symbol-like Number Variables in Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2501.06141</link>
<guid>https://arxiv.org/abs/2501.06141</guid>
<content:encoded><![CDATA[
arXiv:2501.06141v3 Announce Type: replace-cross 
Abstract: What types of numeric representations emerge in neural systems, and what would a satisfying answer to this question look like? In this work, we interpret Neural Network (NN) solutions to sequence based number tasks using a variety of methods to understand how well we can interpret them through the lens of interpretable Symbolic Algorithms (SAs) -- precise programs describable by rules and typed, mutable variables. We use autoregressive GRUs, LSTMs, and Transformers trained on tasks where the correct tokens depend on numeric information only latent in the task structure. We show through multiple causal and theoretical methods that we can interpret raw NN activity through the lens of simplified SAs when we frame the activity in terms of neural subspaces rather than individual neurons. Using Distributed Alignment Search (DAS), we find that, depending on network architecture, dimensionality, and task specifications, alignments with SA's can be very high, or they can be only approximate, or fail altogether. We extend our analytic toolkit to address the failure cases by expanding the DAS framework to a broader class of alignment functions that more flexibly capture NN activity in terms of interpretable variables from SAs, and we provide theoretic and empirical explorations of Linear Alignment Functions (LAFs) in contrast to the preexisting Orthogonal Alignment Functions (OAFs). Through analyses of specific cases we confirm the usefulness of causal interventions on neural subspaces for NN interpretability, and we show that recurrent models can develop graded, symbol-like number variables in their neural activity. We further show that shallow Transformers learn very different solutions than recurrent networks, and we prove that such models must use anti-Markovian solutions -- solutions that do not rely on cumulative, Markovian hidden states -- in the absence of sufficient attention layers.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2SSP: A Two-Stage Framework for Structured Pruning of LLMs</title>
<link>https://arxiv.org/abs/2501.17771</link>
<guid>https://arxiv.org/abs/2501.17771</guid>
<content:encoded><![CDATA[
arXiv:2501.17771v2 Announce Type: replace-cross 
Abstract: We propose a novel Two-Stage framework for Structured Pruning (\textsc{2SSP}) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron on the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test \textsc{2SSP} on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at https://github.com/FabrizioSandri/2SSP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Exploration for Multi-Reward Multi-Policy Evaluation</title>
<link>https://arxiv.org/abs/2502.02516</link>
<guid>https://arxiv.org/abs/2502.02516</guid>
<content:encoded><![CDATA[
arXiv:2502.02516v3 Announce Type: replace-cross 
Abstract: We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions must be evaluated simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC perspective to achieve $\epsilon$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the MR-NaS exploration scheme to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of this adaptive exploration scheme.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dealing with Annotator Disagreement in Hate Speech Classification</title>
<link>https://arxiv.org/abs/2502.08266</link>
<guid>https://arxiv.org/abs/2502.08266</guid>
<content:encoded><![CDATA[
arXiv:2502.08266v2 Announce Type: replace-cross 
Abstract: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is essential for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate various automatic approaches for aggregating multiple annotations, in the context of hate speech classification in Turkish tweets. Our work highlights the importance of the problem and provides state-of-the-art benchmark results for the detection and understanding of hate speech in online discourse.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Augmented Thyroid Scintigraphy for Robust Classification</title>
<link>https://arxiv.org/abs/2503.00366</link>
<guid>https://arxiv.org/abs/2503.00366</guid>
<content:encoded><![CDATA[
arXiv:2503.00366v2 Announce Type: replace-cross 
Abstract: Purpose: Thyroid scintigraphy plays a vital role in diagnosing a range of thyroid disorders. While deep learning classification models hold significant promise in this domain, their effectiveness is frequently compromised by limited and imbalanced datasets. This study investigates the impact of three data augmentation strategies including Stable Diffusion (SD), Flow Matching (FM), and Conventional Augmentation (CA), on enhancing the performance of a ResNet18 classifier.
  Methods: Anterior thyroid scintigraphy images from 2,954 patients across nine medical centers were classified into four categories: Diffuse Goiter (DG), Nodular Goiter (NG), Normal (NL), and Thyroiditis (TI). Data augmentation was performed using various SD and FM models, resulting in 18 distinct augmentation scenarios. Each augmented dataset was used to train a ResNet18 classifier. Model performance was assessed using class-wise and average precision, recall, F1-score, AUC, and image fidelity metrics (FID and KID).
  Results: FM-based augmentation outperformed all other methods, achieving the highest classification accuracy and lowest FID/KID scores, indicating both improved model generalization and realistic image synthesis. SD1, combining image and prompt inputs in the inference process, was the most effective SD variant, suggesting that physician-generated prompts provide meaningful clinical context. O+FM+CA yielded the most balanced and robust performance across all classes.
  Conclusion: Integrating FM and clinically-informed SD augmentation, especially when guided by expert prompts, substantially improves thyroid scintigraphy classification. These findings highlight the importance of leveraging both structured medical input and advanced generative models for more effective training on limited datasets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensionality reduction for homological stability and global structure preservation</title>
<link>https://arxiv.org/abs/2503.03156</link>
<guid>https://arxiv.org/abs/2503.03156</guid>
<content:encoded><![CDATA[
arXiv:2503.03156v3 Announce Type: replace-cross 
Abstract: We propose a new dimensionality reduction toolkit designed to address some of the challenges faced by traditional methods like UMAP and tSNE such as loss of global structure and computational efficiency. Built on the JAX framework, DiRe leverages modern hardware acceleration to provide an efficient, scalable, and interpretable solution for visualizing complex data structures, and for quantitative analysis of lower-dimensional embeddings. The toolkit shows considerable promise in preserving both local and global structures within the data as compared to state-of-the-art UMAP and tSNE implementations. This makes it suitable for a wide range of applications in machine learning, bio-informatics, and data science.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation</title>
<link>https://arxiv.org/abs/2503.08010</link>
<guid>https://arxiv.org/abs/2503.08010</guid>
<content:encoded><![CDATA[
arXiv:2503.08010v2 Announce Type: replace-cross 
Abstract: We present SKALD, a multi-shot video assembly method that constructs coherent video sequences from candidate shots with minimal reliance on text. Central to our approach is the Learned Clip Assembly (LCA) score, a learning-based metric that measures temporal and semantic relationships between shots to quantify narrative coherence. We tackle the exponential complexity of combining multiple shots with an efficient beam-search algorithm guided by the LCA score. To train our model effectively with limited human annotations, we propose two tasks for the LCA encoder: Shot Coherence Learning, which uses contrastive learning to distinguish coherent and incoherent sequences, and Feature Regression, which converts these learned representations into a real-valued coherence score. We develop two variants: a base SKALD model that relies solely on visual coherence and SKALD-text, which integrates auxiliary text information when available. Experiments on the VSPD and our curated MSV3C datasets show that SKALD achieves an improvement of up to 48.6% in IoU and a 43% speedup over the state-of-the-art methods. A user study further validates our approach, with 45% of participants favoring SKALD-assembled videos, compared to 22% preferring text-based assembly methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alzheimer's Disease Classification Using Retinal OCT: TransnetOCT and Swin Transformer Models</title>
<link>https://arxiv.org/abs/2503.11511</link>
<guid>https://arxiv.org/abs/2503.11511</guid>
<content:encoded><![CDATA[
arXiv:2503.11511v2 Announce Type: replace-cross 
Abstract: Retinal optical coherence tomography (OCT) images are the biomarkers for neurodegenerative diseases, which are rising in prevalence. Early detection of Alzheimer's disease using retinal OCT is a primary challenging task. This work utilizes advanced deep learning techniques to classify retinal OCT images of subjects with Alzheimer's disease (AD) and healthy controls (CO). The goal is to enhance diagnostic capabilities through efficient image analysis. In the proposed model, Raw OCT images have been preprocessed with ImageJ and given to various deep-learning models to evaluate the accuracy. The best classification architecture is TransNetOCT, which has an average accuracy of 98.18% for input OCT images and 98.91% for segmented OCT images for five-fold cross-validation compared to other models, and the Swin Transformer model has achieved an accuracy of 93.54%. The evaluation accuracy metric demonstrated TransNetOCT and Swin transformer models capability to classify AD and CO subjects reliably, contributing to the potential for improved diagnostic processes in clinical settings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models</title>
<link>https://arxiv.org/abs/2503.15904</link>
<guid>https://arxiv.org/abs/2503.15904</guid>
<content:encoded><![CDATA[
arXiv:2503.15904v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases. This study introduces a novel evaluation framework to uncover gender biases in LLMs: using free-form storytelling to surface biases embedded within the models. A systematic analysis of ten prominent LLMs shows a consistent pattern of overrepresenting female characters across occupations, likely due to supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Paradoxically, despite this overrepresentation, the occupational gender distributions produced by these LLMs align more closely with human stereotypes than with real-world labor data. This highlights the challenge and importance of implementing balanced mitigation measures to promote fairness and prevent the establishment of potentially new biases. We release the prompts and LLM-generated stories at GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PVChat: Personalized Video Chat with One-Shot Learning</title>
<link>https://arxiv.org/abs/2503.17069</link>
<guid>https://arxiv.org/abs/2503.17069</guid>
<content:encoded><![CDATA[
arXiv:2503.17069v4 Announce Type: replace-cross 
Abstract: Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborating with AI Agents: Field Experiments on Teamwork, Productivity, and Performance</title>
<link>https://arxiv.org/abs/2503.18238</link>
<guid>https://arxiv.org/abs/2503.18238</guid>
<content:encoded><![CDATA[
arXiv:2503.18238v2 Announce Type: replace-cross 
Abstract: To uncover how AI agents change productivity, performance, and work processes, we introduce Pairit -- an experimentation platform enabling humans and AI agents to collaborate in integrative workspaces. In a large-scale marketing experiment on the platform, 2310 participants were randomly assigned to human-human and human-AI teams. The teams exchanged 183,691 messages and created 63,656 image edits, 1,960,095 ad copy edits, and 10,375 AI-generated images while producing 11,138 ads for a large think tank. Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 63% and allowed humans to engage in 71% less direct text editing. While human-AI teams engaged in 18% more process and content communication, human-human teams engaged in 29% more social and emotional communication. Humans in human-AI teams experienced 73% greater productivity per worker and produced higher-quality ad copy, while human-human teams produced higher-quality images, suggesting AI agents require fine-tuning for multimodal workflows. Field tests of the ad campaigns accumulated ~5M ad impressions and revealed that ads with higher image quality (produced by human-human collaborations) and higher text quality (produced by human-AI collaborations) performed significantly better on click-through rates, view through rates, and cost per click metrics. Together, these results suggest that human collaboration with AI agents significantly reshapes communication patterns and work processes and increases productivity, while improving some dimensions of output quality and deteriorating others. We hope the release of the extensible Pairit platform will accelerate RCTs of human-AI collaboration across a variety of work tasks and contexts.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Long Horizon Manipulation with Closed-loop Code Generation and Incremental Few-shot Adaptation</title>
<link>https://arxiv.org/abs/2503.21969</link>
<guid>https://arxiv.org/abs/2503.21969</guid>
<content:encoded><![CDATA[
arXiv:2503.21969v2 Announce Type: replace-cross 
Abstract: Embodied long-horizon manipulation requires robotic systems to process multimodal inputs-such as vision and natural language-and translate them into executable actions. However, existing learning-based approaches often depend on large, task-specific datasets and struggle to generalize to unseen scenarios. Recent methods have explored using large language models (LLMs) as high-level planners that decompose tasks into subtasks using natural language and guide pretrained low-level controllers. Yet, these approaches assume perfect execution from low-level policies, which is unrealistic in real-world environments with noise or suboptimal behaviors. To overcome this, we fully discard the pretrained low-level policy and instead use the LLM to directly generate executable code plans within a closed-loop framework. Our planner employs chain-of-thought (CoT)-guided few-shot learning with incrementally structured examples to produce robust and generalizable task plans. Complementing this, a reporter evaluates outcomes using RGB-D and delivers structured feedback, enabling recovery from misalignment and replanning under partial observability. This design eliminates per-step inference, reduces computational overhead, and limits error accumulation that was observed in previous methods. Our framework achieves state-of-the-art performance on 30+ diverse seen and unseen long-horizon tasks across LoHoRavens, CALVIN, Franka Kitchen, and cluttered real-world settings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectR: Dynamically Composing LM Experts with Spectral Routing</title>
<link>https://arxiv.org/abs/2504.03454</link>
<guid>https://arxiv.org/abs/2504.03454</guid>
<content:encoded><![CDATA[
arXiv:2504.03454v2 Announce Type: replace-cross 
Abstract: Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title>
<link>https://arxiv.org/abs/2504.04823</link>
<guid>https://arxiv.org/abs/2504.04823</guid>
<content:encoded><![CDATA[
arXiv:2504.04823v2 Announce Type: replace-cross 
Abstract: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
arXiv:2504.05410v2 Announce Type: replace-cross 
Abstract: The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models</title>
<link>https://arxiv.org/abs/2504.07402</link>
<guid>https://arxiv.org/abs/2504.07402</guid>
<content:encoded><![CDATA[
arXiv:2504.07402v3 Announce Type: replace-cross 
Abstract: We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction built upon the LauraGPT backbone. LauraTSE employs a small-scale auto-regressive decoder-only language model that generates the initial layers of the target speech's discrete codec representations from the continuous embeddings of both the mixture and reference speech. These outputs serve as coarse-grained predictions. To refine them, a one-step encoder-only language model reconstructs the full codec representation by integrating information from both the mixture and the reference speech, adding fine-grained details. Experimental results show that our approach can achieve promising performance. Additionally, we conduct ablation studies to investigate the data scalability and the contribution of the encoder-only model.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process</title>
<link>https://arxiv.org/abs/2504.12488</link>
<guid>https://arxiv.org/abs/2504.12488</guid>
<content:encoded><![CDATA[
arXiv:2504.12488v2 Announce Type: replace-cross 
Abstract: As generative AI tools like ChatGPT become integral to everyday writing, critical questions arise about how to preserve writers' sense of agency and ownership when using these tools. Yet, a systematic understanding of how AI assistance affects different aspects of the writing process - and how this shapes writers' agency - remains underexplored. To address this gap, we conducted a systematic review of 109 HCI papers using the PRISMA approach. From this literature, we identify four overarching design strategies for AI writing support: structured guidance, guided exploration, active co-writing, and critical feedback - mapped across the four key cognitive processes in writing: planning, translating, reviewing, and monitoring. We complement this analysis with interviews of 15 writers across diverse domains. Our findings reveal that writers' desired levels of AI intervention vary across the writing process: content-focused writers (e.g., academics) prioritize ownership during planning, while form-focused writers (e.g., creatives) value control over translating and reviewing. Writers' preferences are also shaped by contextual goals, values, and notions of originality and authorship. By examining when ownership matters, what writers want to own, and how AI interactions shape agency, we surface both alignment and gaps between research and user needs. Our findings offer actionable design guidance for developing human-centered writing tools for co-writing with AI, on human terms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration</title>
<link>https://arxiv.org/abs/2504.12609</link>
<guid>https://arxiv.org/abs/2504.12609</guid>
<content:encoded><![CDATA[
arXiv:2504.12609v3 Announce Type: replace-cross 
Abstract: Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels and human-robot embodiment differences. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the embodiment gap without relying on wearables, teleoperation, or large-scale data collection. From the video, we extract: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. These components enable effective policy learning without any task-specific reward tuning. In the single human demo regime, Human2Sim2Robot outperforms object-aware replay by over 55% and imitation learning by over 68% on grasping, non-prehensile manipulation, and multi-step tasks. Website: https://human2sim2robot.github.io
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRL: Learning Scalable Planning Policies with Simple Rewards</title>
<link>https://arxiv.org/abs/2504.17838</link>
<guid>https://arxiv.org/abs/2504.17838</guid>
<content:encoded><![CDATA[
arXiv:2504.17838v2 Announce Type: replace-cross 
Abstract: We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpness-Aware Minimization with Z-Score Gradient Filtering</title>
<link>https://arxiv.org/abs/2505.02369</link>
<guid>https://arxiv.org/abs/2505.02369</guid>
<content:encoded><![CDATA[
arXiv:2505.02369v4 Announce Type: replace-cross 
Abstract: Deep neural networks achieve high performance across many domains but can still face challenges in generalization when optimization is influenced by small or noisy gradient components. Sharpness-Aware Minimization improves generalization by perturbing parameters toward directions of high curvature, but it uses the entire gradient vector, which means that small or noisy components may affect the ascent step and cause the optimizer to miss optimal solutions. We propose Z-Score Filtered Sharpness-Aware Minimization, which applies Z-score based filtering to gradients in each layer. Instead of using all gradient components, a mask is constructed to retain only the top percentile with the largest absolute Z-scores. The percentile threshold $Q_p$ determines how many components are kept, so that the ascent step focuses on directions that stand out most compared to the average of the layer. This selective perturbation refines the search toward flatter minima while reducing the influence of less significant gradients. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet with architectures including ResNet, VGG, and Vision Transformers show that the proposed method consistently improves test accuracy compared to Sharpness-Aware Minimization and its variants.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04860</link>
<guid>https://arxiv.org/abs/2505.04860</guid>
<content:encoded><![CDATA[
arXiv:2505.04860v2 Announce Type: replace-cross 
Abstract: Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: https://dcodaaug.github.io/D-CODA/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
arXiv:2505.06987v5 Announce Type: replace-cross 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuB: Learning Extreme Humanoid Balance</title>
<link>https://arxiv.org/abs/2505.07294</link>
<guid>https://arxiv.org/abs/2505.07294</guid>
<content:encoded><![CDATA[
arXiv:2505.07294v2 Announce Type: replace-cross 
Abstract: The human body demonstrates exceptional motor capabilities-such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters-both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose HuB (Humanoid Balance), a unified framework that integrates reference motion refinement, balance-aware policy learning, and sim-to-real robustness training, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy remains stable even under strong physical disturbances-such as a forceful soccer strike-while baseline methods consistently fail to complete these tasks. Project website: https://hub-robot.github.io
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2505.07879</link>
<guid>https://arxiv.org/abs/2505.07879</guid>
<content:encoded><![CDATA[
arXiv:2505.07879v2 Announce Type: replace-cross 
Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAG: Scalable Language-Augmented Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08124</link>
<guid>https://arxiv.org/abs/2505.08124</guid>
<content:encoded><![CDATA[
arXiv:2505.08124v2 Announce Type: replace-cross 
Abstract: Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-Cache: Training-Free Retrieval for Real-Time Manipulation</title>
<link>https://arxiv.org/abs/2505.09040</link>
<guid>https://arxiv.org/abs/2505.09040</guid>
<content:encoded><![CDATA[
arXiv:2505.09040v2 Announce Type: replace-cross 
Abstract: Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Invariant Risk Minimization</title>
<link>https://arxiv.org/abs/2505.12506</link>
<guid>https://arxiv.org/abs/2505.12506</guid>
<content:encoded><![CDATA[
arXiv:2505.12506v2 Announce Type: replace-cross 
Abstract: We propose a novel unsupervised framework for \emph{Invariant Risk Minimization} (IRM), extending the concept of invariance to settings where labels are unavailable. Traditional IRM methods rely on labeled data to learn representations that are robust to distributional shifts across environments. In contrast, our approach redefines invariance through feature distribution alignment, enabling robust representation learning from unlabeled data. We introduce two methods within this framework: Principal Invariant Component Analysis (PICA), a linear method that extracts invariant directions under Gaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep generative model that disentangles environment-invariant and environment-dependent latent factors. Our approach is based on a novel ``unsupervised'' structural causal model and supports environment-conditioned sample-generation and intervention. Empirical evaluations on synthetic dataset and modified versions of MNIST demonstrate the effectiveness of our methods in capturing invariant structure, preserving relevant information, and generalizing across environments without access to labels.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation</title>
<link>https://arxiv.org/abs/2505.14978</link>
<guid>https://arxiv.org/abs/2505.14978</guid>
<content:encoded><![CDATA[
arXiv:2505.14978v2 Announce Type: replace-cross 
Abstract: This paper presents JARVIS, a novel multi-agent framework that leverages Large Language Models (LLMs) and domain expertise to generate high-quality scripts for specialized Electronic Design Automation (EDA) tasks. By combining a domain-specific LLM trained with synthetically generated data, a custom compiler for structural verification, rule enforcement, code fixing capabilities, and advanced retrieval mechanisms, our approach achieves significant improvements over state-of-the-art domain-specific models. Our framework addresses the challenges of data scarcity and hallucination errors in LLMs, demonstrating the potential of LLMs in specialized engineering domains. We evaluate our framework on multiple benchmarks and show that it outperforms existing models in terms of accuracy and reliability. Our work sets a new precedent for the application of LLMs in EDA and paves the way for future innovations in this field.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation</title>
<link>https://arxiv.org/abs/2505.16752</link>
<guid>https://arxiv.org/abs/2505.16752</guid>
<content:encoded><![CDATA[
arXiv:2505.16752v3 Announce Type: replace-cross 
Abstract: Deep Learning Recommendation Models (DLRMs) often rely on extensive manual feature engineering to improve accuracy and user experience, which increases system complexity and limits scalability of model performance with respect to computational resources. Recently, Meta introduced a generative ranking paradigm based on HSTU block that enables end-to-end learning from raw user behavior sequences and demonstrates scaling law on large datasets that can be regarded as the state-of-the-art (SOTA). However, splitting user behaviors into interleaved item and action information significantly increases the input sequence length, which adversely affects both training and inference efficiency. To address this issue, we propose the Dual-Flow Generative Ranking Network (DFGR), that employs a dual-flow mechanism to optimize interaction modeling, ensuring efficient training and inference through end-to-end token processing. DFGR duplicates the original user behavior sequence into a real flow and a fake flow based on the authenticity of the action information, and then defines a novel interaction method between the real flow and the fake flow within the QKV module of the self-attention mechanism. This design reduces computational overhead and improves both training efficiency and inference performance compared to Meta's HSTU-based model. Experiments on both open-source and real industrial datasets show that DFGR outperforms DLRM, which serves as the industrial online baseline with extensive feature engineering, as well as Meta's HSTU and other common recommendation models such as DIN, DCN, DIEN, and DeepFM. Furthermore, we investigate optimal parameter allocation strategies under computational constraints, establishing DFGR as an efficient and effective next-generation generative ranking paradigm.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2505.20152</link>
<guid>https://arxiv.org/abs/2505.20152</guid>
<content:encoded><![CDATA[
arXiv:2505.20152v2 Announce Type: replace-cross 
Abstract: Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our hard negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further conduct ablation studies to analyze three key factors: hard negative types, the efficiency of image-based negatives, and training configurations. These analyses yield important insights into optimizing hard negative strategies for geometric reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Large Language Models with gSMILE</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language</title>
<link>https://arxiv.org/abs/2505.22146</link>
<guid>https://arxiv.org/abs/2505.22146</guid>
<content:encoded><![CDATA[
arXiv:2505.22146v3 Announce Type: replace-cross 
Abstract: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Human evaluation studies validate our framework's alignment with human decision-making patterns, and generalization experiments demonstrate effective performance on novel tool categories. Ablation studies revealed that manipulation-related attributes (graspability, elongation, hand-relatedness) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization</title>
<link>https://arxiv.org/abs/2505.24269</link>
<guid>https://arxiv.org/abs/2505.24269</guid>
<content:encoded><![CDATA[
arXiv:2505.24269v2 Announce Type: replace-cross 
Abstract: In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical Manufacturing Scale-Up</title>
<link>https://arxiv.org/abs/2505.24584</link>
<guid>https://arxiv.org/abs/2505.24584</guid>
<content:encoded><![CDATA[
arXiv:2505.24584v3 Announce Type: replace-cross 
Abstract: Recent advances in generative AI have accelerated the discovery of novel chemicals and materials. However, scaling these discoveries to industrial production remains a major bottleneck due to the synthesis gap -- the need to develop entirely new manufacturing processes. This challenge requires detailed engineering blueprints: PFDs for equipment layouts and material/energy flows, and PIDs for process plant operations. Current AI systems cannot yet reliably generate these critical engineering schematics, creating a fundamental obstacle to manufacturing scale-up of novel discoveries. We present a closed-loop, physics-aware framework for automated generation of industrially viable PFDs and PIDs. The framework integrates three key components: (1) domain-specialized small language models (SLMs) trained for auto-generation of PFDs and PIDs, (2) a hierarchical knowledge graph containing process flow and instrumentation descriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation (GRAG), and (3) an open-source chemical process simulator for modeling, simulation, optimization, and analysis of novel chemical processes. The SLMs are trained through a multi-stage pipeline on synthetic datasets, with process simulator-in-the-loop validation ensuring feasibility. To enhance computational efficiency, the framework implements structural pruning (width and depth) guided by importance heuristics to reduce language model size while preserving accuracy, followed by advanced inference optimizations including FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization, and Test-Time Inference Scaling. Experimental results demonstrate that our framework generates simulator-validated process descriptions with high fidelity.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges</title>
<link>https://arxiv.org/abs/2506.02048</link>
<guid>https://arxiv.org/abs/2506.02048</guid>
<content:encoded><![CDATA[
arXiv:2506.02048v2 Announce Type: replace-cross 
Abstract: We present 'Random-Crypto', a procedurally generated cryptographic Capture The Flag (CTF) dataset designed to unlock the potential of Reinforcement Learning (RL) for LLM-based agents in security-sensitive domains. Cryptographic reasoning offers an ideal RL testbed: it combines precise validation, structured multi-step inference, and reliance on reliable computational tool use. Leveraging these properties, we fine-tune a Python tool-augmented Llama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure execution environment. The resulting agent achieves a significant improvement in Pass@8 on previously unseen challenges. Moreover, the improvements generalize to two external benchmarks: 'picoCTF', spanning both crypto and non-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135 cryptography questions. Ablation studies attribute the gains to enhanced tool usage and procedural reasoning. These findings position 'Random-Crypto' as a rich training ground for building intelligent, adaptable LLM agents capable of handling complex cybersecurity tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL</title>
<link>https://arxiv.org/abs/2506.04147</link>
<guid>https://arxiv.org/abs/2506.04147</guid>
<content:encoded><![CDATA[
arXiv:2506.04147v4 Announce Type: replace-cross 
Abstract: Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors. More information and robot videos at robo-rl.github.io
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Search, Retrieval, and Composition via Task Similarity in Collaborative Agentic Systems</title>
<link>https://arxiv.org/abs/2506.05577</link>
<guid>https://arxiv.org/abs/2506.05577</guid>
<content:encoded><![CDATA[
arXiv:2506.05577v2 Announce Type: replace-cross 
Abstract: Agentic AI aims to create systems that set their own goals, adapt proactively to change, and refine behavior through continuous experience. Recent advances suggest that, when facing multiple and unforeseen tasks, agents could benefit from sharing machine-learned knowledge and reuse policies that have already been fully or partially learned by other agents. However, how to query, select, and retrieve policies from a pool of agents, and how to integrate such policies remains a largely unexplored area. This study explores how an agent decides what knowledge to select, from whom, and when and how to integrate it in its own policy in order to accelerate its own learning. The proposed algorithm, \emph{Modular Sharing and Composition in Collective Learning} (MOSAIC), improves learning in agentic collectives by combining (1) knowledge selection using performance signals and cosine similarity on Wasserstein task embeddings, (2) modular and transferable neural representations via masks, and (3) policy integration, composition and fine-tuning. MOSAIC outperforms isolated learners and global sharing approaches in both learning speed and overall performance, and in some cases solves tasks that isolated agents cannot. The results also demonstrate that selective, goal-driven reuse leads to less susceptibility to task interference. We also observe the emergence of self-organization, where agents solving simpler tasks accelerate the learning of harder ones through shared knowledge.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards an Explainable Comparison and Alignment of Feature Embeddings</title>
<link>https://arxiv.org/abs/2506.06231</link>
<guid>https://arxiv.org/abs/2506.06231</guid>
<content:encoded><![CDATA[
arXiv:2506.06231v3 Announce Type: replace-cross 
Abstract: While several feature embedding models have been developed in the literature, comparisons of these embeddings have largely focused on their numerical performance in classification-related downstream applications. However, an interpretable comparison of different embeddings requires identifying and analyzing mismatches between sample groups clustered within the embedding spaces. In this work, we propose the \emph{Spectral Pairwise Embedding Comparison (SPEC)} framework to compare embeddings and identify their differences in clustering a reference dataset. Our approach examines the kernel matrices derived from two embeddings and leverages the eigendecomposition of the difference kernel matrix to detect sample clusters that are captured differently by the two embeddings. We present a scalable implementation of this kernel-based approach, with computational complexity that grows linearly with the sample size. Furthermore, we introduce an optimization problem using this framework to align two embeddings, ensuring that clusters identified in one embedding are also captured in the other model. We provide numerical results demonstrating the SPEC's application to compare and align embeddings on large-scale datasets such as ImageNet and MS-COCO. The project page is available at https://mjalali.github.io/SPEC/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias</title>
<link>https://arxiv.org/abs/2506.06280</link>
<guid>https://arxiv.org/abs/2506.06280</guid>
<content:encoded><![CDATA[
arXiv:2506.06280v2 Announce Type: replace-cross 
Abstract: Diagnosing deep neural networks (DNNs) by analyzing the eigenspectrum of their weights has been an active area of research in recent years. One of the main approaches involves measuring the heavytailness of the empirical spectral densities (ESDs) of weight matrices. This analysis has been shown to provide insights to help diagnose whether a model is well-trained or undertrained, and has been used to guide training methods involving layer-wise hyperparameter assignment. In this paper, we address an often-overlooked challenge in estimating the heavytailness of these ESDs: the impact of the aspect ratio of weight matrices. We demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in estimating the heavytailness of ESDs, leading to inaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we propose FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We show that this method effectively mitigates the aspect ratio bias. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum analysis while enabling more effective layer-wise hyperparameter assignment. In one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3% when compared with state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Geometric Embedding for Node Influence Maximization</title>
<link>https://arxiv.org/abs/2506.07435</link>
<guid>https://arxiv.org/abs/2506.07435</guid>
<content:encoded><![CDATA[
arXiv:2506.07435v2 Announce Type: replace-cross 
Abstract: Computing classical centrality measures such as betweenness and closeness is computationally expensive on large-scale graphs. In this work, we introduce an efficient force layout algorithm that embeds a graph into a low-dimensional space, where the radial distance from the origin serves as a proxy for various centrality measures. We evaluate our method on multiple graph families and demonstrate strong correlations with degree, PageRank, and paths-based centralities. As an application, it turns out that the proposed embedding allows to find high-influence nodes in a network, and provides a fast and scalable alternative to the standard greedy algorithm.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Teacher to Student: Tracking Memorization Through Model Distillation</title>
<link>https://arxiv.org/abs/2506.16170</link>
<guid>https://arxiv.org/abs/2506.16170</guid>
<content:encoded><![CDATA[
arXiv:2506.16170v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are known to memorize parts of their training data, raising important concerns around privacy and security. While previous research has focused on studying memorization in pre-trained models, much less is known about how knowledge distillation (KD) affects memorization.In this study, we explore how different KD methods influence the memorization of fine-tuned task data when a large teacher model is distilled into smaller student variants.This study demonstrates that distilling a larger teacher model, fine-tuned on a dataset, into a smaller variant not only lowers computational costs and model size but also significantly reduces the memorization risks compared to standard fine-tuning approaches.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning with Columnar Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2506.17169</link>
<guid>https://arxiv.org/abs/2506.17169</guid>
<content:encoded><![CDATA[
arXiv:2506.17169v2 Announce Type: replace-cross 
Abstract: Continual learning is a key feature of biological neural systems, but artificial neural networks often suffer from catastrophic forgetting. Instead of backpropagation, biologically plausible learning algorithms may enable stable continual learning. This study proposes columnar-organized spiking neural networks (SNNs) with local learning rules for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that its microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). We evaluate CoLaNET on two benchmarks: Permuted MNIST (ten sequential pixel-permuted tasks) and a two-task MNIST/EMNIST setup. Our model learns ten sequential tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title>
<link>https://arxiv.org/abs/2506.17208</link>
<guid>https://arxiv.org/abs/2506.17208</guid>
<content:encoded><![CDATA[
arXiv:2506.17208v2 Announce Type: replace-cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Generation with Equivariant Variational Flow Matching</title>
<link>https://arxiv.org/abs/2506.18340</link>
<guid>https://arxiv.org/abs/2506.18340</guid>
<content:encoded><![CDATA[
arXiv:2506.18340v2 Announce Type: replace-cross 
Abstract: We derive a controlled generation objective within the framework of Variational Flow Matching (VFM), which casts flow matching as a variational inference problem. We demonstrate that controlled generation can be implemented two ways: (1) by way of end-to-end training of conditional generative models, or (2) as a Bayesian inference problem, enabling post hoc control of unconditional models without retraining. Furthermore, we establish the conditions required for equivariant generation and provide an equivariant formulation of VFM tailored for molecular generation, ensuring invariance to rotations, translations, and permutations. We evaluate our approach on both uncontrolled and controlled molecular generation, achieving state-of-the-art performance on uncontrolled generation and outperforming state-of-the-art models in controlled generation, both with end-to-end training and in the Bayesian inference setting. This work strengthens the connection between flow-based generative modeling and Bayesian inference, offering a scalable and principled framework for constraint-driven and symmetry-aware generation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geological Everything Model 3D: A Physics-informed Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding</title>
<link>https://arxiv.org/abs/2507.00419</link>
<guid>https://arxiv.org/abs/2507.00419</guid>
<content:encoded><![CDATA[
arXiv:2507.00419v3 Announce Type: replace-cross 
Abstract: Understanding Earth's subsurface is critical for energy transition, natural hazard mitigation, and planetary science. Yet subsurface analysis remains fragmented, with separate models required for structural interpretation, stratigraphic analysis, geobody segmentation, and property modeling-each tightly coupled to specific data distributions and task formulations. We introduce the Geological Everything Model 3D (GEM), a unified generative architecture that reformulates all these tasks as prompt-conditioned inference along latent structural frameworks derived from subsurface imaging. This formulation moves beyond task-specific models by enabling a shared inference mechanism, where GEM propagates human-provided prompts-such as well logs, masks, or structural sketches-along inferred structural frameworks to produce geologically coherent outputs. Through this mechanism, GEM achieves zero-shot generalization across tasks with heterogeneous prompt types, without retraining for new tasks or data sources. This capability emerges from a two-stage training process that combines self-supervised representation learning on large-scale field seismic data with adversarial fine-tuning using mixed prompts and labels across diverse subsurface tasks. GEM demonstrates broad applicability across surveys and tasks, including Martian radar stratigraphy analysis, structural interpretation in subduction zones, full seismic stratigraphic interpretation, geobody segmentation, and property modeling. By bridging expert knowledge with generative reasoning in a structurally aware manner, GEM lays the foundation for scalable, human-in-the-loop geophysical AI-transitioning from fragmented pipelines to a vertically integrated, promptable reasoning system. Project page: https://douyimin.github.io/GEM
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2FGL: Spatial Spectral Federated Graph Learning</title>
<link>https://arxiv.org/abs/2507.02409</link>
<guid>https://arxiv.org/abs/2507.02409</guid>
<content:encoded><![CDATA[
arXiv:2507.02409v4 Announce Type: replace-cross 
Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the semantic knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drift occurs, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate the challenge of poor semantic knowledge caused by label signal disruption. Furthermore, we design a frequency alignment to address spectral client drift. The combination of Spatial and Spectral strategies forms our framework S2FGL. Extensive experiments on multiple datasets demonstrate the superiority of S2FGL. The code is available at https://github.com/Wonder7racer/S2FGL.git.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Auditory Scene Analysis</title>
<link>https://arxiv.org/abs/2507.02755</link>
<guid>https://arxiv.org/abs/2507.02755</guid>
<content:encoded><![CDATA[
arXiv:2507.02755v2 Announce Type: replace-cross 
Abstract: Auditory scene analysis (ASA) aims to retrieve information from the acoustic environment, by carrying out three main tasks: sound source location, separation, and classification. These tasks are traditionally executed with a linear data flow, where the sound sources are first located; then, using their location, each source is separated into its own audio stream; from each of which, information is extracted that is relevant to the application scenario (audio event detection, speaker identification, emotion classification, etc.). However, running these tasks linearly increases the overall response time, while making the last tasks (separation and classification) highly sensitive to errors of the first task (location). A considerable amount of effort and computational complexity has been employed in the state-of-the-art to develop techniques that are the least error-prone possible. However, doing so gives rise to an ASA system that is non-viable in many applications that require a small computational footprint and a low response time, such as bioacoustics, hearing-aid design, search and rescue, human-robot interaction, etc. To this effect, in this work, a multi-agent approach is proposed to carry out ASA where the tasks are run in parallel, with feedback loops between them to compensate for local errors, such as: using the quality of the separation output to correct the location error; and using the classification result to reduce the localization's sensitivity towards interferences. The result is a multi-agent auditory scene analysis (MASA) system that is robust against local errors, without a considerable increase in complexity, and with a low response time. The complete proposed MASA system is provided as a framework that uses open-source tools for sound acquisition and reproduction (JACK) and inter-agent communication (ROS2), allowing users to add their own agents.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference</title>
<link>https://arxiv.org/abs/2507.03865</link>
<guid>https://arxiv.org/abs/2507.03865</guid>
<content:encoded><![CDATA[
arXiv:2507.03865v2 Announce Type: replace-cross 
Abstract: Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Approach for Estimating Largest Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning</title>
<link>https://arxiv.org/abs/2507.04868</link>
<guid>https://arxiv.org/abs/2507.04868</guid>
<content:encoded><![CDATA[
arXiv:2507.04868v2 Announce Type: replace-cross 
Abstract: Understanding and quantifying chaos from data remains challenging. We present a data-driven method for estimating the largest Lyapunov exponent (LLE) from one-dimensional chaotic time series using machine learning. A predictor is trained to produce out-of-sample, multi-horizon forecasts; the LLE is then inferred from the exponential growth of the geometrically averaged forecast error (GMAE) across the horizon, which serves as a proxy for trajectory divergence. We validate the approach on four canonical 1D maps-logistic, sine, cubic, and Chebyshev-achieving R2pos > 0.99 against reference LLE curves with series as short as M = 450. Among baselines, KNN yields the closest fits (KNN-R comparable; RF larger deviations). By design the estimator targets positive exponents: in periodic/stable regimes it returns values indistinguishable from zero. Noise robustness is assessed by adding zero-mean white measurement noise and summarizing performance versus the average SNR over parameter sweeps: accuracy saturates for SNRm > 30 dB and collapses below 27 dB, a conservative sensor-level benchmark. The method is simple, computationally efficient, and model-agnostic, requiring only stationarity and the presence of a dominant positive exponent. It offers a practical route to LLE estimation in experimental settings where only scalar time-series measurements are available, with extensions to higher-dimensional and irregularly sampled data left for future work.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks</title>
<link>https://arxiv.org/abs/2507.05346</link>
<guid>https://arxiv.org/abs/2507.05346</guid>
<content:encoded><![CDATA[
arXiv:2507.05346v2 Announce Type: replace-cross 
Abstract: The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Complexity Landscape and Model Structure Functions</title>
<link>https://arxiv.org/abs/2507.13543</link>
<guid>https://arxiv.org/abs/2507.13543</guid>
<content:encoded><![CDATA[
arXiv:2507.13543v2 Announce Type: replace-cross 
Abstract: We develop a framework for dualizing the Kolmogorov structure function $h_x(\alpha)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering</title>
<link>https://arxiv.org/abs/2507.14784</link>
<guid>https://arxiv.org/abs/2507.14784</guid>
<content:encoded><![CDATA[
arXiv:2507.14784v2 Announce Type: replace-cross 
Abstract: Video Question Answering (VideoQA) requires identifying sparse critical moments in long videos and reasoning about their causal relationships to answer semantically complex questions. While recent advances in multimodal learning have improved alignment and fusion, current approaches remain limited by two prevalent but fundamentally flawed strategies: (1) task-agnostic sampling indiscriminately processes all frames, overwhelming key events with irrelevant content; and (2) heuristic retrieval captures superficial patterns but misses causal-temporal structures needed for complex reasoning. To address these challenges, we introduce LeAdQA, an innovative approach that bridges these gaps through synergizing causal-aware query refinement with fine-grained visual grounding. Our method first leverages LLMs to reformulate question-option pairs, resolving causal ambiguities and sharpening temporal focus. These refined queries subsequently direct a temporal grounding model to precisely retrieve the most salient segments, complemented by an adaptive fusion mechanism dynamically integrating the evidence to maximize relevance. The integrated visual-textual cues are then processed by an MLLM to generate accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and NExT-GQA demonstrate that our method's precise visual grounding substantially enhances the understanding of video-question relationships, achieving state-of-the-art (SOTA) performance on complex reasoning tasks while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems</title>
<link>https://arxiv.org/abs/2507.14850</link>
<guid>https://arxiv.org/abs/2507.14850</guid>
<content:encoded><![CDATA[
arXiv:2507.14850v2 Announce Type: replace-cross 
Abstract: We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models</title>
<link>https://arxiv.org/abs/2507.20094</link>
<guid>https://arxiv.org/abs/2507.20094</guid>
<content:encoded><![CDATA[
arXiv:2507.20094v2 Announce Type: replace-cross 
Abstract: Diffusion models have become a powerful backbone for text-to-image generation, producing high-quality visuals from natural language prompts. However, when prompts involve multiple objects alongside global or local style instructions, the outputs often drift in style and lose spatial coherence, limiting their reliability for controlled, style-consistent scene generation. We present Local Prompt Adaptation (LPA), a lightweight, training-free method that splits the prompt into content and style tokens, then injects them selectively into the U-Net's attention layers at chosen timesteps. By conditioning object tokens early and style tokens later in the denoising process, LPA improves both layout control and stylistic uniformity without additional training cost. We conduct extensive ablations across parser settings and injection windows, finding that the best configuration -- lpa late only with a 300-650 step window -- delivers the strongest balance of prompt alignment and style consistency. On the T2I benchmark, LPA improves CLIP-prompt alignment over vanilla SDXL by +0.41% and over SD1.5 by +0.34%, with no diversity loss. On our custom 50-prompt style-rich benchmark, LPA achieves +0.09% CLIP-prompt and +0.08% CLIP-style gains over baseline. Our method is model-agnostic, easy to integrate, and requires only a single configuration change, making it a practical choice for controllable, style-consistent multi-object generation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning with Probing for Sequential User-Centric Selection</title>
<link>https://arxiv.org/abs/2507.20112</link>
<guid>https://arxiv.org/abs/2507.20112</guid>
<content:encoded><![CDATA[
arXiv:2507.20112v2 Announce Type: replace-cross 
Abstract: We formalize sequential decision-making with information acquisition as the probing-augmented user-centric selection (PUCS) framework, where a learner first probes a subset of arms to obtain side information on resources and rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such as ridesharing, wireless scheduling, and content recommendation, in which both resources and payoffs are initially unknown and probing is costly. For the offline setting with known distributions, we present a greedy probing algorithm with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the online setting with unknown distributions, we introduce OLPA, a stochastic combinatorial bandit algorithm that achieves a regret bound $\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound $\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic factors. Experiments on real-world data demonstrate the effectiveness of our solutions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation</title>
<link>https://arxiv.org/abs/2508.01772</link>
<guid>https://arxiv.org/abs/2508.01772</guid>
<content:encoded><![CDATA[
arXiv:2508.01772v2 Announce Type: replace-cross 
Abstract: Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Confidence-Diversity Framework for Calibrating AI Judgement in Accessible Qualitative Coding Tasks</title>
<link>https://arxiv.org/abs/2508.02029</link>
<guid>https://arxiv.org/abs/2508.02029</guid>
<content:encoded><![CDATA[
arXiv:2508.02029v2 Announce Type: replace-cross 
Abstract: LLMs enable qualitative coding at large scale, but assessing reliability remains challenging where human experts seldom agree. We investigate confidence-diversity calibration as a quality assessment framework for accessible coding tasks where LLMs already demonstrate strong performance but exhibit overconfidence. Analysing 5,680 coding decisions from eight state-of-the-art LLMs across ten categories, we find that mean self-confidence tracks inter-model agreement closely (Pearson r=0.82). Adding model diversity quantified as normalised Shannon entropy produces a dual signal explaining agreement almost completely (R-squared=0.979), though this high predictive power likely reflects task simplicity for current LLMs. The framework enables a three-tier workflow auto-accepting 35 percent of segments with less than 5 percent error, cutting manual effort by 65 percent. Cross-domain validation confirms transferability (kappa improvements of 0.20 to 0.78). While establishing a methodological foundation for AI judgement calibration, the true potential likely lies in more challenging scenarios where LLMs may demonstrate comparative advantages over human cognitive limitations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration</title>
<link>https://arxiv.org/abs/2508.02069</link>
<guid>https://arxiv.org/abs/2508.02069</guid>
<content:encoded><![CDATA[
arXiv:2508.02069v2 Announce Type: replace-cross 
Abstract: Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, offer a distinctive approach for capturing the complexities of temporal data. However, their potential for spatial modeling in multivariate time-series forecasting remains largely unexplored. To bridge this gap, we introduce a brand new SNN architecture, which is among the first to seamlessly integrate graph structural learning with spike-based temporal processing for multivariate time-series forecasting. Specifically, we first embed time features and an adaptive matrix, eliminating the need for predefined graph structures. We then further learn sequence features through the Observation (OBS) Block. Building upon this, our Multi-Scale Spike Aggregation (MSSA) hierarchically aggregates neighborhood information through spiking SAGE layers, enabling multi-hop feature extraction while eliminating the need for floating-point operations. Finally, we propose a Dual-Path Spike Fusion (DSF) Block to integrate spatial graph features and temporal dynamics via a spike-gated mechanism, combining LSTM-processed sequences with spiking self-attention outputs, effectively improve the model accuracy of long sequence datasets. Experiments show that our model surpasses the state-of-the-art SNN-based iSpikformer on all datasets and outperforms traditional temporal models at long horizons, thereby establishing a new paradigm for efficient spatial-temporal modeling.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from B Cell Evolution: Adaptive Multi-Expert Diffusion for Antibody Design via Online Optimization</title>
<link>https://arxiv.org/abs/2508.02834</link>
<guid>https://arxiv.org/abs/2508.02834</guid>
<content:encoded><![CDATA[
arXiv:2508.02834v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models have shown remarkable potential for antibody design, yet existing approaches apply uniform generation strategies that cannot adapt to each antigen's unique requirements. Inspired by B cell affinity maturation, where antibodies evolve through multi-objective optimization balancing affinity, stability, and self-avoidance, we propose the first biologically-motivated framework that leverages physics-based domain knowledge within an online meta-learning system. Our method employs multiple specialized experts (van der Waals, molecular recognition, energy balance, and interface geometry) whose parameters evolve during generation based on iterative feedback, mimicking natural antibody refinement cycles. Instead of fixed protocols, this adaptive guidance discovers personalized optimization strategies for each target. Our experiments demonstrate that this approach: (1) discovers optimal SE(3)-equivariant guidance strategies for different antigen classes without pre-training, preserving molecular symmetries throughout optimization; (2) significantly enhances hotspot coverage and interface quality through target-specific adaptation, achieving balanced multi-objective optimization characteristic of therapeutic antibodies; (3) establishes a paradigm for iterative refinement where each antibody-antigen system learns its unique optimization profile through online evaluation; (4) generalizes effectively across diverse design challenges, from small epitopes to large protein interfaces, enabling precision-focused campaigns for individual targets.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Deep Learning Fails: Limitations of Recurrent Models on Stroke-Based Handwriting for Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2508.03773</link>
<guid>https://arxiv.org/abs/2508.03773</guid>
<content:encoded><![CDATA[
arXiv:2508.03773v2 Announce Type: replace-cross 
Abstract: Alzheimer's disease detection requires expensive neuroimaging or invasive procedures, limiting accessibility. This study explores whether deep learning can enable non-invasive Alzheimer's disease detection through handwriting analysis. Using a dataset of 34 distinct handwriting tasks collected from healthy controls and Alzheimer's disease patients, we evaluate and compare three recurrent neural architectures (LSTM, GRU, RNN) against traditional machine learning models. A crucial distinction of our approach is that the recurrent models process pre-extracted features from discrete strokes, not raw temporal signals. This violates the assumption of a continuous temporal flow that recurrent networks are designed to capture. Results reveal that they exhibit poor specificity and high variance. Traditional ensemble methods significantly outperform all deep architectures, achieving higher accuracy with balanced metrics. This demonstrates that recurrent architectures, designed for continuous temporal sequences, fail when applied to feature vectors extracted from ambiguously segmented strokes. Despite their complexity, deep learning models cannot overcome the fundamental disconnect between their architectural assumptions and the discrete, feature-based nature of stroke-level handwriting data. Although performance is limited, the study highlights several critical issues in data representation and model compatibility, pointing to valuable directions for future research.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Finetuned LLMs in AMR Parsing</title>
<link>https://arxiv.org/abs/2508.05028</link>
<guid>https://arxiv.org/abs/2508.05028</guid>
<content:encoded><![CDATA[
arXiv:2508.05028v3 Announce Type: replace-cross 
Abstract: AMR (Abstract Meaning Representation) is a semantic formalism that encodes sentence meaning as rooted, directed, acyclic graphs, where nodes represent concepts and edges denote semantic relations. Finetuning decoder only Large Language Models (LLMs) represent a promising novel straightfoward direction for AMR parsing. This paper presents a comprehensive evaluation of finetuning four distinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that straightfoward finetuning of decoder only LLMs can achieve comparable performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2 demonstrates competitive performance against SOTA AMR parsers given a straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5 excels in structural validity.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Expression Generation for Referring Image Segmentation and Grounding</title>
<link>https://arxiv.org/abs/2508.05123</link>
<guid>https://arxiv.org/abs/2508.05123</guid>
<content:encoded><![CDATA[
arXiv:2508.05123v2 Announce Type: replace-cross 
Abstract: Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges</title>
<link>https://arxiv.org/abs/2508.05668</link>
<guid>https://arxiv.org/abs/2508.05668</guid>
<content:encoded><![CDATA[
arXiv:2508.05668v2 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMCE-Net++: Feature Map Convergence Evaluation and Training</title>
<link>https://arxiv.org/abs/2508.06109</link>
<guid>https://arxiv.org/abs/2508.06109</guid>
<content:encoded><![CDATA[
arXiv:2508.06109v2 Announce Type: replace-cross 
Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their opaque internal representations. While Feature Map Convergence Evaluation (FMCE) quantifies module-level convergence via Feature Map Convergence Scores (FMCS), it lacks experimental validation and closed-loop integration. To address this limitation, we propose FMCE-Net++, a novel training framework that integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module generates FMCS predictions, which, combined with task labels, jointly supervise backbone optimization through a Representation Auxiliary Loss. The RAL dynamically balances the primary classification loss and feature convergence optimization via a tunable \Representation Abstraction Factor. Extensive experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100 demonstrate that FMCE-Net++ consistently enhances model performance without architectural modifications or additional data. Key experimental outcomes include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp (ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate state-of-the-art performance ceilings.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Equation-VAE: Disentangled Latent Representations for Tabular Data</title>
<link>https://arxiv.org/abs/2508.06347</link>
<guid>https://arxiv.org/abs/2508.06347</guid>
<content:encoded><![CDATA[
arXiv:2508.06347v2 Announce Type: replace-cross 
Abstract: Learning interpretable latent representations from tabular data remains a challenge in deep generative modeling. We introduce SE-VAE (Structural Equation-Variational Autoencoder), a novel architecture that embeds measurement structure directly into the design of a variational autoencoder. Inspired by structural equation modeling, SE-VAE aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate construct-specific confounding variation. This modular architecture enables disentanglement through design rather than through statistical regularizers alone. We evaluate SE-VAE on a suite of simulated tabular datasets and benchmark its performance against a series of leading baselines using standard disentanglement metrics. SE-VAE consistently outperforms alternatives in factor recovery, interpretability, and robustness to nuisance variation. Ablation results reveal that architectural structure, rather than regularization strength, is the key driver of performance. SE-VAE offers a principled framework for white-box generative modeling in scientific and social domains where latent constructs are theory-driven and measurement validity is essential.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
<link>https://arxiv.org/abs/2508.07423</link>
<guid>https://arxiv.org/abs/2508.07423</guid>
<content:encoded><![CDATA[
arXiv:2508.07423v2 Announce Type: replace-cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library</title>
<link>https://arxiv.org/abs/2508.07970</link>
<guid>https://arxiv.org/abs/2508.07970</guid>
<content:encoded><![CDATA[
arXiv:2508.07970v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite the notable advances enabled by existing RLHF training frameworks, significant challenges remain to scale to complex multimodal workflows and adapt to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT Yet Another Transformer Trainer in WeChat, a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across diverse experimental scenarios, demonstrating its substantial throughput improvements over state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models that support WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications. We have made WeChat-YATT publicly available at https://www.github.com/tencent/WeChat-YATT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShoulderShot: Generating Over-the-Shoulder Dialogue Videos</title>
<link>https://arxiv.org/abs/2508.07597</link>
<guid>https://arxiv.org/abs/2508.07597</guid>
<content:encoded><![CDATA[
<div> Dialogue videos, over-the-shoulder, visual variety, emotional connection, video generation 
Summary: 
ShoulderShot introduces a novel framework for generating over-the-shoulder dialogue videos, addressing challenges such as character consistency, spatial continuity, and dialogue length limitations. By combining dual-shot generation with looping video, ShoulderShot surpasses existing methods in shot-reverse-shot layout, spatial continuity, and dialogue flexibility. This framework opens up new possibilities for practical dialogue video generation and provides a solution for creating extended dialogues while maintaining character consistency. The results of ShoulderShot demonstrate its capabilities in generating high-quality dialogue scenes, enhancing visual variety and emotional connection in films, short dramas, and advertisements. The framework's efficiency and effectiveness make it a valuable tool for video generation research, offering a solution to the underexplored area of dialogue generation in video content. <div>
arXiv:2508.07597v2 Announce Type: replace-cross 
Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and advertisements, providing visual variety and enhancing viewers' emotional connection. Despite their importance, such dialogue scenes remain largely underexplored in video generation research. The main challenges include maintaining character consistency across different shots, creating a sense of spatial continuity, and generating long, multi-turn dialogues within limited computational budgets. Here, we present ShoulderShot, a framework that combines dual-shot generation with looping video, enabling extended dialogues while preserving character consistency. Our results demonstrate capabilities that surpass existing methods in terms of shot-reverse-shot layout, spatial continuity, and flexibility in dialogue length, thereby opening up new possibilities for practical dialogue video generation. Videos and comparisons are available at https://shouldershot.github.io.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Rule-Based Argumentation Using Datalog</title>
<link>https://arxiv.org/abs/2508.10976</link>
<guid>https://arxiv.org/abs/2508.10976</guid>
<content:encoded><![CDATA[
<div> Keywords: ASPIC+, rule-based argumentation, first-order rules, grounding, Datalog

Summary:
In this study, the authors address the challenge of reasoning over first-order rules in ASPIC+, a key framework for rule-based argumentation in AI. While first-order rules are commonly used in ASPIC+, most existing approaches support only propositional rules, requiring a preliminary grounding step for first-order instances. The authors propose an intelligent grounding procedure that translates first-order ASPIC+ instances into a Datalog program and queries a Datalog engine to obtain ground substitutions. They also suggest simplifications specific to ASPIC+ to avoid unnecessary grounding. An empirical evaluation of their prototypical implementation demonstrates scalability. This research contributes to advancing the capabilities of reasoning over rule-based argumentation systems using first-order rules. 

<br /><br />Summary: <div>
arXiv:2508.10976v1 Announce Type: new 
Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for AI. Although first-order rules are commonly used in ASPIC+ examples, most existing approaches to reason over rule-based argumentation only support propositional rules. To enable reasoning over first-order instances, a preliminary grounding step is required. As groundings can lead to an exponential increase in the size of the input theories, intelligent procedures are needed. However, there is a lack of dedicated solutions for ASPIC+. Therefore, we propose an intelligent grounding procedure that keeps the size of the grounding manageable while preserving the correctness of the reasoning process. To this end, we translate the first-order ASPIC+ instance into a Datalog program and query a Datalog engine to obtain ground substitutions to perform the grounding of rules and contraries. Additionally, we propose simplifications specific to the ASPIC+ formalism to avoid grounding of rules that have no influence on the reasoning process. Finally, we performed an empirical evaluation of a prototypical implementation to show scalability.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching</title>
<link>https://arxiv.org/abs/2508.11070</link>
<guid>https://arxiv.org/abs/2508.11070</guid>
<content:encoded><![CDATA[
<div> framework, multi-agent algorithmic recourse, capacitated weighted bipartite matching problem, social welfare, system-level design<br />
Summary:<br />
The article introduces a new framework for multi-agent algorithmic recourse in situations where multiple stakeholders are involved, addressing the limitations of existing individual-focused approaches. The framework models the interaction between multiple recourse seekers and providers as a capacitated weighted bipartite matching problem, optimizing for social welfare while considering both recourse costs and provider capacity. It proposes a three-layer optimization approach to maximize welfare while minimizing modifications to system settings. Experimental validation on various datasets shows that the framework can achieve near-optimal social welfare with minimal adjustments. This work extends algorithmic recourse from individual recommendations to system-level design, offering a feasible path towards balancing social welfare and individual actionability. <br /> <div>
arXiv:2508.11070v1 Announce Type: new 
Abstract: Decision makers are increasingly relying on machine learning in sensitive situations. In such settings, algorithmic recourse aims to provide individuals with actionable and minimally costly steps to reverse unfavorable AI-driven decisions. While existing research predominantly focuses on single-individual (i.e., seeker) and single-model (i.e., provider) scenarios, real-world applications often involve multiple interacting stakeholders. Optimizing outcomes for seekers under an individual welfare approach overlooks the inherently multi-agent nature of real-world systems, where individuals interact and compete for limited resources. To address this, we introduce a novel framework for multi-agent algorithmic recourse that accounts for multiple recourse seekers and recourse providers. We model this many-to-many interaction as a capacitated weighted bipartite matching problem, where matches are guided by both recourse cost and provider capacity. Edge weights, reflecting recourse costs, are optimized for social welfare while quantifying the welfare gap between individual welfare and this collectively feasible outcome. We propose a three-layer optimization framework: (1) basic capacitated matching, (2) optimal capacity redistribution to minimize the welfare gap, and (3) cost-aware optimization balancing welfare maximization with capacity adjustment costs. Experimental validation on synthetic and real-world datasets demonstrates that our framework enables the many-to-many algorithmic recourse to achieve near-optimal welfare with minimum modification in system settings. This work extends algorithmic recourse from individual recommendations to system-level design, providing a tractable path toward higher social welfare while maintaining individual actionability.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to optimize for automatic proton PBS treatment planning for H&amp;N cancers</title>
<link>https://arxiv.org/abs/2508.11085</link>
<guid>https://arxiv.org/abs/2508.11085</guid>
<content:encoded><![CDATA[
<div> Inverse optimizer, data-driven, automatic treatment planning, PPO, H&amp;N cancer<br />
<br />
Summary:
A new automatic treatment planning framework utilizing a data-driven inverse optimizer and PPO-based virtual planner has been proposed for proton PBS treatment of H&amp;N cancers. The inverse optimizer, based on L2O method, predicts update steps from task-specific data. Techniques for long-context processing are integrated into a Transformer-based framework to address scalability concerns. The PPO framework acts as a virtual planner, adjusting objective parameters through a policy network, while the dose predictor initializes these parameters. Compared to traditional L-BFGSB methods, the L2O-based inverse optimizer shows a 22.97% improvement in effectiveness and 36.41% increase in efficiency. Plans generated by the framework within an average of 2.55 hours demonstrate improved or comparable OAR sparing and superior target coverage for patients with varying characteristics, including prescription dose levels and beam angles, compared to manually generated plans. <div>
arXiv:2508.11085v1 Announce Type: new 
Abstract: Proton PBS treatment planning for H&amp;N cancers involves numerous conflicting objectives, requiring significant effort from human planners to balance and satisfy multiple clinical goals during planning. To achieve this, experience-demanding objective parameter adjustment and computationally expensive inverse optimization are performed iteratively. Extensive efforts have been made to automatically adjust objective parameters, but the most time-consuming component, i.e., inverse optimization, still relies heavily on theory-driven approaches. We propose a data-driven inverse optimizer and integrate it into a PPO-based automatic treatment planning framework to automatically generate high-quality plans within a clinical acceptable planning time. The inverse optimizer is a L2O method that predicts update steps by learning from the task-specific data distribution. For the first time, we integrate techniques designed for long-context processing, originally developed for LLMs, into a Transformer-based L2O framework to address the scalability issue of existing L2O methods. The PPO framework functions as an outer-loop virtual planner, autonomously adjusting objective parameters through a policy network, and the dose predictor is used to initialize objective parameters. The inner-loop L2O inverse optimizer computes machine-deliverable MU values based on objectives refined by the PPO policy network. 97 patients are collected in this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves the effectiveness and efficiency by 22.97% and 36.41%, respectively. In conjunction with the PPO-based learned virtual planner, plans generated by our framework within an average of 2.55 hours show improved or comparable OAR sparing with superior target coverage for patients with different prescription dose levels, number of target volumes, beam angles, etc., compared with human-generated plans.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation</title>
<link>https://arxiv.org/abs/2508.11182</link>
<guid>https://arxiv.org/abs/2508.11182</guid>
<content:encoded><![CDATA[
<div> admissibility, assumption-based argumentation, strong admissibility, weak admissibility, bipolar set-based argumentation frameworks

Summary: 
This work explores admissibility notions in assumption-based argumentation, specifically investigating strong and weak admissibility in non-flat ABA using abstract bipolar set-based argumentation frameworks (BSAFs). The preferred, complete, and grounded semantics are introduced for non-flat ABA, extending prior research on weak admissibility in flat ABA. Strong admissibility is newly introduced for non-flat ABA, maintaining the modularization property. Shared limitations with standard admissible semantics are identified in strong and weakly admissible semantics, prompting discussions on addressing these shortcomings. <div>
arXiv:2508.11182v1 Announce Type: new 
Abstract: In this work, we broaden the investigation of admissibility notions in the context of assumption-based argumentation (ABA). More specifically, we study two prominent alternatives to the standard notion of admissibility from abstract argumentation, namely strong and weak admissibility, and introduce the respective preferred, complete and grounded semantics for general (sometimes called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation frameworks (BSAFs) as formal playground since they concisely capture the relations between assumptions and are expressive enough to represent general non-flat ABA frameworks, as recently shown. While weak admissibility has been recently investigated for a restricted fragment of ABA in which assumptions cannot be derived (flat ABA), strong admissibility has not been investigated for ABA so far. We introduce strong admissibility for ABA and investigate desirable properties. We furthermore extend the recent investigations of weak admissibility in the flat ABA fragment to the non-flat case. We show that the central modularization property is maintained under classical, strong, and weak admissibility. We also show that strong and weakly admissible semantics in non-flat ABA share some of the shortcomings of standard admissible semantics and discuss ways to address these.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information</title>
<link>https://arxiv.org/abs/2508.11252</link>
<guid>https://arxiv.org/abs/2508.11252</guid>
<content:encoded><![CDATA[
<div> mathematics, reasoning models, dataset, incomplete problems, intelligence

Summary:
Large Reasoning Models (LRMs) have shown impressive problem-solving skills in mathematics but lack the ability to proactively ask for information in incomplete problems. A new dataset with diverse contexts is proposed to address this limitation. The evaluation of LRMs on the dataset reveals their inability to ask for information, leading to behaviors like overthinking and hallucination. The study highlights the challenges and potential of supervised fine-tuning in enhancing LRMs' abilities. The goal is to advance LRMs towards genuine intelligence that goes beyond problem-solving to include proactive information-seeking. 

<br /><br />Summary: <div>
arXiv:2508.11252v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding</title>
<link>https://arxiv.org/abs/2508.11347</link>
<guid>https://arxiv.org/abs/2508.11347</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge graph embedding, continual learning, dynamic evolution, embedding dimensions, SAGE framework

Summary:<br />
Traditional knowledge graph embedding methods focus on static graphs, but real-world knowledge graphs evolve dynamically. The SAGE framework addresses this by considering the varying scales of updates and adapting the embedding dimensions accordingly. By using a Dynamic Distillation mechanism, SAGE balances preserving learned knowledge with incorporating new facts. Experimental results on seven benchmarks show that SAGE outperforms existing methods, with improvements in MRR, H@1, and H@10. Comparisons with methods using fixed embedding dimensions demonstrate the importance of adaptive dimensions in continual knowledge graph embedding. The SAGE framework is publicly available on GitHub for further research and implementation. <br /><br />Summary: <div>
arXiv:2508.11347v1 Announce Type: new 
Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities and relations in a low-dimensional space, primarily focusing on static graphs. However, real-world KGs are dynamically evolving with the constant addition of entities, relations and facts. To address such dynamic nature of KGs, several continual knowledge graph embedding (CKGE) methods have been developed to efficiently update KG embeddings to accommodate new facts while maintaining learned knowledge. As KGs grow at different rates and scales in real-world scenarios, existing CKGE methods often fail to consider the varying scales of updates and lack systematic evaluation throughout the entire update process. In this paper, we propose SAGE, a scale-aware gradual evolution framework for CKGE. Specifically, SAGE firstly determine the embedding dimensions based on the update scales and expand the embedding space accordingly. The Dynamic Distillation mechanism is further employed to balance the preservation of learned knowledge and the incorporation of new facts. We conduct extensive experiments on seven benchmarks, and the results show that SAGE consistently outperforms existing baselines, with a notable improvement of 1.38% in MRR, 1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with methods using fixed embedding dimensions show that SAGE achieves optimal performance on every snapshot, demonstrating the importance of adaptive embedding dimensions in CKGE. The codes of SAGE are publicly available at: https://github.com/lyfxjtu/Dynamic-Embedding.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</title>
<link>https://arxiv.org/abs/2508.11360</link>
<guid>https://arxiv.org/abs/2508.11360</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, GUI environments, Curriculum learning, Group Relative Policy Optimization, Fine-grained policy optimization 

Summary: 
CRAFT-GUI introduces a curriculum learning framework based on Group Relative Policy Optimization to address limitations in RL approaches for GUI tasks. It considers varying task difficulty and provides nuanced feedback through a reward function combining rule-based signals and model-judged evaluation. Experimental results show significant performance improvements over existing methods, outperforming them by 5.6% on public benchmarks Android Control and 10.3% on internal online benchmarks. This integration of RL with curriculum learning demonstrates the effectiveness of adapting learning processes to account for task-specific complexities and providing agents with richer feedback for more efficient policy updates. By enhancing adaptability and optimizing policy at a finer granularity, CRAFT-GUI successfully advances automated task execution in GUI environments. 

<br /><br />Summary: <div>
arXiv:2508.11360v1 Announce Type: new 
Abstract: As autonomous agents become adept at understanding and interacting with graphical user interface (GUI) environments, a new era of automated task execution is emerging. Recent studies have demonstrated that Reinforcement Learning (RL) can effectively enhance agents' performance in dynamic interactive GUI environments. However, these methods face two key limitations: (1) they overlook the significant variation in difficulty across different GUI tasks by treating the entire training data as a uniform set, which hampers the agent's ability to adapt its learning process; and (2) most approaches collapse task-specific nuances into a single, coarse reward, leaving the agent with a uniform signal that yields inefficient policy updates. To address these limitations, we propose CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO) that explicitly accounts for the varying difficulty across trajectories. To enable more fine-grained policy optimization, we design a reward function that combines simple rule-based signals with model-judged evaluation, providing richer and more nuanced feedback during training. Experimental results demonstrate that our method achieves significant improvements over previous state-of-the-art approaches, outperforming them by 5.6% on public benchmarks Android Control and 10.3% on our internal online benchmarks, respectively. These findings empirically validate the effectiveness of integrating reinforcement learning with curriculum learning in GUI interaction tasks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager</title>
<link>https://arxiv.org/abs/2508.11416</link>
<guid>https://arxiv.org/abs/2508.11416</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, inventory levels, decision bias, supply chain management, decision support systems

Summary:
The article introduces AIM-Bench, a novel benchmark to assess LLM agents' decision-making behavior in uncertain supply chain management scenarios. It explores the decision biases present in LLM agents, similar to those observed in humans, and suggests strategies to mitigate biases such as the pull-to-center effect and the bullwhip effect. The study reveals that different LLMs exhibit varying degrees of decision bias, highlighting the need for considering biases in deploying LLMs for inventory decision-making. Implementing cognitive reflection and information sharing are identified as potential strategies to address biases and improve decision-making in supply chains. The findings emphasize the importance of developing human-centered decision support systems for supply chains to mitigate biases and enhance decision-making processes. 

<br /><br />Summary: <div>
arXiv:2508.11416v1 Announce Type: new 
Abstract: Recent advances in mathematical reasoning and the long-term planning capabilities of large language models (LLMs) have precipitated the development of agents, which are being increasingly leveraged in business operations processes. Decision models to optimize inventory levels are one of the core elements of operations management. However, the capabilities of the LLM agent in making inventory decisions in uncertain contexts, as well as the decision-making biases (e.g. framing effect, etc.) of the agent, remain largely unexplored. This prompts concerns regarding the capacity of LLM agents to effectively address real-world problems, as well as the potential implications of biases that may be present. To address this gap, we introduce AIM-Bench, a novel benchmark designed to assess the decision-making behaviour of LLM agents in uncertain supply chain management scenarios through a diverse series of inventory replenishment experiments. Our results reveal that different LLMs typically exhibit varying degrees of decision bias that are similar to those observed in human beings. In addition, we explored strategies to mitigate the pull-to-centre effect and the bullwhip effect, namely cognitive reflection and implementation of information sharing. These findings underscore the need for careful consideration of the potential biases in deploying LLMs in Inventory decision-making scenarios. We hope that these insights will pave the way for mitigating human decision bias and developing human-centred decision support systems for supply chains.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title>
<link>https://arxiv.org/abs/2508.11452</link>
<guid>https://arxiv.org/abs/2508.11452</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multimodal Large Language Models, Inclusion Arena, Bradley-Terry model, human feedback<br />
Summary:
Inclusion Arena introduces a live leaderboard for ranking models based on human feedback from AI-powered applications. The platform integrates pairwise model comparisons during natural user interactions to ensure practical usage scenarios are reflected accurately. The Bradley-Terry model is utilized with innovations like Placement Matches and Proximity Sampling to enhance model ranking accuracy and stability. Empirical analyses show that Inclusion Arena provides reliable rankings with high data transitivity and reduces the risk of manipulation. By fostering collaboration between foundational models and real-world applications, the platform aims to accelerate the development of optimized Large Language Models and Multimodal Large Language Models for user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.<br /><br />Summary: <div>
arXiv:2508.11452v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Landmark-Assisted Monte Carlo Planning</title>
<link>https://arxiv.org/abs/2508.11493</link>
<guid>https://arxiv.org/abs/2508.11493</guid>
<content:encoded><![CDATA[
<div> Keywords: landmarks, probabilistic planning, UCT algorithm, MDPs, subgoals

Summary: 
Probabilistic landmarks, akin to classical planning's landmarks, set conditions that must be met in solution plans. This study introduces probabilistic landmarks and integrates them into the UCT algorithm for decomposing MDPs into subgoals. The adaptation involves finding a balance between achieving landmarks greedily and accomplishing the final goal. Results from benchmark domains demonstrate that strategically selected landmarks can notably enhance the performance of UCT in online probabilistic planning. The optimal trade-off between greedy landmark attainment and long-term goal completion varies depending on the specific problem at hand. The findings suggest that landmarks can offer valuable guidance for anytime algorithms tackling MDPs.<br /><br />Summary: <div>
arXiv:2508.11493v1 Announce Type: new 
Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in every solution plan$\unicode{x2013}$have contributed to major advancements in classical planning, but they have seldom been used in stochastic domains. We formalize probabilistic landmarks and adapt the UCT algorithm to leverage them as subgoals to decompose MDPs; core to the adaptation is balancing between greedy landmark achievement and final goal achievement. Our results in benchmark domains show that well-chosen landmarks can significantly improve the performance of UCT in online probabilistic planning, while the best balance of greedy versus long-term goal achievement is problem-dependent. The results suggest that landmarks can provide helpful guidance for anytime algorithms solving MDPs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</title>
<link>https://arxiv.org/abs/2508.11524</link>
<guid>https://arxiv.org/abs/2508.11524</guid>
<content:encoded><![CDATA[
<div> large-scale planning problems, Large Language Models (LLMs), problem decomposition, LLM4Inspire, LLM4Predict

Summary:<br />
Addressing large-scale planning problems is a key challenge in the planning community due to the state-space explosion. Researchers have explored using Large Language Models (LLMs) to generate actions and states to reduce the search space. However, integrating LLMs with domain-specific knowledge is crucial for valid plans. This paper proposes an LLM-assisted planner with problem decomposition, dividing complex problems into simpler sub-tasks. Two paradigms, LLM4Inspire and LLM4Predict, are introduced to aid problem decomposition by providing heuristic guidance and utilizing domain-specific knowledge. Experimental validation across multiple domains shows the planner's effectiveness in search space partitioning. The results demonstrate that LLMs can locate solutions and incorporating domain-specific knowledge in LLMs, specifically LLM4Predict, shows promise over LLM4Inspire's general knowledge. 

<br /><br />Summary: <div>
arXiv:2508.11524v1 Announce Type: new 
Abstract: Addressing large-scale planning problems has become one of the central challenges in the planning community, deriving from the state-space explosion caused by growing objects and actions. Recently, researchers have explored the effectiveness of leveraging Large Language Models (LLMs) to generate helpful actions and states to prune the search space. However, prior works have largely overlooked integrating LLMs with domain-specific knowledge to ensure valid plans. In this paper, we propose a novel LLM-assisted planner integrated with problem decomposition, which first decomposes large planning problems into multiple simpler sub-tasks. Then we explore two novel paradigms to utilize LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where LLM4Inspire provides heuristic guidance according to general knowledge and LLM4Predict employs domain-specific knowledge to infer intermediate conditions. We empirically validate the effectiveness of our planner across multiple domains, demonstrating the ability of search space partition when solving large-scale planning problems. The experimental results show that LLMs effectively locate feasible solutions when pruning the search space, where infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds particular promise compared with LLM4Inspire, which offers general knowledge within LLMs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A weighted U statistic for association analysis considering genetic heterogeneity</title>
<link>https://arxiv.org/abs/1504.08319</link>
<guid>https://arxiv.org/abs/1504.08319</guid>
<content:encoded><![CDATA[
<div> genetic heterogeneity, complex diseases, association analysis, HWU method, nicotine dependence <br />
<br />
Summary: <br />
The article discusses the impact of genetic heterogeneity on complex diseases and highlights the limitations of existing statistical methods that assume homogeneous genetic effects. The HWU method is proposed to address this issue and improve the power of association analyses for diseases with heterogeneous genetic etiologies. Through simulations, the superiority and robustness of the HWU method are demonstrated, showing its ability to effectively analyze high-dimensional genetic data. Applying the HWU method to the SAGE dataset, a genome-wide analysis of nicotine dependence identifies two new genes (CYP3A5 and IKBKB) with heterogeneous effects on the disease. The study emphasizes the importance of considering genetic heterogeneity in genetic studies of complex diseases and provides a valuable tool for identifying novel genetic factors associated with disease susceptibility. <div>
arXiv:1504.08319v2 Announce Type: cross 
Abstract: Converging evidence suggests that common complex diseases with the same or similar clinical manifestations could have different underlying genetic etiologies. While current research interests have shifted toward uncovering rare variants and structural variations predisposing to human diseases, the impact of heterogeneity in genetic studies of complex diseases has been largely overlooked. Most of the existing statistical methods assume the disease under investigation has a homogeneous genetic effect and could, therefore, have low power if the disease undergoes heterogeneous pathophysiological and etiological processes. In this paper, we propose a heterogeneity weighted U (HWU) method for association analyses considering genetic heterogeneity. HWU can be applied to various types of phenotypes (e.g., binary and continuous) and is computationally effcient for high- dimensional genetic data. Through simulations, we showed the advantage of HWU when the underlying genetic etiology of a disease was heterogeneous, as well as the robustness of HWU against different model assumptions (e.g., phenotype distributions). Using HWU, we conducted a genome-wide analysis of nicotine dependence from the Study of Addiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis of nearly one million genetic markers took 7 hours, identifying heterogeneous effects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data</title>
<link>https://arxiv.org/abs/1505.01179</link>
<guid>https://arxiv.org/abs/1505.01179</guid>
<content:encoded><![CDATA[
<div> genetic association studies, sequencing data, high-dimensional genotypes, multiple disease phenotypes, generalized similarity U test <br />
Summary: <br />
- The paper discusses the challenges in genetic association studies using sequencing data due to the high dimensionality of genotypes and phenotypes. 
- Traditional statistical methods are not suitable for analyzing such data, especially when multiple disease phenotypes follow different distributions. 
- The proposed generalized similarity U test (GSU) addresses these challenges by being a similarity-based test that can handle high-dimensional data. 
- The study includes theoretical analysis of GSU, efficient p-value calculation, sample size, and power calculations. 
- Simulation results show that GSU outperforms existing methods in terms of power and robustness to phenotype distributions. 
- Application of GSU to the Dallas Heart Study data led to the identification of a joint association of 4 genes with 5 metabolic-related phenotypes. <br /> <div>
arXiv:1505.01179v3 Announce Type: cross 
Abstract: Sequencing-based studies are emerging as a major tool for genetic association studies of complex diseases. These studies pose great challenges to the traditional statistical methods (e.g., single-locus analyses based on regression methods) because of the high-dimensionality of data and the low frequency of genetic variants. In addition, there is a great interest in biology and epidemiology to identify genetic risk factors contributed to multiple disease phenotypes. The multiple phenotypes can often follow different distributions, which violates the assumptions of most current methods. In this paper, we propose a generalized similarity U test, referred to as GSU. GSU is a similarity-based test and can handle high-dimensional genotypes and phenotypes. We studied the theoretical properties of GSU, and provided the efficient p-value calculation for association test as well as the sample size and power calculation for the study design. Through simulation, we found that GSU had advantages over existing methods in terms of power and robustness to phenotype distributions. Finally, we used GSU to perform a multivariate analysis of sequencing data in the Dallas Heart Study and identified a joint association of 4 genes with 5 metabolic related phenotypes.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weighted U Statistic for Genetic Association Analyses of Sequencing Data</title>
<link>https://arxiv.org/abs/1505.01204</link>
<guid>https://arxiv.org/abs/1505.01204</guid>
<content:encoded><![CDATA[
<div> rare variants, next generation sequencing, genetic etiology, high-dimensional data analysis, weighted U statistic <br />
Summary: <br />
Advancements in next generation sequencing have led to a vast amount of data that can be used to study rare variants in complex diseases. Traditional statistical methods struggle with the high dimensionality and low frequency of variants in this data. To address this, the WU-seq method was developed, a weighted U statistic that does not rely on specific assumptions about the disease model or phenotype distribution. Simulation and empirical studies demonstrated that WU-SEQ outperformed the commonly used SKAT method, particularly when assumptions were violated. Even under ideal conditions, WU-SEQ showed comparable performance to SKAT. Applying WU-seq to sequencing data from the Dallas Heart Study revealed an association between ANGPTL 4 and very low density lipoprotein cholesterol. This method offers a powerful tool for exploring the genetic basis of complex diseases using high-dimensional sequencing data. <br /> <div>
arXiv:1505.01204v1 Announce Type: cross 
Abstract: With advancements in next generation sequencing technology, a massive amount of sequencing data are generated, offering a great opportunity to comprehensively investigate the role of rare variants in the genetic etiology of complex diseases. Nevertheless, this poses a great challenge for the statistical analysis of high-dimensional sequencing data. The association analyses based on traditional statistical methods suffer substantial power loss because of the low frequency of genetic variants and the extremely high dimensionality of the data. We developed a weighted U statistic, referred to as WU-seq, for the high-dimensional association analysis of sequencing data. Based on a non-parametric U statistic, WU-SEQ makes no assumption of the underlying disease model and phenotype distribution, and can be applied to a variety of phenotypes. Through simulation studies and an empirical study, we showed that WU-SEQ outperformed a commonly used SKAT method when the underlying assumptions were violated (e.g., the phenotype followed a heavy-tailed distribution). Even when the assumptions were satisfied, WU-SEQ still attained comparable performance to SKAT. Finally, we applied WU-seq to sequencing data from the Dallas Heart Study (DHS), and detected an association between ANGPTL 4 and very low density lipoprotein cholesterol.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci</title>
<link>https://arxiv.org/abs/1505.01206</link>
<guid>https://arxiv.org/abs/1505.01206</guid>
<content:encoded><![CDATA[
<div> genetic variants, joint association analysis, multifactor dimensionality reduction, Crohn's disease, genome wide association studies
Summary:
The study introduces an efficient approach, Trees Assembling Mann whitney (TAMW), for joint association analysis of low marginal effect (LME) genetic variants in complex diseases. TAMW outperforms existing methods like multifactor dimensionality reduction (MDR) and likelihood ratio based Mann whitney approach (LRMW) in detecting interactions among multiple LME loci. In simulations and an empirical study on Crohn's disease (CD) loci, TAMW demonstrates higher power and accuracy in identifying joint associations. Applying TAMW to a GWAS on CD, significant joint associations predisposing to the disease are discovered, implicating genes such as ATG16L1 and LACC1 in CD pathophysiology. The analysis of 459K SNPs in the GWAS is efficiently completed in 40 hours through parallel computing. TAMW proves to be a valuable tool for identifying complex disease associations involving multiple genetic variants and interactions. <br /><br />Summary: <div>
arXiv:1505.01206v1 Announce Type: cross 
Abstract: Common complex diseases are likely influenced by the interplay of hundreds, or even thousands, of genetic variants. Converging evidence shows that genetic variants with low marginal effects (LME) play an important role in disease development. Despite their potential significance, discovering LME genetic variants and assessing their joint association on high dimensional data (e.g., genome wide association studies) remain a great challenge. To facilitate joint association analysis among a large ensemble of LME genetic variants, we proposed a computationally efficient and powerful approach, which we call Trees Assembling Mann whitney (TAMW). Through simulation studies and an empirical data application, we found that TAMW outperformed multifactor dimensionality reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW) when the underlying complex disease involves multiple LME loci and their interactions. For instance, in a simulation with 20 interacting LME loci, TAMW attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW (power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci, TAMW also identified a stronger joint association with CD than those detected by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct a genome wide analysis. The analysis of 459K single nucleotide polymorphisms was completed in 40 hours using parallel computing, and revealed a joint association predisposing to CD (p-value=2.763e-19). Further analysis of the newly discovered association suggested that 13 genes, such as ATG16L1 and LACC1, may play an important role in CD pathophysiological and etiological processes.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Similarity U: A Non-parametric Test of Association Based on Similarity</title>
<link>https://arxiv.org/abs/1801.01220</link>
<guid>https://arxiv.org/abs/1801.01220</guid>
<content:encoded><![CDATA[
<div> Genetic association studies using second-generation sequencing technologies aim to identify sets of genetic variants that contribute to various phenotypes. A novel similarity-based test, generalized similarity U (GSU), was proposed to test the association between complex objects such as genotype and phenotype. Theoretical analysis showed that the Laplacian kernel-based similarity used in GSU enhances power and robustness. Simulation studies demonstrated that GSU outperformed existing methods in terms of power and robustness. In a whole-genome sequencing scan of Alzheimer's Disease Neuroimaging Initiative (ADNI) data, three genes  APOE, APOC1, and TOMM40  were identified as associated with imaging phenotypes. A C++ package for the analysis of whole-genome sequencing data using GSU was developed and made available for download. 

Keywords: genetic association studies, second-generation sequencing, generalized similarity U, Laplacian kernel, robustness<br /><br />Summary: 
Genetic association studies utilizing second-generation sequencing technologies aim to pinpoint genetic variants influencing various phenotypes. The novel GSU test, based on similarity measures, proved superior in power and robustness compared to traditional methods. Utilizing a Laplacian kernel further amplified these advantages. Application of GSU to ADNI data revealed genes associated with imaging phenotypes. Additionally, a user-friendly C++ package for whole-genome sequencing analysis using GSU was developed and shared on GitHub. <div>
arXiv:1801.01220v1 Announce Type: cross 
Abstract: Second generation sequencing technologies are being increasingly used for genetic association studies, where the main research interest is to identify sets of genetic variants that contribute to various phenotype. The phenotype can be univariate disease status, multivariate responses and even high-dimensional outcomes. Considering the genotype and phenotype as two complex objects, this also poses a general statistical problem of testing association between complex objects. We here proposed a similarity-based test, generalized similarity U (GSU), that can test the association between complex objects. We first studied the theoretical properties of the test in a general setting and then focused on the application of the test to sequencing association studies. Based on theoretical analysis, we proposed to use Laplacian kernel based similarity for GSU to boost power and enhance robustness. Through simulation, we found that GSU did have advantages over existing methods in terms of power and robustness. We further performed a whole genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative (ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with imaging phenotype. We developed a C++ package for analysis of whole genome sequencing data using GSU. The source codes can be downloaded at https://github.com/changshuaiwei/gsu.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning</title>
<link>https://arxiv.org/abs/2508.07264</link>
<guid>https://arxiv.org/abs/2508.07264</guid>
<content:encoded><![CDATA[
<div> Query Transforms, Fusion Scheme, Gating Mechanism, Q-Bottleneck, Mixture-of-Experts<br />
<br />
Summary: <br />
The study introduces a novel approach called \textsc{FLUID} for multimodal classification, focusing on integrating visual and textual signals efficiently. \textsc{FLUID} comprises three key components: Q-transforms for retaining important token-level features, a fusion scheme ensuring cross-modal consistency and task-specific fusion, and a lightweight Mixture-of-Experts model for specialized semantic pattern recognition. Experimental results on the GLAMI-1M benchmark show \textsc{FLUID} achieving a remarkable 91% accuracy, surpassing previous methods and demonstrating resistance to noise, class imbalance, and semantic diversity. Ablation studies validate the effectiveness of individual components and their combined synergies, highlighting \textsc{FLUID} as a scalable and robust solution for multimodal product classification. <br /><br /> <div>
arXiv:2508.07264v1 Announce Type: cross 
Abstract: Multimodal classification requires robust integration of visual and textual signals, yet common fusion strategies are brittle and vulnerable to modality-specific noise. In this paper, we present \textsc{FLUID}-Flow-Latent Unified Integration via Token Distillation for Expert Specialization, a principled token-level pipeline that improves cross-modal robustness and scalability. \textsc{FLUID} contributes three core elements: (1) \emph{Q-transforms}, learnable query tokens that distill and retain salient token-level features from modality-specific backbones; (2) a two-stage fusion scheme that enforces cross-modal consistency via contrastive alignment and then performs adaptive, task-aware fusion through a gating mechanism and a \emph{Q-bottleneck} that selectively compresses information for downstream reasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at prediction time that enables efficient specialization to diverse semantic patterns. Extensive experiments demonstrate that \textsc{FLUID} attains \(91\%\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior baselines and exhibiting strong resilience to label noise, long-tail class imbalance, and semantic heterogeneity. Targeted ablation studies corroborate both the individual and synergistic benefits of the proposed components, positioning \textsc{FLUID} as a scalable, noise-resilient solution for multimodal product classification.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization</title>
<link>https://arxiv.org/abs/2508.10913</link>
<guid>https://arxiv.org/abs/2508.10913</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, energy efficiency, single-timestep computation, Self-Dropping Neuron mechanism, Bayesian optimization

Summary:
Spiking Neural Networks (SNNs) offer energy efficiency benefits through sparse encoding but suffer from increased latency. A novel single-timestep SNN approach is proposed, optimizing spike generation and temporal parameters. The inclusion of a Self-Dropping Neuron mechanism enhances information capacity by adjusting thresholds and selectively suppressing spikes. By leveraging Bayesian optimization, an efficient inference mode with a single time step is achieved. Experimental results on various datasets show improved accuracy and reduced energy consumption compared to traditional SNN models. The proposed method achieves classification accuracies of 93.72%, 92.20%, and 69.45% on different datasets, while reducing energy consumption by 56%, 21%, and 22% respectively. These findings highlight the potential of single-timestep SNNs in improving efficiency and accuracy in edge computing scenarios. 

<br /><br />Summary: <div>
arXiv:2508.10913v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs), as an emerging biologically inspired computational model, demonstrate significant energy efficiency advantages due to their event-driven information processing mechanism. Compared to traditional Artificial Neural Networks (ANNs), SNNs transmit information through discrete spike signals, which substantially reduces computational energy consumption through their sparse encoding approach. However, the multi-timestep computation model significantly increases inference latency and energy, limiting the applicability of SNNs in edge computing scenarios. We propose a single-timestep SNN, which enhances accuracy and reduces computational energy consumption in a single timestep by optimizing spike generation and temporal parameters. We design a Self-Dropping Neuron mechanism, which enhances information-carrying capacity through dynamic threshold adjustment and selective spike suppression. Furthermore, we employ Bayesian optimization to globally search for time parameters and obtain an efficient inference mode with a single time step. Experimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that, compared to traditional multi-timestep SNNs employing the Leaky Integrate-and-Fire (LIF) model, our method achieves classification accuracies of 93.72%, 92.20%, and 69.45%, respectively, using only single-timestep spikes, while maintaining comparable or even superior accuracy. Additionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Quantitative Measures for Multiparty Behaviour Evaluation</title>
<link>https://arxiv.org/abs/2508.10916</link>
<guid>https://arxiv.org/abs/2508.10916</guid>
<content:encoded><![CDATA[
<div> Keywords: digital humans, multiparty interactions, evaluation metrics, skeletal motion data, intervention-driven framework

Summary: 
Digital humans are becoming increasingly prevalent in multiparty interactions, necessitating the development of comprehensive evaluation metrics. This study introduces a novel framework that assesses social behavior in skeletal motion data across three dimensions: synchrony, temporal alignment, and structural similarity. By applying perturbations to group interactions and conducting mixed-effects analyses, the framework's sensitivity and effectiveness are demonstrated. The results show that gesture kinematic dampening increases determinism but reduces beat consistency, while delays weaken participant coupling and pitch flattening elevates costs related to F0 Soft-DTW. Additionally, a perception study comparing full-video and skeleton-only renderings highlights representation effects. The three metrics offer valuable insights into spatial structure, timing alignment, and behavioral variability, providing a robust toolkit for evaluating and improving socially intelligent agents. The code for the framework is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.10916v1 Announce Type: cross 
Abstract: Digital humans are emerging as autonomous agents in multiparty interactions, yet existing evaluation metrics largely ignore contextual coordination dynamics. We introduce a unified, intervention-driven framework for objective assessment of multiparty social behaviour in skeletal motion data, spanning three complementary dimensions: (1) synchrony via Cross-Recurrence Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode Decompositionbased Beat Consistency, and (3) structural similarity via Soft Dynamic Time Warping. We validate metric sensitivity through three theory-driven perturbations -- gesture kinematic dampening, uniform speech-gesture delays, and prosodic pitch-variance reduction-applied to $\approx 145$ 30-second thin slices of group interactions from the DnD dataset. Mixed-effects analyses reveal predictable, joint-independent shifts: dampening increases CRQA determinism and reduces beat consistency, delays weaken cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A complementary perception study ($N=27$) compares judgments of full-video and skeleton-only renderings to quantify representation effects. Our three measures deliver orthogonal insights into spatial structure, timing alignment, and behavioural variability. Thereby forming a robust toolkit for evaluating and refining socially intelligent agents. Code available on \href{https://github.com/tapri-lab/gig-interveners}{GitHub}.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>