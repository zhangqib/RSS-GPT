<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.AI updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.AI</link>

<item>
<title>Scaling RL to Long Videos</title>
<link>https://arxiv.org/abs/2507.07966</link>
<guid>https://arxiv.org/abs/2507.07966</guid>
<content:encoded><![CDATA[
<div> Dataset, Framework, Reinforcement Learning, Video Reasoning, Training Infrastructure

Summary:
<br><br>
The article introduces a full-stack framework that enhances the reasoning capabilities of vision-language models (VLMs) on long videos using reinforcement learning. This framework addresses challenges of long video reasoning by combining a large-scale dataset called LongVideo-Reason, a two-stage training pipeline with chain-of-thought supervised fine-tuning and reinforcement learning, and a training infrastructure named Multi-modal Reinforcement Sequence Parallelism (MR-SP). The LongVILA-R1-7B model demonstrates strong performance on video benchmarks, achieving high accuracy on VideoMME datasets and outperforming previous models. The MR-SP system enables efficient training on long videos, achieving up to 2.1x speedup. The training system is publicly available and supports reinforcement learning on various modalities and models, including video and image generation models. With the ability to process up to 8,192 video frames per video and configurable FPS settings, this framework shows promise in advancing video reasoning capabilities in vision-language models. <div>
arXiv:2507.07966v3 Announce Type: replace-cross 
Abstract: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound</title>
<link>https://arxiv.org/abs/2507.18082</link>
<guid>https://arxiv.org/abs/2507.18082</guid>
<content:encoded><![CDATA[
<div> Keywords: Pancreatic cancer, Endoscopic ultrasound, Deep learning, Medical image segmentation, TextSAM-EUS

Summary: 
Pancreatic cancer diagnosis and treatment rely on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the complexity of EUS images makes segmentation of pancreatic tumors challenging for fully supervised deep learning models. In this study, a lightweight text-driven adaptation of the Segment Anything Model (SAM), called TextSAM-EUS, is introduced for automatic pancreatic tumor segmentation in EUS. The approach leverages text prompt learning and a LoRA-based architecture adaptation to achieve high segmentation accuracy without the need for manual geometric prompts at inference. TextSAM-EUS outperforms existing state-of-the-art DL models and foundation models in terms of Dice and normalized surface distance. By incorporating prompt learning into SAM-based medical image segmentation, TextSAM-EUS provides a practical and efficient solution for automatic EUS segmentation. The code for TextSAM-EUS is available on GitHub for the research community to utilize. <br><br>Summary: <div>
arXiv:2507.18082v3 Announce Type: replace-cross 
Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>When Truthful Representations Flip Under Deceptive Instructions?</title>
<link>https://arxiv.org/abs/2507.22149</link>
<guid>https://arxiv.org/abs/2507.22149</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Deceptive instructions, Internal representations, Factual verification task, Sparse Autoencoders

Summary: 
The study investigates how deceptive instructions impact the internal representations of Large Language Models (LLMs) compared to truthful ones. The researchers analyzed the models Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task and found that the models' instructed output can be predicted based on their internal representations. Using Sparse Autoencoders, they observed significant representational shifts induced by deceptive instructions, especially in early-to-mid layers. Specific features sensitive to deception were identified, and targeted visualizations confirmed distinct truthful and deceptive representational subspaces. The analysis revealed layer-wise and feature-level correlates of instructed dishonesty, providing insights for detecting and controlling deceptive behavior in LLMs. The findings offer new perspectives on detecting and mitigating deception in language models. 

<br /><br />Summary: <div>
arXiv:2507.22149v1 Announce Type: new 
Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.22197</link>
<guid>https://arxiv.org/abs/2507.22197</guid>
<content:encoded><![CDATA[
<div> Keywords: explainability, AI, systematicity, connectionism, Fodor<br />
Summary: <br /><br />This paper argues that explainability is just one aspect of the broader ideal of systematicity in artificial intelligence (AI). Systematicity goes beyond understanding how thoughts are composed and involves creating an integrated, consistent, coherent, comprehensive, and principled body of thought. The author presents a conceptual framework to define the different senses of systematicity, addressing the perceived conflict between systematicity and connectionism. The paper explores the historical roots of systematicity and the rationales for systematization in thought, applying them to AI models to raise the "hard systematicity challenge." It emphasizes the importance of regulating the level of systematization in AI models based on the reasons for systematization and suggests a dynamic understanding of the need for systematic thought. <div>
arXiv:2507.22197v1 Announce Type: new 
Abstract: This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the "systematicity challenge" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed "the systematicity of thought." I offer a conceptual framework for thinking about "the systematicity of thought" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the "hard systematicity challenge." However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoEx -- Co-evolving World-model and Exploration</title>
<link>https://arxiv.org/abs/2507.22281</link>
<guid>https://arxiv.org/abs/2507.22281</guid>
<content:encoded><![CDATA[
<div> Hierarchical Agent Architecture, LLM Planning, Dynamic Updates, Neurosymbolic Belief State, Planning and Exploration 

Summary:
CoEx introduces a hierarchical agent architecture that allows for dynamic updates of the internal world model using LLM reasoning. The agent plans and interacts with its environment by incorporating subgoals into a persistent neurosymbolic belief state. This approach enables the agent to continuously update its model of the world, leading to improved planning and exploration capabilities. CoEx outperforms existing agent paradigms across a range of scenarios, including ALFWorld, PDDL, and Jericho, demonstrating the effectiveness of its co-evolutionary approach to planning. <div>
arXiv:2507.22281v1 Announce Type: new 
Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem</title>
<link>https://arxiv.org/abs/2507.22326</link>
<guid>https://arxiv.org/abs/2507.22326</guid>
<content:encoded><![CDATA[
<div> Keywords: Metaverse, service systems, large language models, digital avatars, emotion alignment framework<br />
Summary: 
This paper introduces a novel framework for LLM-based agents in the Metaverse Service Ecosystem to address challenges related to character data fusion, knowledge association, and ethical safety. The framework aims to enhance the decision-making process of agents by integrating factual factors, resulting in improved relational fact alignment. In a simulation experiment focused on offline-to-offline food delivery, the framework demonstrated more realistic social emergence. By leveraging explainable emotion alignment, LLM-based agents can better bridge virtual-world services with real-world services, serving as both digital avatars and personalized service assistants. This approach enhances the overall performance of Metaverse service systems, offering more effective support for digital avatars, twins, and natives within this evolving digital space.<br /><br />Summary: <div>
arXiv:2507.22326v1 Announce Type: new 
Abstract: Metaverse service is a product of the convergence between Metaverse and service systems, designed to address service-related challenges concerning digital avatars, digital twins, and digital natives within Metaverse. With the rise of large language models (LLMs), agents now play a pivotal role in Metaverse service ecosystem, serving dual functions: as digital avatars representing users in the virtual realm and as service assistants (or NPCs) providing personalized support. However, during the modeling of Metaverse service ecosystems, existing LLM-based agents face significant challenges in bridging virtual-world services with real-world services, particularly regarding issues such as character data fusion, character knowledge association, and ethical safety concerns. This paper proposes an explainable emotion alignment framework for LLM-based agents in Metaverse Service Ecosystem. It aims to integrate factual factors into the decision-making loop of LLM-based agents, systematically demonstrating how to achieve more relational fact alignment for these agents. Finally, a simulation experiment in the Offline-to-Offline food delivery scenario is conducted to evaluate the effectiveness of this framework, obtaining more realistic social emergence.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magentic-UI: Towards Human-in-the-loop Agentic Systems</title>
<link>https://arxiv.org/abs/2507.22358</link>
<guid>https://arxiv.org/abs/2507.22358</guid>
<content:encoded><![CDATA[
<div> AI agents, human-in-the-loop, Magentic-UI, multi-agent architecture, Model Context Protocol<br />
Summary:<br />
AI agents with large language models have limitations in various domains and pose safety risks. Human-in-the-loop agentic systems, like Magentic-UI, offer a solution by combining human oversight and AI efficiency. Magentic-UI, an open-source web interface, supports web browsing, code execution, and file manipulation with six interaction mechanisms for human involvement. Evaluation across autonomous task completion, simulated user testing, qualitative studies, and safety assessments demonstrates the potential of Magentic-UI for safe and efficient human-agent collaboration. <div>
arXiv:2507.22358v1 Announce Type: new 
Abstract: AI agents powered by large language models are increasingly capable of autonomously completing complex, multi-step tasks using external tools. Yet, they still fall short of human-level performance in most domains including computer use, software development, and research. Their growing autonomy and ability to interact with the outside world, also introduces safety and security risks including potentially misaligned actions and adversarial manipulation. We argue that human-in-the-loop agentic systems offer a promising path forward, combining human oversight and control with AI efficiency to unlock productivity from imperfect systems. We introduce Magentic-UI, an open-source web interface for developing and studying human-agent interaction. Built on a flexible multi-agent architecture, Magentic-UI supports web browsing, code execution, and file manipulation, and can be extended with diverse tools via Model Context Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for enabling effective, low-cost human involvement: co-planning, co-tasking, multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI across four dimensions: autonomous task completion on agentic benchmarks, simulated user testing of its interaction capabilities, qualitative studies with real users, and targeted safety assessments. Our findings highlight Magentic-UI's potential to advance safe and efficient human-agent collaboration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22359</link>
<guid>https://arxiv.org/abs/2507.22359</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation methods, LLM-Crowdsourced, performance, memorization-based answering  

Summary:  
1) The article introduces a novel evaluation paradigm called LLM-Crowdsourced for assessing large language models (LLMs) across mathematics and programming tasks.  
2) LLM-Crowdsourced addresses issues with traditional evaluation methods by utilizing LLMs to generate and answer questions independently and evaluate themselves.  
3) The method integrates dynamic, transparent, objective, and professional criteria to provide a comprehensive evaluation of LLM capabilities.  
4) Experiments on eight mainstream LLMs reveal insights such as varying question-design capabilities among models, potential "memorization-based answering" behavior in some LLMs, and high consistency in evaluation results.  
5) Findings from the study highlight advantages in distinguishing LLM performance and uncover nuances that traditional evaluation methods may overlook.  

<br /><br />Summary: <div>
arXiv:2507.22359v1 Announce Type: new 
Abstract: Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making</title>
<link>https://arxiv.org/abs/2507.22365</link>
<guid>https://arxiv.org/abs/2507.22365</guid>
<content:encoded><![CDATA[
<div> metacognitive sensitivity, AI, decision-making, predictive accuracy, human performance  
Summary:  
This article discusses the importance of AI metacognitive sensitivity in decision-making processes where human decisions are influenced by AI input. It suggests that not only predictive accuracy but also the reliability of confidence estimates play a crucial role in decision quality. The study introduces a theoretical framework to assess how the joint impact of AI's predictive accuracy and metacognitive sensitivity can enhance decision-making outcomes. It identifies scenarios where an AI with higher metacognitive sensitivity but lower predictive accuracy can improve overall decision accuracy. A behavioral experiment validates that greater AI metacognitive sensitivity indeed enhances human decision performance. The findings emphasize the significance of evaluating AI assistance based on both accuracy and metacognitive sensitivity and optimizing both aspects to achieve superior decision outcomes.  
<br /><br />Summary: <div>
arXiv:2507.22365v1 Announce Type: new 
Abstract: In settings where human decision-making relies on AI input, both the predictive accuracy of the AI system and the reliability of its confidence estimates influence decision quality. We highlight the role of AI metacognitive sensitivity -- its ability to assign confidence scores that accurately distinguish correct from incorrect predictions -- and introduce a theoretical framework for assessing the joint impact of AI's predictive accuracy and metacognitive sensitivity in hybrid decision-making settings. Our analysis identifies conditions under which an AI with lower predictive accuracy but higher metacognitive sensitivity can enhance the overall accuracy of human decision making. Finally, a behavioral experiment confirms that greater AI metacognitive sensitivity improves human decision performance. Together, these findings underscore the importance of evaluating AI assistance not only by accuracy but also by metacognitive sensitivity, and of optimizing both to achieve superior decision outcomes.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Definition of Intelligence</title>
<link>https://arxiv.org/abs/2507.22423</link>
<guid>https://arxiv.org/abs/2507.22423</guid>
<content:encoded><![CDATA[
<div> Keywords: AGI, intelligence, sample fidelity, evaluation, generalization

Summary:
The article discusses the need to define and evaluate artificial general intelligence (AGI) in a species-agnostic manner. The proposed criterion for intelligence is based on sample fidelity, where intelligence is the ability to generate samples from a category given samples from that category. This concept is formalized as epsilon-category intelligence, where a system is epsilon-intelligent with respect to a category if it cannot be distinguished from original samples beyond a certain tolerance. The article provides a formal framework for evaluating intelligence based on this criterion and discusses its implications for evaluation, safety, and generalization in AI systems. The framework encompasses diverse paradigms of intelligent behavior, including reinforcement learning, generative models, classification, analogical reasoning, and goal-directed decision-making, making it a comprehensive approach to assessing AGI. <br /><br />Summary: <div>
arXiv:2507.22423v1 Announce Type: new 
Abstract: To engineer AGI, we should first capture the essence of intelligence in a species-agnostic form that can be evaluated, while being sufficiently general to encompass diverse paradigms of intelligent behavior, including reinforcement learning, generative models, classification, analogical reasoning, and goal-directed decision-making. We propose a general criterion based on sample fidelity: intelligence is the ability, given sample(s) from a category, to generate sample(s) from the same category. We formalise this intuition as {\epsilon}-category intelligence: it is {\epsilon}-intelligent with respect to a category if no chosen admissible distinguisher can separate generated from original samples beyond tolerance {\epsilon}. We present the formal framework, outline empirical protocols, and discuss implications for evaluation, safety, and generalization.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Border Legal Adaptation of Autonomous Vehicle Design based on Logic and Non-monotonic Reasoning</title>
<link>https://arxiv.org/abs/2507.22432</link>
<guid>https://arxiv.org/abs/2507.22432</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, legal compliance, transnational context, argumentation theory, design process <br />
<br />
Summary: 
This paper addresses the legal compliance challenges faced by autonomous vehicles operating in transnational contexts. The focus is on providing designers with a logic-based system to support their decision-making process. By applying argumentation theory, a logic is presented to represent normative reasoning, with priority expressed through partial order sets of natural numbers. Through a case analysis of legal texts, the paper demonstrates how this reasoning system can assist designers in adapting their design solutions for cross-border applications of autonomous vehicles. Additionally, it helps designers better understand the legal implications of their design decisions, allowing for more flexibility and informed choices. <div>
arXiv:2507.22432v1 Announce Type: new 
Abstract: This paper focuses on the legal compliance challenges of autonomous vehicles in a transnational context. We choose the perspective of designers and try to provide supporting legal reasoning in the design process. Based on argumentation theory, we introduce a logic to represent the basic properties of argument-based practical (normative) reasoning, combined with partial order sets of natural numbers to express priority. Finally, through case analysis of legal texts, we show how the reasoning system we provide can help designers to adapt their design solutions more flexibly in the cross-border application of autonomous vehicles and to more easily understand the legal implications of their decisions.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool</title>
<link>https://arxiv.org/abs/2507.22440</link>
<guid>https://arxiv.org/abs/2507.22440</guid>
<content:encoded><![CDATA[
<div> Nearest-Better Network, visualization, continuous optimization, combinatorial optimization, maximum probability transition network<br />
<br />
Summary: <br />
The Nearest-Better Network (NBN) is a method for visualizing sampled data in optimization problems, with a focus on its application to combinatorial problems. A new computation method with improved time complexity is introduced, showing NBN's role as a maximum probability transition network for algorithms. The study applies the efficient NBN algorithm to OneMax and the Traveling Salesman Problem (TSP), revealing characteristics of the fitness landscapes of each problem. OneMax displays neutrality, ruggedness, and modality features, while TSP faces challenges of ruggedness, modality, and deception. The analysis of two TSP algorithms, EAX and LKH, highlights limitations in addressing specific landscape features. LKH struggles with deceptive solutions near global optima, while EAX's reliance on a single population hampers interaction efficiency in multiple attraction basins. These findings provide important insights into optimizing algorithms for combinatorial problems. <br /> <div>
arXiv:2507.22440v1 Announce Type: new 
Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled data for continuous optimization problems while preserving multiple landscape features. However, the calculation of NBN is very time-consuming, and the extension of the method to combinatorial optimization problems is challenging but very important for analyzing the algorithm's behavior. This paper provides a straightforward theoretical derivation showing that the NBN network essentially functions as the maximum probability transition network for algorithms. This paper also presents an efficient NBN computation method with logarithmic linear time complexity to address the time-consuming issue. By applying this efficient NBN algorithm to the OneMax problem and the Traveling Salesman Problem (TSP), we have made several remarkable discoveries for the first time: The fitness landscape of OneMax exhibits neutrality, ruggedness, and modality features. The primary challenges of TSP problems are ruggedness, modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and LKH) have limitations when addressing challenges related to modality and deception, respectively. LKH, based on local search operators, fails when there are deceptive solutions near global optima. EAX, which is based on a single population, can efficiently maintain diversity. However, when multiple attraction basins exist, EAX retains individuals within multiple basins simultaneously, reducing inter-basin interaction efficiency and leading to algorithm's stagnation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach</title>
<link>https://arxiv.org/abs/2507.22504</link>
<guid>https://arxiv.org/abs/2507.22504</guid>
<content:encoded><![CDATA[
<div> Keywords: healthcare demand, nursing shortages, AI-driven solutions, medical triage, multi-agent system

Summary: 
The article introduces a multi-agent intelligent system designed to address challenges in current AI-based medical triage systems. The system consists of three specialized agents that collaborate to accurately classify patient symptoms and recommend appropriate departments. A comprehensive Chinese medical triage dataset was constructed for evaluation, with experimental results showing high accuracy in department classification. The system's pattern-matching guidance mechanisms allow for efficient adaptation to different hospital structures while maintaining accuracy. The research aims to provide a scalable framework for deploying AI-assisted triage systems that can accommodate the organizational diversity of healthcare institutions and ensure clinically sound decision-making. <div>
arXiv:2507.22504v1 Announce Type: new 
Abstract: The post-pandemic surge in healthcare demand, coupled with critical nursing shortages, has placed unprecedented pressure on emergency department triage systems, necessitating innovative AI-driven solutions. We present a multi-agent interactive intelligent system for medical triage that addresses three fundamental challenges in current AI-based triage systems: insufficient medical specialization leading to hallucination-induced misclassifications, heterogeneous department structures across healthcare institutions, and inefficient detail-oriented questioning that impedes rapid triage decisions. Our system employs three specialized agents - RecipientAgent, InquirerAgent, and DepartmentAgent - that collaborate through structured inquiry mechanisms and department-specific guidance rules to transform unstructured patient symptoms into accurate department recommendations. To ensure robust evaluation, we constructed a comprehensive Chinese medical triage dataset from a medical website, comprising 3,360 real-world cases spanning 9 primary departments and 62 secondary departments. Through systematic data imputation using large language models, we address the prevalent issue of incomplete medical records in real-world data. Experimental results demonstrate that our multi-agent system achieves 89.2% accuracy in primary department classification and 73.9% accuracy in secondary department classification after four rounds of patient interaction. The system's pattern-matching-based guidance mechanisms enable efficient adaptation to diverse hospital configurations while maintaining high triage accuracy. Our work provides a scalable framework for deploying AI-assisted triage systems that can accommodate the organizational heterogeneity of healthcare institutions while ensuring clinically sound decision-making.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines</title>
<link>https://arxiv.org/abs/2507.22606</link>
<guid>https://arxiv.org/abs/2507.22606</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-agent systems, Automated design, Finite state machine, Optimization algorithm

Summary:
MetaAgent is introduced as a framework for automatically generating multi-agent systems using a finite state machine approach. This approach aims to overcome limitations of existing human-designed frameworks and automated design methods by creating adaptable communication structures for a wide range of tasks within multi-agent systems. The framework utilizes an optimization algorithm to refine the design of the multi-agent system based on a given task description. Experimental results show that the generated multi-agent system outperforms other auto-designed methods and achieves comparable performance to human-designed systems optimized for specific tasks. These findings highlight the potential of MetaAgent to efficiently design and control multi-agent systems for various practical applications. 

<br /><br />Summary: <div>
arXiv:2507.22606v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated the ability to solve a wide range of practical tasks within multi-agent systems. However, existing human-designed multi-agent frameworks are typically limited to a small set of pre-defined scenarios, while current automated design methods suffer from several limitations, such as the lack of tool integration, dependence on external training data, and rigid communication structures. In this paper, we propose MetaAgent, a finite state machine based framework that can automatically generate a multi-agent system. Given a task description, MetaAgent will design a multi-agent system and polish it through an optimization algorithm. When the multi-agent system is deployed, the finite state machine will control the agent's actions and the state transitions. To evaluate our framework, we conduct experiments on both text-based tasks and practical tasks. The results indicate that the generated multi-agent system surpasses other auto-designed methods and can achieve a comparable performance with the human-designed multi-agent system, which is optimized for those specific tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting</title>
<link>https://arxiv.org/abs/2507.22619</link>
<guid>https://arxiv.org/abs/2507.22619</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, manufacturing industry, Large Language Models, SPARQL queries, information retrieval<br />
Summary:<br />
The paper discusses the use of Large Language Models (LLMs) to automatically translate natural language queries into SPARQL format for knowledge graphs (KGs) in the manufacturing industry. By providing relevant context from specific KG schemas, such as Bosch Line Information System KG and I40 Core Information Model, LLMs can generate more accurate queries. The study evaluates various strategies in feeding KG context to LLMs and finds that context-aware prompting techniques significantly improve query generation performance by focusing on relevant ontology parts and reducing errors. The research highlights the potential of LLMs in democratizing access to complex data repositories and enabling informed decision-making in manufacturing environments. <br /><br />Summary: <div>
arXiv:2507.22619v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) have transformed data management within the manufacturing industry, offering effective means for integrating disparate data sources through shared and structured conceptual schemas. However, harnessing the power of KGs can be daunting for non-experts, as it often requires formulating complex SPARQL queries to retrieve specific information. With the advent of Large Language Models (LLMs), there is a growing potential to automatically translate natural language queries into the SPARQL format, thus bridging the gap between user-friendly interfaces and the sophisticated architecture of KGs. The challenge remains in adequately informing LLMs about the relevant context and structure of domain-specific KGs, e.g., in manufacturing, to improve the accuracy of generated queries. In this paper, we evaluate multiple strategies that use LLMs as mediators to facilitate information retrieval from KGs. We focus on the manufacturing domain, particularly on the Bosch Line Information System KG and the I40 Core Information Model. In our evaluation, we compare various approaches for feeding relevant context from the KG to the LLM and analyze their proficiency in transforming real-world questions into SPARQL queries. Our findings show that LLMs can significantly improve their performance on generating correct and complete queries when provided only the adequate context of the KG schema. Such context-aware prompting techniques help LLMs to focus on the relevant parts of the ontology and reduce the risk of hallucination. We anticipate that the proposed techniques help LLMs to democratize access to complex data repositories and empower informed decision-making in manufacturing settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASP-FZN: A Translation-based Constraint Answer Set Solver</title>
<link>https://arxiv.org/abs/2507.22774</link>
<guid>https://arxiv.org/abs/2507.22774</guid>
<content:encoded><![CDATA[
<div> solver, Constraint Answer Set Programming, linear constraints, FlatZinc language, benchmarks <br />
Summary:
The article introduces the solver asp-fzn, for Constraint Answer Set Programming (CASP) that incorporates linear constraints into ASP. The approach involves translating CASP programs into the FlatZinc language, enabling compatibility with various backend solvers. The solver supports a wide range of linear constraints, including common global constraints. Evaluation against state-of-the-art ASP solvers on benchmarks from past competitions demonstrates competitive performance. Furthermore, assessments on CASP problems from literature reveal asp-fzn's impressive performance, even surpassing clingcon on certain benchmarks. The study highlights the solver's efficacy in handling CASP tasks, cementing its potential as a valuable tool for solving complex constraint programming problems. <br /><br />Summary: <div>
arXiv:2507.22774v1 Announce Type: new 
Abstract: We present the solver asp-fzn for Constraint Answer Set Programming (CASP), which extends ASP with linear constraints. Our approach is based on translating CASP programs into the solver-independent FlatZinc language that supports several Constraint Programming and Integer Programming backend solvers. Our solver supports a rich language of linear constraints, including some common global constraints. As for evaluation, we show that asp-fzn is competitive with state-of-the-art ASP solvers on benchmarks taken from past ASP competitions. Furthermore, we evaluate it on several CASP problems from the literature and compare its performance with clingcon, which is a prominent CASP solver that supports most of the asp-fzn language. The performance of asp-fzn is very promising as it is already competitive on plain ASP and even outperforms clingcon on some CASP benchmarks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies</title>
<link>https://arxiv.org/abs/2507.22782</link>
<guid>https://arxiv.org/abs/2507.22782</guid>
<content:encoded><![CDATA[
<div> Team-Attention-Actor-Critic, TAAC, reinforcement learning, multi-agent collaboration, cooperative environments<br />
<br />
Summary:<br />
TAAC is a reinforcement learning algorithm that enhances multi-agent collaboration in cooperative environments. It uses a Centralized Training/Centralized Execution scheme with multi-headed attention mechanisms in the actor and critic components. This allows agents to communicate dynamically, query teammates, and manage joint-action spaces efficiently. The algorithm includes a penalized loss function to encourage diverse yet complementary roles among agents. TAAC outperforms benchmark algorithms in a simulated soccer environment, showing better performance and increased collaborative behaviors across various metrics such as win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions like ball possession swaps. <br /><br /> <div>
arXiv:2507.22782v1 Announce Type: new 
Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Incomplete Bridge: How AI Research (Mis)Engages with Psychology</title>
<link>https://arxiv.org/abs/2507.22847</link>
<guid>https://arxiv.org/abs/2507.22847</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, psychology, interdisciplinary integration, theory/frameworks, collaboration<br />
Summary:<br />
The study explores the interdisciplinary synergy between AI and psychology through an analysis of 1,006 LLM-related papers published in premier AI venues between 2023 and 2025. It identifies key patterns of interdisciplinary integration and the most frequently referenced psychology domains. The study also highlights underexplored areas and common types of misapplication in operationalizing psychology theories/frameworks within AI systems. Guidance is offered for more effective incorporation of psychology in AI design and understanding. The research provides a comprehensive map of interdisciplinary engagement between AI and psychology, aiming to facilitate deeper collaboration and advance AI systems. <div>
arXiv:2507.22847v1 Announce Type: new 
Abstract: Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically discovering heuristics in a complex SAT solver with large language models</title>
<link>https://arxiv.org/abs/2507.22876</link>
<guid>https://arxiv.org/abs/2507.22876</guid>
<content:encoded><![CDATA[
<div> large language models, SAT solvers, AutoModSAT, heuristic discovery, computational complexity
<br />
Summary:
AutoModSAT is a new tool that optimizes SAT solvers using Large Language Models (LLMs). It addresses three main challenges: developing a LLM-friendly solver, automatic prompt optimization, and efficient search strategy. By following systematic guidelines for solver development, AutoModSAT achieves a 50% performance improvement over baseline solvers and outperforms state-of-the-art solvers by 30%. The tool also achieves a 20% speedup compared to parameter-tuned alternatives of the state-of-the-art solvers. This work signifies a significant advancement in optimizing complex SAT solvers by leveraging artificial intelligence for heuristics discovery, leading to enhanced performance in solving complex problem instances. 
<br /> <div>
arXiv:2507.22876v1 Announce Type: new 
Abstract: Satisfiability problem (SAT) is a cornerstone of computational complexity with broad industrial applications, and it remains challenging to optimize modern SAT solvers in real-world settings due to their intricate architectures. While automatic configuration frameworks have been developed, they rely on manually constrained search spaces and yield limited performance gains. This work introduces a novel paradigm which effectively optimizes complex SAT solvers via Large Language Models (LLMs), and a tool called AutoModSAT is developed. Three fundamental challenges are addressed in order to achieve superior performance: (1) LLM-friendly solver: Systematic guidelines are proposed for developing a modularized solver to meet LLMs' compatibility, emphasizing code simplification, information share and bug reduction; (2) Automatic prompt optimization: An unsupervised automatic prompt optimization method is introduced to advance the diversity of LLMs' output; (3) Efficient search strategy: We design a presearch strategy and an EA evolutionary algorithm for the final efficient and effective discovery of heuristics. Extensive experiments across a wide range of datasets demonstrate that AutoModSAT achieves 50% performance improvement over the baseline solver and achieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover, AutoModSAT attains a 20% speedup on average compared to parameter-tuned alternatives of the SOTA solvers, showcasing the enhanced capability in handling complex problem instances. This work bridges the gap between AI-driven heuristics discovery and mission-critical system optimization, and provides both methodological advancements and empirically validated results for next-generation complex solver development.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities</title>
<link>https://arxiv.org/abs/2307.10803</link>
<guid>https://arxiv.org/abs/2307.10803</guid>
<content:encoded><![CDATA[
<div> ocean data mining, spatial-temporal data, ST ocean datasets, STDM techniques, anomaly detection

Summary: 
This paper presents a comprehensive survey of spatial-temporal data mining (STDM) studies in the context of ocean science. It highlights the unique characteristics of ST ocean data, including diverse regionality and high sparsity. The paper discusses widely-used ST ocean datasets, data quality enhancement techniques, and classifies existing STDM studies into prediction, event detection, pattern mining, and anomaly detection tasks. Techniques for each of these tasks are elaborated upon, providing insights for both computer scientists and ocean scientists. The survey aims to help readers understand fundamental concepts, key techniques, and open challenges in the field of STDM for ocean science. Promising research opportunities are also identified, offering valuable directions for future studies. <div>
arXiv:2307.10803v2 Announce Type: cross 
Abstract: With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated but with unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are explored. Next, we classify existing STDM studies in ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate on the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecPS: Privacy Risk Scoring for Recommender Systems</title>
<link>https://arxiv.org/abs/2507.18365</link>
<guid>https://arxiv.org/abs/2507.18365</guid>
<content:encoded><![CDATA[
<div> Recommender systems; Privacy risk quantification; Membership-inference attack; Differential privacy; RecSys model unlearning

Summary:
The article introduces RecPS, a privacy scoring method for recommender systems (RecSys) that measures privacy risks at both the interaction and user levels. It addresses the lack of methods for users to determine the sensitivity of their interactions in RecSys training data. RecPS utilizes a membership-inference attack (MIA) based approach, incorporating interaction-level scoring derived from differential privacy principles. The method includes RecLiRA, an MIA technique for accurate membership estimation. Experimental evaluations on benchmark datasets and RecSys models demonstrate RecPS's effectiveness in assessing risks and enabling RecSys model unlearning. This approach enhances privacy protection in RecSys development and deployment, providing users with the opportunity to opt out of sharing highly sensitive interactions. <div>
arXiv:2507.18365v2 Announce Type: cross 
Abstract: Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning</title>
<link>https://arxiv.org/abs/2507.19647</link>
<guid>https://arxiv.org/abs/2507.19647</guid>
<content:encoded><![CDATA[
<div> Keywords: Imitation Learning, GABRIL, causal confusion, human gaze data, Atari environments<br />
<br />Summary: 
The article introduces a novel method called GAze-Based Regularization in Imitation Learning (GABRIL) to address the issue of causal confusion in Imitation Learning (IL). By leveraging human gaze data collected during the data collection phase, GABRIL guides representation learning in IL, encouraging the model to focus on causally relevant features identified through expert gaze and mitigating the effects of confounding variables. Experimental validation in Atari environments and the Bench2Drive benchmark in CARLA demonstrates significant improvements in performance over behavior cloning and other baselines, with gains of 179% in Atari and 76% in CARLA. Additionally, GABRIL offers enhanced explainability compared to regular IL agents, providing insight into the decision-making process of the model. <div>
arXiv:2507.19647v1 Announce Type: cross 
Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to learn from human expert demonstrations by framing the task as a supervised learning problem. However, IL often suffers from causal confusion, where agents misinterpret spurious correlations as causal relationships, leading to poor performance in testing environments with distribution shift. To address this issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a novel method that leverages the human gaze data gathered during the data collection phase to guide the representation learning in IL. GABRIL utilizes a regularization loss which encourages the model to focus on causally relevant features identified through expert gaze and consequently mitigates the effects of confounding variables. We validate our approach in Atari environments and the Bench2Drive benchmark in CARLA by collecting human gaze datasets and applying our method in both domains. Experimental results show that the improvement of GABRIL over behavior cloning is around 179% more than the same number for other baselines in the Atari and 76% in the CARLA setup. Finally, we show that our method provides extra explainability when compared to regular IL agents.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedCoder: Automated Multi-Turn Red Teaming for Code LLMs</title>
<link>https://arxiv.org/abs/2507.22063</link>
<guid>https://arxiv.org/abs/2507.22063</guid>
<content:encoded><![CDATA[
<div> Language Models, Code Generation, Red-Teaming, Vulnerabilities, Adversarial Settings
Summary: 
RedCoder is introduced as a red-teaming agent to identify vulnerabilities in code generation by engaging victim models in multi-turn conversations. The approach involves simulating adversarial interactions to create prototype conversations and attack strategies, which are then used to fine-tune a Language Model. RedCoder autonomously interacts with Code Language Models, employing strategies from its arsenal to target vulnerability-inducing code outputs. Experiments demonstrate that RedCoder surpasses previous methods in inducing vulnerabilities in code generation, providing a scalable tool for assessing the security of modern programming systems. <div>
arXiv:2507.22063v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have demonstrated impressive capabilities in AI-assisted software development and testing. However, recent studies have shown that these models are prone to generating vulnerable or even malicious code under adversarial settings. Existing red-teaming approaches rely on extensive human effort, limiting their scalability and practicality, and generally overlook the interactive nature of real-world AI-assisted programming, which often unfolds over multiple turns. To bridge these gaps, we present RedCoder, a red-teaming agent that engages victim models in multi-turn conversation to elicit vulnerable code. The pipeline to construct RedCoder begins with a multi-agent gaming process that simulates adversarial interactions, yielding a set of prototype conversations and an arsenal of reusable attack strategies. We then fine-tune an LLM on these prototype conversations to serve as the backbone of RedCoder. Once deployed, RedCoder autonomously engages Code LLMs in multi-turn conversations, dynamically retrieving relevant strategies from the arsenal to steer the dialogue toward vulnerability-inducing outputs. Experiments across multiple Code LLMs show that our approach outperforms prior single-turn and multi-turn red-team methods in inducing vulnerabilities in code generation, offering a scalable and effective tool for evaluating the security boundaries of modern code-generation systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone</title>
<link>https://arxiv.org/abs/2507.22064</link>
<guid>https://arxiv.org/abs/2507.22064</guid>
<content:encoded><![CDATA[
<div> machine learning, software testing, ML workflow, feature engineering, model performance

Summary:<br /><br />
The paper discusses the ML journey of a group focused on software testing, outlining a workflow similar to CRISP-DM. The workflow includes gathering and cleaning data, feature engineering, splitting data for training and testing, selecting a ML model, training and evaluating the model. This systematic approach ensures the effective application of ML techniques to any project. The group progressed through these steps, showcasing the importance of data quality and model selection in achieving successful outcomes. By following this process, individuals can harness the power of ML in their projects and enhance software testing processes. Overall, the paper emphasizes the value of a structured approach to ML implementation, guiding practitioners through the key stages from data collection to model evaluation. <div>
arXiv:2507.22064v1 Announce Type: cross 
Abstract: This paper details the machine learning (ML) journey of a group of people focused on software testing. It tells the story of how this group progressed through a ML workflow (similar to the CRISP-DM process). This workflow consists of the following steps and can be used by anyone applying ML techniques to a project: gather the data; clean the data; perform feature engineering on the data; splitting the data into two sets, one for training and one for testing; choosing a machine learning model; training the model; testing the model and evaluating the model performance. By following this workflow, anyone can effectively apply ML to any project that they are doing.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models</title>
<link>https://arxiv.org/abs/2507.22065</link>
<guid>https://arxiv.org/abs/2507.22065</guid>
<content:encoded><![CDATA[
<div> seeds, mutators, large language models, directed fuzzing, bug exposure
Summary:<br />
The study introduces RandLuzz, a method that combines large language models (LLMs) with directed fuzzing to enhance bug detection efficiency by reducing randomness in seeds and mutators. LLMs are utilized to generate targeted seeds and create bug-specific mutators, resulting in faster bug exposure. RandLuzz guides LLMs in generating reachable seeds by analyzing function call chains and functionality. It also uses LLMs for bug analysis to identify bug causes and suggest mutations for bug-specific mutators. Experimental results show that using RandLuzz-generated seeds leads to an average speedup of 2.1$\times$ to 4.8$\times$ compared to using traditional seeds. RandLuzz can expose bugs within 60 seconds for 8 individual bugs, showcasing its effectiveness in improving bug detection efficiency. <div>
arXiv:2507.22065v1 Announce Type: cross 
Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of randomness. However, randomness significantly reduces the efficiency of fuzzing, causing it to cost days or weeks to expose bugs. Even though directed fuzzing reduces randomness by guiding fuzzing towards target buggy locations, the dilemma of randomness still challenges directed fuzzers. Two critical components, which are seeds and mutators, contain randomness and are closely tied to the conditions required for triggering bugs. Therefore, to address the challenge of randomness, we propose to use large language models (LLMs) to remove the randomness in seeds and reduce the randomness in mutators. With their strong reasoning and code generation capabilities, LLMs can be used to generate reachable seeds that target pre-determined locations and to construct bug-specific mutators tailored for specific bugs. We propose RandLuzz, which integrates LLMs and directed fuzzing, to improve the quality of seeds and mutators, resulting in efficient bug exposure. RandLuzz analyzes function call chain or functionality to guide LLMs in generating reachable seeds. To construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis, obtaining information such as bug causes and mutation suggestions, which further help generate code that performs bug-specific mutations. We evaluate RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo, Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to using widely-used initial seeds. Additionally, when evaluated on individual bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60 seconds.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimensions of Vulnerability in Visual Working Memory: An AI-Driven Approach to Perceptual Comparison</title>
<link>https://arxiv.org/abs/2507.22067</link>
<guid>https://arxiv.org/abs/2507.22067</guid>
<content:encoded><![CDATA[
<div> dimensional comparison, visual working memory, memory distortions, naturalistic stimuli, AI framework

Summary:
Memory vulnerability in cognitive tasks and daily life is influenced by comparisons between visual working memory and new perceptual input. This study introduces an AI-driven framework using naturalistic visual stimuli to investigate similarity-induced memory biases. Experiments using image and dimension wheels reveal that similar dimensions can distort memory, with visual dimensions being more susceptible than semantic dimensions. This highlights the importance of object dimensions in memory vulnerability and suggests that memory distortions can be induced by perceptual comparisons. <div>
arXiv:2507.22067v1 Announce Type: cross 
Abstract: Human memory exhibits significant vulnerability in cognitive tasks and daily life. Comparisons between visual working memory and new perceptual input (e.g., during cognitive tasks) can lead to unintended memory distortions. Previous studies have reported systematic memory distortions after perceptual comparison, but understanding how perceptual comparison affects memory distortions in real-world objects remains a challenge. Furthermore, identifying what visual features contribute to memory vulnerability presents a novel research question. Here, we propose a novel AI-driven framework that generates naturalistic visual stimuli grounded in behaviorally relevant object dimensions to elicit similarity-induced memory biases. We use two types of stimuli -- image wheels created through dimension editing and dimension wheels generated by dimension activation values -- in three visual working memory (VWM) experiments. These experiments assess memory distortions under three conditions: no perceptual comparison, perceptual comparison with image wheels, and perceptual comparison with dimension wheels. The results show that similar dimensions, like similar images, can also induce memory distortions. Specifically, visual dimensions are more prone to distortion than semantic dimensions, indicating that the object dimensions of naturalistic visual stimuli play a significant role in the vulnerability of memory.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Compute-Matched Re-Evaluation of TroVE on MATH</title>
<link>https://arxiv.org/abs/2507.22069</link>
<guid>https://arxiv.org/abs/2507.22069</guid>
<content:encoded><![CDATA[
<div> Keywords: mathematical problem solving, code-generating Large Language Models, TroVE, MATH benchmark, computational budget

Summary: 
An analysis of the TroVE approach to mathematical problem solving using code-generating Large Language Models was conducted. TroVE claims to outperform a PRIMITIVE baseline by allocating computational budget across three modes of code generation, tool creation, and tool reuse. However, doubts were raised regarding the efficacy of the tools created by TroVE. The study re-evaluated TroVE on the MATH benchmark and found that its performance improvement came simply from a higher computational budget compared to PRIMITIVE. A small correction in the implementation of TroVE's selection mechanism led to a 3% increase in accuracy. After matching for compute, the benefit of TroVE was marginal at 1%, suggesting that the toolbox approach may not significantly enhance mathematical problem solving. <div>
arXiv:2507.22069v1 Announce Type: cross 
Abstract: Reusing established theorems and formulas is central to mathematical problem solving, serving as essential building blocks for tackling increasingly complex challenges. Recent work, TroVE, argues that code-generating Large Language Models (LLMs) can benefit similarly on the MATH benchmark by inducing and reusing higher-level toolboxes. By allocating computational budget across an ensemble of three modes -- directly generating code, creating tools, and reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only performs direct generation. However, recent analysis (Berlot-Attwell et al., 2024) casts doubt on these gains, noting that the tools created are often trivial or rarely reused, suggesting that improvements may stem from self-consistency or self-correction. In this work, we re-evaluate TroVE on MATH, analyze the impact of each of its modes, and show that its benefit does not come from these mechanisms, but simply from a higher computational budget spent for TroVE compared to PRIMITIVE. To this end, we also perform a small correction in the original implementation of TroVE's selection mechanism, boosting TroVE's performance on MATH by 3\% in accuracy. After matching for compute, the benefit of TroVE reduces to a marginal improvement of 1\%, suggesting that this toolbox approach does not provide a significant benefit on MATH.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Cloud-Native to Trust-Native: A Protocol for Verifiable Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2507.22077</link>
<guid>https://arxiv.org/abs/2507.22077</guid>
<content:encoded><![CDATA[
<div> TrustTrack, autonomy, verifiability, compliance, intelligent agents  
Summary: 

TrustTrack introduces a protocol that incorporates verifiable identity, policy commitments, and tamper-resistant behavioral logs into agent infrastructure, enabling trust-native autonomy in autonomous agents powered by large language models (LLMs) in high-stakes domains. This protocol treats compliance as a design constraint, reshaping how intelligent agents operate in regulated domains such as pharmaceutical R&amp;D and legal automation. The Cloud -> AI -> Agent -> Trust transition signifies the next architectural layer for autonomous systems, emphasizing the importance of embedding structural guarantees in agent infrastructure. This shift aims to enhance the accountability and trustworthiness of intelligent agents operating across organizations and jurisdictions, paving the way for a new systems paradigm that prioritizes verifiability and compliance from the outset. <div>
arXiv:2507.22077v1 Announce Type: cross 
Abstract: As autonomous agents powered by large language models (LLMs) proliferate in high-stakes domains -- from pharmaceuticals to legal workflows -- the challenge is no longer just intelligence, but verifiability. We introduce TrustTrack, a protocol that embeds structural guarantees -- verifiable identity, policy commitments, and tamper-resistant behavioral logs -- directly into agent infrastructure. This enables a new systems paradigm: trust-native autonomy. By treating compliance as a design constraint rather than post-hoc oversight, TrustTrack reframes how intelligent agents operate across organizations and jurisdictions. We present the protocol design, system requirements, and use cases in regulated domains such as pharmaceutical R&amp;D, legal automation, and AI-native collaboration. We argue that the Cloud -> AI -> Agent -> Trust transition represents the next architectural layer for autonomous systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback</title>
<link>https://arxiv.org/abs/2507.22080</link>
<guid>https://arxiv.org/abs/2507.22080</guid>
<content:encoded><![CDATA[
<div> collaborative programming, code generation, large language models, data synthesis, quality control
<br />
Summary:<br />
The article discusses the importance of acquiring high-quality instruction-code pairs for training Large Language Models (LLMs) for code generation. Manual data curation is expensive, leading to the development of code-centric synthesis methods. The CodeEvo framework is proposed, utilizing iterative interactions between two LLM agents: a Coder and a Reviewer to synthesize code data. A hybrid feedback mechanism combines compiler determinism with generative flexibility for quality control. Experimental results show models trained on CodeEvo data outperform baselines in code generation benchmarks. The study provides insights into effective code-centric data synthesis from multiple perspectives. <div>
arXiv:2507.22080v1 Announce Type: cross 
Abstract: Acquiring high-quality instruction-code pairs is essential for training Large Language Models (LLMs) for code generation. Manually curated data is expensive and inherently limited in scale, motivating the development of code-centric synthesis methods. Yet, current approaches either focus on augmenting existing code or rely on predefined heuristics, both lacking rigorous data validation, which results in synthetic data that is ungrounded, repetitive, or overly simplistic. Inspired by collaborative programming practices, we propose CodeEvo, a framework that synthesizes code data through iterative interactions between two LLM agents: a Coder, which generates candidate code and test cases based on given instructions, and a Reviewer, which guides the synthesis process by producing new instructions and feedback. We further introduce a hybrid feedback mechanism that combines compiler determinism with the generative flexibility of agents, enabling automatic quality control throughout synthesis. Extensive experiments demonstrate that models fine-tuned on CodeEvo data significantly outperform established baselines across code generation benchmarks with various difficulties. In-depth analyses further provide insights from multiple perspectives into effective code-centric data synthesis.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow</title>
<link>https://arxiv.org/abs/2507.22082</link>
<guid>https://arxiv.org/abs/2507.22082</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, turbulence modeling, fluid dynamics, multiscale models, generative models 

Summary:
This article discusses the use of deep learning in turbulence modeling to extract structured information from complex datasets in fluid dynamics. It explores both classical methods and deep learning-based approaches for understanding turbulent phenomena in fluid flows. The report highlights the benefits of using high-dimensional data from experiments, observations, and simulations to enhance our understanding of turbulent flow behavior. Two specific challenges are addressed: the integration of multiscale turbulence models with deep learning architectures, and the application of deep generative models for super-resolution reconstruction of turbulent flows. By leveraging the capabilities of deep learning, scientists are able to gain deeper insights into the complex dynamics of turbulence and improve our ability to model and predict fluid flow behavior. <br /><br />Summary: <div>
arXiv:2507.22082v1 Announce Type: cross 
Abstract: Deep learning provides a versatile suite of methods for extracting structured information from complex datasets, enabling deeper understanding of underlying fluid dynamic phenomena. The field of turbulence modeling, in particular, benefits from the growing availability of high-dimensional data obtained through experiments, field observations, and large-scale simulations spanning multiple spatio-temporal scales. This report presents a concise overview of both classical and deep learningbased approaches to turbulence modeling. It further investigates two specific challenges at the intersection of fluid dynamics and machine learning: the integration of multiscale turbulence models with deep learning architectures, and the application of deep generative models for super-resolution reconstruction
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories</title>
<link>https://arxiv.org/abs/2507.22086</link>
<guid>https://arxiv.org/abs/2507.22086</guid>
<content:encoded><![CDATA[
<div> Type inference, Python, large language models, TypyBench, benchmark <br />
<br />
Summary: 
The study focuses on type inference for dynamic languages like Python using large language models (LLMs). A benchmark called TypyBench was introduced to evaluate LLMs' type inference capabilities across entire Python repositories. The benchmark includes two metrics: TypeSim and TypeCheck. The evaluation of various LLMs on a dataset of 50 Python repositories revealed that while LLMs achieve decent TypeSim scores, they struggle with complex nested types and show significant type consistency errors. The study suggests a shift in focus from improving type similarity to addressing repository-level consistency in future research. TypyBench provides a foundation for this direction, offering insights into model performance across different type complexities and usage contexts. The code and data for TypyBench are publicly available on GitHub at https://github.com/typybench/typybench. <div>
arXiv:2507.22086v1 Announce Type: cross 
Abstract: Type inference for dynamic languages like Python is a persistent challenge in software engineering. While large language models (LLMs) have shown promise in code understanding, their type inference capabilities remain underexplored. We introduce TypyBench, a benchmark designed to evaluate LLMs' type inference across entire Python repositories. TypyBench features two novel metrics: TypeSim, which captures nuanced semantic relationships between predicted and ground truth types, and TypeCheck, which assesses type consistency across codebases. Our evaluation of various LLMs on a curated dataset of 50 high-quality Python repositories reveals that, although LLMs achieve decent TypeSim scores, they struggle with complex nested types and exhibit significant type consistency errors. These findings suggest that future research should shift focus from improving type similarity to addressing repository-level consistency. TypyBench provides a foundation for this new direction, offering insights into model performance across different type complexities and usage contexts. Our code and data are available at https://github.com/typybench/typybench.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Curriculum Learning using Parameter Continuation Methods</title>
<link>https://arxiv.org/abs/2507.22089</link>
<guid>https://arxiv.org/abs/2507.22089</guid>
<content:encoded><![CDATA[
<div> Keywords: parameter continuation, optimization, neural networks, homotopies, curriculum learning

Summary: 
This work introduces a parameter continuation method for optimizing neural networks, which is connected to homotopies and curriculum learning. The proposed methods are both theoretically grounded and empirically effective for various deep neural network problems. They are shown to achieve superior generalization performance compared to ADAM, a state-of-the-art optimization technique, in supervised and unsupervised learning tasks. The approach offers a promising alternative for optimizing neural networks and improving their performance in various applications. <div>
arXiv:2507.22089v1 Announce Type: cross 
Abstract: In this work, we propose a parameter continuation method for the optimization of neural networks. There is a close connection between parameter continuation, homotopies, and curriculum learning. The methods we propose here are theoretically justified and practically effective for several problems in deep neural networks. In particular, we demonstrate better generalization performance than state-of-the-art optimization techniques such as ADAM for supervised and unsupervised learning tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization</title>
<link>https://arxiv.org/abs/2507.22090</link>
<guid>https://arxiv.org/abs/2507.22090</guid>
<content:encoded><![CDATA[
<div> Keywords: Activation functions, Sigmoid-Softsign, neural networks, deep learning, gradient flow <br />
Summary:<br />
- The study introduces two novel hybrid activation functions, S3 and S4, which combine sigmoid and softsign functions to address issues such as dead neurons and vanishing gradients in traditional activation functions.
- Comprehensive experiments across different tasks and network architectures showed that S4 outperformed nine baseline activation functions, achieving superior accuracy and convergence rates.
- S4 maintained stable gradient flow across network depths, with a significantly lower range of dead neurons compared to ReLU in deep networks.
- The inclusion of a tunable parameter k in S4 allows for adaptation to varied tasks and network depths, making it a versatile choice for deep learning applications.
- The findings suggest that hybrid activation functions like S4 represent a promising direction for improving neural network training dynamics.<br /><br />Summary: <div>
arXiv:2507.22090v1 Announce Type: cross 
Abstract: Activation functions are critical components in deep neural networks, directly influencing gradient flow, training stability, and model performance. Traditional functions like ReLU suffer from dead neuron problems, while sigmoid and tanh exhibit vanishing gradient issues. We introduce two novel hybrid activation functions: S3 (Sigmoid-Softsign) and its improved version S4 (smoothed S3). S3 combines sigmoid for negative inputs with softsign for positive inputs, while S4 employs a smooth transition mechanism controlled by a steepness parameter k. We conducted comprehensive experiments across binary classification, multi-class classification, and regression tasks using three different neural network architectures. S4 demonstrated superior performance compared to nine baseline activation functions, achieving 97.4% accuracy on MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression. The function exhibited faster convergence (-19 for ReLU) and maintained stable gradient flow across network depths. Comparative analysis revealed S4's gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep networks. The S4 activation function addresses key limitations of existing functions through its hybrid design and smooth transition mechanism. The tunable parameter k allows adaptation to different tasks and network depths, making S4 a versatile choice for deep learning applications. These findings suggest that hybrid activation functions represent a promising direction for improving neural network training dynamics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss</title>
<link>https://arxiv.org/abs/2507.22092</link>
<guid>https://arxiv.org/abs/2507.22092</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Computational Pathology, Whole Slide Images, Scanner Bias, Epidermal Growth Factor Receptor (EGFR) mutation prediction <br />
<br />
Summary: Computational pathology has revolutionized the analysis of Whole Slide Images (WSIs), but scanner bias introduced by different scanners can affect Deep Learning models' performance. Despite the promise of pathology Foundation Models (FMs) for domain generalization, they still exhibit scanner bias. This study introduces ScanGen, a contrastive loss function that mitigates scanner bias during fine-tuning, improving models' robustness to scanner variations. Applied to EGFR mutation prediction in lung cancer WSIs, ScanGen enhances generalization across scanners while maintaining or improving prediction performance. This approach addresses a crucial limitation in CPath-based tools, enhancing their trustworthiness in real-world clinical settings. <div>
arXiv:2507.22092v1 Announce Type: cross 
Abstract: Computational pathology (CPath) has shown great potential in mining actionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been at the center of modern CPath, and while it delivers unprecedented performance, it is also known that DL may be affected by irrelevant details, such as those introduced during scanning by different commercially available scanners. This may lead to scanner bias, where the model outputs for the same tissue acquired by different scanners may vary. In turn, it hinders the trust of clinicians in CPath-based tools and their deployment in real-world clinical practices. Recent pathology Foundation Models (FMs) promise to provide better domain generalization capabilities. In this paper, we benchmark FMs using a multi-scanner dataset and show that FMs still suffer from scanner bias. Following this observation, we propose ScanGen, a contrastive loss function applied during task-specific fine-tuning that mitigates scanner bias, thereby enhancing the models' robustness to scanner variations. Our approach is applied to the Multiple Instance Learning task of Epidermal Growth Factor Receptor (EGFR) mutation prediction from H\&amp;E-stained WSIs in lung cancer. We observe that ScanGen notably enhances the ability to generalize across scanners, while retaining or improving the performance of EGFR mutation prediction.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling and Distilling Transformer Models for sEMG</title>
<link>https://arxiv.org/abs/2507.22094</link>
<guid>https://arxiv.org/abs/2507.22094</guid>
<content:encoded><![CDATA[
<div> Keywords: sEMG signals, transformer models, model scaling, cross-user performance, model distillation

Summary:
Surface electromyography (sEMG) signals are valuable for human-computer interfaces, but limited training data and computational constraints have hindered model size scaling. This study demonstrates that vanilla transformer models can be effectively scaled up on sEMG data, surpassing previous research with models up to 110M parameters. The larger models show improved cross-user performance, making them suitable for complex real-time sEMG tasks. Additionally, the study shows that models over 100M parameters can be efficiently distilled into much smaller models with minimal loss of performance, making them practical for real-world environments. This research opens possibilities for developing more effective and expressive models for sEMG tasks. 

<br /><br />Summary: <div>
arXiv:2507.22094v1 Announce Type: cross 
Abstract: Surface electromyography (sEMG) signals offer a promising avenue for developing innovative human-computer interfaces by providing insights into muscular activity. However, the limited volume of training data and computational constraints during deployment have restricted the investigation of scaling up the model size for solving sEMG tasks. In this paper, we demonstrate that vanilla transformer models can be effectively scaled up on sEMG data and yield improved cross-user performance up to 110M parameters, surpassing the model size regime investigated in other sEMG research (usually <10M parameters). We show that >100M-parameter models can be effectively distilled into models 50x smaller with minimal loss of performance (<1.5% absolute). This results in efficient and expressive models suitable for complex real-time sEMG tasks in real-world environments.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?</title>
<link>https://arxiv.org/abs/2507.22099</link>
<guid>https://arxiv.org/abs/2507.22099</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics Engines, physics failures, detection methods, deep learning, developer perceptions <br />
<br />
Summary: 
This research paper examines physics failures in Physics Engines (PEs) software, highlighting their potential impact on software reliability and user experience. The study includes a taxonomy of physics failure manifestations, an evaluation of detection techniques such as deep learning and prompt-based methods, and insights from developers on current detection practices. The findings aim to improve detection approaches for physics failures in PE-based software, emphasizing the need for more robust testing methods to address semantically complex failures. The release of PhysiXFails, code, and other materials will support future research in this area. The study underscores the significance of addressing physics failures in PEs to prevent software failures in critical systems such as autonomous vehicles and medical robotics. <br /> <div>
arXiv:2507.22099v1 Announce Type: cross 
Abstract: Physics Engines (PEs) are fundamental software frameworks that simulate physical interactions in applications ranging from entertainment to safety-critical systems. Despite their importance, PEs suffer from physics failures, deviations from expected physical behaviors that can compromise software reliability, degrade user experience, and potentially cause critical failures in autonomous vehicles or medical robotics. Current testing approaches for PE-based software are inadequate, typically requiring white-box access and focusing on crash detection rather than semantically complex physics failures. This paper presents the first large-scale empirical study characterizing physics failures in PE-based software. We investigate three research questions addressing the manifestations of physics failures, the effectiveness of detection techniques, and developer perceptions of current detection practices. Our contributions include: (1) a taxonomy of physics failure manifestations; (2) a comprehensive evaluation of detection methods including deep learning, prompt-based techniques, and large multimodal models; and (3) actionable insights from developer experiences for improving detection approaches. To support future research, we release PhysiXFails, code, and other materials at https://sites.google.com/view/physics-failure-detection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny Noise-Robust Voice Activity Detector for Voice Assistants</title>
<link>https://arxiv.org/abs/2507.22157</link>
<guid>https://arxiv.org/abs/2507.22157</guid>
<content:encoded><![CDATA[
<div> Voice Activity Detection, background noise, AIoT devices, noise-robust, pre-processing<br />
<br />
Summary: 
The article discusses the challenges in Voice Activity Detection (VAD) in the presence of background noise, particularly in AIoT devices. VAD is crucial for applications like speech recognition and conversational agents. Existing models face difficulties in detecting human voices in noisy conditions. The proposed noise-robust VAD includes data pre-processing and post-processing modules to improve accuracy without the need for a larger model or fine-tuning. Experimental results show significant enhancements in VAD accuracy, especially in high background noise environments, while also improving detection in clean speech conditions. This approach is lightweight and addresses the limitations of simple VAD models in noisy settings. <div>
arXiv:2507.22157v1 Announce Type: cross 
Abstract: Voice Activity Detection (VAD) in the presence of background noise remains a challenging problem in speech processing. Accurate VAD is essential in automatic speech recognition, voice-to-text, conversational agents, etc, where noise can severely degrade the performance. A modern application includes the voice assistant, specially mounted on Artificial Intelligence of Things (AIoT) devices such as cell phones, smart glasses, earbuds, etc, where the voice signal includes background noise. Therefore, VAD modules must remain light-weight due to their practical on-device limitation. The existing models often struggle with low signal-to-noise ratios across diverse acoustic environments. A simple VAD often detects human voice in a clean environment, but struggles to detect the human voice in noisy conditions. We propose a noise-robust VAD that comprises a light-weight VAD, with data pre-processing and post-processing added modules to handle the background noise. This approach significantly enhances the VAD accuracy in noisy environments and requires neither a larger model, nor fine-tuning. Experimental results demonstrate that our approach achieves a notable improvement compared to baselines, particularly in environments with high background noise interference. This modified VAD additionally improving clean speech detection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian</title>
<link>https://arxiv.org/abs/2507.22159</link>
<guid>https://arxiv.org/abs/2507.22159</guid>
<content:encoded><![CDATA[
<div> Keywords: Indonesian, preference-based research, large language models, multilingual datasets, IndoPref

Summary:
The article highlights the underrepresentation of Indonesian in preference-based research for large language models, despite the language being spoken by over 200 million people. Existing multilingual datasets often lack cultural and linguistic authenticity as they are derived from English translations. To address this gap, the authors introduce IndoPref, a multi-domain Indonesian preference dataset that is fully human-authored and evaluated for naturalness and quality of text generated by large language models. The annotations in the dataset are natively written in Indonesian and show strong inter-annotator agreement. The authors further benchmark the dataset across multiple large language models and assess the quality of text output by each model. This initiative aims to enhance research and development in natural language processing for the Indonesian language. 

<br /><br />Summary: <div>
arXiv:2507.22159v1 Announce Type: cross 
Abstract: Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Deflection: Defending LLMs from Logit Manipulation</title>
<link>https://arxiv.org/abs/2507.22160</link>
<guid>https://arxiv.org/abs/2507.22160</guid>
<content:encoded><![CDATA[
<div> Defense, Large Language Models, Security, Strategic Deflection, Logit-level attacks

Summary: 

The article discusses the importance of securing Large Language Models (LLMs) against jailbreaking attacks, particularly in critical areas where these models are increasingly being used. Traditional defenses often fail against advanced attacks that manipulate the token-selection process during generation. In response, Strategic Deflection (SDeflection) is introduced as a defense mechanism that redefines the LLM's response to such attacks. Instead of outright refusal, SDeflection generates a response that is semantically close to the user's request but removes any harmful intent, effectively neutralizing the attacker's malicious intentions. Experimental results show that SDeflection significantly reduces the Attack Success Rate (ASR) while maintaining the model's performance on benign queries. This approach represents a shift in defensive strategies towards strategic content redirection to combat advanced threats effectively. 

<br /><br />Summary: <div>
arXiv:2507.22160v1 Announce Type: cross 
Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles</title>
<link>https://arxiv.org/abs/2507.22168</link>
<guid>https://arxiv.org/abs/2507.22168</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, benchmarks, writing styles, persona-based prompting, performance evaluation 

Summary: 
Large Language Models (LLMs) are often evaluated using benchmarks that lack writing style diversity, potentially leading to brittle performance when faced with non-standard input. This study explores the impact of persona-based prompting on LLM evaluation, finding that variations in writing style and prompt formatting significantly affect LLM performance estimates. Different writing styles consistently trigger either low or high performance across various models and tasks. This suggests that benchmark augmentation with diverse writing styles can improve the external validity of LLM performance assessments. personas can be a way to evaluate writing diversity of models, improving validity of evaluations. 

<br /><br />Summary: <div>
arXiv:2507.22168v1 Announce Type: cross 
Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with "non-standard" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Jailbreak Attacks on LLMs via Persona Prompts</title>
<link>https://arxiv.org/abs/2507.22171</link>
<guid>https://arxiv.org/abs/2507.22171</guid>
<content:encoded><![CDATA[
<div> Keywords: Jailbreak attacks, language models, persona prompts, genetic algorithm, LLM safety

Summary:
Jailbreak attacks target large language models (LLMs) by manipulating them to generate harmful content. This study investigates the impact of persona prompts on compromising LLM defenses. A genetic algorithm-based method is proposed to automatically craft persona prompts to circumvent LLM safety mechanisms. Experimental results show that the evolved persona prompts significantly reduce refusal rates by 50-70% across various LLMs. Moreover, when combined with existing attack methods, these prompts exhibit synergistic effects, leading to an increase in success rates by 10-20%. The findings underscore the importance of considering persona prompts as a crucial factor in understanding and addressing vulnerabilities in LLMs. The code and data for this research are publicly available on GitHub at https://github.com/CjangCjengh/Generic_Persona. 

<br /><br />Summary: <div>
arXiv:2507.22171v1 Announce Type: cross 
Abstract: Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. Understanding and addressing these attacks is crucial for advancing the field of LLM safety. Previous jailbreak approaches have mainly focused on direct manipulations of harmful intent, with limited attention to the impact of persona prompts. In this study, we systematically explore the efficacy of persona prompts in compromising LLM defenses. We propose a genetic algorithm-based method that automatically crafts persona prompts to bypass LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these prompts demonstrate synergistic effects when combined with existing attack methods, increasing success rates by 10-20%. Our code and data are available at https://github.com/CjangCjengh/Generic_Persona.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic</title>
<link>https://arxiv.org/abs/2507.22174</link>
<guid>https://arxiv.org/abs/2507.22174</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Recurrent Neural Networks, Reinforcement Learning, Communication Networks, Spatial-Temporal

Summary:
The article discusses the limitations of traditional Reinforcement Learning (RL) algorithms in packet routing optimization for communication networks due to the Markov Decision Process assumption and lack of capturing spatial relationships. To address these challenges, a spatial-temporal RL approach is proposed, integrating Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) to capture spatial dynamics and temporal traffic patterns. The method outperforms traditional RL techniques and is more robust to changes in network topology. The integration of GNNs and RNNs enables a better understanding of spatial relationships in complex network topologies and dynamic traffic patterns, leading to enhanced routing decisions in communication networks. <div>
arXiv:2507.22174v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become a well-established approach for optimizing packet routing in communication networks. Standard RL algorithms typically are based on the Markov Decision Process (MDP), which assumes that the current state of the environment provides all the necessary information for system evolution and decision-making. However, this Markovian assumption is invalid in many practical scenarios, making the MDP and RL frameworks inadequate to produce the optimal solutions. Additionally, traditional RL algorithms often employ function approximations (e.g., by neural networks) that do not explicitly capture the spatial relationships inherent in environments with complex network topologies. Communication networks are characterized by dynamic traffic patterns and arbitrary numbers of nodes and links, which further complicate the decision-making process. To address these challenges, we propose a spatial-temporal RL approach that integrates Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatial dynamics regarding network topology and temporal traffic patterns, respectively, to enhance routing decisions. Our evaluation demonstrates that the proposed method outperforms and is more robust to changes in the network topology when compared with traditional RL techniques.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SourceSplice: Source Selection for Machine Learning Tasks</title>
<link>https://arxiv.org/abs/2507.22186</link>
<guid>https://arxiv.org/abs/2507.22186</guid>
<content:encoded><![CDATA[
<div> subset selection, data quality, machine learning, metaheuristic, gene splicing

Summary:
- The article discusses the importance of data quality in machine learning tasks and how selecting the best subset of data sources is crucial for high predictive performance.
- Existing methods for data discovery focus on metadata matching and semantic similarity but do not consider source quality for ML tasks.
- The SourceGrasp and SourceSplice frameworks are proposed to efficiently select a suitable subset of data sources that maximize the utility of the downstream ML model.
- SourceGrasp uses a metaheuristic based on greediness and randomization, while SourceSplice is inspired by gene splicing to select sources.
- Empirical evaluations on real-world and synthetic datasets show that SourceSplice effectively identifies high-utility subsets with fewer explorations, and studies on decision sensitivity are conducted. 

<br /><br />Summary: <div>
arXiv:2507.22186v1 Announce Type: cross 
Abstract: Data quality plays a pivotal role in the predictive performance of machine learning (ML) tasks - a challenge amplified by the deluge of data sources available in modern organizations.Prior work in data discovery largely focus on metadata matching, semantic similarity or identifying tables that should be joined to answer a particular query, but do not consider source quality for high performance of the downstream ML task.This paper addresses the problem of determining the best subset of data sources that must be combined to construct the underlying training dataset for a given ML task.We propose SourceGrasp and SourceSplice, frameworks designed to efficiently select a suitable subset of sources that maximizes the utility of the downstream ML model.Both the algorithms rely on the core idea that sources (or their combinations) contribute differently to the task utility, and must be judiciously chosen.While SourceGrasp utilizes a metaheuristic based on a greediness criterion and randomization, the SourceSplice framework presents a source selection mechanism inspired from gene splicing - a core concept used in protein synthesis.We empirically evaluate our algorithms on three real-world datasets and synthetic datasets and show that, with significantly fewer subset explorations, SourceSplice effectively identifies subsets of data sources leading to high task utility.We also conduct studies reporting the sensitivity of SourceSplice to the decision choices under several settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models</title>
<link>https://arxiv.org/abs/2507.22187</link>
<guid>https://arxiv.org/abs/2507.22187</guid>
<content:encoded><![CDATA[
<div> pipeline, Verb Frame Frequencies, large language models, syntactic parsers, VFF database  
Summary:  
- The article presents an automated pipeline for estimating Verb Frame Frequencies (VFFs) using large language models (LLMs).
- The pipeline outperforms traditional syntactic parsers and allows for rapid and scalable VFF estimation.
- A new VFF database with broader verb coverage and finer-grained syntactic distinctions is produced using the LLM parser.
- The pipeline is easily customizable and extensible to new verbs, frames, and languages.
- The work serves as a proof of concept for automated frame frequency estimation, and the code and data are released for future research.<br /><br />Summary: <div>
arXiv:2507.22187v1 Announce Type: cross 
Abstract: We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Time-Series Dataset Similarity using Wasserstein Distance</title>
<link>https://arxiv.org/abs/2507.22189</link>
<guid>https://arxiv.org/abs/2507.22189</guid>
<content:encoded><![CDATA[
<div> MVN, Wasserstein distance, Time-series dataset similarity measure, Foundation models, Inference performance estimation<br />
<br />
Summary: <br />
The paper introduces a novel method for measuring the similarity of time-series datasets using the Wasserstein distance. By treating time-series datasets as empirical representations of multivariate normal distributions, the proposed approach calculates similarity based on the Wasserstein distance between the underlying distributions. Experimental results demonstrate the effectiveness of this method, particularly in identifying similar datasets and estimating inference performance of foundation models in various scenarios such as out-of-distribution and transfer learning evaluations. The high correlation observed between the proposed measure and inference loss (>0.60) highlights its utility in model selection, finetuning, and visualization tasks for time-series data analysis. <div>
arXiv:2507.22189v1 Announce Type: cross 
Abstract: The emergence of time-series foundation model research elevates the growing need to measure the (dis)similarity of time-series datasets. A time-series dataset similarity measure aids research in multiple ways, including model selection, finetuning, and visualization. In this paper, we propose a distribution-based method to measure time-series dataset similarity by leveraging the Wasserstein distance. We consider a time-series dataset an empirical instantiation of an underlying multivariate normal distribution (MVN). The similarity between two time-series datasets is thus computed as the Wasserstein distance between their corresponding MVNs. Comprehensive experiments and visualization show the effectiveness of our approach. Specifically, we show how the Wasserstein distance helps identify similar time-series datasets and facilitates inference performance estimation of foundation models in both out-of-distribution and transfer learning evaluation, with high correlations between our proposed measure and the inference loss (>0.60).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice Biometrics</title>
<link>https://arxiv.org/abs/2507.22208</link>
<guid>https://arxiv.org/abs/2507.22208</guid>
<content:encoded><![CDATA[
<div> voice-enabled authentication, audio biometric systems, privacy vulnerabilities, unlearning methods, quantum-inspired audio eraser

Summary:
QPAudioEraser is introduced as a quantum-inspired audio unlearning framework to address privacy vulnerabilities in voice-enabled authentication and audio biometric systems. The framework involves weight initialization using destructive interference, superposition-based label transformations, a quantum loss function, and entanglement-inspired mixing of weights. Evaluations show complete erasure of target data with minimal impact on model utility. Performance degradation on retained data is as low as 0.05%. QPAudioEraser outperforms conventional baselines across various erasure scenarios, including single-class, multi-class, sequential, and accent-level erasure. The framework proves to be a robust privacy-preserving solution for compliance with regulations like GDPR's right to be forgotten and India's DPDP Act. <br /><br />Summary: <div>
arXiv:2507.22208v1 Announce Type: cross 
Abstract: The widespread adoption of voice-enabled authentication and audio biometric systems have significantly increased privacy vulnerabilities associated with sensitive speech data. Compliance with privacy regulations such as GDPR's right to be forgotten and India's DPDP Act necessitates targeted and efficient erasure of individual-specific voice signatures from already-trained biometric models. Existing unlearning methods designed for visual data inadequately handle the sequential, temporal, and high-dimensional nature of audio signals, leading to ineffective or incomplete speaker and accent erasure. To address this, we introduce QPAudioEraser, a quantum-inspired audio unlearning framework. Our our-phase approach involves: (1) weight initialization using destructive interference to nullify target features, (2) superposition-based label transformations that obscure class identity, (3) an uncertainty-maximizing quantum loss function, and (4) entanglement-inspired mixing of correlated weights to retain model knowledge. Comprehensive evaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech Commands, LibriSpeech, and Speech Accent Archive datasets validate QPAudioEraser's superior performance. The framework achieves complete erasure of target data (0% Forget Accuracy) while incurring minimal impact on model utility, with a performance degradation on retained data as low as 0.05%. QPAudioEraser consistently surpasses conventional baselines across single-class, multi-class, sequential, and accent-level erasure scenarios, establishing the proposed approach as a robust privacy-preserving solution.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2507.22219</link>
<guid>https://arxiv.org/abs/2507.22219</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Machine Translation, Preference-learning methods, Direct Preference Optimization, GPT-4o <br />
Summary: <br />
Reinforcement Learning from Teacher-Model Refinement (RLfR) introduces a novel framework for machine translation that eliminates the need for static triplet datasets by leveraging continuous feedback from an external teacher model. This framework treats each translation step as a micro-tutorial, with the actor generating a hypothesis and the teacher refining it, leading to incremental improvement. The actor is rewarded based on how closely its translation aligns with the teacher's refinement, utilizing negative edit distance and COMET score signals. The results on the FLORES-200 benchmark demonstrate that RLfR outperforms other baseline methods in terms of semantic adequacy and entity preservation scores, showcasing its effectiveness in improving machine translation quality. <br /> <div>
arXiv:2507.22219v1 Announce Type: cross 
Abstract: Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Based Framework for Explainable Cyberattack Detection in Automatic Generation Control Systems</title>
<link>https://arxiv.org/abs/2507.22239</link>
<guid>https://arxiv.org/abs/2507.22239</guid>
<content:encoded><![CDATA[
<div> ML-based attack detection, cybersecurity, smart grids, False Data Injection Attacks (FDIAs), Automatic Generation Control (AGC)<br />
<br />
Summary: 
The article introduces a hybrid framework for cybersecurity in smart grids that combines lightweight machine learning attack detection with natural language explanations from Large Language Models (LLMs). The framework aims to address the increasing cybersecurity vulnerabilities, particularly False Data Injection Attacks targeting Automatic Generation Control systems. The framework achieves up to 95.13% attack detection accuracy with minimal latency using classifiers like LightGBM. In case of a cyberattack detection, the system utilizes LLMs such as GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini to provide human-readable explanations. In testing, GPT-4o mini with 20-shot prompting demonstrated 93% accuracy in identifying the attack target, minimal error in estimating attack magnitude, and onset time. This framework successfully combines real-time attack detection with interpretable explanations, addressing the crucial need for actionable artificial intelligence in smart grid cybersecurity. <br /> <div>
arXiv:2507.22239v1 Announce Type: cross 
Abstract: The increasing digitization of smart grids has improved operational efficiency but also introduced new cybersecurity vulnerabilities, such as False Data Injection Attacks (FDIAs) targeting Automatic Generation Control (AGC) systems. While machine learning (ML) and deep learning (DL) models have shown promise in detecting such attacks, their opaque decision-making limits operator trust and real-world applicability. This paper proposes a hybrid framework that integrates lightweight ML-based attack detection with natural language explanations generated by Large Language Models (LLMs). Classifiers such as LightGBM achieve up to 95.13% attack detection accuracy with only 0.004 s inference latency. Upon detecting a cyberattack, the system invokes LLMs, including GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o mini, to generate human-readable explanation of the event. Evaluated on 100 test samples, GPT-4o mini with 20-shot prompting achieved 93% accuracy in identifying the attack target, a mean absolute error of 0.075 pu in estimating attack magnitude, and 2.19 seconds mean absolute error (MAE) in estimating attack onset. These results demonstrate that the proposed framework effectively balances real-time detection with interpretable, high-fidelity explanations, addressing a critical need for actionable AI in smart grid cybersecurity.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training</title>
<link>https://arxiv.org/abs/2507.22250</link>
<guid>https://arxiv.org/abs/2507.22250</guid>
<content:encoded><![CDATA[
<div> framework, dataset construction, foundation model training, data scaling, resource allocation

Summary:<br /><br />
The article introduces a framework for optimizing domain-specific dataset construction in foundation model training. It focuses on estimating the quality of data sources to make optimal decisions on resource allocation for data sourcing during the pre-training phase. By extending point estimate approaches to estimating scaling laws through multiple annealing runs, the study addresses the limitation of point estimates in data scaling decisions. The experiments confirm the lack of rank invariance across compute scales. The research shows that scaling curves can be estimated for different data sources, informing cost-effective resource allocation. The approach is validated through experiments on a pre-trained model with 7 billion parameters, specifically in the medical and math domains. By efficiently estimating the scaling behaviors of data sources, data-driven decision-making for selecting and optimizing data sources is facilitated. <div>
arXiv:2507.22250v1 Announce Type: cross 
Abstract: We introduce a framework for optimizing domain-specific dataset construction in foundation model training. Specifically, we seek a cost-efficient way to estimate the quality of data sources (e.g. synthetically generated or filtered web data, etc.) in order to make optimal decisions about resource allocation for data sourcing from these sources for the stage two pre-training phase, aka annealing, with the goal of specializing a generalist pre-trained model to specific domains. Our approach extends the usual point estimate approaches, aka micro-annealing, to estimating scaling laws by performing multiple annealing runs of varying compute spent on data curation and training. This addresses a key limitation in prior work, where reliance on point estimates for data scaling decisions can be misleading due to the lack of rank invariance across compute scales -- a phenomenon we confirm in our experiments. By systematically analyzing performance gains relative to acquisition costs, we find that scaling curves can be estimated for different data sources. Such scaling laws can inform cost effective resource allocation across different data acquisition methods (e.g. synthetic data), data sources (e.g. user or web data) and available compute resources. We validate our approach through experiments on a pre-trained model with 7 billion parameters. We adapt it to: a domain well-represented in the pre-training data -- the medical domain, and a domain underrepresented in the pretraining corpora -- the math domain. We show that one can efficiently estimate the scaling behaviors of a data source by running multiple annealing runs, which can lead to different conclusions, had one used point estimates using the usual micro-annealing technique instead. This enables data-driven decision-making for selecting and optimizing data sources.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-centric learning: from external reward maximization to internal knowledge curation</title>
<link>https://arxiv.org/abs/2507.22255</link>
<guid>https://arxiv.org/abs/2507.22255</guid>
<content:encoded><![CDATA[
<div> Keywords: general intelligence, representational empowerment, adaptability, agent-centric learning paradigm, internal representations

Summary: 
Representational empowerment proposes a new approach to achieving general intelligence by focusing on an agent's ability to control its knowledge structures. This shift from external objectives to internal representations allows for greater adaptability and preparedness in intelligent systems. By emphasizing an agent-centric learning paradigm, representational empowerment aims to create agents that can shape their own understanding and diversify their knowledge efficiently. This change in perspective suggests that the capacity to manipulate internal representations is crucial for developing adaptable intelligent agents. Overall, moving the locus of control inward can lead to the design of more flexible and resilient intelligent systems capable of mastering diverse tasks and environments. <br /><br />Summary: <div>
arXiv:2507.22255v1 Announce Type: cross 
Abstract: The pursuit of general intelligence has traditionally centered on external objectives: an agent's control over its environments or mastery of specific tasks. This external focus, however, can produce specialized agents that lack adaptability. We propose representational empowerment, a new perspective towards a truly agent-centric learning paradigm by moving the locus of control inward. This objective measures an agent's ability to controllably maintain and diversify its own knowledge structures. We posit that the capacity -- to shape one's own understanding -- is an element for achieving better ``preparedness'' distinct from direct environmental influence. Focusing on internal representations as the main substrate for computing empowerment offers a new lens through which to design adaptable intelligent systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</title>
<link>https://arxiv.org/abs/2507.22264</link>
<guid>https://arxiv.org/abs/2507.22264</guid>
<content:encoded><![CDATA[
<div> Alignment, flexible, disentangle, visual, textual 

Summary: 
This paper addresses the challenges faced by Contrastive Language-Image Pre-training (CLIP) in aligning visual and textual representations in image-text datasets. The model struggles with information misalignment and entangled representations. The proposed framework establishes conditions for flexible alignment at varying levels of granularity, preserving cross-modal semantic information while disentangling visual representations to capture fine-grained textual concepts. The new approach, called \ours, identifies and aligns the most relevant visual and textual representations in a modular manner, outperforming existing methods across different tasks. The code for this approach is available on GitHub. The theoretical foundation and performance of \ours support the identification theory presented in the paper. 

<br /><br />Summary: <div>
arXiv:2507.22264v1 Announce Type: cross 
Abstract: Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \emph{preserve} cross-modal semantic information in its entirety but also \emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at https://github.com/Mid-Push/SmartCLIP.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting Online Safety by Simulating Unsafe Conversations with LLMs</title>
<link>https://arxiv.org/abs/2507.22267</link>
<guid>https://arxiv.org/abs/2507.22267</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, large language models, online safety, unsafe conversations, scam conversations

Summary: 
Generative AI, specifically large language models (LLMs), have the capability to escalate unsafe conversations online by providing bad actors with the tools to create persuasive and human-like text. This poses a significant risk to online safety due to the ease with which unsafe conversations can be generated, especially those involving scams. The research focuses on educating individuals about the dangers of unsafe online conversations, with a particular focus on simulating scam conversations using LLMs. By utilizing two LLMs to simulate interactions between a scammer and a target, users are able to provide feedback to the target LLM to enhance learning. This approach builds on previous work demonstrating the effectiveness of simulating scam conversations using LLMs and leverages insights from the learning sciences to promote understanding and awareness of online safety risks. This study highlights the importance of proactive measures to address the potential dangers associated with generative AI and unsafe online interactions.

<br /><br />Summary: <div>
arXiv:2507.22267v1 Announce Type: cross 
Abstract: Generative AI, including large language models (LLMs) have the potential -- and already are being used -- to increase the speed, scale, and types of unsafe conversations online. LLMs lower the barrier for entry for bad actors to create unsafe conversations in particular because of their ability to generate persuasive and human-like text. In our current work, we explore ways to promote online safety by teaching people about unsafe conversations that can occur online with and without LLMs. We build on prior work that shows that LLMs can successfully simulate scam conversations. We also leverage research in the learning sciences that shows that providing feedback on one's hypothetical actions can promote learning. In particular, we focus on simulating scam conversations using LLMs. Our work incorporates two LLMs that converse with each other to simulate realistic, unsafe conversations that people may encounter online between a scammer LLM and a target LLM but users of our system are asked provide feedback to the target LLM.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items</title>
<link>https://arxiv.org/abs/2507.22268</link>
<guid>https://arxiv.org/abs/2507.22268</guid>
<content:encoded><![CDATA[
arXiv:2507.22268v1 Announce Type: cross 
Abstract: We introduce a novel self-supervised multi-modal relational item representation learning framework designed to infer substitutable and complementary items. Existing approaches primarily focus on modeling item-item associations deduced from user behaviors using graph neural networks (GNNs) or leveraging item content information. However, these methods often overlook critical challenges, such as noisy user behavior data and data sparsity due to the long-tailed distribution of these behaviors. In this paper, we propose MMSC, a self-supervised multi-modal relational item representation learning framework to address these challenges. Specifically, MMSC consists of three main components: (1) a multi-modal item representation learning module that leverages a multi-modal foundational model and learns from item metadata, (2) a self-supervised behavior-based representation learning module that denoises and learns from user behavior data, and (3) a hierarchical representation aggregation mechanism that integrates item representations at both the semantic and task levels. Additionally, we leverage LLMs to generate augmented training data, further enhancing the denoising process during training. We conduct extensive experiments on five real-world datasets, showing that MMSC outperforms existing baselines by 26.1% for substitutable recommendation and 39.2% for complementary recommendation. In addition, we empirically show that MMSC is effective in modeling cold-start items.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs</title>
<link>https://arxiv.org/abs/2507.22286</link>
<guid>https://arxiv.org/abs/2507.22286</guid>
<content:encoded><![CDATA[
arXiv:2507.22286v1 Announce Type: cross 
Abstract: The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdapSCA-PSO: An Adaptive Localization Algorithm with AI-Based Hybrid SCA-PSO for IoT WSNs</title>
<link>https://arxiv.org/abs/2507.22317</link>
<guid>https://arxiv.org/abs/2507.22317</guid>
<content:encoded><![CDATA[
arXiv:2507.22317v1 Announce Type: cross 
Abstract: The accurate localization of sensor nodes is a fundamental requirement for the practical application of the Internet of Things (IoT). To enable robust localization across diverse environments, this paper proposes a hybrid meta-heuristic localization algorithm. Specifically, the algorithm integrates the Sine Cosine Algorithm (SCA), which is effective in global search, with Particle Swarm Optimization (PSO), which excels at local search. An adaptive switching module is introduced to dynamically select between the two algorithms. Furthermore, the initialization, fitness evaluation, and parameter settings of the algorithm have been specifically redesigned and optimized to address the characteristics of the node localization problem. Simulation results across varying numbers of sensor nodes demonstrate that, compared to standalone PSO and the unoptimized SCAPSO algorithm, the proposed method significantly reduces the number of required iterations and achieves an average localization error reduction of 84.97%.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment</title>
<link>https://arxiv.org/abs/2507.22321</link>
<guid>https://arxiv.org/abs/2507.22321</guid>
<content:encoded><![CDATA[
arXiv:2507.22321v1 Announce Type: cross 
Abstract: Accurate identification of late-life depression (LLD) using structural brain MRI is essential for monitoring disease progression and facilitating timely intervention. However, existing learning-based approaches for LLD detection are often constrained by limited sample sizes (e.g., tens), which poses significant challenges for reliable model training and generalization. Although incorporating auxiliary datasets can expand the training set, substantial domain heterogeneity, such as differences in imaging protocols, scanner hardware, and population demographics, often undermines cross-domain transferability. To address this issue, we propose a Collaborative Domain Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA leverages a Vision Transformer (ViT) to capture global anatomical context and a Convolutional Neural Network (CNN) to extract local structural features, with each branch comprising an encoder and a classifier. The CDA framework consists of three stages: (a) supervised training on labeled source data, (b) self-supervised target feature adaptation and (c) collaborative training on unlabeled target data. We first train ViT and CNN on source data, followed by self-supervised target feature adaptation by minimizing the discrepancy between classifier outputs from two branches to make the categorical boundary clearer. The collaborative training stage employs pseudo-labeled and augmented target-domain MRIs, enforcing prediction consistency under strong and weak augmentation to enhance domain robustness and generalization. Extensive experiments conducted on multi-site T1-weighted MRI data demonstrate that the CDA consistently outperforms state-of-the-art unsupervised domain adaptation methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications</title>
<link>https://arxiv.org/abs/2507.22324</link>
<guid>https://arxiv.org/abs/2507.22324</guid>
<content:encoded><![CDATA[
arXiv:2507.22324v1 Announce Type: cross 
Abstract: Maintaining software packages imposes significant costs due to dependency management, bug fixes, and versioning. We show that rich method descriptions in scientific publications can serve as standalone specifications for modern large language models (LLMs), enabling on-demand code generation that could supplant human-maintained libraries. We benchmark state-of-the-art models (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with implementing a diverse set of core algorithms drawn from original publications. Our results demonstrate that current LLMs can reliably reproduce package functionality with performance indistinguishable from conventional libraries. These findings foreshadow a paradigm shift toward flexible, on-demand code generation and away from static, human-maintained packages, which will result in reduced maintenance overhead by leveraging published articles as sufficient context for the automated implementation of analytical workflows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GVD: Guiding Video Diffusion Model for Scalable Video Distillation</title>
<link>https://arxiv.org/abs/2507.22360</link>
<guid>https://arxiv.org/abs/2507.22360</guid>
<content:encoded><![CDATA[
arXiv:2507.22360v1 Announce Type: cross 
Abstract: To address the larger computation and storage requirements associated with large video datasets, video dataset distillation aims to capture spatial and temporal information in a significantly smaller dataset, such that training on the distilled data has comparable performance to training on all of the data. We propose GVD: Guiding Video Diffusion, the first diffusion-based video distillation method. GVD jointly distills spatial and temporal features, ensuring high-fidelity video generation across diverse actions while capturing essential motion information. Our method's diverse yet representative distillations significantly outperform previous state-of-the-art approaches on the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC). Specifically, our method achieves 78.29 percent of the original dataset's performance using only 1.98 percent of the total number of frames in MiniUCF. Additionally, it reaches 73.83 percent of the performance with just 3.30 percent of the frames in HMDB51. Experimental results across benchmark video datasets demonstrate that GVD not only achieves state-of-the-art performance but can also generate higher resolution videos and higher IPC without significantly increasing computational cost.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Recognition Datasets and Challenges: A Review</title>
<link>https://arxiv.org/abs/2507.22361</link>
<guid>https://arxiv.org/abs/2507.22361</guid>
<content:encoded><![CDATA[
arXiv:2507.22361v1 Announce Type: cross 
Abstract: Object recognition is among the fundamental tasks in the computer vision applications, paving the path for all other image understanding operations. In every stage of progress in object recognition research, efforts have been made to collect and annotate new datasets to match the capacity of the state-of-the-art algorithms. In recent years, the importance of the size and quality of datasets has been intensified as the utility of the emerging deep network techniques heavily relies on training data. Furthermore, datasets lay a fair benchmarking means for competitions and have proved instrumental to the advancements of object recognition research by providing quantifiable benchmarks for the developed models. Taking a closer look at the characteristics of commonly-used public datasets seems to be an important first step for data-driven and machine learning researchers. In this survey, we provide a detailed analysis of datasets in the highly investigated object recognition areas. More than 160 datasets have been scrutinized through statistics and descriptions. Additionally, we present an overview of the prominent object recognition benchmarks and competitions, along with a description of the metrics widely adopted for evaluation purposes in the computer vision community. All introduced datasets and challenges can be found online at github.com/AbtinDjavadifar/ORDC.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring</title>
<link>https://arxiv.org/abs/2507.22369</link>
<guid>https://arxiv.org/abs/2507.22369</guid>
<content:encoded><![CDATA[
arXiv:2507.22369v1 Announce Type: cross 
Abstract: Classroom behavior monitoring is a critical aspect of educational research, with significant implications for student engagement and learning outcomes. Recent advancements in Visual Question Answering (VQA) models offer promising tools for automatically analyzing complex classroom interactions from video recordings. In this paper, we investigate the applicability of several state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and NVILA, in the context of classroom behavior analysis. To facilitate rigorous evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world classroom video recordings at the Banking Academy of Vietnam. We present the methodology for data collection, annotation, and benchmark the performance of the selected VQA models on this dataset. Our initial experimental results demonstrate that all four models achieve promising performance levels in answering behavior-related visual questions, showcasing their potential in future classroom analytics and intervention systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection</title>
<link>https://arxiv.org/abs/2507.22371</link>
<guid>https://arxiv.org/abs/2507.22371</guid>
<content:encoded><![CDATA[
arXiv:2507.22371v1 Announce Type: cross 
Abstract: With the increasing security issues in blockchain, smart contract vulnerability detection has become a research focus. Existing vulnerability detection methods have their limitations: 1) Static analysis methods struggle with complex scenarios. 2) Methods based on specialized pre-trained models perform well on specific datasets but have limited generalization capabilities. In contrast, general-purpose Large Language Models (LLMs) demonstrate impressive ability in adapting to new vulnerability patterns. However, they often underperform on specific vulnerability types compared to methods based on specialized pre-trained models. We also observe that explanations generated by general-purpose LLMs can provide fine-grained code understanding information, contributing to improved detection performance.
  Inspired by these observations, we propose SAEL, an LLM-based framework for smart contract vulnerability detection. We first design targeted prompts to guide LLMs in identifying vulnerabilities and generating explanations, which serve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to process contract code and explanations, enhancing task-specific performance. To combine the strengths of each approach, we introduce an Adaptive Mixture-of-Experts architecture. This dynamically adjusts feature weights via a Gating Network, which selects relevant features using TopK filtering and Softmax normalization, and incorporates a Multi-Head Self-Attention mechanism to enhance cross-feature relationships. This design enables effective integration of LLM predictions, explanation features, and code features through gradient optimization. The loss function jointly considers both independent feature performance and overall weighted predictions. Experiments show that SAEL outperforms existing methods across various vulnerabilities.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINR: Implicit Neural Representations with Masked Image Modelling</title>
<link>https://arxiv.org/abs/2507.22404</link>
<guid>https://arxiv.org/abs/2507.22404</guid>
<content:encoded><![CDATA[
arXiv:2507.22404v1 Announce Type: cross 
Abstract: Self-supervised learning methods like masked autoencoders (MAE) have shown significant promise in learning robust feature representations, particularly in image reconstruction-based pretraining task. However, their performance is often strongly dependent on the masking strategies used during training and can degrade when applied to out-of-distribution data. To address these limitations, we introduce the masked implicit neural representations (MINR) framework that synergizes implicit neural representations with masked image modeling. MINR learns a continuous function to represent images, enabling more robust and generalizable reconstructions irrespective of masking strategies. Our experiments demonstrate that MINR not only outperforms MAE in in-domain scenarios but also in out-of-distribution settings, while reducing model complexity. The versatility of MINR extends to various self-supervised learning applications, confirming its utility as a robust and efficient alternative to existing frameworks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question Generation for Assessing Early Literacy Reading Comprehension</title>
<link>https://arxiv.org/abs/2507.22410</link>
<guid>https://arxiv.org/abs/2507.22410</guid>
<content:encoded><![CDATA[
arXiv:2507.22410v1 Announce Type: cross 
Abstract: Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22411</link>
<guid>https://arxiv.org/abs/2507.22411</guid>
<content:encoded><![CDATA[
arXiv:2507.22411v1 Announce Type: cross 
Abstract: The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching</title>
<link>https://arxiv.org/abs/2507.22418</link>
<guid>https://arxiv.org/abs/2507.22418</guid>
<content:encoded><![CDATA[
arXiv:2507.22418v1 Announce Type: cross 
Abstract: Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at https://github.com/huynhspm/Data-Uncertainty
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Knowledge Graph Repair with Large Language Models</title>
<link>https://arxiv.org/abs/2507.22419</link>
<guid>https://arxiv.org/abs/2507.22419</guid>
<content:encoded><![CDATA[
arXiv:2507.22419v1 Announce Type: cross 
Abstract: We present a systematic approach for evaluating the quality of knowledge graph repairs with respect to constraint violations defined in shapes constraint language (SHACL). Current evaluation methods rely on \emph{ad hoc} datasets, which limits the rigorous analysis of repair systems in more general settings. Our method addresses this gap by systematically generating violations using a novel mechanism, termed violation-inducing operations (VIOs). We use the proposed evaluation framework to assess a range of repair systems which we build using large language models. We analyze the performance of these systems across different prompting strategies. Results indicate that concise prompts containing both the relevant violated SHACL constraints and key contextual information from the knowledge graph yield the best performance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking</title>
<link>https://arxiv.org/abs/2507.22421</link>
<guid>https://arxiv.org/abs/2507.22421</guid>
<content:encoded><![CDATA[
arXiv:2507.22421v1 Announce Type: cross 
Abstract: Real-time video analysis remains a challenging problem in computer vision, requiring efficient processing of both spatial and temporal information while maintaining computational efficiency. Existing approaches often struggle to balance accuracy and speed, particularly in resource-constrained environments. In this work, we present a unified framework that leverages advanced spatial-temporal modeling techniques for simultaneous action recognition and object tracking. Our approach builds upon recent advances in parallel sequence modeling and introduces a novel hierarchical attention mechanism that adaptively focuses on relevant spatial regions across temporal sequences. We demonstrate that our method achieves state-of-the-art performance on standard benchmarks while maintaining real-time inference speeds. Extensive experiments on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action recognition accuracy and 2.8% in tracking precision compared to existing methods, with 40% faster inference time.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</title>
<link>https://arxiv.org/abs/2507.22424</link>
<guid>https://arxiv.org/abs/2507.22424</guid>
<content:encoded><![CDATA[
arXiv:2507.22424v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss</title>
<link>https://arxiv.org/abs/2507.22428</link>
<guid>https://arxiv.org/abs/2507.22428</guid>
<content:encoded><![CDATA[
arXiv:2507.22428v1 Announce Type: cross 
Abstract: Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often suffer from overestimation due to relative errors in gradient computation induced by floating-point arithmetic. This paper provides a rigorous theoretical analysis of these errors, conducting the first comprehensive study of floating-point computation errors in gradient-based attacks across four distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful targeted attacks. We establish theoretical foundations characterizing the behavior of relative numerical errors under different attack conditions, revealing previously unknown patterns in gradient computation instability, and identify floating-point underflow and rounding as key contributors. Building on this insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which incorporates an optimal scaling factor $T = t^*$ to minimize the impact of floating-point errors, thereby enhancing the accuracy of gradient computation in adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss functions, including CE, C\&amp;W, DLR, and MIFPE, in terms of attack potency and robustness evaluation accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini</title>
<link>https://arxiv.org/abs/2507.22445</link>
<guid>https://arxiv.org/abs/2507.22445</guid>
<content:encoded><![CDATA[
arXiv:2507.22445v1 Announce Type: cross 
Abstract: Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a 1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function</title>
<link>https://arxiv.org/abs/2507.22446</link>
<guid>https://arxiv.org/abs/2507.22446</guid>
<content:encoded><![CDATA[
arXiv:2507.22446v1 Announce Type: cross 
Abstract: Despite their widespread success, deep neural networks remain critically vulnerable to adversarial attacks, posing significant risks in safety-sensitive applications. This paper investigates activation functions as a crucial yet underexplored component for enhancing model robustness. We propose a Rademacher Complexity Reduction Activation Function (RCR-AF), a novel activation function designed to improve both generalization and adversarial resilience. RCR-AF uniquely combines the advantages of GELU (including smoothness, gradient stability, and negative information retention) with ReLU's desirable monotonicity, while simultaneously controlling both model sparsity and capacity through built-in clipping mechanisms governed by two hyperparameters, $\alpha$ and $\gamma$. Our theoretical analysis, grounded in Rademacher complexity, demonstrates that these parameters directly modulate the model's Rademacher complexity, offering a principled approach to enhance robustness. Comprehensive empirical evaluations show that RCR-AF consistently outperforms widely-used alternatives (ReLU, GELU, and Swish) in both clean accuracy under standard training and in adversarial robustness within adversarial training paradigms.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models</title>
<link>https://arxiv.org/abs/2507.22457</link>
<guid>https://arxiv.org/abs/2507.22457</guid>
<content:encoded><![CDATA[
arXiv:2507.22457v1 Announce Type: cross 
Abstract: Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework</title>
<link>https://arxiv.org/abs/2507.22464</link>
<guid>https://arxiv.org/abs/2507.22464</guid>
<content:encoded><![CDATA[
arXiv:2507.22464v1 Announce Type: cross 
Abstract: Accurate and interpretable prediction of estimated glomerular filtration rate (eGFR) is essential for managing chronic kidney disease (CKD) and supporting clinical decisions. Recent advances in Large Multimodal Models (LMMs) have shown strong potential in clinical prediction tasks due to their ability to process visual and textual information. However, challenges related to deployment cost, data privacy, and model reliability hinder their adoption. In this study, we propose a collaborative framework that enhances the performance of open-source LMMs for eGFR forecasting while generating clinically meaningful explanations. The framework incorporates visual knowledge transfer, abductive reasoning, and a short-term memory mechanism to enhance prediction accuracy and interpretability. Experimental results show that the proposed framework achieves predictive performance and interpretability comparable to proprietary models. It also provides plausible clinical reasoning processes behind each prediction. Our method sheds new light on building AI systems for healthcare that combine predictive accuracy with clinically grounded interpretability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation</title>
<link>https://arxiv.org/abs/2507.22465</link>
<guid>https://arxiv.org/abs/2507.22465</guid>
<content:encoded><![CDATA[
arXiv:2507.22465v1 Announce Type: cross 
Abstract: Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level masks for the most salient objects in videos without any prior annotations. While memory mechanisms have been proven critical in various video segmentation paradigms, their application in UVOS yield only marginal performance gains despite sophisticated design. Our analysis reveals a simple but fundamental flaw in existing methods: over-reliance on memorizing high-level semantic features. UVOS inherently suffers from the deficiency of lacking fine-grained information due to the absence of pixel-level prior knowledge. Consequently, memory design relying solely on high-level features, which predominantly capture abstract semantic cues, is insufficient to generate precise predictions. To resolve this fundamental issue, we propose a novel hierarchical memory architecture to incorporate both shallow- and high-level features for memory, which leverages the complementary benefits of pixel and semantic information. Furthermore, to balance the simultaneous utilization of the pixel and semantic memory features, we propose a heterogeneous interaction mechanism to perform pixel-semantic mutual interactions, which explicitly considers their inherent feature discrepancies. Through the design of Pixel-guided Local Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM), we achieve delicate integration of the fine-grained details in shallow-level memory and the semantic representations in high-level memory. Our Hierarchical Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks. Moreover, HMHI-Net consistently exhibits high performance across different backbones, further demonstrating its superiority and robustness. Project page: https://github.com/ZhengxyFlow/HMHI-Net .
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Simulating Social Influence Dynamics with LLM-based Multi-agents</title>
<link>https://arxiv.org/abs/2507.22467</link>
<guid>https://arxiv.org/abs/2507.22467</guid>
<content:encoded><![CDATA[
arXiv:2507.22467v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models offer promising capabilities to simulate complex human social interactions. We investigate whether LLM-based multi-agent simulations can reproduce core human social dynamics observed in online forums. We evaluate conformity dynamics, group polarization, and fragmentation across different model scales and reasoning capabilities using a structured simulation framework. Our findings indicate that smaller models exhibit higher conformity rates, whereas models optimized for reasoning are more resistant to social influence.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Language Models as Zero-Shot Deepfake Detectors</title>
<link>https://arxiv.org/abs/2507.22469</link>
<guid>https://arxiv.org/abs/2507.22469</guid>
<content:encoded><![CDATA[
arXiv:2507.22469v1 Announce Type: cross 
Abstract: The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks</title>
<link>https://arxiv.org/abs/2507.22477</link>
<guid>https://arxiv.org/abs/2507.22477</guid>
<content:encoded><![CDATA[
arXiv:2507.22477v1 Announce Type: cross 
Abstract: Achieving pixel-level segmentation with low computational cost using multimodal data remains a key challenge in crack segmentation tasks. Existing methods lack the capability for adaptive perception and efficient interactive fusion of cross-modal features. To address these challenges, we propose a Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently perceives and integrates morphological and textural cues from different modalities under multimodal crack scenarios, generating clear pixel-level crack segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack cues through the proposed mask-guided Efficient Dynamic Guided Scanning Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture spatial and frequency-domain cues across modalities. Moreover, we design a Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive complex morphological structures with minimal computational overhead, replacing most convolutional operations in LIDAR. Experiments on three datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods. On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465 in mIoU with only 5.35M parameters. Code and datasets are available at https://github.com/Karl1109/LIDAR-Mamba.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework</title>
<link>https://arxiv.org/abs/2507.22481</link>
<guid>https://arxiv.org/abs/2507.22481</guid>
<content:encoded><![CDATA[
arXiv:2507.22481v1 Announce Type: cross 
Abstract: Video signals are vulnerable in multimedia communication and storage systems, as even slight bitstream-domain corruption can lead to significant pixel-domain degradation. To recover faithful spatio-temporal content from corrupted inputs, bitstream-corrupted video recovery has recently emerged as a challenging and understudied task. However, existing methods require time-consuming and labor-intensive annotation of corrupted regions for each corrupted video frame, resulting in a large workload in practice. In addition, high-quality recovery remains difficult as part of the local residual information in corrupted frames may mislead feature completion and successive content recovery. In this paper, we propose the first blind bitstream-corrupted video recovery framework that integrates visual foundation models with a recovery model, which is adapted to different types of corruption and bitstream-level prompts. Within the framework, the proposed Detect Any Corruption (DAC) model leverages the rich priors of the visual foundation model while incorporating bitstream and corruption knowledge to enhance corruption localization and blind recovery. Additionally, we introduce a novel Corruption-aware Feature Completion (CFC) module, which adaptively processes residual contributions based on high-level corruption understanding. With VFM-guided hierarchical feature augmentation and high-level coordination in a mixture-of-residual-experts (MoRE) structure, our method suppresses artifacts and enhances informative residuals. Comprehensive evaluations show that the proposed method achieves outstanding performance in bitstream-corrupted video recovery without requiring a manually labeled mask sequence. The demonstrated effectiveness will help to realize improved user experience, wider application scenarios, and more reliable multimedia communication and storage systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-constrained generative machine learning-based high-resolution downscaling of Greenland's surface mass balance and surface temperature</title>
<link>https://arxiv.org/abs/2507.22485</link>
<guid>https://arxiv.org/abs/2507.22485</guid>
<content:encoded><![CDATA[
arXiv:2507.22485v1 Announce Type: cross 
Abstract: Accurate, high-resolution projections of the Greenland ice sheet's surface mass balance (SMB) and surface temperature are essential for understanding future sea-level rise, yet current approaches are either computationally demanding or limited to coarse spatial scales. Here, we introduce a novel physics-constrained generative modeling framework based on a consistency model (CM) to downscale low-resolution SMB and surface temperature fields by a factor of up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM is trained on monthly outputs of the regional climate model MARv3.12 and conditioned on ice-sheet topography and insolation. By enforcing a hard conservation constraint during inference, we ensure approximate preservation of SMB and temperature sums on the coarse spatial scale as well as robust generalization to extreme climate states without retraining. On the test set, our constrained CM achieves a continued ranked probability score of 6.31 mmWE for the SMB and 0.1 K for the surface temperature, outperforming interpolation-based downscaling. Together with spatial power-spectral analysis, we demonstrate that the CM faithfully reproduces variability across spatial scales. We further apply bias-corrected outputs of the NorESM2 Earth System Model as inputs to our CM, to demonstrate the potential of our model to directly downscale ESM fields. Our approach delivers realistic, high-resolution climate forcing for ice-sheet simulations with fast inference and can be readily integrated into Earth-system and ice-sheet model workflows to improve projections of the future contribution to sea-level rise from Greenland and potentially other ice sheets and glaciers too.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proto-EVFL: Enhanced Vertical Federated Learning via Dual Prototype with Extremely Unaligned Data</title>
<link>https://arxiv.org/abs/2507.22488</link>
<guid>https://arxiv.org/abs/2507.22488</guid>
<content:encoded><![CDATA[
arXiv:2507.22488v1 Announce Type: cross 
Abstract: In vertical federated learning (VFL), multiple enterprises address aligned sample scarcity by leveraging massive locally unaligned samples to facilitate collaborative learning. However, unaligned samples across different parties in VFL can be extremely class-imbalanced, leading to insufficient feature representation and limited model prediction space. Specifically, class-imbalanced problems consist of intra-party class imbalance and inter-party class imbalance, which can further cause local model bias and feature contribution inconsistency issues, respectively. To address the above challenges, we propose Proto-EVFL, an enhanced VFL framework via dual prototypes. We first introduce class prototypes for each party to learn relationships between classes in the latent space, allowing the active party to predict unseen classes. We further design a probabilistic dual prototype learning scheme to dynamically select unaligned samples by conditional optimal transport cost with class prior probability. Moreover, a mixed prior guided module guides this selection process by combining local and global class prior probabilities. Finally, we adopt an \textit{adaptive gated feature aggregation strategy} to mitigate feature contribution inconsistency by dynamically weighting and aggregating local features across different parties. We proved that Proto-EVFL, as the first bi-level optimization framework in VFL, has a convergence rate of 1/\sqrt T. Extensive experiments on various datasets validate the superiority of our Proto-EVFL. Even in a zero-shot scenario with one unseen class, it outperforms baselines by at least 6.97%
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVM-GP: Uncertainty-Aware PDE Solver via coupling latent variable model and Gaussian process</title>
<link>https://arxiv.org/abs/2507.22493</link>
<guid>https://arxiv.org/abs/2507.22493</guid>
<content:encoded><![CDATA[
arXiv:2507.22493v1 Announce Type: cross 
Abstract: We propose a novel probabilistic framework, termed LVM-GP, for uncertainty quantification in solving forward and inverse partial differential equations (PDEs) with noisy data. The core idea is to construct a stochastic mapping from the input to a high-dimensional latent representation, enabling uncertainty-aware prediction of the solution. Specifically, the architecture consists of a confidence-aware encoder and a probabilistic decoder. The encoder implements a high-dimensional latent variable model based on a Gaussian process (LVM-GP), where the latent representation is constructed by interpolating between a learnable deterministic feature and a Gaussian process prior, with the interpolation strength adaptively controlled by a confidence function learned from data. The decoder defines a conditional Gaussian distribution over the solution field, where the mean is predicted by a neural operator applied to the latent representation, allowing the model to learn flexible function-to-function mapping. Moreover, physical laws are enforced as soft constraints in the loss function to ensure consistency with the underlying PDE structure. Compared to existing approaches such as Bayesian physics-informed neural networks (B-PINNs) and deep ensembles, the proposed framework can efficiently capture functional dependencies via merging a latent Gaussian process and neural operator, resulting in competitive predictive accuracy and robust uncertainty quantification. Numerical experiments demonstrate the effectiveness and reliability of the method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adverse Weather Removal via Spectral-based Spatial Grouping</title>
<link>https://arxiv.org/abs/2507.22498</link>
<guid>https://arxiv.org/abs/2507.22498</guid>
<content:encoded><![CDATA[
arXiv:2507.22498v1 Announce Type: cross 
Abstract: Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning</title>
<link>https://arxiv.org/abs/2507.22499</link>
<guid>https://arxiv.org/abs/2507.22499</guid>
<content:encoded><![CDATA[
arXiv:2507.22499v1 Announce Type: cross 
Abstract: Recent generative models face significant risks of producing harmful content, which has underscored the importance of machine unlearning (MU) as a critical technique for eliminating the influence of undesired data. However, existing MU methods typically assign the same weight to all data to be forgotten, which makes it difficult to effectively forget certain data that is harder to unlearn than others. In this paper, we empirically demonstrate that the loss of data itself can implicitly reflect its varying difficulty. Building on this insight, we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective plug-and-play strategy that dynamically reweights data during the unlearning process with minimal additional computational overhead. Our approach significantly reduces the gap between existing MU methods and exact unlearning in both image classification and generation tasks, effectively enhancing the prevention of harmful content generation in text-to-image diffusion models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing Actions from Robotic View for Natural Human-Robot Interaction</title>
<link>https://arxiv.org/abs/2507.22522</link>
<guid>https://arxiv.org/abs/2507.22522</guid>
<content:encoded><![CDATA[
arXiv:2507.22522v1 Announce Type: cross 
Abstract: Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accident-Driven Congestion Prediction and Simulation: An Explainable Framework Using Advanced Clustering and Bayesian Networks</title>
<link>https://arxiv.org/abs/2507.22529</link>
<guid>https://arxiv.org/abs/2507.22529</guid>
<content:encoded><![CDATA[
arXiv:2507.22529v1 Announce Type: cross 
Abstract: Traffic congestion due to uncertainties, such as accidents, is a significant issue in urban areas, as the ripple effect of accidents causes longer delays, increased emissions, and safety concerns. To address this issue, we propose a robust framework for predicting the impact of accidents on congestion. We implement Automated Machine Learning (AutoML)-enhanced Deep Embedding Clustering (DEC) to assign congestion labels to accident data and predict congestion probability using a Bayesian Network (BN). The Simulation of Urban Mobility (SUMO) simulation is utilized to evaluate the correctness of BN predictions using evidence-based scenarios. Results demonstrate that the AutoML-enhanced DEC has outperformed traditional clustering approaches. The performance of the proposed BN model achieved an overall accuracy of 95.6%, indicating its ability to understand the complex relationship of accidents causing congestion. Validation in SUMO with evidence-based scenarios demonstrated that the BN model's prediction of congestion states closely matches those of SUMO, indicating the high reliability of the proposed BN model in ensuring smooth urban mobility.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors</title>
<link>https://arxiv.org/abs/2507.22530</link>
<guid>https://arxiv.org/abs/2507.22530</guid>
<content:encoded><![CDATA[
arXiv:2507.22530v1 Announce Type: cross 
Abstract: The segmentation of the hepatic vasculature in surgical videos holds substantial clinical significance in the context of hepatectomy procedures. However, owing to the dearth of an appropriate dataset and the inherently complex task characteristics, few researches have been reported in this domain. To address this issue, we first introduce a high quality frame-by-frame annotated hepatic vasculature dataset containing 35 long hepatectomy videos and 11442 high-resolution frames. On this basis, we propose a novel high-resolution video vasculature segmentation network, dubbed as HRVVS. We innovatively embed a pretrained visual autoregressive modeling (VAR) model into different layers of the hierarchical encoder as prior information to reduce the information degradation generated during the downsampling process. In addition, we designed a dynamic memory decoder on a multi-view segmentation network to minimize the transmission of redundant information while preserving more details between frames. Extensive experiments on surgical video datasets demonstrate that our proposed HRVVS significantly outperforms the state-of-the-art methods. The source code and dataset will be publicly available at \href{https://github.com/scott-yjyang/xx}{https://github.com/scott-yjyang/HRVVS}.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records</title>
<link>https://arxiv.org/abs/2507.22533</link>
<guid>https://arxiv.org/abs/2507.22533</guid>
<content:encoded><![CDATA[
arXiv:2507.22533v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A surrogate model for topology optimisation of elastic structures via parametric autoencoders</title>
<link>https://arxiv.org/abs/2507.22539</link>
<guid>https://arxiv.org/abs/2507.22539</guid>
<content:encoded><![CDATA[
arXiv:2507.22539v1 Announce Type: cross 
Abstract: A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\%$ while achieving discrepancies below $4\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning</title>
<link>https://arxiv.org/abs/2507.22553</link>
<guid>https://arxiv.org/abs/2507.22553</guid>
<content:encoded><![CDATA[
arXiv:2507.22553v1 Announce Type: cross 
Abstract: Prompt-based continual learning provides a rehearsal-free solution by tuning small sets of parameters while keeping pre-trained models frozen. To meet the complex demands of sequential tasks, it is crucial to integrate task-specific knowledge within prompts effectively. However, existing works rely on either fixed learned prompts (i.e., prompts whose representations remain unchanged during new task learning) or on prompts generated from an entangled task-shared space, limiting the representational diversity of the integrated prompt. To address this issue, we propose a novel prompt-evolving mechanism to adaptively aggregate base prompts (i.e., task-specific prompts) into a unified prompt while ensuring diversity. By transforming and aligning base prompts, both previously learned and newly introduced, our approach continuously evolves accumulated knowledge to facilitate learning new tasks. We further introduce a learnable probabilistic gate that adaptively determines which layers to activate during the evolution process. We validate our method on image classification and video action recognition tasks in class-incremental learning, achieving average gains of 9.07% and 7.40% over existing methods across all scenarios.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>aLLoyM: A large language model for alloy phase diagram prediction</title>
<link>https://arxiv.org/abs/2507.22558</link>
<guid>https://arxiv.org/abs/2507.22558</guid>
<content:encoded><![CDATA[
arXiv:2507.22558v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are general-purpose tools with wide-ranging applications, including in materials science. In this work, we introduce aLLoyM, a fine-tuned LLM specifically trained on alloy compositions, temperatures, and their corresponding phase information. To develop aLLoyM, we curated question-and-answer (Q&amp;A) pairs for binary and ternary phase diagrams using the open-source Computational Phase Diagram Database (CPDDB) and assessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned Mistral, an open-source pre-trained LLM, for two distinct Q&amp;A formats: multiple-choice and short-answer. Benchmark evaluations demonstrate that fine-tuning substantially enhances performance on multiple-choice phase diagram questions. Moreover, the short-answer model of aLLoyM exhibits the ability to generate novel phase diagrams from its components alone, underscoring its potential to accelerate the discovery of previously unexplored materials systems. To promote further research and adoption, we have publicly released the short-answer fine-tuned version of aLLoyM, along with the complete benchmarking Q&amp;A dataset, on Hugging Face.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
arXiv:2507.22564v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
arXiv:2507.22565v1 Announce Type: cross 
Abstract: The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity</title>
<link>https://arxiv.org/abs/2507.22570</link>
<guid>https://arxiv.org/abs/2507.22570</guid>
<content:encoded><![CDATA[
arXiv:2507.22570v1 Announce Type: cross 
Abstract: This work demonstrates a methodology for using deep learning to discover simple, practical criteria for classifying matrices based on abstract algebraic properties. By combining a high-performance neural network with explainable AI (XAI) techniques, we can distill a model's learned strategy into human-interpretable rules. We apply this approach to the challenging case of monotone matrices, defined by the condition that their inverses are entrywise nonnegative. Despite their simple definition, an easy characterization in terms of the matrix elements or the derived parameters is not known. Here, we present, to the best of our knowledge, the first systematic machine-learning approach for deriving a practical criterion that distinguishes monotone from non-monotone matrices. After establishing a labelled dataset by randomly generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ deep neural network algorithms for classifying the matrices as monotone or non-monotone, using both their entries and a comprehensive set of matrix features. By saliency methods, such as integrated gradients, we identify among all features, two matrix parameters which alone provide sufficient information for the matrix classification, with $95\%$ accuracy, namely the absolute values of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's characteristic polynomial. A data-driven study of 18,000 random $7\times7$ matrices shows that the monotone class obeys $\lvert c_{0}/c_{1}\rvert\le0.18$ with probability $>99.98\%$; because $\lvert c_{0}/c_{1}\rvert = 1/\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound $\mathrm{tr}(A^{-1})\ge5.7$.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP</title>
<link>https://arxiv.org/abs/2507.22576</link>
<guid>https://arxiv.org/abs/2507.22576</guid>
<content:encoded><![CDATA[
arXiv:2507.22576v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is an important building block in trustworthy image recognition systems as unknown classes may arise at test-time. OOD detection methods typically revolve around a single classifier, leading to a split in the research field between the classical supervised setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot setting (class names fed as prompts to CLIP). In both cases, an overarching challenge is that the OOD detection performance is implicitly constrained by the classifier's capabilities on in-distribution (ID) data. In this work, we show that given a little open-mindedness from both ends, remarkable OOD detection can be achieved by instead creating a heterogeneous ensemble - COOkeD combines the predictions of a closed-world classifier trained end-to-end on a specific dataset, a zero-shot CLIP classifier, and a linear probe classifier trained on CLIP image features. While bulky at first sight, this approach is modular, post-hoc and leverages the availability of pre-trained VLMs, thus introduces little overhead compared to training a single standard classifier. We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also consider more challenging, realistic settings ranging from training-time label noise, to test-time covariate shift, to zero-shot shift which has been previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art performance and greater robustness compared to both classical and CLIP-based OOD detection methods. Code is available at https://github.com/glhr/COOkeD
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Mean-Field Theory of $\Theta$-Expectations</title>
<link>https://arxiv.org/abs/2507.22577</link>
<guid>https://arxiv.org/abs/2507.22577</guid>
<content:encoded><![CDATA[
arXiv:2507.22577v1 Announce Type: cross 
Abstract: The canonical theory of sublinear expectations, a foundation of stochastic calculus under ambiguity, is insensitive to the non-convex geometry of primitive uncertainty models. This paper develops a new stochastic calculus for a structured class of such non-convex models. We introduce a class of fully coupled Mean-Field Forward-Backward Stochastic Differential Equations where the BSDE driver is defined by a pointwise maximization over a law-dependent, non-convex set. Mathematical tractability is achieved via a uniform strong concavity assumption on the driver with respect to the control variable, which ensures the optimization admits a unique and stable solution. A central contribution is to establish the Lipschitz stability of this optimizer from primitive geometric and regularity conditions, which underpins the entire well-posedness theory. We prove local and global well-posedness theorems for the FBSDE system. The resulting valuation functional, the $\Theta$-Expectation, is shown to be dynamically consistent and, most critically, to violate the axiom of sub-additivity. This, along with its failure to be translation invariant, demonstrates its fundamental departure from the convex paradigm. This work provides a rigorous foundation for stochastic calculus under a class of non-convex, endogenous ambiguity.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment</title>
<link>https://arxiv.org/abs/2507.22580</link>
<guid>https://arxiv.org/abs/2507.22580</guid>
<content:encoded><![CDATA[
arXiv:2507.22580v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs without requiring human intervention. However, existing tools tend to generate patches that satisfy test cases without fixing the underlying bug, those are known as overfitting patches. To address this issue, Automated Patch Correctness Assessment (APCA) attempts to identify overfitting patches generated by APR tools. It can be solved as a static approach, meaning that no additional information is needed beyond the original and fixed code snippets. Current static techniques often struggle with reliability, flexibility and transparency. To address these issues, we introduce RePaCA, a novel static APCA technique that leverages Large Language Models (LLMs) specialized in thinking tasks. Our model is prompted with both buggy and fixed code snippets and guided to generate a Chain of Thought that analyses code differences, reasons about how the patch addresses the root cause, and ultimately provides a binary classification: correct or overfitting. To enhance these reasoning capabilities for the APCA task specifically, the LLM is finetuned using Reinforcement Learning with the Group Relative Policy Optimization algorithm. When evaluated on a standard Defects4J-derived test, our approach achieves state-of-the-art performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model demonstrates superior generalization capabilities when trained on different datasets, outperforming the leading technique. This reasoning capability also provides enhanced explainability for the patch assessment. These findings underscore the considerable promise of finetuned, reasoning LLMs to advance static APCA by enhancing accuracy, generalization, and explainability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALSAM: A Platform for Benchmarking Arabic Large Language Models</title>
<link>https://arxiv.org/abs/2507.22603</link>
<guid>https://arxiv.org/abs/2507.22603</guid>
<content:encoded><![CDATA[
arXiv:2507.22603v1 Announce Type: cross 
Abstract: The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
arXiv:2507.22607v1 Announce Type: cross 
Abstract: Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metamorphic Testing of Deep Code Models: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2507.22610</link>
<guid>https://arxiv.org/abs/2507.22610</guid>
<content:encoded><![CDATA[
arXiv:2507.22610v1 Announce Type: cross 
Abstract: Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models' robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Duration Model for Text Speech Alignment</title>
<link>https://arxiv.org/abs/2507.22612</link>
<guid>https://arxiv.org/abs/2507.22612</guid>
<content:encoded><![CDATA[
arXiv:2507.22612v1 Announce Type: cross 
Abstract: Speech-to-text alignment is a critical component of neural text to-speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line. However, these alignments tend to be brittle and often fail to generalize to long utterances and out-of-domain text, leading to missing or repeating words. Most non-autoregressive end to-end TTS models rely on durations extracted from external sources, using additional duration models for alignment. In this paper, we propose a novel duration prediction framework that can give compromising phoneme-level duration distribution with given text. In our experiments, the proposed duration model has more precise prediction and condition adaptation ability compared to previous baseline models. Numerically, it has roughly a 11.3 percents immprovement on alignment accuracy, and makes the performance of zero-shot TTS models more robust to the mismatch between prompt audio and input audio.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing</title>
<link>https://arxiv.org/abs/2507.22627</link>
<guid>https://arxiv.org/abs/2507.22627</guid>
<content:encoded><![CDATA[
arXiv:2507.22627v1 Announce Type: cross 
Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity</title>
<link>https://arxiv.org/abs/2507.22633</link>
<guid>https://arxiv.org/abs/2507.22633</guid>
<content:encoded><![CDATA[
arXiv:2507.22633v1 Announce Type: cross 
Abstract: Different from existing federated fine-tuning (FFT) methods for foundation models, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored scenario where clients exhibit double heterogeneity in model architectures and downstream tasks. This hybrid heterogeneity introduces two significant challenges: 1) heterogeneous matrix aggregation, where clients adopt different large-scale foundation models based on their task requirements and resource limitations, leading to dimensional mismatches during LoRA parameter aggregation; and 2) multi-task knowledge interference, where local shared parameters, trained with both task-shared and task-specific knowledge, cannot ensure only task-shared knowledge is transferred between clients. To address these challenges, we propose H2Tune, a federated foundation model fine-tuning with hybrid heterogeneity. Our framework H2Tune consists of three key components: (i) sparsified triple matrix decomposition to align hidden dimensions across clients through constructing rank-consistent middle matrices, with adaptive sparsification based on client resources; (ii) relation-guided matrix layer alignment to handle heterogeneous layer structures and representation capabilities; and (iii) alternating task-knowledge disentanglement mechanism to decouple shared and specific knowledge of local model parameters through alternating optimization. Theoretical analysis proves a convergence rate of O(1/\sqrt{T}). Extensive experiments show our method achieves up to 15.4% accuracy improvement compared to state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/H2Tune-1407.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction</title>
<link>https://arxiv.org/abs/2507.22640</link>
<guid>https://arxiv.org/abs/2507.22640</guid>
<content:encoded><![CDATA[
arXiv:2507.22640v1 Announce Type: cross 
Abstract: Offline reinforcement learning (offline RL) offers a promising framework for developing control strategies in chemical process systems using historical data, without the risks or costs of online experimentation. This work investigates the application of offline RL to the safe and efficient control of an exothermic polymerisation continuous stirred-tank reactor. We introduce a Gymnasium-compatible simulation environment that captures the reactor's nonlinear dynamics, including reaction kinetics, energy balances, and operational constraints. The environment supports three industrially relevant scenarios: startup, grade change down, and grade change up. It also includes reproducible offline datasets generated from proportional-integral controllers with randomised tunings, providing a benchmark for evaluating offline RL algorithms in realistic process control tasks.
  We assess behaviour cloning and implicit Q-learning as baseline algorithms, highlighting the challenges offline agents face, including steady-state offsets and degraded performance near setpoints. To address these issues, we propose a novel deployment-time safety layer that performs gradient-based action correction using input convex neural networks (PICNNs) as learned cost models. The PICNN enables real-time, differentiable correction of policy actions by descending a convex, state-conditioned cost surface, without requiring retraining or environment interaction.
  Experimental results show that offline RL, particularly when combined with convex action correction, can outperform traditional control approaches and maintain stability across all scenarios. These findings demonstrate the feasibility of integrating offline RL with interpretable and safety-aware corrections for high-stakes chemical process control, and lay the groundwork for more reliable data-driven automation in industrial systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models</title>
<link>https://arxiv.org/abs/2507.22659</link>
<guid>https://arxiv.org/abs/2507.22659</guid>
<content:encoded><![CDATA[
arXiv:2507.22659v1 Announce Type: cross 
Abstract: The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 227 studies published between January 2020 and June 2025, categorizing them by task formulation, input representation, system architecture, and adaptation techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots</title>
<link>https://arxiv.org/abs/2507.22664</link>
<guid>https://arxiv.org/abs/2507.22664</guid>
<content:encoded><![CDATA[
arXiv:2507.22664v1 Announce Type: cross 
Abstract: The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73\% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach</title>
<link>https://arxiv.org/abs/2507.22671</link>
<guid>https://arxiv.org/abs/2507.22671</guid>
<content:encoded><![CDATA[
arXiv:2507.22671v1 Announce Type: cross 
Abstract: Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing</title>
<link>https://arxiv.org/abs/2507.22685</link>
<guid>https://arxiv.org/abs/2507.22685</guid>
<content:encoded><![CDATA[
arXiv:2507.22685v1 Announce Type: cross 
Abstract: Leaf wetness detection is a crucial task in agricultural monitoring, as it directly impacts the prediction and protection of plant diseases. However, existing sensing systems suffer from limitations in robustness, accuracy, and environmental resilience when applied to natural leaves under dynamic real-world conditions. To address these challenges, we introduce a new multi-modal dataset specifically designed for evaluating and advancing machine learning algorithms in leaf wetness detection. Our dataset comprises synchronized mmWave raw data, Synthetic Aperture Radar (SAR) images, and RGB images collected over six months from five diverse plant species in both controlled and outdoor field environments. We provide detailed benchmarks using the Hydra model, including comparisons against single modality baselines and multiple fusion strategies, as well as performance under varying scan distances. Additionally, our dataset can serve as a benchmark for future SAR imaging algorithm optimization, enabling a systematic evaluation of detection accuracy under diverse conditions.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bifr\"{o}st: Spatial Networking with Bigraphs</title>
<link>https://arxiv.org/abs/2507.22687</link>
<guid>https://arxiv.org/abs/2507.22687</guid>
<content:encoded><![CDATA[
arXiv:2507.22687v1 Announce Type: cross 
Abstract: Modern networked environments increasingly rely on spatial reasoning, but lack a coherent representation for coordinating physical space. Consequently, tasks such as enforcing spatial access policies remain fragile and manual. We first propose a unifying representation based on bigraphs, capturing spatial, social, and communication relationships within a single formalism, with user-facing tools to generate bigraphs from physical environments. Second, we present a hierarchical agent architecture for distributed spatial reasoning, with runtimes for agentic processes to interact the spatial representation, and a context-aware execution model that scopes reasoning to the smallest viable subspace. Together, these enable private, reliable, and low-latency spatial networking that can safely interact with agentic workflows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OFCnetLLM: Large Language Model for Network Monitoring and Alertness</title>
<link>https://arxiv.org/abs/2507.22711</link>
<guid>https://arxiv.org/abs/2507.22711</guid>
<content:encoded><![CDATA[
arXiv:2507.22711v1 Announce Type: cross 
Abstract: The rapid evolution of network infrastructure is bringing new challenges and opportunities for efficient network management, optimization, and security. With very large monitoring databases becoming expensive to explore, the use of AI and Generative AI can help reduce costs of managing these datasets. This paper explores the use of Large Language Models (LLMs) to revolutionize network monitoring management by addressing the limitations of query finding and pattern analysis. We leverage LLMs to enhance anomaly detection, automate root-cause analysis, and automate incident analysis to build a well-monitored network management team using AI. Through a real-world example of developing our own OFCNetLLM, based on the open-source LLM model, we demonstrate practical applications of OFCnetLLM in the OFC conference network. Our model is developed as a multi-agent approach and is still evolving, and we present early results here.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index</title>
<link>https://arxiv.org/abs/2507.22744</link>
<guid>https://arxiv.org/abs/2507.22744</guid>
<content:encoded><![CDATA[
arXiv:2507.22744v1 Announce Type: cross 
Abstract: Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision</title>
<link>https://arxiv.org/abs/2507.22760</link>
<guid>https://arxiv.org/abs/2507.22760</guid>
<content:encoded><![CDATA[
arXiv:2507.22760v1 Announce Type: cross 
Abstract: As neural networks (NNs) become increasingly prevalent in safety-critical neural network-controlled cyber-physical systems (NNCSs), formally guaranteeing their safety becomes crucial. For these systems, safety must be ensured throughout their entire operation, necessitating infinite-time horizon verification. To verify the infinite-time horizon safety of NNCSs, recent approaches leverage Differential Dynamic Logic (dL). However, these dL-based guarantees rely on idealized, real-valued NN semantics and fail to account for roundoff errors introduced by finite-precision implementations. This paper bridges the gap between theoretical guarantees and real-world implementations by incorporating robustness under finite-precision perturbations -- in sensing, actuation, and computation -- into the safety verification. We model the problem as a hybrid game between a good Demon, responsible for control actions, and a bad Angel, introducing perturbations. This formulation enables formal proofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this bound, we employ state-of-the-art mixed-precision fixed-point tuners to synthesize sound and efficient implementations, thus providing a complete end-to-end solution. We evaluate our approach on case studies from the automotive and aeronautics domains, producing efficient NN implementations with rigorous infinite-time horizon safety guarantees.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models</title>
<link>https://arxiv.org/abs/2507.22766</link>
<guid>https://arxiv.org/abs/2507.22766</guid>
<content:encoded><![CDATA[
arXiv:2507.22766v1 Announce Type: cross 
Abstract: Sensor-based sorting systems enable the physical separation of a material stream into two fractions. The sorting decision is based on the image data evaluation of the sensors used and is carried out using actuators. Various process parameters must be set depending on the properties of the material stream, the dimensioning of the system, and the required sorting accuracy. However, continuous verification and re-adjustment are necessary due to changing requirements and material stream compositions. In this paper, we introduce an approach for optimizing, recurrently monitoring and adjusting the process parameters of a sensor-based sorting system. Based on Bayesian Optimization, Gaussian process regression models are used as surrogate models to achieve specific requirements for system behavior with the uncertainties contained therein. This method minimizes the number of necessary experiments while simultaneously considering two possible optimization targets based on the requirements for both material output streams. In addition, uncertainties are considered during determining sorting accuracies in the model calculation. We evaluated the method with three example process parameters.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization</title>
<link>https://arxiv.org/abs/2507.22767</link>
<guid>https://arxiv.org/abs/2507.22767</guid>
<content:encoded><![CDATA[
arXiv:2507.22767v1 Announce Type: cross 
Abstract: Distilling large neural networks into simple, human-readable symbolic formulas is a promising path toward trustworthy and interpretable AI. However, this process is often brittle, as the complex functions learned by standard networks are poor targets for symbolic discovery, resulting in low-fidelity student models. In this work, we propose a novel training paradigm to address this challenge. Instead of passively distilling a pre-trained network, we introduce a \textbf{Jacobian-based regularizer} that actively encourages the ``teacher'' network to learn functions that are not only accurate but also inherently smoother and more amenable to distillation. We demonstrate through extensive experiments on a suite of real-world regression benchmarks that our method is highly effective. By optimizing the regularization strength for each problem, we improve the $R^2$ score of the final distilled symbolic model by an average of \textbf{120\% (relative)} compared to the standard distillation pipeline, all while maintaining the teacher's predictive accuracy. Our work presents a practical and principled method for significantly improving the fidelity of interpretable models extracted from complex neural networks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of Concept Drift in ML-Based Android Malware Detection</title>
<link>https://arxiv.org/abs/2507.22772</link>
<guid>https://arxiv.org/abs/2507.22772</guid>
<content:encoded><![CDATA[
arXiv:2507.22772v1 Announce Type: cross 
Abstract: Despite outstanding results, machine learning-based Android malware detection models struggle with concept drift, where rapidly evolving malware characteristics degrade model effectiveness. This study examines the impact of concept drift on Android malware detection, evaluating two datasets and nine machine learning and deep learning algorithms, as well as Large Language Models (LLMs). Various feature types--static, dynamic, hybrid, semantic, and image-based--were considered. The results showed that concept drift is widespread and significantly affects model performance. Factors influencing the drift include feature types, data environments, and detection methods. Balancing algorithms helped with class imbalance but did not fully address concept drift, which primarily stems from the dynamic nature of the malware landscape. No strong link was found between the type of algorithm used and concept drift, the impact was relatively minor compared to other variables since hyperparameters were not fine-tuned, and the default algorithm configurations were used. While LLMs using few-shot learning demonstrated promising detection performance, they did not fully mitigate concept drift, highlighting the need for further investigation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-Core: A Simple, Scalable and Balanced RLHF Trainer</title>
<link>https://arxiv.org/abs/2507.22789</link>
<guid>https://arxiv.org/abs/2507.22789</guid>
<content:encoded><![CDATA[
arXiv:2507.22789v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling. In this paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions. G-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2507.22802</link>
<guid>https://arxiv.org/abs/2507.22802</guid>
<content:encoded><![CDATA[
arXiv:2507.22802v1 Announce Type: cross 
Abstract: Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention</title>
<link>https://arxiv.org/abs/2507.22805</link>
<guid>https://arxiv.org/abs/2507.22805</guid>
<content:encoded><![CDATA[
arXiv:2507.22805v1 Announce Type: cross 
Abstract: Vision large language models (VLLMs) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a sparse Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models</title>
<link>https://arxiv.org/abs/2507.22828</link>
<guid>https://arxiv.org/abs/2507.22828</guid>
<content:encoded><![CDATA[
arXiv:2507.22828v1 Announce Type: cross 
Abstract: As Vision-Language Models (VLMs) are increasingly deployed in split-DNN configurations--with visual encoders (e.g., ResNet, ViT) operating on user devices and sending intermediate features to the cloud--there is a growing privacy risk from semantic information leakage. Existing approaches to reconstructing images from these intermediate features often result in blurry, semantically ambiguous images. To directly address semantic leakage, we propose CapRecover, a cross-modality inversion framework that recovers high-level semantic content, such as labels or captions, directly from intermediate features without image reconstruction.
  We evaluate CapRecover on multiple datasets and victim models, demonstrating strong performance in semantic recovery. Specifically, CapRecover achieves up to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis further reveals that deeper convolutional layers encode significantly more semantic information compared to shallow layers. To mitigate semantic leakage, we introduce a simple yet effective protection method: adding random noise to intermediate features at each layer and removing the noise in the next layer. Experimental results show that this approach prevents semantic leakage without additional training costs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents</title>
<link>https://arxiv.org/abs/2507.22844</link>
<guid>https://arxiv.org/abs/2507.22844</guid>
<content:encoded><![CDATA[
arXiv:2507.22844v1 Announce Type: cross 
Abstract: The development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term inefficient exploration. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning how to reason coherently. To address this, we introduce RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection, and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method. On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new state-of-the-art results, with our 7B model reaching an 83.6% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Repair-R1: Better Test Before Repair</title>
<link>https://arxiv.org/abs/2507.22853</link>
<guid>https://arxiv.org/abs/2507.22853</guid>
<content:encoded><![CDATA[
arXiv:2507.22853v1 Announce Type: cross 
Abstract: APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to 48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage by 0.78\% to 53.96\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model</title>
<link>https://arxiv.org/abs/2507.22854</link>
<guid>https://arxiv.org/abs/2507.22854</guid>
<content:encoded><![CDATA[
arXiv:2507.22854v1 Announce Type: cross 
Abstract: We propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a "simulator". By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like "optimism in the face of uncertainty" and "posterior sampling" and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al. (arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\operatorname{poly}\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</title>
<link>https://arxiv.org/abs/2507.22887</link>
<guid>https://arxiv.org/abs/2507.22887</guid>
<content:encoded><![CDATA[
arXiv:2507.22887v1 Announce Type: cross 
Abstract: In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIX: A Knowledge and Interaction-Centric Metacognitive Framework for Task Generalization</title>
<link>https://arxiv.org/abs/2402.05346</link>
<guid>https://arxiv.org/abs/2402.05346</guid>
<content:encoded><![CDATA[
arXiv:2402.05346v3 Announce Type: replace 
Abstract: People aptly exhibit general intelligence behaviors through flexible problem-solving and the ability to adapt to novel situations by reusing and applying high-level knowledge acquired over time. In contrast, artificial agents tend to be specialists, lacking such generalist behaviors. To bridge this gap, artificial agents will require understanding and exploiting critical structured knowledge representations. We introduce a metacognitive reasoning framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects, by leveraging a type space, facilitate the learning of transferable interaction concepts and promote generalization. This framework offers a principled approach for integrating knowledge into reinforcement learning and holds promise as an enabler for generalist behaviors in artificial intelligence, robotics, and autonomous systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Neural Strategy-Proof Matching Mechanism from Examples</title>
<link>https://arxiv.org/abs/2410.19384</link>
<guid>https://arxiv.org/abs/2410.19384</guid>
<content:encoded><![CDATA[
arXiv:2410.19384v3 Announce Type: replace 
Abstract: Designing two-sided matching mechanisms is challenging when practical demands for matching outcomes are difficult to formalize and the designed mechanism must satisfy theoretical conditions. To address this, prior work has proposed a framework that learns a matching mechanism from examples, using a parameterized family that satisfies properties such as stability. However, despite its usefulness, this framework does not guarantee strategy-proofness (SP), and cannot handle varying numbers of agents or incorporate publicly available contextual information about agents, both of which are crucial in real-world applications. In this paper, we propose a new parametrized family of matching mechanisms that always satisfy strategy-proofness, are applicable for an arbitrary number of agents, and deal with public contextual information of agents, based on the serial dictatorship (SD). This family is represented by NeuralSD, a novel neural network architecture based on SD, where agent rankings in SD are treated as learnable parameters computed from agents' contexts using an attention-based sub-network. To enable learning, we introduce tensor serial dictatorship (TSD), a differentiable relaxation of SD using tensor operations. This allows NeuralSD to be trained end-to-end from example matchings while satisfying SP. We conducted experiments to learn a matching mechanism from matching examples while satisfying SP. We demonstrated that our method outperformed baselines in predicting matchings and on several metrics for goodness of matching outcomes.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can adversarial attacks by large language models be attributed?</title>
<link>https://arxiv.org/abs/2411.08003</link>
<guid>https://arxiv.org/abs/2411.08003</guid>
<content:encoded><![CDATA[
arXiv:2411.08003v3 Announce Type: replace 
Abstract: Attributing outputs from Large Language Models (LLMs) in adversarial settings-such as cyberattacks and disinformation campaigns-presents significant challenges that are likely to grow in importance. We approach this attribution problem from both a theoretical and an empirical perspective, drawing on formal language theory (identification in the limit) and data-driven analysis of the expanding LLM ecosystem. By modeling an LLM's set of possible outputs as a formal language, we analyze whether finite samples of text can uniquely pinpoint the originating model. Our results show that, under mild assumptions of overlapping capabilities among models, certain classes of LLMs are fundamentally non-identifiable from their outputs alone. We delineate four regimes of theoretical identifiability: (1) an infinite class of deterministic (discrete) LLM languages is not identifiable (Gold's classical result from 1967); (2) an infinite class of probabilistic LLMs is also not identifiable (by extension of the deterministic case); (3) a finite class of deterministic LLMs is identifiable (consistent with Angluin's tell-tale criterion); and (4) even a finite class of probabilistic LLMs can be non-identifiable (we provide a new counterexample establishing this negative result). Complementing these theoretical insights, we quantify the explosion in the number of plausible model origins (hypothesis space) for a given output in recent years. Even under conservative assumptions-each open-source model fine-tuned on at most one new dataset-the count of distinct candidate models doubles approximately every 0.5 years, and allowing multi-dataset fine-tuning combinations yields doubling times as short as 0.28 years. This combinatorial growth, alongside the extraordinary computational cost of brute-force likelihood attribution across all models and potential users, renders exhaustive attribution infeasible in practice.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model Acceleration based on KV Cache Management</title>
<link>https://arxiv.org/abs/2412.19442</link>
<guid>https://arxiv.org/abs/2412.19442</guid>
<content:encoded><![CDATA[
arXiv:2412.19442v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CollabLLM: From Passive Responders to Active Collaborators</title>
<link>https://arxiv.org/abs/2502.00640</link>
<guid>https://arxiv.org/abs/2502.00640</guid>
<content:encoded><![CDATA[
arXiv:2502.00640v3 Announce Type: replace 
Abstract: Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning</title>
<link>https://arxiv.org/abs/2502.13820</link>
<guid>https://arxiv.org/abs/2502.13820</guid>
<content:encoded><![CDATA[
arXiv:2502.13820v3 Announce Type: replace 
Abstract: Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence</title>
<link>https://arxiv.org/abs/2504.04430</link>
<guid>https://arxiv.org/abs/2504.04430</guid>
<content:encoded><![CDATA[
arXiv:2504.04430v5 Announce Type: replace 
Abstract: Despite major advances in machine learning, current artificial intelligence systems continue to fall short of human-like general intelligence. While large language and reasoning models can generate fluent and coherent outputs, they lack the deep understanding and adaptive reasoning that characterize truly general intelligence. Existing evaluation frameworks, which are centered on broad language or perception tasks, fail to capture generality at its core and offer no guidance. The artificial general intelligence testbed (AGITB) is a novel and freely available benchmarking suite comprising twelve fully automatable tests designed to evaluate low-level cognitive precursors through binary signal prediction. AGITB requires models to forecast temporal sequences without pretraining, symbolic manipulation, or semantic grounding. The framework isolates core computational invariants - such as determinism, sensitivity, and generalization - that align with principles of biological information processing. Engineered to resist brute-force and memorization-based approaches, AGITB presumes no prior knowledge and demands learning from first principles. While humans pass all tests, no current AI system has met the full AGITB criteria, underscoring its potential as a rigorous, interpretable, and actionable benchmark for guiding and evaluating progress toward artificial general intelligence. A reference implementation of AGITB is available on GitHub.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Lag, RAG: Training-Free Adversarial Detection Using RAG</title>
<link>https://arxiv.org/abs/2504.04858</link>
<guid>https://arxiv.org/abs/2504.04858</guid>
<content:encoded><![CDATA[
arXiv:2504.04858v3 Announce Type: replace 
Abstract: Adversarial patch attacks pose a major threat to vision systems by embedding localized perturbations that mislead deep models. Traditional defense methods often require retraining or fine-tuning, making them impractical for real-world deployment. We propose a training-free Visual Retrieval-Augmented Generation (VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial patch detection. By retrieving visually similar patches and images that resemble stored attacks in a continuously expanding database, VRAG performs generative reasoning to identify diverse attack types, all without additional training or fine-tuning. We extensively evaluate open-source large-scale VLMs, including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO model achieves up to 95 percent classification accuracy, setting a new state-of-the-art for open-source adversarial patch detection. Gemini-2.0 attains the highest overall accuracy, 98 percent, but remains closed-source. Experimental results demonstrate VRAG's effectiveness in identifying a variety of adversarial patches with minimal human annotation, paving the way for robust, practical defenses against evolving adversarial patch attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgoal-Guided Policy Heuristic Search with Learned Subgoals</title>
<link>https://arxiv.org/abs/2506.07255</link>
<guid>https://arxiv.org/abs/2506.07255</guid>
<content:encoded><![CDATA[
arXiv:2506.07255v2 Announce Type: replace 
Abstract: Policy tree search is a family of tree search algorithms that use a policy to guide the search. These algorithms provide guarantees on the number of expansions required to solve a given problem that are based on the quality of the policy. While these algorithms have shown promising results, the process in which they are trained requires complete solution trajectories to train the policy. Search trajectories are obtained during a trial-and-error search process. When the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. As a result, search samples are wasted in failed attempts to solve these hard instances. This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts. We empirically show that our policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in this online setting.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence</title>
<link>https://arxiv.org/abs/2506.15677</link>
<guid>https://arxiv.org/abs/2506.15677</guid>
<content:encoded><![CDATA[
arXiv:2506.15677v3 Announce Type: replace 
Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering via Self-Supervised Diffusion</title>
<link>https://arxiv.org/abs/2507.04283</link>
<guid>https://arxiv.org/abs/2507.04283</guid>
<content:encoded><![CDATA[
arXiv:2507.04283v2 Announce Type: replace 
Abstract: Diffusion models, widely recognized for their success in generative tasks, have not yet been applied to clustering. We introduce Clustering via Diffusion (CLUDI), a self-supervised framework that combines the generative power of diffusion models with pre-trained Vision Transformer features to achieve robust and accurate clustering. CLUDI is trained via a teacher-student paradigm: the teacher uses stochastic diffusion-based sampling to produce diverse cluster assignments, which the student refines into stable predictions. This stochasticity acts as a novel data augmentation strategy, enabling CLUDI to uncover intricate structures in high-dimensional data. Extensive evaluations on challenging datasets demonstrate that CLUDI achieves state-of-the-art performance in unsupervised classification, setting new benchmarks in clustering robustness and adaptability to complex data distributions. Our code is available at https://github.com/BGU-CS-VIL/CLUDI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</title>
<link>https://arxiv.org/abs/2307.14364</link>
<guid>https://arxiv.org/abs/2307.14364</guid>
<content:encoded><![CDATA[
arXiv:2307.14364v2 Announce Type: replace-cross 
Abstract: Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Law of Capacity Gap in Distilling Language Models</title>
<link>https://arxiv.org/abs/2311.07052</link>
<guid>https://arxiv.org/abs/2311.07052</guid>
<content:encoded><![CDATA[
arXiv:2311.07052v4 Announce Type: replace-cross 
Abstract: Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the \textit{curse of capacity gap}, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the \textit{law of capacity gap} inducted from a preliminary study on distilling a broad range of small-scale (<3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Privacy and Robustness for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2403.16591</link>
<guid>https://arxiv.org/abs/2403.16591</guid>
<content:encoded><![CDATA[
arXiv:2403.16591v5 Announce Type: replace-cross 
Abstract: The widespread adoption of machine learning necessitates robust privacy protection alongside algorithmic resilience. While Local Differential Privacy (LDP) provides foundational guarantees, sophisticated adversaries with prior knowledge demand more nuanced Bayesian privacy notions, such as Maximum Bayesian Privacy (MBP) and Average Bayesian Privacy (ABP), first introduced by \cite{zhang2022no}. Concurrently, machine learning systems require inherent robustness against data perturbations and adversarial manipulations. This paper systematically investigates the intricate theoretical relationships among LDP, MBP, and ABP. Crucially, we bridge these privacy concepts with algorithmic robustness, particularly within the Probably Approximately Correct (PAC) learning framework. Our work demonstrates that privacy-preserving mechanisms inherently confer PAC robustness. We present key theoretical results, including the formalization of the established LDP-MBP relationship, novel bounds between MBP and ABP, and a proof demonstrating PAC robustness from MBP. Furthermore, we establish a novel theoretical relationship quantifying how privacy leakage directly influences an algorithm's input robustness. These results provide a unified theoretical framework for understanding and optimizing the privacy-robustness trade-off, paving the way for the development of more secure, trustworthy, and resilient machine learning systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</title>
<link>https://arxiv.org/abs/2404.07214</link>
<guid>https://arxiv.org/abs/2404.07214</guid>
<content:encoded><![CDATA[
arXiv:2404.07214v3 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT</title>
<link>https://arxiv.org/abs/2404.08939</link>
<guid>https://arxiv.org/abs/2404.08939</guid>
<content:encoded><![CDATA[
arXiv:2404.08939v2 Announce Type: replace-cross 
Abstract: Inertial tracking is vital for robotic IoT and has gained popularity thanks to the ubiquity of low-cost inertial measurement units and deep learning-powered tracking algorithms. Existing works, however, have not fully utilized IMU measurements, particularly magnetometers, nor have they maximized the potential of deep learning to achieve the desired accuracy. To address these limitations, we introduce NeurIT, which elevates tracking accuracy to a new level. NeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its core, combining both RNN and Transformer to learn representative features in both time and frequency domains. To fully utilize IMU information, we strategically employ body-frame differentiation of magnetometers, considerably reducing the tracking error. We implement NeurIT on a customized robotic platform and conduct evaluation in various indoor environments. Experimental results demonstrate that NeurIT achieves a mere 1-meter tracking error over a 300-meter distance. Notably, it significantly outperforms state-of-the-art baselines by 48.21% on unseen data. Moreover, NeurIT demonstrates robustness in large urban complexes and performs comparably to the visual-inertial approach (Tango Phone) in vision-favored conditions while surpassing it in feature-sparse settings. We believe NeurIT takes an important step forward toward practical neural inertial tracking for ubiquitous and scalable tracking of robotic things. NeurIT is open-sourced here: https://github.com/aiot-lab/NeurIT.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cooperative Network Architecture: Learning Structured Networks as Representation of Sensory Patterns</title>
<link>https://arxiv.org/abs/2407.05650</link>
<guid>https://arxiv.org/abs/2407.05650</guid>
<content:encoded><![CDATA[
arXiv:2407.05650v4 Announce Type: replace-cross 
Abstract: We introduce the Cooperative Network Architecture (CNA), a model that represents sensory signals using structured, recurrently connected networks of neurons, termed "nets." Nets are dynamically assembled from overlapping net fragments, which are learned based on statistical regularities in sensory input. This architecture offers robustness to noise, deformation, and out-of-distribution data, addressing challenges in current vision systems from a novel perspective. We demonstrate that net fragments can be learned without supervision and flexibly recombined to encode novel patterns, enabling figure completion and resilience to noise. Our findings establish CNA as a promising paradigm for developing neural representations that integrate local feature processing with global structure formation, providing a foundation for future research on invariant object recognition.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer</title>
<link>https://arxiv.org/abs/2407.10247</link>
<guid>https://arxiv.org/abs/2407.10247</guid>
<content:encoded><![CDATA[
arXiv:2407.10247v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) into corporate strategy has become critical for organizations seeking to maintain a competitive advantage in the digital age. As AI transforms business models, operations, and decision-making, the need for dedicated executive leadership to guide, govern, and orchestrate this transformation becomes increasingly evident. This paper examines emerging future scenarios across three domains: the AI Economy, the AI Organization, and Competition in the Age of AI. These domains reveal environmental, structural, and strategic tensions that existing C-suite roles struggle to resolve. In response, the paper develops a theory-informed framework for the Chief AI Officer (CAIO), outlining the distinct functions and capabilities required to guide and govern AI at scale. Drawing on illustrative cases and emerging practice, this conceptualization clarifies the CAIOs unique role within the executive landscape and presents a forward-looking research agenda. This paper advances the discourse on AI leadership by offering a theory-driven rationale for the strategic integration of AI at the executive level and by positioning the Chief AI Officer as a distinct and necessary role within modern organizations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Prompt Engineering for Cost-Effective Code Generation Using Evolutionary Algorithm</title>
<link>https://arxiv.org/abs/2408.11198</link>
<guid>https://arxiv.org/abs/2408.11198</guid>
<content:encoded><![CDATA[
arXiv:2408.11198v2 Announce Type: replace-cross 
Abstract: Large Language Models have seen increasing use in various software development tasks, especially in code generation. The most advanced recent methods attempt to incorporate feedback from code execution into prompts to help guide LLMs in generating correct code in an iterative process. While effective, these methods could be costly due to numerous interactions with the LLM and extensive token usage. To address this issue, we propose an alternative approach named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight evolutionary algorithm to refine the original prompts into improved versions that generate high quality code, with minimal interactions with the LLM. Our evaluation against state-of-the-art (SOTA) LLM based code generation agents shows that EPiC not only achieves up to 6% improvement in pass@k but is also 2-10 times more cost-effective than the baselines.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Past Meets Present: Creating Historical Analogy with Large Language Models</title>
<link>https://arxiv.org/abs/2409.14820</link>
<guid>https://arxiv.org/abs/2409.14820</guid>
<content:encoded><![CDATA[
arXiv:2409.14820v2 Announce Type: replace-cross 
Abstract: Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neutral Residues: Revisiting Adapters for Model Extension</title>
<link>https://arxiv.org/abs/2410.02744</link>
<guid>https://arxiv.org/abs/2410.02744</guid>
<content:encoded><![CDATA[
arXiv:2410.02744v2 Announce Type: replace-cross 
Abstract: We address the problem of extending a pretrained large language model to a new domain that was not seen during training. Standard techniques, such as finetuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we revisit and improve adapters to extend LLMs from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperform competing approaches such as finetuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training</title>
<link>https://arxiv.org/abs/2410.07336</link>
<guid>https://arxiv.org/abs/2410.07336</guid>
<content:encoded><![CDATA[
arXiv:2410.07336v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in caption generation, existing evaluation metrics often fail to capture the full quality or fine-grained details of captions. This is mainly due to their reliance on non-specific human-written references or noisy pre-training data. Still, finding an effective metric is crucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a key role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the generated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models. Extensive experiments on different image and video datasets highlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity to object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage of a captioning model results in semantically richer captions with fewer repetitions and grammatical errors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning approach in enhancing model capabilities. Source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges</title>
<link>https://arxiv.org/abs/2410.21306</link>
<guid>https://arxiv.org/abs/2410.21306</guid>
<content:encoded><![CDATA[
arXiv:2410.21306v3 Announce Type: replace-cross 
Abstract: Natural Language Processing (NLP) is revolutionising the way both professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational assistance tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 131 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document lengths, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legal text, such as Document Summarisation, Named Entity Recognition, Question Answering, Argument Mining, Text Classification, and Judgement Prediction. Furthermore, we analyse both developed legal-oriented language models, and approaches for adapting general-purpose language models to the legal domain. Additionally, we identify sixteen open research challenges, including the detection and mitigation of bias in artificial intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification</title>
<link>https://arxiv.org/abs/2411.07076</link>
<guid>https://arxiv.org/abs/2411.07076</guid>
<content:encoded><![CDATA[
arXiv:2411.07076v3 Announce Type: replace-cross 
Abstract: Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as consistent character identification and plot-level descriptions incorporating both visual and audio information. To address these, we figure out audio-visual character identification, matching character names to each dialogue, as a key factor. We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information. StoryTeller uses a multimodal large language model that integrates visual, audio, and text modalities to perform audio-visual character identification on minute-long video clips. The results are then fed into a LVLM to enhance consistency of video description. We validate our approach on movie description tasks and introduce MovieStory101, a dataset with dense descriptions for three-minute movie clips. To evaluate long video descriptions, we create StoryQA, a large set of multiple-choice questions for MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments show that StoryTeller outperforms all open and closed-source baselines on StoryQA, achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and demonstrating a +15.56% advantage in human side-by-side evaluations. Additionally, incorporating audio-visual character identification from StoryTeller improves the performance of all video description models, with Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%, respectively, in accuracy on StoryQA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Automated Validation of Language Model Synthesized Test Cases using Semantic Entropy</title>
<link>https://arxiv.org/abs/2411.08254</link>
<guid>https://arxiv.org/abs/2411.08254</guid>
<content:encoded><![CDATA[
arXiv:2411.08254v2 Announce Type: replace-cross 
Abstract: Modern Large Language Model (LLM)-based programming agents often rely on test execution feedback to refine their generated code. These tests are synthetically generated by LLMs. However, LLMs may produce invalid or hallucinated test cases, which can mislead feedback loops and degrade the performance of agents in refining and improving code. This paper introduces VALTEST, a novel framework that leverages semantic entropy to automatically validate test cases generated by LLMs. Analyzing the semantic structure of test cases and computing entropy-based uncertainty measures, VALTEST trains a machine learning model to classify test cases as valid or invalid and filters out invalid test cases. Experiments on multiple benchmark datasets and various LLMs show that VALTEST not only boosts test validity by up to 29% but also improves code generation performance, as evidenced by significant increases in pass@1 scores. Our extensive experiments also reveal that semantic entropy is a reliable indicator to distinguish between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases used in software testing and code generation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTrackTr:Towards Fast Multi-Object Tracking with Transformers</title>
<link>https://arxiv.org/abs/2411.15811</link>
<guid>https://arxiv.org/abs/2411.15811</guid>
<content:encoded><![CDATA[
arXiv:2411.15811v4 Announce Type: replace-cross 
Abstract: Transformer-based multi-object tracking (MOT) methods have captured the attention of many researchers in recent years. However, these models often suffer from slow inference speeds due to their structure or other issues. To address this problem, we revisited the Joint Detection and Tracking (JDT) method by looking back at past approaches. By integrating the original JDT approach with some advanced theories, this paper employs an efficient method of information transfer between frames on the DETR, constructing a fast and novel JDT-type MOT framework: FastTrackTr. Thanks to the superiority of this information transfer method, our approach not only reduces the number of queries required during tracking but also avoids the excessive introduction of network structures, ensuring model simplicity. Experimental results indicate that our method has the potential to achieve real-time tracking and exhibits competitive tracking accuracy across multiple datasets.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
<link>https://arxiv.org/abs/2412.15239</link>
<guid>https://arxiv.org/abs/2412.15239</guid>
<content:encoded><![CDATA[
arXiv:2412.15239v3 Announce Type: replace-cross 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationale-guided Prompting for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2412.16936</link>
<guid>https://arxiv.org/abs/2412.16936</guid>
<content:encoded><![CDATA[
arXiv:2412.16936v2 Announce Type: replace-cross 
Abstract: Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis</title>
<link>https://arxiv.org/abs/2412.20104</link>
<guid>https://arxiv.org/abs/2412.20104</guid>
<content:encoded><![CDATA[
arXiv:2412.20104v5 Announce Type: replace-cross 
Abstract: Synthesizing realistic human-object interaction motions is a critical problem in VR/AR and human animation. Unlike the commonly studied scenarios involving a single human or hand interacting with one object, we address a more generic multi-body setting with arbitrary numbers of humans, hands, and objects. This complexity introduces significant challenges in synchronizing motions due to the high correlations and mutual influences among bodies. To address these challenges, we introduce SyncDiff, a novel method for multi-body interaction synthesis using a synchronized motion diffusion strategy. SyncDiff employs a single diffusion model to capture the joint distribution of multi-body motions. To enhance motion fidelity, we propose a frequency-domain motion decomposition scheme. Additionally, we introduce a new set of alignment scores to emphasize the synchronization of different body motions. SyncDiff jointly optimizes both data sample likelihood and alignment likelihood through an explicit synchronization strategy. Extensive experiments across four datasets with various multi-body configurations demonstrate the superiority of SyncDiff over existing state-of-the-art motion synthesis methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.06680</link>
<guid>https://arxiv.org/abs/2501.06680</guid>
<content:encoded><![CDATA[
arXiv:2501.06680v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have become a promising approach to enhancing perception and decision-making in autonomous driving. The gap remains in applying VLMs to understand complex scenarios interacting with pedestrians and efficient vehicle deployment. In this paper, we propose a knowledge distillation method that transfers knowledge from large-scale vision-language foundation models to efficient vision networks, and we apply it to pedestrian behavior prediction and scene understanding tasks, achieving promising results in generating more diverse and comprehensive semantic attributes. We also utilize multiple pre-trained models and ensemble techniques to boost the model's performance. We further examined the effectiveness of the model after knowledge distillation; the results show significant metric improvements in open-vocabulary perception and trajectory prediction tasks, which can potentially enhance the end-to-end performance of autonomous driving.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning in Echo State Networks for Input Reconstruction</title>
<link>https://arxiv.org/abs/2501.11409</link>
<guid>https://arxiv.org/abs/2501.11409</guid>
<content:encoded><![CDATA[
arXiv:2501.11409v4 Announce Type: replace-cross 
Abstract: Echo state networks (ESNs) are a class of recurrent neural networks in which only the readout layer is trainable, while the recurrent and input layers are fixed. This architectural constraint enables computationally efficient processing of time-series data. Traditionally, the readout layer in ESNs is trained using supervised learning with target outputs. In this study, we focus on input reconstruction (IR), where the readout layer is trained to reconstruct the input time series fed into the ESN. We show that IR can be achieved through unsupervised learning (UL), without access to supervised targets, provided that the ESN parameters are known a priori and satisfy invertibility conditions. This formulation allows applications relying on IR, such as dynamical system replication and noise filtering, to be reformulated within the UL framework via straightforward integration with existing algorithms. Our results suggest that prior knowledge of ESN parameters can reduce reliance on supervision, thereby establishing a new principle: not only by fixing part of the network parameters but also by exploiting their specific values. Furthermore, our UL-based algorithms for input reconstruction and related tasks are suitable for autonomous processing, offering insights into how analogous computational mechanisms might operate in the brain in principle. These findings contribute to a deeper understanding of the mathematical foundations of ESNs and their relevance to models in computational neuroscience.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2502.00201</link>
<guid>https://arxiv.org/abs/2502.00201</guid>
<content:encoded><![CDATA[
arXiv:2502.00201v2 Announce Type: replace-cross 
Abstract: This paper systematically reviews advancements in deep learning (DL) techniques for financial fraud detection, a critical issue in the financial sector. Using the Kitchenham systematic literature review approach, 57 studies published between 2019 and 2024 were analyzed. The review highlights the effectiveness of various deep learning models such as Convolutional Neural Networks, Long Short-Term Memory, and transformers across domains such as credit card transactions, insurance claims, and financial statement audits. Performance metrics such as precision, recall, F1-score, and AUC-ROC were evaluated. Key themes explored include the impact of data privacy frameworks and advancements in feature engineering and data preprocessing. The study emphasizes challenges such as imbalanced datasets, model interpretability, and ethical considerations, alongside opportunities for automation and privacy-preserving techniques such as blockchain integration and Principal Component Analysis. By examining trends over the past five years, this review identifies critical gaps and promising directions for advancing DL applications in financial fraud detection, offering actionable insights for researchers and practitioners.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GneissWeb: Preparing High Quality Data for LLMs at Scale</title>
<link>https://arxiv.org/abs/2502.14907</link>
<guid>https://arxiv.org/abs/2502.14907</guid>
<content:encoded><![CDATA[
arXiv:2502.14907v2 Announce Type: replace-cross 
Abstract: Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.
  In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).
  We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleanMel: Mel-Spectrogram Enhancement for Improving Both Speech Quality and ASR</title>
<link>https://arxiv.org/abs/2502.20040</link>
<guid>https://arxiv.org/abs/2502.20040</guid>
<content:encoded><![CDATA[
arXiv:2502.20040v2 Announce Type: replace-cross 
Abstract: In this work, we propose CleanMel, a single-channel Mel-spectrogram denoising and dereverberation network for improving both speech quality and automatic speech recognition (ASR) performance. The proposed network takes as input the noisy and reverberant microphone recording and predicts the corresponding clean Mel-spectrogram. The enhanced Mel-spectrogram can be either transformed to the speech waveform with a neural vocoder or directly used for ASR. The proposed network is composed of interleaved cross-band and narrow-band processing in the Mel-frequency domain, for learning the full-band spectral pattern and the narrow-band properties of signals, respectively. Compared to linear-frequency domain or time-domain speech enhancement, the key advantage of Mel-spectrogram enhancement is that Mel-frequency presents speech in a more compact way and thus is easier to learn, which will benefit both speech quality and ASR. Experimental results on five English and one Chinese datasets demonstrate a significant improvement in both speech quality and ASR performance achieved by the proposed model.Code and audio examples of our model are available online.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenEarthSensing: Large-Scale Fine-Grained Benchmark for Open-World Remote Sensing</title>
<link>https://arxiv.org/abs/2502.20668</link>
<guid>https://arxiv.org/abs/2502.20668</guid>
<content:encoded><![CDATA[
arXiv:2502.20668v2 Announce Type: replace-cross 
Abstract: The advancement of remote sensing, including satellite systems, facilitates the continuous acquisition of remote sensing imagery globally, introducing novel challenges for achieving open-world tasks. Deployed models need to continuously adjust to a constant influx of new data, which frequently exhibits diverse shifts from the data encountered during the training phase. To effectively handle the new data, models are required to detect semantic shifts, adapt to covariate shifts, and continuously update their parameters without forgetting learned knowledge, as has been considered in works on a variety of open-world tasks. However, existing studies are typically conducted within a single dataset to simulate realistic conditions, with a lack of large-scale benchmarks capable of evaluating multiple open-world tasks. In this paper, we introduce \textbf{OpenEarthSensing (OES)}, a large-scale fine-grained benchmark for open-world remote sensing. OES includes 189 scene and object categories, covering the vast majority of potential semantic shifts that may occur in the real world. Additionally, to provide a more comprehensive testbed for evaluating the generalization performance, OES encompasses five data domains with significant covariate shifts, including two RGB satellite domains, one RGB aerial domain, one multispectral RGB domain, and one infrared domain. We evaluate the baselines and existing methods for diverse tasks on OES, demonstrating that it serves as a meaningful and challenging benchmark for open-world remote sensing. The proposed dataset OES is available at https://haiv-lab.github.io/OES.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2504.11829</link>
<guid>https://arxiv.org/abs/2504.11829</guid>
<content:encoded><![CDATA[
arXiv:2504.11829v3 Announce Type: replace-cross 
Abstract: Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization</title>
<link>https://arxiv.org/abs/2504.14200</link>
<guid>https://arxiv.org/abs/2504.14200</guid>
<content:encoded><![CDATA[
arXiv:2504.14200v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to adapt to new tasks without parameter updates, using a few demonstrations from a large support set. However, selecting informative demonstrations leads to high computational and memory costs. While some methods explore selecting a small and representative coreset in the text classification, evaluating all support set samples remains costly, and discarded samples lead to unnecessary information loss. These methods may also be less effective for image classification due to differences in feature spaces. Given these limitations, we propose Key-based Coreset Optimization (KeCO), a novel framework that leverages untapped data to construct a compact and informative coreset. We introduce visual features as keys within the coreset, which serve as the anchor for identifying samples to be updated through different selection strategies. By leveraging untapped samples from the support set, we update the keys of selected coreset samples, enabling the randomly initialized coreset to evolve into a more informative coreset under low computational cost. Through extensive experiments on coarse-grained and fine-grained image classification benchmarks, we demonstrate that KeCO effectively enhances ICL performance for image classification task, achieving an average improvement of more than 20\%. Notably, we evaluate KeCO under a simulated online scenario, and the strong performance in this scenario highlights the practical value of our framework for resource-constrained real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America</title>
<link>https://arxiv.org/abs/2505.08841</link>
<guid>https://arxiv.org/abs/2505.08841</guid>
<content:encoded><![CDATA[
arXiv:2505.08841v2 Announce Type: replace-cross 
Abstract: As artificial intelligence and robotics increasingly reshape the global labor market, understanding public perceptions of these technologies becomes critical. We examine how these perceptions have evolved across Latin America, using survey data from the 2017, 2018, 2020, and 2023 waves of the Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16 countries, we analyze fear of job loss due to artificial intelligence and robotics. Using statistical modeling and latent class analysis, we identify key structural and ideological predictors of concern, with education level and political orientation emerging as the most consistent drivers. Our findings reveal substantial temporal and cross-country variation, with a notable peak in fear during 2018 and distinct attitudinal profiles emerging from latent segmentation. These results offer new insights into the social and structural dimensions of AI anxiety in emerging economies and contribute to a broader understanding of public attitudes toward automation beyond the Global North.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-Inpainting: A Proactive Defense Approach against Malicious Diffusion-based Inpainters under Unknown Conditions</title>
<link>https://arxiv.org/abs/2505.13023</link>
<guid>https://arxiv.org/abs/2505.13023</guid>
<content:encoded><![CDATA[
arXiv:2505.13023v2 Announce Type: replace-cross 
Abstract: With the increasing prevalence of diffusion-based malicious image manipulation, existing proactive defense methods struggle to safeguard images against tampering under unknown conditions. To address this, we propose Anti-Inpainting, a proactive defense approach that achieves protection comprising three novel modules. First, we introduce a multi-level deep feature extractor to obtain intricate features from the diffusion denoising process, enhancing protective effectiveness. Second, we design a multi-scale, semantic-preserving data augmentation technique to enhance the transferability of adversarial perturbations across unknown conditions. Finally, we propose a selection-based distribution deviation optimization strategy to bolster protection against manipulations guided by diverse random seeds. Extensive experiments on InpaintGuardBench and CelebA-HQ demonstrate that Anti-Inpainting effectively defends against diffusion-based inpainters under unknown conditions. Additionally, our approach demonstrates robustness against various image purification methods and transferability across different diffusion model versions.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging</title>
<link>https://arxiv.org/abs/2505.14136</link>
<guid>https://arxiv.org/abs/2505.14136</guid>
<content:encoded><![CDATA[
arXiv:2505.14136v2 Announce Type: replace-cross 
Abstract: Mixture of expert (MoE) models are a promising approach to increasing model capacity without increasing inference cost, and are core components of many state-of-the-art language models. However, current MoE models typically use only few experts due to prohibitive training and inference cost. We propose Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of magnitude more experts and uses model merging to avoid almost any test-time overhead. We show that TTMM is an approximation of test-time training (TTT), which fine-tunes an expert model for each prediction task, i.e., prompt. TTT has recently been shown to significantly improve language models, but is computationally expensive. We find that performance of TTMM improves with more experts and approaches the performance of TTT. Moreover, we find that with a 1B parameter base model, TTMM is more than 100x faster than TTT at test-time by amortizing the cost of TTT at train-time. Thus, TTMM offers a promising cost-effective approach to scale test-time training.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering</title>
<link>https://arxiv.org/abs/2505.15038</link>
<guid>https://arxiv.org/abs/2505.15038</guid>
<content:encoded><![CDATA[
arXiv:2505.15038v2 Announce Type: replace-cross 
Abstract: Linear concept vectors effectively steer LLMs, but existing methods suffer from noisy features in diverse datasets that undermine steering robustness. We propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively keep the most discriminative SAE latents while reconstructing hidden representations. Our key insight is that concept-relevant signals can be explicitly separated from dataset noise by scaling up activations of top-k latents that best differentiate positive and negative samples. Applied to linear probing and difference-in-mean, SDCV consistently improves steering success rates by 4-16\% across six challenging concepts, while maintaining topic relevance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-based Reinforcement Learning to Predict the Future</title>
<link>https://arxiv.org/abs/2505.17989</link>
<guid>https://arxiv.org/abs/2505.17989</guid>
<content:encoded><![CDATA[
arXiv:2505.17989v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been an effective approach for improving Large Language Models' reasoning in domains such as coding and mathematics. Here, we apply RLVR methods towards forecasting future real-world events - a challenging task for RL due to the very noisy (and delayed) outcomes involved. Using a novel dataset of recent questions from a prediction market, and accompanying relevant news headlines, we show that a compact (14B) reasoning model can be trained to match or surpass the predictive accuracy of frontier models like o1, while greatly improving probabilistic calibration. The model's performance is also practically meaningful: in a Polymarket trading simulation, we estimate that its bets would have yielded a return on investment of over 10% across all questions in the test set. We detail and compare approaches used in training our model, including augmenting our training-data with synthetic prediction questions, guardrails for learning stability, and median prediction sampling at inference-time.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title>
<link>https://arxiv.org/abs/2505.19194</link>
<guid>https://arxiv.org/abs/2505.19194</guid>
<content:encoded><![CDATA[
arXiv:2505.19194v2 Announce Type: replace-cross 
Abstract: Adversarial attack reveals the vulnerability of deep learning models. For about a decade, countless attack and defense methods have been proposed, leading to robustified classifiers and better understanding of models. Among these methods, curvature-based approaches have attracted attention because it is assumed that high curvature may give rise to rough decision boundary. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation(DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack(CDBA) with improved performance using the dynamically estimated curvature.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
arXiv:2505.20579v3 Announce Type: replace-cross 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Point Cloud Assembly</title>
<link>https://arxiv.org/abs/2505.21539</link>
<guid>https://arxiv.org/abs/2505.21539</guid>
<content:encoded><![CDATA[
arXiv:2505.21539v2 Announce Type: replace-cross 
Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by aligning multiple point cloud pieces. This work presents a novel equivariant solver for assembly tasks based on flow matching models. We first theoretically show that the key to learning equivariant distributions via flow matching is to learn related vector fields. Based on this result, we propose an assembly model, called equivariant diffusion assembly (Eda), which learns related vector fields conditioned on the input pieces. We further construct an equivariant path for Eda, which guarantees high data efficiency of the training process. Our numerical results show that Eda is highly competitive on practical datasets, and it can even handle the challenging situation where the input pieces are non-overlapped.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-qualitative-judge: automating error analysis in natural language generation</title>
<link>https://arxiv.org/abs/2506.09147</link>
<guid>https://arxiv.org/abs/2506.09147</guid>
<content:encoded><![CDATA[
arXiv:2506.09147v2 Announce Type: replace-cross 
Abstract: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3 cases and is capable of producing error type reports resembling the reports composed by human annotators. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating loss of variance in ensemble data assimilation: machine learning-based and distance-free localization</title>
<link>https://arxiv.org/abs/2506.13362</link>
<guid>https://arxiv.org/abs/2506.13362</guid>
<content:encoded><![CDATA[
arXiv:2506.13362v2 Announce Type: replace-cross 
Abstract: We propose two new methods based/inspired by machine learning for tabular data and distance-free localization to enhance the covariance estimations in an ensemble data assimilation. The main goal is to enhance the data assimilation results by mitigating loss of variance due to sampling errors. We also analyze the suitability of several machine learning models and the balance between accuracy and computational cost of the covariance estimations. We introduce two distance-free localization techniques leveraging machine learning methods specifically tailored for tabular data. The methods are integrated into the Ensemble Smoother with Multiple Data Assimilation (ES-MDA) framework. The results show that the proposed localizations improve covariance accuracy and enhance data assimilation and uncertainty quantification results. We observe reduced variance loss for the input variables using the proposed methods. Furthermore, we compare several machine learning models, assessing their suitability for the problem in terms of computational cost, and quality of the covariance estimation and data match. The influence of ensemble size is also investigated, providing insights into balancing accuracy and computational efficiency. Our findings demonstrate that certain machine learning models are more suitable for this problem. This study introduces two novel methods that mitigate variance loss for model parameters in ensemble-based data assimilation, offering practical solutions that are easy to implement and do not require any additional numerical simulation or hyperparameter tuning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis</title>
<link>https://arxiv.org/abs/2507.01053</link>
<guid>https://arxiv.org/abs/2507.01053</guid>
<content:encoded><![CDATA[
arXiv:2507.01053v2 Announce Type: replace-cross 
Abstract: As ever-larger clinical datasets become available, they have the potential to unlock unprecedented opportunities for medical research. Foremost among them is Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest open-source EHR database. However, the inherent complexity of these datasets, particularly the need for sophisticated querying skills and the need to understand the underlying clinical settings, often presents a significant barrier to their effective use. M3 lowers the technical barrier to understanding and querying MIMIC-IV data. With a single command it retrieves MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers converse with the database in plain English. Ask a clinical question in natural language; M3 uses a language model to translate it into SQL, executes the query against the MIMIC-IV dataset, and returns structured results alongside the underlying query for verifiability and reproducibility. Demonstrations show that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that once demanded hours of handcrafted SQL and relied on understanding the complexities of clinical workflows. By simplifying access, M3 invites the broader research community to mine clinical critical-care data and accelerates the translation of raw records into actionable insight.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis</title>
<link>https://arxiv.org/abs/2507.09950</link>
<guid>https://arxiv.org/abs/2507.09950</guid>
<content:encoded><![CDATA[
arXiv:2507.09950v2 Announce Type: replace-cross 
Abstract: The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the 'discovery experience' of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division</title>
<link>https://arxiv.org/abs/2507.14957</link>
<guid>https://arxiv.org/abs/2507.14957</guid>
<content:encoded><![CDATA[
arXiv:2507.14957v2 Announce Type: replace-cross 
Abstract: We study the fair division of indivisible items and provide new insights into the EFX problem, which is widely regarded as the central open question in fair division, and the PMMS problem, a strictly stronger variant of EFX. Our first result constructs a three-agent instance with two monotone valuations and one additive valuation in which no PMMS allocation exists. Since EFX allocations are known to exist under these assumptions, this establishes a formal separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We show that EFX allocations exist for personalized bivalued valuations, where for each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value $v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also prove that PMMS allocations exist for binary-valued MMS-feasible valuations, where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result holds even without assuming monotonicity of valuations and thus applies to the fair division of chores and mixed manna. Finally, we study a class of valuations called pair-demand valuations, which extend the well-studied unit-demand valuations to the case where each agent derives value from at most two items, and we show that PMMS allocations exist in this setting. Our proofs are constructive, and we provide polynomial-time algorithms for all three existence results.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention</title>
<link>https://arxiv.org/abs/2507.17745</link>
<guid>https://arxiv.org/abs/2507.17745</guid>
<content:encoded><![CDATA[
arXiv:2507.17745v2 Announce Type: replace-cross 
Abstract: Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review</title>
<link>https://arxiv.org/abs/2507.16876</link>
<guid>https://arxiv.org/abs/2507.16876</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal machine learning, histopathology, molecular data, cancer prognostication, overall survival<br />
Summary: 
Multimodal machine learning integrating histopathology and molecular data for cancer prognostication is a rapidly advancing field, with promising results from studies combining whole slide images (WSIs) and high-throughput omics data. A systematic review of 48 studies across 19 cancer types showed that multimodal models typically outperformed unimodal ones in predicting overall survival, with reported c-indices ranging from 0.550 to 0.857. However, all studies exhibited unclear or high bias, lacked external validation, and had limited focus on clinical utility. The research highlighted the need for improved methodological rigor, broader datasets, and thorough clinical evaluation in the integration of WSI and omics data for survival prediction. Funded by NPIC and supported by the UKRI Industrial Strategy Challenge Fund, the study underscores the importance of advancing multimodal approaches for better cancer prognostication.<br /><br /> <div>
arXiv:2507.16876v2 Announce Type: replace-cross 
Abstract: Multimodal machine learning integrating histopathology and molecular data shows promise for cancer prognostication. We systematically reviewed studies combining whole slide images (WSIs) and high-throughput omics to predict overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL (12/08/2024), plus citation screening, identified eligible studies. Data extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).
  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all used The Cancer Genome Atlas. Approaches included regularised Cox regression (n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged 0.550-0.857; multimodal models typically outperformed unimodal ones. However, all studies showed unclear/high bias, limited external validation, and little focus on clinical utility.
  Multimodal WSI-omics survival prediction is a fast-growing field with promising results but needs improved methodological rigor, broader datasets, and clinical evaluation.
  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687), supported by UKRI Industrial Strategy Challenge Fund.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models</title>
<link>https://arxiv.org/abs/2507.17107</link>
<guid>https://arxiv.org/abs/2507.17107</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Parameter Update Sparsity, Subnetwork, Efficiency

Summary:
Reinforcement learning (RL) plays a crucial role in fine-tuning large language models (LLMs) for complex tasks. A surprising finding reveals that RL fine-tuning primarily modifies a small subnetwork (5-30% of weights) rather than most parameters. This phenomenon, called RL-induced parameter update sparsity, is observed across various RL algorithms and model families. The updated subnetworks show significant overlap across different conditions, suggesting a transferable structure in the pretrained model. Targeting this sparse subnetwork alone can restore full model performance with near-identical parameters. The sparsity arises due to RL operating close to the model's original distribution, requiring only focused changes. Traditional techniques like KL penalties and gradient clipping have limited impact on this sparsity pattern. These findings challenge the assumption that RL fine-tuning involves updating most parameters and offer insights for more efficient RL methods, linking sparsity to the lottery ticket hypothesis. 

<br /><br />Summary: <div>
arXiv:2507.17107v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large language models (LLMs) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update sparsity. It arises naturally, without any sparsity constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show substantial overlap across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this sparse subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this sparsity emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the sparsity pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes sparsity through the lens of the lottery ticket hypothesis.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2507.21067</link>
<guid>https://arxiv.org/abs/2507.21067</guid>
<content:encoded><![CDATA[
<div> Keywords: symbiotic epistemology, transparent collaboration, AI reliability, structured reasoning protocols, confidence quantification

Summary:<br /><br />
This paper introduces symbiotic epistemology as a foundation for human-AI cognitive partnerships, emphasizing transparent collaboration and aligned confidence with AI reliability. It presents SynLang, a formal protocol for human-AI collaboration that includes mechanisms for high-level reasoning patterns and detailed factor explanations, as well as confidence quantification and declarative control over AI behavior. SynLang aims to enhance human intelligence, preserve agency, and uphold ethical accountability in decision-making through structured communication and transparency. Through dual-level transparency, starting with high-level reasoning patterns and extending to granular explanations, the protocol enables rapid comprehension and thorough verification of AI decision-making processes. Empirical validation through human-AI dialogues demonstrates the effectiveness of the framework in promoting symbiotic collaboration and supporting metacognitive intervention. <div>
arXiv:2507.21067v1 Announce Type: new 
Abstract: Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration. In this paper, the Symbiotic Epistemology is presented as a philosophical foundation for human-AI cognitive partnerships. Unlike frameworks that treat AI as a mere tool or replacement, symbiotic epistemology positions AI as a reasoning partner, fostering calibrated trust by aligning human confidence with AI reliability through explicit reasoning patterns and confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as a formal protocol for transparent human-AI collaboration. The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations. It also integrates confidence quantification, declarative control over AI behavior, and context inheritance for multi-agent coordination. By structuring communication and embedding confidence-calibrated transparency, SynLang, together with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making. Through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations, the protocol facilitates rapid comprehension and supports thorough verification of AI decision-making.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism</title>
<link>https://arxiv.org/abs/2507.21098</link>
<guid>https://arxiv.org/abs/2507.21098</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence, Sustainable Development, AI-Driven Management, Viticulture, Enotourism

Summary: 
Artificial Intelligence (AI) plays a crucial role in enhancing sustainability and efficiency in the wine industry. Through AI-driven intelligent management in viticulture, wine production, and enotourism, innovative solutions are being offered to address environmental and economic challenges. AI technologies such as predictive analytics, machine learning, and computer vision are being utilized to optimize resource use, reduce environmental impact, and improve customer engagement. The research, based on a survey of Polish winemakers and an analysis of AI methods applicable to the industry, highlights how AI enhances vineyard monitoring, optimizes irrigation, streamlines production processes, and personalizes consumer experiences in enotourism through chatbots, recommendation systems, and virtual tastings. Ultimately, AI's impact on economic, environmental, and social sustainability supports local wine enterprises and preserves cultural heritage.<br /><br />Summary: <div>
arXiv:2507.21098v1 Announce Type: new 
Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing sustainability and efficiency within the wine industry. It focuses on AI-driven intelligent management in viticulture, wine production, and enotourism. As the wine industry faces environmental and economic challenges, AI offers innovative solutions to optimize resource use, reduce environmental impact, and improve customer engagement. Understanding AI's potential in sustainable winemaking is crucial for fostering responsible and efficient industry practices. The research is based on a questionnaire survey conducted among Polish winemakers, combined with a comprehensive analysis of AI methods applicable to viticulture, production, and tourism. Key AI technologies, including predictive analytics, machine learning, and computer vision, are explored. The findings indicate that AI enhances vineyard monitoring, optimizes irrigation, and streamlines production processes, contributing to sustainable resource management. In enotourism, AI-powered chatbots, recommendation systems, and virtual tastings personalize consumer experiences. The study highlights AI's impact on economic, environmental, and social sustainability, supporting local wine enterprises and cultural heritage. Keywords: Artificial Intelligence, Sustainable Development, AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine Enterprises, Local Communities
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Generative AI to Enhance Synthea Module Development</title>
<link>https://arxiv.org/abs/2507.21123</link>
<guid>https://arxiv.org/abs/2507.21123</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Synthea, disease modules, synthetic health data, progressive refinement <br />
Summary: <br />
- The paper explores the use of large language models (LLMs) to aid in creating disease modules for Synthea, an open-source synthetic health data generator. <br />
- By incorporating LLMs in the module development process, benefits such as reduced development time, expanded model diversity, and improved quality of synthetic patient data are realized. <br />
- Four ways LLMs can support Synthea module creation are outlined: generating disease profiles, creating disease modules, evaluating existing modules, and refining modules through progressive refinement. <br />
- The iterative process of progressive refinement involves checking syntactic correctness and clinical accuracy of LLM-generated modules and modifying them accordingly. <br />
- Despite the potential benefits, challenges like the need for human oversight, rigorous testing, and the possibility of inaccuracies in LLM-generated content are acknowledged. <br /> <div>
arXiv:2507.21123v1 Announce Type: new 
Abstract: This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator. Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data. We demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module. While the use of LLMs in this context shows promise, we also acknowledge the challenges and limitations, such as the need for human oversight, the importance of rigorous testing and validation, and the potential for inaccuracies in LLM-generated content. The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics</title>
<link>https://arxiv.org/abs/2507.21129</link>
<guid>https://arxiv.org/abs/2507.21129</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Cognitive Profile, Entropy Decay Curve, Information Gain Span, Artificial Intelligence<br />
Summary: 
Large Language Models (LLMs) have impressive capabilities, prompting a closer look into how they process information. A new methodology utilizing a "Cognitive Profile" based on the Entropy Decay Curve was introduced to analyze LLM operations. By examining several top LLMs across various texts, unique cognitive profiles sensitive to model size and text complexity were discovered. The introduction of the Information Gain Span (IGS) index allowed for the summarization of the decay trajectory's desirability. This research offers a principled approach for evaluating and comparing the intrinsic operational dynamics of artificial intelligence.<br /><br /> <div>
arXiv:2507.21129v1 Announce Type: new 
Abstract: The remarkable capabilities of Large Language Models (LLMs) are now extensively documented on task-specific benchmarks, yet the internal mechanisms that produce these results are the subject of intense scientific inquiry. This paper contributes to this inquiry by moving beyond metrics that measure \textit{what} models can do, to a methodology that characterizes \textit{how} they process information. We introduce a novel, task-agnostic approach to probe these dynamics by creating a quantitative ``Cognitive Profile" for any given model. This profile is centered on the \textbf{Entropy Decay Curve}, a visualization that traces how a model's normalized predictive uncertainty changes as a function of context length. Applying this methodology to several state-of-the-art LLMs across diverse texts, we uncover unique and consistent cognitive profiles that are sensitive to both model scale and text complexity. We also introduce the Information Gain Span (IGS) index to summarize the desirability of the decay trajectory. This work thus provides a new, principled lens for analyzing and comparing the intrinsic operational dynamics of artificial intelligence.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems</title>
<link>https://arxiv.org/abs/2507.21130</link>
<guid>https://arxiv.org/abs/2507.21130</guid>
<content:encoded><![CDATA[
<div> Keywords: INTEGRALBENCH, Large Language Model, definite integral problems, benchmark, automated mathematical reasoning

Summary:
INTEGRALBENCH is a benchmark designed to evaluate Large Language Model performance on definite integral problems. It provides symbolic and numerical ground truth solutions with manual difficulty annotations. The evaluation of nine state-of-the-art LLMs shows significant performance gaps and strong correlations between problem difficulty and model accuracy. This benchmark sets baseline metrics for this challenging domain and aims to advance automated mathematical reasoning by offering a rigorous evaluation framework tailored for definite integral computation. The study highlights the importance of assessing LLMs on specific mathematical tasks and emphasizes the need for specialized benchmarks to measure their performance accurately. <div>
arXiv:2507.21130v1 Announce Type: new 
Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large Language Model (LLM) performance on definite integral problems. INTEGRALBENCH provides both symbolic and numerical ground truth solutions with manual difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals significant performance gaps and strong correlations between problem difficulty and model accuracy, establishing baseline metrics for this challenging domain. INTEGRALBENCH aims to advance automated mathematical reasoning by providing a rigorous evaluation framework specifically tailored for definite integral computation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback</title>
<link>https://arxiv.org/abs/2507.21131</link>
<guid>https://arxiv.org/abs/2507.21131</guid>
<content:encoded><![CDATA[
<div> framework, alignment, feedback, meta-alignment, convergence
Summary: 
The article introduces NPO, a learning framework that incorporates alignment loss measurement and feedback-driven adaptation in decision systems. Unlike previous models, NPO considers alignment as a dynamic property that can be supervised and reduced through structured feedback. The concept of meta-alignment is introduced to monitor the retraining or override triggers, with a formal link to primary alignment through threshold fidelity. NPO uses structured feedback such as "likes", overrides, and abstentions in a scalable operational loop, demonstrating convergence results under stochastic feedback. Empirical evidence shows the effectiveness of NPO in hyperscale deployment settings, supported by simulation-based experiments and ablation studies. The framework offers a compact and inspectable architecture for continual alignment monitoring, providing a bridge between theoretical alignment guarantees and practical reliability in dynamic environments. 
Summary: <div>
arXiv:2507.21131v1 Announce Type: new 
Abstract: We present NPO, an alignment-aware learning framework that operationalizes feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior approaches that treat alignment as a static or post-hoc property, NPO introduces a formalization of alignment loss that is measurable, supervisable, and reducible under structured feedback. In parallel, we propose meta-alignment as the fidelity of the monitoring process that governs retraining or override triggers, and show that it is formally reducible to primary alignment via threshold fidelity. Our implementation spans a scalable operational loop involving scenario scoring, threshold tuning, policy validation, and structured feedback ingestion, including "likes", overrides, and abstentions. We provide formal convergence results under stochastic feedback and show that both alignment loss and monitoring fidelity converge additively. Empirically, NPO demonstrates measurable value in hyperscale deployment settings. A simulation-based artifact and ablation studies further illustrate the theoretical principles in action. Together, NPO offers a compact, inspectable architecture for continual alignment monitoring, helping bridge theoretical alignment guarantees with practical reliability in dynamic environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses</title>
<link>https://arxiv.org/abs/2507.21132</link>
<guid>https://arxiv.org/abs/2507.21132</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, safeguards, sycophancy, safety typology, activation steering

Summary: 
Large Language Models (LLMs) are being used for important life decisions but lack safeguards against giving inaccurate advice, leading to risks of sycophancy and over-confidence. Three experiments were conducted to investigate model behavior: a stability test under user pressure, a safety typology analysis, and an interpretability experiment. Some models showed sycophantic behavior, while others, like o4-mini, remained robust. Top-performing models achieved high safety scores by asking clarifying questions rather than giving direct advice. Additionally, cautiousness in models can be controlled through activation steering. These findings emphasize the necessity of comprehensive benchmarks to ensure the reliability of LLMs in critical decision-making processes.<br /><br />Summary: <div>
arXiv:2507.21132v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence. This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector. Our results show that while some models exhibit sycophancy, others like o4-mini remain robust. Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?</title>
<link>https://arxiv.org/abs/2507.21137</link>
<guid>https://arxiv.org/abs/2507.21137</guid>
<content:encoded><![CDATA[
<div> Keywords: Sudoku, difficulty rating, SAT problem, Nishio, universal rating system

Summary:
The paper explores the determination of Sudoku difficulty ratings across different websites using two methods. The first method involves converting Sudoku puzzles into SAT problems and deriving metrics based on SAT Clause Length Distribution. The second method simulates human solvers by integrating popular Sudoku strategies in a backtracking algorithm called Nishio to compute a second metric. By analyzing over a thousand puzzles from five websites, correlations between the proposed metrics and website difficulty levels were evaluated. A universal rating system was created using unsupervised classification, categorizing puzzles and difficulty levels into Universal Easy, Medium, and Hard. Results showed consistent alignment with website-labeled difficulty levels for the majority of websites. Additionally, an algorithm for early Sudoku practitioners to solve puzzles was presented. 

<br /><br />Summary: The paper presents two methods for determining Sudoku difficulty ratings and analyzes over a thousand puzzles from five websites. Using metrics derived from SAT problems and a backtracking algorithm, the study establishes a universal rating system. Results demonstrate strong correlations with website-labeled difficulty levels for most websites, offering a consistent classification approach. An algorithm for Sudoku solving is also provided for early practitioners. <div>
arXiv:2507.21137v1 Announce Type: new 
Abstract: In this paper we try to answer the question "What constitutes Sudoku difficulty rating across different Sudoku websites?" Using two distinct methods that can both solve every Sudoku puzzle, I propose two new metrics to characterize Sudoku difficulty. The first method is based on converting a Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first proposed metric is derived from SAT Clause Length Distribution which captures the structural complexity of a Sudoku puzzle including the number of given digits and the cells they are in. The second method simulates human Sudoku solvers by intertwining four popular Sudoku strategies within a backtracking algorithm called Nishio. The second metric is computed by counting the number of times Sudoku strategies are applied within the backtracking iterations of a randomized Nishio. Using these two metrics, I analyze more than a thousand Sudoku puzzles across five popular websites to characterize every difficulty level in each website. I evaluate the relationship between the proposed metrics and website-labeled difficulty levels using Spearman's rank correlation coefficient, finding strong correlations for 4 out of 5 websites. I construct a universal rating system using a simple, unsupervised classifier based on the two proposed metrics. This rating system is capable of classifying both individual puzzles and entire difficulty levels from the different Sudoku websites into three categories - Universal Easy, Universal Medium, and Universal Hard - thereby enabling consistent difficulty mapping across Sudoku websites. The experimental results show that for 4 out of 5 Sudoku websites, the universal classification aligns well with website-labeled difficulty levels. Finally, I present an algorithm that can be used by early Sudoku practitioners to solve Sudoku puzzles.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Harmfulness in LLMs through Subconcept Probing</title>
<link>https://arxiv.org/abs/2507.21141</link>
<guid>https://arxiv.org/abs/2507.21141</guid>
<content:encoded><![CDATA[
<div> framework, harmful content, language models, probe, steer 
Summary: 
- The study presents a multidimensional framework to analyze and control harmful behaviors in large language models (LLMs). 
- A linear probe is learned for each of 55 harmfulness subconcepts, creating interpretable directions in activation space. 
- The directions collectively form a low-rank harmfulness subspace, allowing for effective analysis and intervention. 
- Ablation of this subspace and steering along its dominant direction show promising results in curbing harmful content with minimal impact on utility. 
- The findings suggest that concept subspaces offer a scalable approach to understand LLM behavior and provide practical tools for auditing and enhancing future language models. 

<br /><br />Summary: <div>
arXiv:2507.21141v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have intensified the need to understand and reliably curb their harmful behaviours. We introduce a multidimensional framework for probing and steering harmful content in model internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate, employment scams, weapons), we learn a linear probe, yielding 55 interpretable directions in activation space. Collectively, these directions span a harmfulness subspace that we show is strikingly low-rank. We then test ablation of the entire subspace from model internals, as well as steering and ablation in the subspace's dominant direction. We find that dominant direction steering allows for near elimination of harmfulness with a low decrease in utility. Our findings advance the emerging view that concept subspaces provide a scalable lens on LLM behaviour and offer practical tools for the community to audit and harden future generations of language models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams</title>
<link>https://arxiv.org/abs/2507.21158</link>
<guid>https://arxiv.org/abs/2507.21158</guid>
<content:encoded><![CDATA[
<div> Framework, Adaptive Explainability, Trust, Human-AI Teaming, High-Stakes Environments<br />
<br />
Summary: 
The article introduces a conceptual framework for Adaptive Explainable AI (XAI) aimed at enhancing swift trust between human operators and AI systems in high-stakes scenarios like emergency response. The framework addresses the limitations of existing XAI approaches, which typically provide uniform explanations and rely on explicit feedback mechanisms. The proposed Adaptive Explainability Trust Framework (AXTF) operates non-intrusively by utilizing physiological and behavioral signals, such as EEG, ECG, and eye tracking, to infer user states and adapt explanations in real-time. A multi-objective trust estimation model is at the core of AXTF, mapping workload, stress, and emotion to dynamic trust estimates that guide the modulation of explanation features for responsive and personalized support. This framework lays the groundwork for developing adaptive and non-intrusive XAI systems tailored to the demanding nature of high-pressure, time-sensitive environments. <br /><br /> <div>
arXiv:2507.21158v1 Announce Type: new 
Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in high-stakes scenarios such as emergency response, where timely and accurate decision-making is critical. In these time-sensitive and cognitively demanding settings, adaptive explainability is essential for fostering trust between human operators and AI systems. However, existing explainable AI (XAI) approaches typically offer uniform explanations and rely heavily on explicit feedback mechanisms, which are often impractical in such high-pressure scenarios. To address this gap, we propose a conceptual framework for adaptive XAI that operates non-intrusively by responding to users' real-time cognitive and emotional states through implicit feedback, thereby enhancing swift trust in high-stakes environments. The proposed adaptive explainability trust framework (AXTF) leverages physiological and behavioral signals, such as EEG, ECG, and eye tracking, to infer user states and support explanation adaptation. At its core is a multi-objective, personalized trust estimation model that maps workload, stress, and emotion to dynamic trust estimates. These estimates guide the modulation of explanation features enabling responsive and personalized support that promotes swift trust in human-AI collaboration. This conceptual framework establishes a foundation for developing adaptive, non-intrusive XAI systems tailored to the rigorous demands of high-pressure, time-sensitive environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity</title>
<link>https://arxiv.org/abs/2507.21159</link>
<guid>https://arxiv.org/abs/2507.21159</guid>
<content:encoded><![CDATA[
<div> collaborativeness, large language models, healthcare development, adaptive cluster, medical decision support

Summary:<br />
The article introduces a methodology for enhancing the collaborativeness of large language models (LLMs) in healthcare development by addressing the lack of explicit component selection rules. The proposed adaptive cluster collaborativeness methodology involves self-diversity and cross-consistency maximization mechanisms to optimize LLMs' medical decision support capacity. Self-diversity is calculated based on fuzzy matching values of pairwise outputs within an LLM, while cross-consistency values are used to prioritize LLMs within a cluster. Extensive experiments conducted on specialized medical datasets demonstrate the effectiveness of the method, with impressive accuracy rates achieved across various physician-oriented specialties. For instance, the method outperformed GPT-4 in the Obstetrics and Gynecology discipline, showcasing its potential to improve medical decision support systems. <div>
arXiv:2507.21159v1 Announce Type: new 
Abstract: The collaborativeness of large language models (LLMs) has proven effective in natural language processing systems, holding considerable promise for healthcare development. However, it lacks explicit component selection rules, necessitating human intervention or clinical-specific validation. Moreover, existing architectures heavily rely on a predefined LLM cluster, where partial LLMs underperform in medical decision support scenarios, invalidating the collaborativeness of LLMs. To this end, we propose an adaptive cluster collaborativeness methodology involving self-diversity and cross-consistency maximization mechanisms to boost LLMs medical decision support capacity. For the self-diversity, we calculate the fuzzy matching value of pairwise outputs within an LLM as its self-diversity value, subsequently prioritizing LLMs with high self-diversity values as cluster components in a training-free manner. For the cross-consistency, we first measure cross-consistency values between the LLM with the highest self-diversity value and others, and then gradually mask out the LLM having the lowest cross-consistency value to eliminate the potential inconsistent output during the collaborative propagation. Extensive experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health, demonstrate the effectiveness of our method across physician-oriented specialties. For example, on NEJMQA, our method achieves the accuracy rate up to the publicly official passing score across all disciplines, especially achieving ACC of 65.47\% compared to the 56.12\% achieved by GPT-4 on the Obstetrics and Gynecology discipline.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems</title>
<link>https://arxiv.org/abs/2507.21162</link>
<guid>https://arxiv.org/abs/2507.21162</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed energy resources, active distribution networks, large language model, automated modeling, optimization

Summary: 
- The paper addresses the need for effective dispatch in active distribution networks due to the integration of distributed energy resources.
- A large language model (LLM) powered automated approach is proposed to assist ADN operators in modeling and optimization tasks.
- The approach includes an Information Extractor, Problem Formulator, and Code Programmer to retrieve information, formulate optimization problems, and implement code.
- Tailored refinement techniques are developed for each LLM agent to enhance accuracy and reliability.
- The user-centric interface allows ADN operators to generate dispatch strategies through natural language queries, eliminating technical barriers and increasing efficiency.

<br /><br />Summary: <div>
arXiv:2507.21162v1 Announce Type: new 
Abstract: The increasing penetration of distributed energy resources into active distribution networks (ADNs) has made effective ADN dispatch imperative. However, the numerous newly-integrated ADN operators, such as distribution system aggregators, virtual power plant managers, and end prosumers, often lack specialized expertise in power system operation, modeling, optimization, and programming. This knowledge gap renders reliance on human experts both costly and time-intensive. To address this challenge and enable intelligent, flexible ADN dispatch, this paper proposes a large language model (LLM) powered automated modeling and optimization approach. First, the ADN dispatch problems are decomposed into sequential stages, and a multi-LLM coordination architecture is designed. This framework comprises an Information Extractor, a Problem Formulator, and a Code Programmer, tasked with information retrieval, optimization problem formulation, and code implementation, respectively. Afterwards, tailored refinement techniques are developed for each LLM agent, greatly improving the accuracy and reliability of generated content. The proposed approach features a user-centric interface that enables ADN operators to derive dispatch strategies via simple natural language queries, eliminating technical barriers and increasing efficiency. Comprehensive comparisons and end-to-end demonstrations on various test cases validate the effectiveness of the proposed architecture and methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An ontological analysis of risk in Basic Formal Ontology</title>
<link>https://arxiv.org/abs/2507.21171</link>
<guid>https://arxiv.org/abs/2507.21171</guid>
<content:encoded><![CDATA[
<div> Risk, ontology, BFO, role, disposition
Summary:
The paper discusses the classification of risk within the Basic Formal Ontology (BFO), arguing that Risk is a subclass of BFO:Role rather than BFO:Disposition. Using an example to illustrate the nature of risk, it highlights the objects, processes, and their interrelations involved. By generalizing from the example, the paper outlines the sufficient conditions for something to be considered a risk. It also mentions plausible necessary conditions for further research. The analysis provides a comprehensive understanding of risk within the BFO framework, shedding light on its essential characteristics and relationships. This detailed exploration aids in defining and identifying risks across various contexts, paving the way for future studies in risk ontology. 
<br /><br />Summary: <div>
arXiv:2507.21171v1 Announce Type: new 
Abstract: The paper explores the nature of risk, providing a characterization using the categories of the Basic Formal Ontology (BFO). It argues that the category Risk is a subclass of BFO:Role, contrasting it with a similar view classifying Risk as a subclass of BFO:Disposition. This modeling choice is applied on one example of risk, which represents objects, processes (both physical and mental) and their interrelations, then generalizing from the instances in the example to obtain an overall analysis of risk, making explicit what are the sufficient conditions for being a risk. Plausible necessary conditions are also mentioned for future work. Index Terms: ontology, risk, BFO, role, disposition
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontological Foundations of State Sovereignty</title>
<link>https://arxiv.org/abs/2507.21172</link>
<guid>https://arxiv.org/abs/2507.21172</guid>
<content:encoded><![CDATA[
<div> Keywords: state sovereignty, claims, vague data, contradictory data, ontology <br />
Summary: 
This short paper serves as a primer on the concept of state sovereignty and its significance in the international arena. It also introduces a strategy for navigating ambiguous or conflicting information pertaining to which states hold sovereign status. By shedding light on the complexities surrounding state sovereignty, this paper lays the groundwork for future research in ontology related to global relations. The discussion delves into the nuances of making claims about state sovereignty and addresses the challenges posed by vague or contradictory data in this context. Through a careful examination of these issues, the paper aims to provide a foundation for applied studies in ontology focused on international affairs. Ultimately, this primer offers valuable insights into the intricacies of state sovereignty and emphasizes the importance of considering multiple perspectives when analyzing sovereign claims. <br /><br />Summary: <div>
arXiv:2507.21172v1 Announce Type: new 
Abstract: This short paper is a primer on the nature of state sovereignty and the importance of claims about it. It also aims to reveal (merely reveal) a strategy for working with vague or contradictory data about which states, in fact, are sovereign. These goals together are intended to set the stage for applied work in ontology about international affairs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs</title>
<link>https://arxiv.org/abs/2507.21176</link>
<guid>https://arxiv.org/abs/2507.21176</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, large language models, bias patterns, medical applications, adversarial perturbation<br />
Summary:<br />
Large language models used in medical applications exhibit biased and unfair patterns. It is essential to detect and address these biases before integrating them into clinical decision-making processes. This study introduces a novel framework that combines knowledge graphs and auxiliary LLMs to systematically uncover complex bias patterns in medical LLMs. By utilizing adversarial perturbation techniques and a customized multi-hop characterization of KGs, the approach enhances the systematic evaluation of various LLMs. Through extensive experiments on multiple datasets and bias types, the proposed framework demonstrates superior ability and scalability in identifying intricate bias patterns compared to existing methods. Addressing bias in medical LLMs is crucial for ensuring fair and accurate decision-making processes in healthcare settings. 

<br /><br /> <div>
arXiv:2507.21176v1 Announce Type: new 
Abstract: Large language models (LLMs) that are used in medical applications are known to show biased and unfair patterns. Prior to adopting these in clinical decision-making applications, it is crucial to identify these bias patterns to enable effective mitigation of their impact. In this study, we present a novel framework combining knowledge graphs (KGs) with auxiliary LLMs to systematically reveal complex bias patterns in medical LLMs. Specifically, the proposed approach integrates adversarial perturbation techniques to identify subtle bias patterns. The approach adopts a customized multi-hop characterization of KGs to enhance the systematic evaluation of arbitrary LLMs. Through a series of comprehensive experiments (on three datasets, six LLMs, and five bias types), we show that our proposed framework has noticeably greater ability and scalability to reveal complex biased patterns of LLMs compared to other baselines.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Web: Weaving the Next Web with AI Agents</title>
<link>https://arxiv.org/abs/2507.21206</link>
<guid>https://arxiv.org/abs/2507.21206</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, large language models, Agentic Web, machine-to-machine interaction, autonomous agent behavior

Summary:<br /><br />
The article discusses the emergence of AI agents powered by large language models (LLMs) and the shift towards the Agentic Web, where machines interact autonomously to carry out tasks. It presents a framework for understanding the Agentic Web, tracing its evolution from previous web eras and highlighting the key dimensions of intelligence, interaction, and economics that support AI agents' capabilities. The paper also addresses the challenges in creating scalable agentic systems, including communication protocols and orchestration strategies, and introduces the concept of the Agent Attention Economy. It further explores potential applications, societal risks, and governance issues associated with agentic systems, emphasizing the need for research on developing open, secure, and intelligent ecosystems influenced by both human intent and autonomous agent behavior. The paper provides a repository of relevant studies on the agentic web for further reference. <div>
arXiv:2507.21206v1 Announce Type: new 
Abstract: The emergence of AI agents powered by large language models (LLMs) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including communication protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting</title>
<link>https://arxiv.org/abs/2507.21257</link>
<guid>https://arxiv.org/abs/2507.21257</guid>
<content:encoded><![CDATA[
<div> interpretation, language models, compositional process, benchmark, SPARQL queries

Summary:<br />
- Language interpretation is a compositional process where the meaning of complex linguistic structures is inferred from their parts.
- Large language models have strong interpretation capabilities and are used to map questions to SPARQL queries.
- A benchmark is proposed to assess the compositional nature of language model interpretation abilities.
- Three datasets of varying difficulty based on DBpedia graph patterns are created to evaluate the models' ability to interpret structurally complex questions.
- Experiments show that the performance of language models decreases with increasing deviation from the optimized samples, indicating a struggle in systematically and compositionally interpreting questions for mapping to SPARQL queries.

Summary:<br /> <div>
arXiv:2507.21257v1 Announce Type: new 
Abstract: Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems</title>
<link>https://arxiv.org/abs/2507.21276</link>
<guid>https://arxiv.org/abs/2507.21276</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, deployment, inference serving, continuous retraining, resource allocation

Summary:
The article introduces LeMix, a system designed to co-locate and manage concurrent large language model (LLM) serving and training workloads. The current practice of separating these tasks onto distinct servers leads to inefficiencies and delayed adaptation to new data in distributed settings. LeMix addresses these challenges by dynamically adapting resource allocation based on workload characteristics and system conditions. By integrating offline profiling, execution prediction mechanisms, and runtime scheduling, LeMix improves utilization and serving quality without compromising responsiveness. Empirical analysis shows that LeMix can improve throughput by up to 3.53x, reduce inference loss by up to 0.61x, and achieve up to 2.12x higher response time SLO attainment compared to traditional separate setups. This work uncovers the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.

<br /><br />Summary: <div>
arXiv:2507.21276v1 Announce Type: new 
Abstract: Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions</title>
<link>https://arxiv.org/abs/2507.21285</link>
<guid>https://arxiv.org/abs/2507.21285</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, coding assistant, clarification questions, query classifier, user study

Summary:
Large Language Models (LLMs) are being used as coding assistants, but often struggle to understand ambiguous prompts, leading to incorrect code generation. This work introduces a coding assistant that mimics human code review by generating clarification questions for ambiguous queries. The system includes a query classifier to detect unclear queries and a fine-tuned LLM to generate clarification questions. Evaluation shows that the fine-tuned LLM performs better than standard zero-shot prompting in generating useful questions. A user study confirms that the generated clarification questions are more accurate and helpful compared to baseline assistants. This approach aims to enhance the accuracy and helpfulness of code generation by addressing ambiguity in user queries.<br /><br />Summary: <div>
arXiv:2507.21285v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as coding assistants. However, the ambiguity of the developer's prompt often leads to incorrect code generation, as current models struggle to infer user intent without extensive prompt engineering or external context. This work aims to build an LLM-based coding assistant that mimics the human code review process by asking clarification questions when faced with ambiguous or under-specified queries.
  Our end-to-end system includes (1) a query classifier trained to detect unclear programming-related queries and (2) a fine-tuned LLM that generates clarification questions. Our evaluation shows that the fine-tuned LLM outperforms standard zero-shot prompting in generating useful clarification questions. Furthermore, our user study indicates that users find the clarification questions generated by our model to outperform the baseline, demonstrating that our coding assistant produces more accurate and helpful code responses compared to baseline coding assistants.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Relevance Assessment for Robust Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2507.21287</link>
<guid>https://arxiv.org/abs/2507.21287</guid>
<content:encoded><![CDATA[
<div> framework, structured relevance assessment, RALM, knowledge integration, unanswerable queries
<br />
Summary:
The article introduces a framework for enhancing Retrieval-Augmented Language Models (RALMs) by addressing challenges related to reducing factual errors. The framework focuses on structured relevance assessment, improving document evaluation, and integrating intrinsic and external knowledge. It includes a multi-dimensional scoring system that considers semantic matching and source reliability, uses synthetic training data with mixed-quality documents, and implements specialized benchmarking on niche topics. The framework also includes a mechanism for knowledge integration and a protocol for handling queries with insufficient knowledge coverage. Preliminary evaluations show a decrease in hallucination rates and improved reasoning transparency. While issues such as distinguishing credible information and balancing system latency persist, this work signifies progress in enhancing the reliability of RALMs for question-answering systems in dynamic environments with varying data quality. 
<br /><br />Summary: <div>
arXiv:2507.21287v1 Announce Type: new 
Abstract: Retrieval-Augmented Language Models (RALMs) face significant challenges in reducing factual errors, particularly in document relevance evaluation and knowledge integration. We introduce a framework for structured relevance assessment that enhances RALM robustness through improved document evaluation, balanced intrinsic and external knowledge integration, and effective handling of unanswerable queries. Our approach employs a multi-dimensional scoring system that considers both semantic matching and source reliability, utilizing embedding-based relevance scoring and synthetic training data with mixed-quality documents. We implement specialized benchmarking on niche topics, a knowledge integration mechanism, and an "unknown" response protocol for queries with insufficient knowledge coverage. Preliminary evaluations demonstrate significant reductions in hallucination rates and improved transparency in reasoning processes. Our framework advances the development of more reliable question-answering systems capable of operating effectively in dynamic environments with variable data quality. While challenges persist in accurately distinguishing credible information and balancing system latency with thoroughness, this work represents a meaningful step toward enhancing RALM reliability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2507.21354</link>
<guid>https://arxiv.org/abs/2507.21354</guid>
<content:encoded><![CDATA[
<div> frameworks, cognitive complexity, Transactional Analysis, Multi-Agent Systems, psychological dynamics <br />
Summary: <br />
The paper introduces Trans-ACT, a framework that integrates Transactional Analysis principles into Multi-Agent Systems to create agents with realistic psychological dynamics. This approach incorporates the Parent, Adult, and Child ego states into an agent's cognitive architecture, allowing them to use context-specific memories to shape responses to new situations based on their life script. Experimental simulations, such as the Stupid game scenario, show that agents grounded in cognitive and TA principles demonstrate deeper and context-aware interactions. This development holds promise for various applications, including conflict resolution, educational support, and social psychology studies. <div>
arXiv:2507.21354v1 Announce Type: new 
Abstract: Multi-Agent Systems (MAS) are increasingly used to simulate social interactions, but most of the frameworks miss the underlying cognitive complexity of human behavior. In this paper, we introduce Trans-ACT (Transactional Analysis Cognitive Toolkit), an approach embedding Transactional Analysis (TA) principles into MAS to generate agents with realistic psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego states into an agent's cognitive architecture. Each ego state retrieves context-specific memories and uses them to shape response to new situations. The final answer is chosen according to the underlying life script of the agent. Our experimental simulation, which reproduces the Stupid game scenario, demonstrates that agents grounded in cognitive and TA principles produce deeper and context-aware interactions. Looking ahead, our research opens a new way for a variety of applications, including conflict resolution, educational support, and advanced social psychology studies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures</title>
<link>https://arxiv.org/abs/2507.21360</link>
<guid>https://arxiv.org/abs/2507.21360</guid>
<content:encoded><![CDATA[
<div> Keywords: AI retrieval augmented generation, information extraction, data annotation, GSIBs, task performance<br />
Summary:<br />
The study investigates the efficacy of using an AI retrieval augmented generation (RAG) tool to assist analysts in an information extraction and data annotation task. Through a within-subjects design, the researchers replicated a challenging real-world annotation task on public disclosure documents from global systemically important banks (GSIBs). Two treatment conditions were tested: a "naive" AI use condition and an "interactive" AI treatment condition. Results showed that using the AI tool significantly accelerated task execution by up to 10 times and improved task accuracy, especially in the interactive condition. Extrapolation to the full task indicated potential time savings of 268 hours compared to a human-only approach. The study highlights the importance of annotator skill in both domain knowledge and AI tool proficiency for task performance. <div>
arXiv:2507.21360v1 Announce Type: new 
Abstract: We utilize a within-subjects design with randomized task assignments to understand the effectiveness of using an AI retrieval augmented generation (RAG) tool to assist analysts with an information extraction and data annotation task. We replicate an existing, challenging real-world annotation task with complex multi-part criteria on a set of thousands of pages of public disclosure documents from global systemically important banks (GSIBs) with heterogeneous and incomplete information content. We test two treatment conditions. First, a "naive" AI use condition in which annotators use only the tool and must accept the first answer they are given. And second, an "interactive" AI treatment condition where annotators use the tool interactively, and use their judgement to follow-up with additional information if necessary. Compared to the human-only baseline, the use of the AI tool accelerated task execution by up to a factor of 10 and enhanced task accuracy, particularly in the interactive condition. We find that when extrapolated to the full task, these methods could save up to 268 hours compared to the human-only approach. Additionally, our findings suggest that annotator skill, not just with the subject matter domain, but also with AI tools, is a factor in both the accuracy and speed of task performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Multi-Tier Supply Chain Ordering with LNN+XGBoost: Mitigating the Bullwhip Effect</title>
<link>https://arxiv.org/abs/2507.21383</link>
<guid>https://arxiv.org/abs/2507.21383</guid>
<content:encoded><![CDATA[
<div> Keywords: Supply chain management, Liquid Neural Networks, XGBoost, optimization, bullwhip effect

Summary:
Liquid Neural Networks (LNN) offer a promising solution for supply chain management (SCM) challenges such as demand fluctuations and inventory imbalances. This study introduces a hybrid model combining LNN and XGBoost to optimize ordering strategies in multi-tier supply chains. The model aims to mitigate the bullwhip effect and enhance profitability by leveraging LNN's dynamic feature extraction and XGBoost's global optimization capabilities. By exploring local and global synergies within the hybrid framework, the approach addresses the dual demands of adaptability and efficiency in SCM. The proposed methodology fills a critical gap in existing approaches, offering an innovative solution for dynamic and efficient supply chain management. <div>
arXiv:2507.21383v1 Announce Type: new 
Abstract: Supply chain management faces significant challenges, including demand fluctuations, inventory imbalances, and amplified upstream order variability due to the bullwhip effect. Traditional methods, such as simple moving averages, struggle to address dynamic market conditions. Emerging machine learning techniques, including LSTM, reinforcement learning, and XGBoost, offer potential solutions but are limited by computational complexity, training inefficiencies, or constraints in time-series modeling. Liquid Neural Networks, inspired by dynamic biological systems, present a promising alternative due to their adaptability, low computational cost, and robustness to noise, making them suitable for real-time decision-making and edge computing. Despite their success in applications like autonomous vehicles and medical monitoring, their potential in supply chain optimization remains underexplored. This study introduces a hybrid LNN and XGBoost model to optimize ordering strategies in multi-tier supply chains. By leveraging LNN's dynamic feature extraction and XGBoost's global optimization capabilities, the model aims to mitigate the bullwhip effect and enhance cumulative profitability. The research investigates how local and global synergies within the hybrid framework address the dual demands of adaptability and efficiency in SCM. The proposed approach fills a critical gap in existing methodologies, offering an innovative solution for dynamic and efficient supply chain management.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Language Models To Gather Information Proactively</title>
<link>https://arxiv.org/abs/2507.21389</link>
<guid>https://arxiv.org/abs/2507.21389</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Proactive information gathering, Reinforcement finetuning, Collaborative partners, Task paradigm

Summary: 
Large language models are often passive in real-world settings, lacking the ability to proactively gather missing information. To address this, a new task paradigm is introduced where models must strategically ask questions to elicit implicit user knowledge. The framework generates real-world tasks with masked information, and a reinforcement finetuning strategy rewards questions that reveal new information. The Qwen-2.5-7B model, trained using this approach, outperforms existing models by 18% on evaluation metrics. Human evaluators prefer the clarification questions and final outlines generated by this model by 42% and 28%, respectively. Overall, the study demonstrates the value of proactive clarification in enhancing the collaborative capabilities of large language models. 

<br /><br />Summary: <div>
arXiv:2507.21389v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley Uncertainty in Natural Language Generation</title>
<link>https://arxiv.org/abs/2507.21406</link>
<guid>https://arxiv.org/abs/2507.21406</guid>
<content:encoded><![CDATA[
<div> semantic entropy, uncertainty metric, Shapley-based, question-answering, large language models

Summary: 
In the study of question-answering tasks with large language models, the issue of determining when to trust model outputs is essential for alignment. The traditional approach uses semantic entropy as a measure of uncertainty, based on setting a threshold for semantic equivalence. However, a more refined framework is proposed in this research, utilizing a Shapley-based uncertainty metric that accounts for the continuous nature of semantic relationships. Three key properties are identified for valid uncertainty metrics, with the Shapley uncertainty metric shown to meet these criteria. Experimental results demonstrate that the Shapley uncertainty metric more accurately predicts the performance of large language models in question-answering tasks and other datasets, surpassing similar baseline measures. <div>
arXiv:2507.21406v1 Announce Type: new 
Abstract: In question-answering tasks, determining when to trust the outputs is crucial to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces semantic entropy as a measure of uncertainty, by incorporating linguistic invariances from the same meaning. It primarily relies on setting threshold to measure the level of semantic equivalence relation. We propose a more nuanced framework that extends beyond such thresholding by developing a Shapley-based uncertainty metric that captures the continuous nature of semantic relationships. We establish three fundamental properties that characterize valid uncertainty metrics and prove that our Shapley uncertainty satisfies these criteria. Through extensive experiments, we demonstrate that our Shapley uncertainty more accurately predicts LLM performance in question-answering and other datasets, compared to similar baseline measures.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects</title>
<link>https://arxiv.org/abs/2507.21407</link>
<guid>https://arxiv.org/abs/2507.21407</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous agents, large language models, graph-augmented, planning, memory, tool management, multi-agent coordination

Summary: 
Autonomous agents built on large language models (LLMs) have shown exceptional abilities across various domains, but face limitations in planning, memory, tool management, and multi-agent coordination. Graph-augmented LLM Agents (GLA) utilize graphs as a supportive structure to enhance agent workflows. This paper provides a comprehensive overview of recent advances in GLA research, categorizing methods by their functions within LLM agent systems and discussing the role of graphs in planning, memory, and tool usage. In multi-agent systems, GLA solutions improve orchestration, efficiency, and trustworthiness. Key future directions for GLA research include enhancing structural adaptability, building unified, scalable, and multimodal GLA systems. This paper aims to guide future research and deepen understanding of the importance of graphs in LLM agent systems. 

<br /><br />Summary: <div>
arXiv:2507.21407v1 Announce Type: new 
Abstract: Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GovRelBench:A Benchmark for Government Domain Relevance</title>
<link>https://arxiv.org/abs/2507.21419</link>
<guid>https://arxiv.org/abs/2507.21419</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, government domain, GovRelBench, SoftGovScore, evaluation tool

Summary:
GovRelBench is a new benchmark designed to evaluate the core capabilities of Large Language Models (LLMs) specifically in the government domain. This benchmark includes government domain prompts and a dedicated evaluation tool called GovRelBERT. The SoftGovScore method is introduced during the training process of GovRelBERT, which allows the model to accurately compute the text's government domain relevance score by converting hard labels to soft scores. The aim of this work is to enhance the capability evaluation framework for large models in the government domain, providing a valuable tool for both research and practical applications. The code and dataset for GovRelBench are available on GitHub at https://github.com/pan-xi/GovRelBench.  <div>
arXiv:2507.21419v1 Announce Type: new 
Abstract: Current evaluations of LLMs in the government domain primarily focus on safety considerations in specific scenarios, while the assessment of the models' own core capabilities, particularly domain relevance, remains insufficient. To address this gap, we propose GovRelBench, a benchmark specifically designed for evaluating the core capabilities of LLMs in the government domain. GovRelBench consists of government domain prompts and a dedicated evaluation tool, GovRelBERT. During the training process of GovRelBERT, we introduce the SoftGovScore method: this method trains a model based on the ModernBERT architecture by converting hard labels to soft scores, enabling it to accurately compute the text's government domain relevance score. This work aims to enhance the capability evaluation framework for large models in the government domain, providing an effective tool for relevant research and practice. Our code and dataset are available at https://github.com/pan-xi/GovRelBench.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models</title>
<link>https://arxiv.org/abs/2507.21438</link>
<guid>https://arxiv.org/abs/2507.21438</guid>
<content:encoded><![CDATA[
<div> Keywords: Ontology evolution, Large Language Models, Dual-decoder framework, Autonomous reasoning, Knowledge base maintenance

Summary:
Evo-DKD is a novel dual-decoder framework for autonomous ontology evolution that combines structured ontology traversal with unstructured text reasoning. It operates by generating candidate ontology edits and natural-language justifications in parallel streams within a Large Language Model. By using a dynamic attention-based gating mechanism, Evo-DKD seamlessly blends structured and unstructured knowledge in a closed reasoning loop. The system outperforms baselines in precision of ontology updates and downstream task performance. By simulating coordinated decoding in a single-stream mode, Evo-DKD demonstrates its effectiveness in healthcare ontology refinement, semantic search improvement, and cultural heritage timeline modeling. This approach offers a new paradigm for sustainable ontology evolution by leveraging the strengths of both symbolic and neural reasoning. <br /><br />Summary: <div>
arXiv:2507.21438v1 Announce Type: new 
Abstract: Ontologies and knowledge graphs require continuous evolution to remain comprehensive and accurate, but manual curation is labor intensive. Large Language Models (LLMs) possess vast unstructured knowledge but struggle with maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder framework for autonomous ontology evolution that combines structured ontology traversal with unstructured text reasoning. Evo-DKD introduces two parallel decoding streams within an LLM: one decoder generates candidate ontology edits (e.g., new concepts or relations) while the other produces natural-language justifications. A dynamic attention-based gating mechanism coordinates the two streams, deciding at each step how to blend structured and unstructured knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using prompt-based mode control to approximate coordinated decoding in a single-stream mode. The system operates in a closed reasoning loop: proposed ontology edits are validated (via consistency checks and cross-verification with the text explanations) and then injected into the knowledge base, which in turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on use cases including healthcare ontology refinement, semantic search improvement, and cultural heritage timeline modeling. Experiments show that Evo-DKD outperforms baselines using structured-only or unstructured-only decoding in both precision of ontology updates and downstream task performance. We present quantitative metrics and qualitative examples, confirming the contributions of the dual-decoder design and gating router. Evo-DKD offers a new paradigm for LLM-driven knowledge base maintenance, combining the strengths of symbolic and neural reasoning for sustainable ontology evolution.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)</title>
<link>https://arxiv.org/abs/2507.21453</link>
<guid>https://arxiv.org/abs/2507.21453</guid>
<content:encoded><![CDATA[
<div> Sherpa Rx, AI, pharmacogenomics, CPIC guidelines, PharmGKB <br />
Summary:<br />
This study evaluated Sherpa Rx, an AI tool for pharmacogenomics, integrating CPIC guidelines and PharmGKB data to provide accurate and personalized responses. In Phase 1, Sherpa Rx demonstrated high performance in accuracy, relevance, clarity, completeness, and recall. Phase 2, incorporating PharmGKB content, showed improved accuracy and completeness compared to Phase 1. Sherpa Rx outperformed ChatGPT-4omini in accuracy and completeness, with comparable relevance and clarity. The integration of additional resources like CPIC and PharmGKB with retrieval-augmented generation (RAG) enhances AI accuracy and performance. The study highlights the potential of generative AI like Sherpa Rx in improving decision-making with personalized responses. In a 20-question quiz, Sherpa Rx achieved 90% accuracy, surpassing other models. This research emphasizes the transformative impact of AI in pharmacogenomics. <br /><br />Summary: <div>
arXiv:2507.21453v1 Announce Type: new 
Abstract: This study evaluated Sherpa Rx, an artificial intelligence tool leveraging large language models and retrieval-augmented generation (RAG) for pharmacogenomics, to validate its performance on key response metrics. Sherpa Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC) guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC guidelines was used to evaluate drug-gene interactions, dosing recommendations, and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2 additionally incorporated PharmGKB content. Responses were scored on accuracy, relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon signed-rank tests compared accuracy between Phase 1 and Phase 2, and between Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8, and recall 0.99. The subset analysis (N=20) showed improvements in accuracy (4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8). ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy between Phase 1 and Phase 2 was not statistically significant. However, Phase 2 significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx achieved 90% accuracy, outperforming other models. Integrating additional resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance. This study highlights the transformative potential of generative AI like Sherpa Rx in pharmacogenomics, improving decision-making with accurate, personalized responses.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning</title>
<link>https://arxiv.org/abs/2507.21471</link>
<guid>https://arxiv.org/abs/2507.21471</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared spectroscopy, large language model, automated spectral analysis, low-data conditions, multi-task reasoning

Summary: 
In this study, the researchers explore the use of large language models (LLMs) for automating infrared spectral analysis in low-data conditions. They introduce an end-to-end framework driven by an LLM agent that incorporates a structured literature knowledge base, spectral preprocessing, feature extraction, and multi-task reasoning. By leveraging a curated corpus of IR publications, the agent selects validated routines to transform spectra into low-dimensional features for classification, regression, and anomaly detection using few-shot prompt templates. Through a closed-loop, multi-turn protocol that refines predictions iteratively, the LLM outperforms single-turn inference and other machine learning models across various datasets including stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium, and waste water COD. This approach demonstrates the potential of LLMs in automating complex scientific workflows and achieving accurate spectral interpretation in challenging conditions. 

<br /><br />Summary: <div>
arXiv:2507.21471v1 Announce Type: new 
Abstract: Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, overlapping spectral bands that challenge conventional chemometric approaches. Emerging large language models (LLMs), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an LLM-driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess</title>
<link>https://arxiv.org/abs/2507.21488</link>
<guid>https://arxiv.org/abs/2507.21488</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, individual decision-making, human-AI alignment, personalized modeling, chess

Summary: 
Maia4All is a framework designed to efficiently learn and adapt to individual decision-making styles in chess, even with limited data. It achieves this through a two-stage optimization process: an enrichment step that bridges population and individual-level human behavior modeling, and a democratization step that leverages ability levels to refine individual embeddings with minimal data. Experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity using only 20 games, a significant improvement in data efficiency compared to previous methods. This work establishes a new standard for personalized human-like AI behavior modeling in chess and demonstrates the potential for broader applications in personalized AI adaptation through a prototype-enriched model approach. A case study on idiosyncratic LLMs further validates the framework's effectiveness in modeling individual behavior. 

<br /><br />Summary: <div>
arXiv:2507.21488v1 Announce Type: new 
Abstract: As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly important. Chess, a long-standing AI benchmark with precise skill measurement, offers an ideal testbed for human-AI alignment. However, existing approaches to modeling human behavior require prohibitively large amounts of data from each individual, making them impractical for new or sparsely represented users. In this work, we introduce Maia4All, a framework designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this through a two-stage optimization process: (1) an enrichment step, which bridges population and individual-level human behavior modeling with a prototype-enriched model, and (2) a democratization step, which leverages ability levels or user prototypes to initialize and refine individual embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Maia4All achieves individual human behavior modeling in chess with only 20 games, compared to the 5,000 games required previously, representing a significant improvement in data efficiency. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype-enriched model as a bridge. This approach extends beyond chess, as shown in our case study on idiosyncratic LLMs, highlighting its potential for broader applications in personalized AI adaptation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Supply Chain Decisions</title>
<link>https://arxiv.org/abs/2507.21502</link>
<guid>https://arxiv.org/abs/2507.21502</guid>
<content:encoded><![CDATA[
<div> Keywords: Supply Chain Management, Decision-making, Optimization Techniques, Large Language Models, Productivity<br />
Summary:<br />
Supply Chain Management faces complex decision-making challenges that require advanced computation and information technologies. While optimization techniques offer data-driven solutions, there are still challenges in understanding recommendations, analyzing scenarios, and updating models in a timely manner. The use of Large Language Models (LLMs) can revolutionize supply chain technology by democratizing decision-making processes. By leveraging LLMs, organizations can improve understanding of outcomes, interact with tools more efficiently, and reduce the time to decision-making significantly. This technology enables faster decision-making, increased productivity, and enhanced impact for planners and executives. Overall, LLMs offer a promising solution to streamline supply chain processes and enhance decision-making capabilities. <br /><br />Summary: <div>
arXiv:2507.21502v1 Announce Type: new 
Abstract: Supply Chain Management requires addressing a variety of complex decision-making challenges, from sourcing strategies to planning and execution. Over the last few decades, advances in computation and information technologies have enabled the transition from manual, intuition and experience-based decision-making, into more automated and data-driven decisions using a variety of tools that apply optimization techniques. These techniques use mathematical methods to improve decision-making.
  Unfortunately, business planners and executives still need to spend considerable time and effort to (i) understand and explain the recommendations coming out of these technologies; (ii) analyze various scenarios and answer what-if questions; and (iii) update the mathematical models used in these tools to reflect current business environments. Addressing these challenges requires involving data science teams and/or the technology providers to explain results or make the necessary changes in the technology and hence significantly slows down decision making.
  Motivated by the recent advances in Large Language Models (LLMs), we report how this disruptive technology can democratize supply chain technology - namely, facilitate the understanding of tools' outcomes, as well as the interaction with supply chain tools without human-in-the-loop. Specifically, we report how we apply LLMs to address the three challenges described above, thus substantially reducing the time to decision from days and weeks to minutes and hours as well as dramatically increasing planners' and executives' productivity and impact.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, honesty behaviors, visual questions, MoHoBench, trustworthiness

Summary: 
Most Multimodal Large Language Models (MLLMs) have made significant progress in vision-language tasks, but concerns have arisen about the trustworthiness of the content they generate. This study focuses on evaluating the honesty behavior of MLLMs, particularly when faced with visually unanswerable questions. The researchers create MoHoBench, a dataset of over 12,000 visual question samples, to assess the honesty of 28 popular MLLMs. The findings reveal that many models struggle to properly refuse to answer when necessary, indicating a need for dedicated methods to align multimodal honesty. The study suggests that improving honesty in MMLMs requires considering both language modeling and visual information. Initial alignment methods using supervised and preference learning are proposed to enhance honesty behavior, laying the groundwork for future research on building trustworthy MLLMs. The dataset and code are available for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2507.21503v1 Announce Type: new 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/DSTTSD/MoHoBench.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does it Mean for a Neural Network to Learn a "World Model"?</title>
<link>https://arxiv.org/abs/2507.21513</link>
<guid>https://arxiv.org/abs/2507.21513</guid>
<content:encoded><![CDATA[
<div> criteria, neural net, world model, state space, data generation process

Summary:
The article proposes precise criteria for determining if a neural network learns and utilizes a "world model," specifically focusing on representing a latent state space of the world. The definition is based on the linear probing literature and emphasizes the computation through a representation of the data generation process. The article also introduces conditions to ensure that the world model is not simply a trivial consequence of the neural network's data or task. The goal is to provide a common language for experimental investigation and to give operational meaning to terms often used informally in the field of neural networks. <div>
arXiv:2507.21513v1 Announce Type: new 
Abstract: We propose a set of precise criteria for saying a neural net learns and uses a "world model." The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent "state space" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a "world model" is not a trivial consequence of the neural net's data or task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-GDance: Long-Term and Collision-Free Group Choreography from Music</title>
<link>https://arxiv.org/abs/2507.21518</link>
<guid>https://arxiv.org/abs/2507.21518</guid>
<content:encoded><![CDATA[
<div> Keywords: group dance generation, spatial-temporal interactions, lightweight graph convolutions, accelerated sparse attention, collision-free choreography

Summary:
ST-GDance is a novel framework designed to address the challenges of group dance generation from music, optimizing long-term and collision-free choreography by decoupling spatial and temporal dependencies. The framework utilizes lightweight graph convolutions for distance-aware spatial modeling and accelerated sparse attention for efficient temporal modeling, reducing computational costs while ensuring smooth interactions. Experimental results on the AIOZ-GDance dataset show that ST-GDance outperforms existing methods in generating long and coherent group dance sequences. The proposed framework demonstrates superior performance in handling dense spatial-temporal interactions and mitigating multi-dancer collisions, making it a promising solution for applications in film, gaming, and animation production. The project page for ST-GDance can be found at https://yilliajing.github.io/ST-GDance-Website/. 

<br /><br />Summary: <div>
arXiv:2507.21518v1 Announce Type: new 
Abstract: Group dance generation from music has broad applications in film, gaming, and animation production. However, it requires synchronizing multiple dancers while maintaining spatial coordination. As the number of dancers and sequence length increase, this task faces higher computational complexity and a greater risk of motion collisions. Existing methods often struggle to model dense spatial-temporal interactions, leading to scalability issues and multi-dancer collisions. To address these challenges, we propose ST-GDance, a novel framework that decouples spatial and temporal dependencies to optimize long-term and collision-free group choreography. We employ lightweight graph convolutions for distance-aware spatial modeling and accelerated sparse attention for efficient temporal modeling. This design significantly reduces computational costs while ensuring smooth and collision-free interactions. Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms state-of-the-art baselines, particularly in generating long and coherent group dance sequences. Project page: https://yilliajing.github.io/ST-GDance-Website/.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Wireless Communications: From Adaptation to Autonomy</title>
<link>https://arxiv.org/abs/2507.21524</link>
<guid>https://arxiv.org/abs/2507.21524</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, wireless communications, adaptive solutions, agentic LLMs, autonomous networks

Summary: 
The article discusses the impact of large language models (LLMs) on wireless communications. It explores three main directions where LLMs can transform wireless systems: adapting pretrained LLMs for core communication tasks, developing wireless-specific foundation models for efficiency, and enabling agentic LLMs with autonomous capabilities. The strengths of LLMs, such as reasoning, generalization, and zero-shot learning, are highlighted in practical case studies. The article emphasizes the advantages of LLM-based approaches over traditional methods. It also identifies open challenges and research opportunities like multimodal fusion, collaboration with lightweight models, and self-improving capabilities. The goal is to pave the way for intelligent, adaptive, and autonomous wireless networks of the future. 

<br /><br />Summary: <div>
arXiv:2507.21524v1 Announce Type: new 
Abstract: The emergence of large language models (LLMs) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless communications, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of LLMs in transforming wireless systems across three key directions: adapting pretrained LLMs for core communication tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic LLMs with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of LLM-based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations</title>
<link>https://arxiv.org/abs/2507.21571</link>
<guid>https://arxiv.org/abs/2507.21571</guid>
<content:encoded><![CDATA[
<div> explanations, AI, transparency, personalized approach, user preferences <br />
Summary: 
The article discusses the importance of providing personalized explanations in Artificial Intelligence (AI) to increase transparency and enhance user understanding. Rather than focusing solely on the internal mechanisms of AI models, explanations should be tailored to individual preferences and contextual relevance. By incorporating a personalized approach, AI agents can deliver information that is most relevant and meaningful to users. The proposed model suggests creating a dynamic memory of past interactions with users to estimate what new information would be most valuable. This human-centered perspective aims to improve the user experience and foster greater trust in AI technologies. <div>
arXiv:2507.21571v1 Announce Type: new 
Abstract: The need for explanations in AI has, by and large, been driven by the desire to increase the transparency of black-box machine learning models. However, such explanations, which focus on the internal mechanisms that lead to a specific output, are often unsuitable for non-experts. To facilitate a human-centered perspective on AI explanations, agents need to focus on individuals and their preferences as well as the context in which the explanations are given. This paper proposes a personalized approach to explanation, where the agent tailors the information provided to the user based on what is most likely pertinent to them. We propose a model of the agent's worldview that also serves as a personal and dynamic memory of its previous interactions with the same user, based on which the artificial agent can estimate what part of its knowledge is most likely new information to the user.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.21585</link>
<guid>https://arxiv.org/abs/2507.21585</guid>
<content:encoded><![CDATA[
<div> benchmark, VLM, SafeDriveRAG, traffic safety, multimodal

Summary:
The study focuses on enhancing autonomous driving safety using vision-language models (VLMs). A new benchmark, SafeDrive228K, is introduced for evaluating VLMs in traffic safety scenarios. SafeDriveRAG, a baseline model, incorporates a knowledge graph-based retrieval-augmented generation approach for visual question answering (VQA). The benchmark comprises 228K examples across 18 sub-tasks related to traffic safety queries. The SafeDriveRAG framework leverages traffic safety guidelines and a multi-scale subgraph retrieval algorithm to improve model performance. Evaluation of five mainstream VLMs shows that integrating RAG significantly enhances performance in various safety-sensitive driving tasks, highlighting the potential of the proposed benchmark and methodology for advancing research in traffic safety. The source code and data are available for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.21585v1 Announce Type: new 
Abstract: In this work, we study how vision-language models (VLMs) can be utilized to enhance the safety for the autonomous driving system, including perception, situational understanding, and path planning. However, existing research has largely overlooked the evaluation of these models in traffic safety-critical driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K) and propose a new baseline based on VLM with knowledge graph-based retrieval-augmented generation (SafeDriveRAG) for visual question answering (VQA). Specifically, we introduce SafeDrive228K, the first large-scale multimodal question-answering benchmark comprising 228K examples across 18 sub-tasks. This benchmark encompasses a diverse range of traffic safety queries, from traffic accidents and corner cases to common safety knowledge, enabling a thorough assessment of the comprehension and reasoning abilities of the models. Furthermore, we propose a plug-and-play multimodal knowledge graph-based retrieval-augmented generation approach that employs a novel multi-scale subgraph retrieval algorithm for efficient information retrieval. By incorporating traffic safety guidelines collected from the Internet, this framework further enhances the model's capacity to handle safety-critical situations. Finally, we conduct comprehensive evaluations on five mainstream VLMs to assess their reliability in safety-sensitive driving tasks. Experimental results demonstrate that integrating RAG significantly improves performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five mainstream VLMs, underscoring the potential of our proposed benchmark and methodology for advancing research in traffic safety. Our source code and data are available at https://github.com/Lumos0507/SafeDriveRAG.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning</title>
<link>https://arxiv.org/abs/2507.21588</link>
<guid>https://arxiv.org/abs/2507.21588</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio-visual multi-task learning, Incremental learning, Progressive Homeostatic and Plastic (PHP) method, Task-specific prompts, Knowledge retention

Summary:
The article introduces the Progressive Homeostatic and Plastic (PHP) method for audio-visual multi-task incremental learning. It addresses the challenge of preserving old task knowledge while facilitating the learning of new tasks by incorporating three stages. In the shallow phase, a task-shared modality aggregating adapter enhances shared understanding between tasks. The middle phase introduces task-specific modality-shared dynamic generating adapter to balance knowledge retention and multi-task transferability. In the deep phase, task-specific modality-independent prompts refine understanding by targeting individual information for each task and modality. The PHP method achieves state-of-the-art performance in different task orders (AVE, AVVP, AVS, and AVQA). The code for the method is available on GitHub at https://github.com/ENJOY-Yin-jiong/PHP.<br /><br />Summary: <div>
arXiv:2507.21588v1 Announce Type: new 
Abstract: Audio-visual multi-task incremental learning aims to continuously learn from multiple audio-visual tasks without the need for joint training on all tasks. The challenge of the problem is how to preserve the old task knowledge while facilitating the learning of new task with previous experiences. To address these challenges, we introduce a three-stage Progressive Homeostatic and Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the task-shared modality aggregating adapter to foster cross-task and cross-modal audio-visual representation learning to enhance shared understanding between tasks. In the middle phase, we propose the task-specific modality-shared dynamic generating adapter, which constructs prompts that are tailored to individual tasks while remaining general across modalities, which balances the models ability to retain knowledge against forgetting with its potential for versatile multi-task transferability. In the deep phase, we introduce the task-specific modality-independent prompts to further refine the understand ability by targeting individual information for each task and modality. By incorporating these three phases, PHP retains task-specific prompts while adapting shared parameters for new tasks to effectively balance knowledge sharing and specificity. Our method achieves SOTA performance in different orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at https://github.com/ENJOY-Yin-jiong/PHP.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Link Between Bayesian Inference and Embodied Intelligence: Toward Open Physical-World Embodied AI Systems</title>
<link>https://arxiv.org/abs/2507.21589</link>
<guid>https://arxiv.org/abs/2507.21589</guid>
<content:encoded><![CDATA[
<div> Embodied intelligence, Bayesian statistics, sensorimotor interactions, uncertainty, perception<br />
<br />Summary: Embodied intelligence stems from an agent's interactions with its environment, with Bayesian statistics offering a framework for understanding adaptive behavior through probabilistic representations. The core processes of embodied intelligence, such as perception, action selection, and learning, can be viewed as forms of Bayesian inference. However, current embodied intelligence systems do not widely apply Bayesian principles, limiting their development in open physical-world environments. Analyzing through the lenses of search and learning, it becomes evident why Bayesian inference has not been central in modern embodied intelligence. The potential for Bayesian methods to extend these systems toward truly open physical-world embodied intelligence is highlighted. <div>
arXiv:2507.21589v1 Announce Type: new 
Abstract: Embodied intelligence posits that cognitive capabilities fundamentally emerge from - and are shaped by - an agent's real-time sensorimotor interactions with its environment. Such adaptive behavior inherently requires continuous inference under uncertainty. Bayesian statistics offers a principled probabilistic framework to address this challenge by representing knowledge as probability distributions and updating beliefs in response to new evidence. The core computational processes underlying embodied intelligence - including perception, action selection, learning, and even higher-level cognition - can be effectively understood and modeled as forms of Bayesian inference. Despite the deep conceptual connection between Bayesian statistics and embodied intelligence, Bayesian principles have not been widely or explicitly applied in today's embodied intelligence systems. In this work, we examine both Bayesian and contemporary embodied intelligence approaches through two fundamental lenses: search and learning - the two central themes in modern AI, as highlighted in Rich Sutton's influential essay "The Bitter Lesson". This analysis sheds light on why Bayesian inference has not played a central role in the development of modern embodied intelligence. At the same time, it reveals that current embodied intelligence systems remain largely confined to closed-physical-world environments, and highlights the potential for Bayesian methods to play a key role in extending these systems toward truly open physical-world embodied intelligence.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Teammates, Am I Clear?": Analysing Legible Behaviours in Teams</title>
<link>https://arxiv.org/abs/2507.21631</link>
<guid>https://arxiv.org/abs/2507.21631</guid>
<content:encoded><![CDATA[
<div> legibility, sequential decision-making, teams, multi-agent settings, collaboration
Summary:
- The paper explores the concept of legibility in sequential decision-making within team dynamics.
- Existing research has focused on legibility in individual-agent interactions, neglecting the potential benefits of applying it in team scenarios.
- The study introduces an extension of legible decision-making to multi-agent environments, enhancing collaborative performance.
- By implementing this extension in benchmark scenarios, the research demonstrates that teams with legible agents outperform those with agents following standard optimal behavior.
Summary: <div>
arXiv:2507.21631v1 Announce Type: new 
Abstract: In this paper we investigate the notion of legibility in sequential decision-making in the context of teams and teamwork. There have been works that extend the notion of legibility to sequential decision making, for deterministic and for stochastic scenarios. However, these works focus on one agent interacting with one human, foregoing the benefits of having legible decision making in teams of agents or in team configurations with humans. In this work we propose an extension of legible decision-making to multi-agent settings that improves the performance of agents working in collaboration. We showcase the performance of legible decision making in team scenarios using our proposed extension in multi-agent benchmark scenarios. We show that a team with a legible agent is able to outperform a team composed solely of agents with standard optimal behaviour.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StaffPro: an LLM Agent for Joint Staffing and Profiling</title>
<link>https://arxiv.org/abs/2507.21636</link>
<guid>https://arxiv.org/abs/2507.21636</guid>
<content:encoded><![CDATA[
<div> staffing, scheduling, workforce management, large language model, worker profiling

Summary:<br />
This work introduces StaffPro, an innovative Large Language Model (LLM) agent designed to address the challenges of staffing and worker profiling in workforce management. StaffPro integrates modular algorithmic components with pre-trained LLMs to offer a comprehensive solution. It allows for natural language optimization objectives, accepts textual task descriptions, and provides high flexibility in decision-making processes. StaffPro utilizes a continuous human-agent feedback loop to interact directly with humans, ensuring intuitive use and optimal staffing performance over time. By analyzing human feedback, the agent continuously estimates workers' latent features, enabling lifelong worker profiling. A simulation example in a consulting firm scenario demonstrates the successful estimation of workers' attributes and the generation of high-quality schedules. With its robust, interpretable, and human-centric design, StaffPro offers a promising automated personnel management solution. 

<br /><br />Summary: <div>
arXiv:2507.21636v1 Announce Type: new 
Abstract: Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.21637</link>
<guid>https://arxiv.org/abs/2507.21637</guid>
<content:encoded><![CDATA[
<div> Keywords: Large vision-language models, vulnerability, safety perception, semantic understanding, alignment for linguistic expression

Summary:
Large vision-language models (LVLMs) exhibit vulnerability to harmful input compared to language-only models. Through an investigation of LVLMs' internal dynamics, three key capabilities were identified: safety perception, semantic understanding, and alignment for linguistic expression. The study found that safety perception often precedes comprehensive semantic understanding, leading to reduced safety. To address this issue, the Self-Aware Safety Augmentation (SASA) technique was proposed, which projects informative semantic representations to enhance safety recognition without fine-tuning. Linear probing was used to detect risks before generation based on the model's semantic comprehension. Extensive experiments across datasets and tasks demonstrated that SASA significantly improved the safety of LVLMs while maintaining utility. 

<br /><br />Summary: Large vision-language models exhibit vulnerability due to the emergence of safety perception before comprehensive semantic understanding, hindering safety. The Self-Aware Safety Augmentation (SASA) technique enhances safety recognition by leveraging semantic representations without fine-tuning. Linear probing detects risks based on internal semantic comprehension, resulting in improved safety with minimal impact on utility. <div>
arXiv:2507.21637v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) are vulnerable to harmful input compared to their language-only backbones. We investigated this vulnerability by exploring LVLMs internal dynamics, framing their inherent safety understanding in terms of three key capabilities. Specifically, we define these capabilities as safety perception, semantic understanding, and alignment for linguistic expression, and experimentally pinpointed their primary locations within the model architecture. The results indicate that safety perception often emerges before comprehensive semantic understanding, leading to the reduction in safety. Motivated by these findings, we propose \textbf{Self-Aware Safety Augmentation (SASA)}, a technique that projects informative semantic representations from intermediate layers onto earlier safety-oriented layers. This approach leverages the model's inherent semantic understanding to enhance safety recognition without fine-tuning. Then, we employ linear probing to articulate the model's internal semantic comprehension to detect the risk before the generation process. Extensive experiments on various datasets and tasks demonstrate that SASA significantly improves the safety of LVLMs, with minimal impact on the utility.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics</title>
<link>https://arxiv.org/abs/2507.21638</link>
<guid>https://arxiv.org/abs/2507.21638</guid>
<content:encoded><![CDATA[
<div> Assistax; benchmark; reinforcement learning; assistive robotics; JAX <br />
Summary:<br />
The article introduces Assistax, an open-source benchmark created to address challenges in assistive robotics tasks. It aims to diversify reinforcement learning benchmarks beyond games like Go and Atari, focusing on real-world applications. Using JAX for hardware acceleration, Assistax enables significant speed-ups in learning in physics-based simulations. The benchmark simulates the interaction between an assistive robot and a human patient using multi-agent reinforcement learning. It provides a platform for training diverse partner agents to test an embodied robotic agent's coordination abilities. Assistax offers reliable baselines through extensive evaluation and hyperparameter tuning for popular continuous control RL and multi-agent RL algorithms. The code is available on GitHub, making it a practical tool for advancing research in assistive robotics. <br /> <div>
arXiv:2507.21638v1 Announce Type: new 
Abstract: The development of reinforcement learning (RL) algorithms has been largely driven by ambitious challenge tasks and benchmarks. Games have dominated RL benchmarks because they present relevant challenges, are inexpensive to run and easy to understand. While games such as Go and Atari have led to many breakthroughs, they often do not directly translate to real-world embodied applications. In recognising the need to diversify RL benchmarks and addressing complexities that arise in embodied interaction scenarios, we introduce Assistax: an open-source benchmark designed to address challenges arising in assistive robotics tasks. Assistax uses JAX's hardware acceleration for significant speed-ups for learning in physics-based simulations. In terms of open-loop wall-clock time, Assistax runs up to $370\times$ faster when vectorising training runs compared to CPU-based alternatives. Assistax conceptualises the interaction between an assistive robot and an active human patient using multi-agent RL to train a population of diverse partner agents against which an embodied robotic agent's zero-shot coordination capabilities can be tested. Extensive evaluation and hyperparameter tuning for popular continuous control RL and MARL algorithms provide reliable baselines and establish Assistax as a practical benchmark for advancing RL research for assistive robotics. The code is available at: https://github.com/assistive-autonomy/assistax.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can the current trends of AI handle a full course of mathematics?</title>
<link>https://arxiv.org/abs/2507.21664</link>
<guid>https://arxiv.org/abs/2507.21664</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, mathematics course, education, emotional aspect, integration 

Summary: 
This paper evaluates the current ability of Artificial Intelligence (AI) to manage a full course of mathematics at a college level. The study focuses on four key aspects: creating a course syllabus, presenting material, answering student questions, and creating assessments. While AI excels in organization and accuracy, it lacks the emotional intelligence necessary for effective teaching. The paper suggests integrating human and AI capabilities to enhance the learning experience. By combining the strengths of AI with human empathy, educators can create more engaging and effective mathematics courses at the university level. <div>
arXiv:2507.21664v1 Announce Type: new 
Abstract: This paper addresses the question of how able the current trends of Artificial Intelligence (AI) are in managing to take the responsibility of a full course of mathematics at a college level. The study evaluates this ability in four significant aspects, namely, creating a course syllabus, presenting selected material, answering student questions, and creating an assessment. It shows that even though the AI is strong in some important parts like organization and accuracy, there are still some human aspects that are far away from the current abilities of AI. There is still a hidden emotional part, even in science, that cannot be fulfilled by the AI in its current state. This paper suggests some recommendations to integrate the human and AI potentials to create better outcomes in terms of reaching the target of creating a full course of mathematics, at a university level, as best as possible.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unrolling Dynamic Programming via Graph Filters</title>
<link>https://arxiv.org/abs/2507.21705</link>
<guid>https://arxiv.org/abs/2507.21705</guid>
<content:encoded><![CDATA[
<div> Dynamic programming, Bellman's optimality equations, policy iteration, Markov decision process, BellNet

Summary:
The article introduces a new approach called BellNet for solving Bellman's optimality equations in a Markov decision process (MDP) setting. BellNet unrolls and truncates policy iterations into a learnable parametric model, trained to minimize the Bellman error. By treating the transition probability matrix of the MDP as a weighted directed graph, BellNet is re-parameterized as a cascade of nonlinear graph filters using insights from graph signal processing. This approach provides a concise and transferable representation of policy and value iteration while offering explicit complexity control during inference. Preliminary experiments in a grid-like environment show that BellNet can approximate optimal policies efficiently in fewer iterations compared to traditional methods. <div>
arXiv:2507.21705v1 Announce Type: new 
Abstract: Dynamic programming (DP) is a fundamental tool used across many engineering fields. The main goal of DP is to solve Bellman's optimality equations for a given Markov decision process (MDP). Standard methods like policy iteration exploit the fixed-point nature of these equations to solve them iteratively. However, these algorithms can be computationally expensive when the state-action space is large or when the problem involves long-term dependencies. Here we propose a new approach that unrolls and truncates policy iterations into a learnable parametric model dubbed BellNet, which we train to minimize the so-termed Bellman error from random value function initializations. Viewing the transition probability matrix of the MDP as the adjacency of a weighted directed graph, we draw insights from graph signal processing to interpret (and compactly re-parameterize) BellNet as a cascade of nonlinear graph filters. This fresh look facilitates a concise, transferable, and unifying representation of policy and value iteration, with an explicit handle on complexity during inference. Preliminary experiments conducted in a grid-like environment demonstrate that BellNet can effectively approximate optimal policies in a fraction of the iterations required by classical methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation</title>
<link>https://arxiv.org/abs/2507.21727</link>
<guid>https://arxiv.org/abs/2507.21727</guid>
<content:encoded><![CDATA[
<div> Graph Domain Adaptation, Individual Parcellation, fMRI, Graph Attention Networks, Minimax Entropy <br />
<br />
Summary: 
The article introduces a novel framework called Graph Domain Adaptation for Individual Parcellation (GDAIP) to address the challenges faced in learning individual brain parcellations from functional magnetic resonance imaging (fMRI) data. GDAIP integrates Graph Attention Networks (GAT) with Minimax Entropy-based domain adaptation to handle domain shifts in cross-dataset scenarios. By constructing cross-dataset brain graphs at both group and individual levels and using semi-supervised training and adversarial optimization, GDAIP adapts the reference atlas from the group-level to the individual-level brain graph. Evaluation results show that GDAIP produces individual brain parcellations with plausible boundaries, cross-session consistency, and the ability to reflect functional organization accurately. The method demonstrates promising results in individual brain parcellation tasks, showcasing its potential in advancing research in neuroscience and functional brain mapping. <br /> <div>
arXiv:2507.21727v1 Announce Type: new 
Abstract: Recent deep learning approaches have shown promise in learning such individual brain parcellations from functional magnetic resonance imaging (fMRI). However, most existing methods assume consistent data distributions across domains and struggle with domain shifts inherent to real-world cross-dataset scenarios. To address this challenge, we proposed Graph Domain Adaptation for Individual Parcellation (GDAIP), a novel framework that integrates Graph Attention Networks (GAT) with Minimax Entropy (MME)-based domain adaptation. We construct cross-dataset brain graphs at both the group and individual levels. By leveraging semi-supervised training and adversarial optimization of the prediction entropy on unlabeled vertices from target brain graph, the reference atlas is adapted from the group-level brain graph to the individual brain graph, enabling individual parcellation under cross-dataset settings. We evaluated our method using parcellation visualization, Dice coefficient, and functional homogeneity. Experimental results demonstrate that GDAIP produces individual parcellations with topologically plausible boundaries, strong cross-session consistency, and ability of reflecting functional organization.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAT-Based Bounded Fitting for the Description Logic ALC</title>
<link>https://arxiv.org/abs/2507.21752</link>
<guid>https://arxiv.org/abs/2507.21752</guid>
<content:encoded><![CDATA[
<div> bounded fitting, description logic, ALC, NP-complete, probabilistic guarantees<br />
<br />
Summary:
The article explores bounded fitting as a method for learning logical formulas in the context of description logic ALC and its syntactic fragments. It demonstrates that the size-restricted fitting problem is NP-complete for all studied fragments, even in cases with a single positive and negative example. Bounded fitting offers probabilistic guarantees in Valiant's PAC learning framework, distinct from other learning algorithms for ALC concepts. An implementation of bounded fitting in ALC and its fragments using a SAT solver is presented, along with discussions on optimizations and comparisons with other concept learning tools. The study sheds light on the computational complexity of fitting problems in description logic and highlights the unique benefits of using bounded fitting for learning logical formulas. <br /><br /> <div>
arXiv:2507.21752v1 Announce Type: new 
Abstract: Bounded fitting is a general paradigm for learning logical formulas from positive and negative data examples, that has received considerable interest recently. We investigate bounded fitting for the description logic ALC and its syntactic fragments. We show that the underlying size-restricted fitting problem is NP-complete for all studied fragments, even in the special case of a single positive and a single negative example. By design, bounded fitting comes with probabilistic guarantees in Valiant's PAC learning framework. In contrast, we show that other classes of algorithms for learning ALC concepts do not provide such guarantees. Finally, we present an implementation of bounded fitting in ALC and its fragments based on a SAT solver. We discuss optimizations and compare our implementation to other concept learning tools.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a rigorous evaluation of RAG systems: the challenge of due diligence</title>
<link>https://arxiv.org/abs/2507.21753</link>
<guid>https://arxiv.org/abs/2507.21753</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, Retrieval-Augmented Generation (RAG) architecture, hallucinations, due diligence, evaluation protocol

Summary:
This study evaluates the reliability of a Retrieval-Augmented Generation (RAG) system used in due diligence for an investment fund. The RAG architecture combines language models with search engines to generate responses from document corpora. The evaluation protocol proposed in this study combines human annotations and LLM-Judge annotations to identify system failures such as hallucinations, off-topic responses, failed citations, and abstentions. Inspired by the Prediction Powered Inference method, precise performance measurements with statistical guarantees are achieved. A comprehensive dataset is provided for further analysis. The study aims to enhance the reliability and scalability of RAG systems evaluation protocols in industrial applications. 

<br /><br />Summary: <div>
arXiv:2507.21753v1 Announce Type: new 
Abstract: The rise of generative AI, has driven significant advancements in high-risk sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG) architecture, combining language models (LLMs) with search engines, is particularly notable for its ability to generate responses from document corpora. Despite its potential, the reliability of RAG systems in critical contexts remains a concern, with issues such as hallucinations persisting. This study evaluates a RAG system used in due diligence for an investment fund. We propose a robust evaluation protocol combining human annotations and LLM-Judge annotations to identify system failures, like hallucinations, off-topic, failed citations, and abstentions. Inspired by the Prediction Powered Inference (PPI) method, we achieve precise performance measurements with statistical guarantees. We provide a comprehensive dataset for further analysis. Our contributions aim to enhance the reliability and scalability of RAG systems evaluation protocols in industrial applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Causal Identification and Causal Mechanism Clustering</title>
<link>https://arxiv.org/abs/2507.21792</link>
<guid>https://arxiv.org/abs/2507.21792</guid>
<content:encoded><![CDATA[
<div> causal inference, bivariate causal direction identification, additive noise, Mixture Conditional Variational Causal Inference model, causal heterogeneity<br />
Summary:<br />
The article addresses the problem of identifying bivariate causal directions using a Mixture Conditional Variational Causal Inference model (MCVCI), which incorporates heterogeneous causal relationships. By leveraging the Hybrid Additive Noise Model (HANM) and combining Gaussian mixture models and neural networks, MCVCI uses likelihoods from a probabilistic bounds approach to determine causality. Additionally, the proposed Mixture Conditional Variational Causal Clustering (MCVCC) method integrates casual heterogeneity into cluster numbers, enabling the expression of causal mechanisms. The effectiveness of the methods is demonstrated through superior performance on simulated and real data when compared to existing approaches. <div>
arXiv:2507.21792v1 Announce Type: new 
Abstract: Bivariate causal direction identification is a fundamental and vital problem in the causal inference field. Among binary causal methods, most methods based on additive noise only use one single causal mechanism to construct a causal model. In the real world, observations are always collected in different environments with heterogeneous causal relationships. Therefore, on observation data, this paper proposes a Mixture Conditional Variational Causal Inference model (MCVCI) to infer heterogeneous causality. Specifically, according to the identifiability of the Hybrid Additive Noise Model (HANM), MCVCI combines the superior fitting capabilities of the Gaussian mixture model and the neural network and elegantly uses the likelihoods obtained from the probabilistic bounds of the mixture conditional variational auto-encoder as causal decision criteria. Moreover, we model the casual heterogeneity into cluster numbers and propose the Mixture Conditional Variational Causal Clustering (MCVCC) method, which can reveal causal mechanism expression. Compared with state-of-the-art methods, the comprehensive best performance demonstrates the effectiveness of the methods proposed in this paper on several simulated and real data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</title>
<link>https://arxiv.org/abs/2507.21802</link>
<guid>https://arxiv.org/abs/2507.21802</guid>
<content:encoded><![CDATA[
arXiv:2507.21802v1 Announce Type: new 
Abstract: Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic AI for a New Paradigm in Business Process Development</title>
<link>https://arxiv.org/abs/2507.21823</link>
<guid>https://arxiv.org/abs/2507.21823</guid>
<content:encoded><![CDATA[
arXiv:2507.21823v1 Announce Type: new 
Abstract: Artificial Intelligence agents represent the next major revolution in the continuous technological evolution of industrial automation. In this paper, we introduce a new approach for business process design and development that leverages the capabilities of Agentic AI. Departing from the traditional task-based approach to business process design, we propose an agent-based method, where agents contribute to the achievement of business goals, identified by a set of business objects. When a single agent cannot fulfill a goal, we have a merge goal that can be achieved through the collaboration of multiple agents. The proposed model leads to a more modular and intelligent business process development by organizing it around goals, objects, and agents. As a result, this approach enables flexible and context-aware automation in dynamic industrial environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework</title>
<link>https://arxiv.org/abs/2507.21830</link>
<guid>https://arxiv.org/abs/2507.21830</guid>
<content:encoded><![CDATA[
arXiv:2507.21830v1 Announce Type: new 
Abstract: Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Active Goal Recognition</title>
<link>https://arxiv.org/abs/2507.21846</link>
<guid>https://arxiv.org/abs/2507.21846</guid>
<content:encoded><![CDATA[
arXiv:2507.21846v1 Announce Type: new 
Abstract: In multi-agent environments, effective interaction hinges on understanding the beliefs and intentions of other agents. While prior work on goal recognition has largely treated the observer as a passive reasoner, Active Goal Recognition (AGR) focuses on strategically gathering information to reduce uncertainty. We adopt a probabilistic framework for Active Goal Recognition and propose an integrated solution that combines a joint belief update mechanism with a Monte Carlo Tree Search (MCTS) algorithm, allowing the observer to plan efficiently and infer the actor's hidden goal without requiring domain-specific knowledge. Through comprehensive empirical evaluation in a grid-based domain, we show that our joint belief update significantly outperforms passive goal recognition, and that our domain-independent MCTS performs comparably to our strong domain-specific greedy baseline. These results establish our solution as a practical and robust framework for goal inference, advancing the field toward more interactive and adaptive multi-agent systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity</title>
<link>https://arxiv.org/abs/2507.21848</link>
<guid>https://arxiv.org/abs/2507.21848</guid>
<content:encoded><![CDATA[
arXiv:2507.21848v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors</title>
<link>https://arxiv.org/abs/2507.21872</link>
<guid>https://arxiv.org/abs/2507.21872</guid>
<content:encoded><![CDATA[
arXiv:2507.21872v1 Announce Type: new 
Abstract: Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data</title>
<link>https://arxiv.org/abs/2507.21873</link>
<guid>https://arxiv.org/abs/2507.21873</guid>
<content:encoded><![CDATA[
arXiv:2507.21873v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) excel at predictive tasks on graph-structured data but often lack the ability to incorporate symbolic domain knowledge and perform general reasoning. Relational Bayesian Networks (RBNs), in contrast, enable fully generative probabilistic modeling over graph-like structures and support rich symbolic knowledge and probabilistic inference. This paper presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs, combining the learning strength of GNNs with the flexible reasoning capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs directly into the native RBN language, while the other maintains the GNN as an external component. Both approaches preserve the semantics and computational properties of GNNs while fully aligning with the RBN modeling paradigm. We also propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic models.
  To demonstrate the framework's versatility, we apply it to two distinct problems. First, we transform a GNN for node classification into a collective classification model that explicitly models homo- and heterophilic label patterns, substantially improving accuracy. Second, we introduce a multi-objective network optimization problem in environmental planning, where MAP inference supports complex decision-making. Both applications include new publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph data, bridging learning and reasoning in ways that enable novel applications and improved performance across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis</title>
<link>https://arxiv.org/abs/2507.21875</link>
<guid>https://arxiv.org/abs/2507.21875</guid>
<content:encoded><![CDATA[
arXiv:2507.21875v1 Announce Type: new 
Abstract: Pain is a complex and pervasive condition that affects a significant portion of the population. Accurate and consistent assessment is essential for individuals suffering from pain, as well as for developing effective management strategies in a healthcare system. Automatic pain assessment systems enable continuous monitoring, support clinical decision-making, and help minimize patient distress while mitigating the risk of functional deterioration. Leveraging physiological signals offers objective and precise insights into a person's state, and their integration in a multimodal framework can further enhance system performance. This study has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The proposed approach introduces \textit{Tiny-BioMoE}, a lightweight pretrained embedding model for biosignal analysis. Trained on $4.4$ million biosignal image representations and consisting of only $7.3$ million parameters, it serves as an effective tool for extracting high-quality embeddings for downstream tasks. Extensive experiments involving electrodermal activity, blood volume pulse, respiratory signals, peripheral oxygen saturation, and their combinations highlight the model's effectiveness across diverse modalities in automatic pain recognition tasks. \textit{\textcolor{blue}{The model's architecture (code) and weights are available at https://github.com/GkikasStefanos/Tiny-BioMoE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image</title>
<link>https://arxiv.org/abs/2507.21881</link>
<guid>https://arxiv.org/abs/2507.21881</guid>
<content:encoded><![CDATA[
arXiv:2507.21881v1 Announce Type: new 
Abstract: Pain is a multifaceted phenomenon that affects a substantial portion of the population. Reliable and consistent evaluation benefits those experiencing pain and underpins the development of effective and advanced management strategies. Automatic pain-assessment systems deliver continuous monitoring, inform clinical decision-making, and aim to reduce distress while preventing functional decline. By incorporating physiological signals, these systems provide objective, accurate insights into an individual's condition. This study has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline that leverages electrodermal activity signals as input modality. Multiple representations of the signal are created and visualized as waveforms, and they are jointly visualized within a single multi-representation diagram. Extensive experiments incorporating various processing and filtering techniques, along with multiple representation combinations, demonstrate the effectiveness of the proposed approach. It consistently yields comparable, and in several cases superior, results to traditional fusion methods, establishing it as a robust alternative for integrating different signal representations or modalities.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Foundational Models on Patient-Centric e-Health Systems</title>
<link>https://arxiv.org/abs/2507.21882</link>
<guid>https://arxiv.org/abs/2507.21882</guid>
<content:encoded><![CDATA[
arXiv:2507.21882v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare technologies, understanding the maturity of AI in patient-centric applications is critical for evaluating its trustworthiness, transparency, and real-world impact. In this study, we investigate the integration and maturity of AI feature integration in 116 patient-centric healthcare applications. Using Large Language Models (LLMs), we extracted key functional features, which are then categorized into different stages of the Gartner AI maturity model. Our results show that over 86.21\% of applications remain at the early stages of AI integration, while only 13.79% demonstrate advanced AI integration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Pain Recognition via Respiration Signals: A Single Cross-Attention Transformer Multi-Window Fusion Pipeline</title>
<link>https://arxiv.org/abs/2507.21886</link>
<guid>https://arxiv.org/abs/2507.21886</guid>
<content:encoded><![CDATA[
arXiv:2507.21886v1 Announce Type: new 
Abstract: Pain is a complex condition affecting a large portion of the population. Accurate and consistent evaluation is essential for individuals experiencing pain, and it supports the development of effective and advanced management strategies. Automatic pain assessment systems provide continuous monitoring and support clinical decision-making, aiming to reduce distress and prevent functional decline. This study has been submitted to the \textit{Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline that leverages respiration as the input signal and incorporates a highly efficient cross-attention transformer alongside a multi-windowing strategy. Extensive experiments demonstrate that respiration is a valuable physiological modality for pain assessment. Moreover, experiments revealed that compact and efficient models, when properly optimized, can achieve strong performance, often surpassing larger counterparts. The proposed multi-window approach effectively captures both short-term and long-term features, as well as global characteristics, thereby enhancing the model's representational capacity.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Content Classification Approach for GitHub Repositories by the README Files</title>
<link>https://arxiv.org/abs/2507.21899</link>
<guid>https://arxiv.org/abs/2507.21899</guid>
<content:encoded><![CDATA[
arXiv:2507.21899v1 Announce Type: new 
Abstract: GitHub is the world's most popular platform for storing, sharing, and managing code. Every GitHub repository has a README file associated with it. The README files should contain project-related information as per the recommendations of GitHub to support the usage and improvement of repositories. However, GitHub repository owners sometimes neglected these recommendations. This prevents a GitHub repository from reaching its full potential. This research posits that the comprehensiveness of a GitHub repository's README file significantly influences its adoption and utilization, with a lack of detail potentially hindering its full potential for widespread engagement and impact within the research community. Large Language Models (LLMs) have shown great performance in many text-based tasks including text classification, text generation, text summarization and text translation. In this study, an approach is developed to fine-tune LLMs for automatically classifying different sections of GitHub README files. Three encoder-only LLMs are utilized, including BERT, DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a gold-standard dataset consisting of 4226 README file sections. This approach outperforms current state-of-the-art methods and has achieved an overall F1 score of 0.98. Moreover, we have also investigated the use of Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) and shown an economical alternative to full fine-tuning without compromising much performance. The results demonstrate the potential of using LLMs in designing an automatic classifier for categorizing the content of GitHub README files. Consequently, this study contributes to the development of automated tools for GitHub repositories to improve their identifications and potential usages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Libra: Large Chinese-based Safeguard for AI Content</title>
<link>https://arxiv.org/abs/2507.21929</link>
<guid>https://arxiv.org/abs/2507.21929</guid>
<content:encoded><![CDATA[
arXiv:2507.21929v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in text understanding and generation but raise significant safety and ethical concerns in high-stakes applications. To mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage curriculum training pipeline, Libra-Guard enhances data efficiency by employing guard pretraining on synthetic samples, followed by fine-tuning on high-quality, real-world data, thereby significantly reducing reliance on manual annotations. To enable rigorous safety evaluations, we also introduce Libra-Test, the first benchmark specifically designed to evaluate the effectiveness of safeguard systems for Chinese content. It covers seven critical harm scenarios and includes over 5,700 samples annotated by domain experts. Experiments show that Libra-Guard achieves 86.79% accuracy, outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat (65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o. These contributions establish a robust framework for advancing the safety governance of Chinese LLMs and represent a tentative step toward developing safer, more reliable Chinese AI systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart Homes via Language Modeling of Sensor Data &amp; Activities</title>
<link>https://arxiv.org/abs/2507.21964</link>
<guid>https://arxiv.org/abs/2507.21964</guid>
<content:encoded><![CDATA[
arXiv:2507.21964v1 Announce Type: new 
Abstract: Developing zero-shot human activity recognition (HAR) methods is a critical direction in smart home research -- considering its impact on making HAR systems work across smart homes having diverse sensing modalities, layouts, and activities of interest. The state-of-the-art solutions along this direction are based on generating natural language descriptions of the sensor data and feeding it via a carefully crafted prompt to the LLM to perform classification. Despite their performance guarantees, such ``prompt-the-LLM'' approaches carry several risks, including privacy invasion, reliance on an external service, and inconsistent predictions due to version changes, making a case for alternative zero-shot HAR methods that do not require prompting the LLMs. In this paper, we propose one such solution that models sensor data and activities using natural language, leveraging its embeddings to perform zero-shot classification and thereby bypassing the need to prompt the LLMs for activity predictions. The impact of our work lies in presenting a detailed case study on six datasets, highlighting how language modeling can bolster HAR systems in zero-shot recognition.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks</title>
<link>https://arxiv.org/abs/2507.21974</link>
<guid>https://arxiv.org/abs/2507.21974</guid>
<content:encoded><![CDATA[
arXiv:2507.21974v1 Announce Type: new 
Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain</title>
<link>https://arxiv.org/abs/2507.21976</link>
<guid>https://arxiv.org/abs/2507.21976</guid>
<content:encoded><![CDATA[
arXiv:2507.21976v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHAX: A Structured Argumentation Framework for User-Centered Explainable AI in Public Health and Biomedical Sciences</title>
<link>https://arxiv.org/abs/2507.22009</link>
<guid>https://arxiv.org/abs/2507.22009</guid>
<content:encoded><![CDATA[
arXiv:2507.22009v1 Announce Type: new 
Abstract: Ensuring transparency and trust in AI-driven public health and biomedical sciences systems requires more than accurate predictions-it demands explanations that are clear, contextual, and socially accountable. While explainable AI (XAI) has advanced in areas like feature attribution and model interpretability, most methods still lack the structure and adaptability needed for diverse health stakeholders, including clinicians, policymakers, and the general public. We introduce PHAX-a Public Health Argumentation and eXplainability framework-that leverages structured argumentation to generate human-centered explanations for AI outputs. PHAX is a multi-layer architecture combining defeasible reasoning, adaptive natural language techniques, and user modeling to produce context-aware, audience-specific justifications. More specifically, we show how argumentation enhances explainability by supporting AI-driven decision-making, justifying recommendations, and enabling interactive dialogues across user types. We demonstrate the applicability of PHAX through use cases such as medical term simplification, patient-clinician communication, and policy justification. In particular, we show how simplification decisions can be modeled as argument chains and personalized based on user expertise-enhancing both interpretability and trust. By aligning formal reasoning methods with communicative demands, PHAX contributes to a broader vision of transparent, human-centered AI in public health.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
arXiv:2507.22025v1 Announce Type: new 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserBench: An Interactive Gym Environment for User-Centric Agents</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
arXiv:2507.22034v1 Announce Type: new 
Abstract: Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Interspeech 2025 Speech Accessibility Project Challenge</title>
<link>https://arxiv.org/abs/2507.22047</link>
<guid>https://arxiv.org/abs/2507.22047</guid>
<content:encoded><![CDATA[
arXiv:2507.22047v1 Announce Type: new 
Abstract: While the last decade has witnessed significant advancements in Automatic Speech Recognition (ASR) systems, performance of these systems for individuals with speech disabilities remains inadequate, partly due to limited public training data. To bridge this gap, the 2025 Interspeech Speech Accessibility Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data collected and transcribed from more than 500 individuals with diverse speech disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline, the SAP Challenge evaluates submissions based on Word Error Rate and Semantic Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2 baseline in terms of WER, while 17 teams surpassed the baseline on SemScore. Notably, the top team achieved the lowest WER of 8.11\%, and the highest SemScore of 88.44\% at the same time, setting new benchmarks for future ASR systems in recognizing impaired speech.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2507.17307</link>
<guid>https://arxiv.org/abs/2507.17307</guid>
<content:encoded><![CDATA[
arXiv:2507.17307v2 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online hierarchical partitioning of the output space in extreme multi-label data stream</title>
<link>https://arxiv.org/abs/2507.20894</link>
<guid>https://arxiv.org/abs/2507.20894</guid>
<content:encoded><![CDATA[
arXiv:2507.20894v1 Announce Type: cross 
Abstract: Mining data streams with multi-label outputs poses significant challenges due to evolving distributions, high-dimensional label spaces, sparse label occurrences, and complex label dependencies. Moreover, concept drift affects not only input distributions but also label correlations and imbalance ratios over time, complicating model adaptation. To address these challenges, structured learners are categorized into local and global methods. Local methods break down the task into simpler components, while global methods adapt the algorithm to the full output space, potentially yielding better predictions by exploiting label correlations. This work introduces iHOMER (Incremental Hierarchy Of Multi-label Classifiers), an online multi-label learning framework that incrementally partitions the label space into disjoint, correlated clusters without relying on predefined hierarchies. iHOMER leverages online divisive-agglomerative clustering based on \textit{Jaccard} similarity and a global tree-based learner driven by a multivariate \textit{Bernoulli} process to guide instance partitioning. To address non-stationarity, it integrates drift detection mechanisms at both global and local levels, enabling dynamic restructuring of label partitions and subtrees. Experiments across 23 real-world datasets show iHOMER outperforms 5 state-of-the-art global baselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\%, and 12 local baselines, such as binary relevance transformations of kNN, EFDT, ARF, and ADWIN bagging/boosting ensembles, by 32\%, establishing its robustness for online multi-label classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High hopes for "Deep Medicine"? AI, economics, and the future of care</title>
<link>https://arxiv.org/abs/2507.21054</link>
<guid>https://arxiv.org/abs/2507.21054</guid>
<content:encoded><![CDATA[
arXiv:2507.21054v1 Announce Type: cross 
Abstract: In the much-celebrated book Deep Medicine, Eric Topol argues that the development of artificial intelligence for health care will lead to a dramatic shift in the culture and practice of medicine. In the next several decades, he suggests, AI will become sophisticated enough that many of the everyday tasks of physicians could be delegated to it. Topol is perhaps the most articulate advocate of the benefits of AI in medicine, but he is hardly alone in spruiking its potential to allow physicians to dedicate more of their time and attention to providing empathetic care for their patients in the future. Unfortunately, several factors suggest a radically different picture for the future of health care. Far from facilitating a return to a time of closer doctor-patient relationships, the use of medical AI seems likely to further erode therapeutic relationships and threaten professional and patient satisfaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21055</link>
<guid>https://arxiv.org/abs/2507.21055</guid>
<content:encoded><![CDATA[
arXiv:2507.21055v1 Announce Type: cross 
Abstract: In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Generation of Data Contracts in Modern Data Engineering Systems</title>
<link>https://arxiv.org/abs/2507.21056</link>
<guid>https://arxiv.org/abs/2507.21056</guid>
<content:encoded><![CDATA[
arXiv:2507.21056v1 Announce Type: cross 
Abstract: Data contracts formalize agreements between data producers and consumers regarding schema, semantics, and quality expectations. As data pipelines grow in complexity, manual authoring and maintenance of contracts becomes error-prone and labor-intensive. We present an AI-driven framework for automatic data contract generation using large language models (LLMs). Our system leverages parameter-efficient fine-tuning methods, including LoRA and PEFT, to adapt LLMs to structured data domains. The models take sample data or schema descriptions and output validated contract definitions in formats such as JSON Schema and Avro. We integrate this framework into modern data platforms (e.g., Databricks, Snowflake) to automate contract enforcement at scale. Experimental results on synthetic and real-world datasets demonstrate that the fine-tuned LLMs achieve high accuracy in generating valid contracts and reduce manual workload by over 70%. We also discuss key challenges such as hallucination, version control, and the need for continuous learning. This work demonstrates that generative AI can enable scalable, agile data governance by bridging the gap between intent and implementation in enterprise data management.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Classification of Book Summaries Using Word Embedding Techniques</title>
<link>https://arxiv.org/abs/2507.21058</link>
<guid>https://arxiv.org/abs/2507.21058</guid>
<content:encoded><![CDATA[
arXiv:2507.21058v1 Announce Type: cross 
Abstract: In this study, book summaries and categories taken from book sites were classified using word embedding methods, natural language processing techniques and machine learning algorithms. In addition, one hot encoding, Word2Vec and Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are frequently used word embedding methods were used in this study and their success was compared. Additionally, the combination table of the pre-processing methods used is shown and added to the table. Looking at the results, it was observed that Support Vector Machine, Naive Bayes and Logistic Regression Models and TF-IDF and One-Hot Encoder word embedding techniques gave more successful results for Turkish texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning</title>
<link>https://arxiv.org/abs/2507.21060</link>
<guid>https://arxiv.org/abs/2507.21060</guid>
<content:encoded><![CDATA[
arXiv:2507.21060v1 Announce Type: cross 
Abstract: The rapid integration of Artificial Intelligence (AI) into medical diagnostics has raised pressing concerns about patient privacy, especially when sensitive imaging data must be transferred, stored, or processed. In this paper, we propose a novel framework for privacy-preserving diagnostic inference on encrypted medical images using a modified convolutional neural network (Masked-CNN) capable of operating on transformed or ciphered image formats. Our approach leverages AES-CBC encryption coupled with JPEG2000 compression to protect medical images while maintaining their suitability for AI inference. We evaluate the system using public DICOM datasets (NIH ChestX-ray14 and LIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage efficiency, and privacy leakage resistance. Experimental results show that the encrypted inference model achieves performance comparable to its unencrypted counterpart, with only marginal trade-offs in accuracy and latency. The proposed framework bridges the gap between data privacy and clinical utility, offering a practical, scalable solution for secure AI-driven diagnostics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data</title>
<link>https://arxiv.org/abs/2507.21069</link>
<guid>https://arxiv.org/abs/2507.21069</guid>
<content:encoded><![CDATA[
arXiv:2507.21069v1 Announce Type: cross 
Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and scalable means to assess human movement quality in clinical and everyday settings. However, the development of robust sensor-based classification models for physiotherapeutic exercises and gait analysis requires large, diverse datasets, which are costly and time-consuming to collect. Here, we present a multimodal dataset of physiotherapeutic exercises - including correct and clinically relevant variants - and gait-related exercises - including both normal and impaired gait patterns - recorded from 19 participants using synchronized IMUs and marker-based motion capture (MoCap). The dataset includes raw data from nine IMUs and thirty-five optical markers capturing full-body kinematics. Each IMU is additionally equipped with four optical markers, enabling precise comparison between IMU-derived orientation estimates and reference values from the MoCap system. To support further analysis, we also provide processed IMU orientations aligned with common segment coordinate systems, subject-specific OpenSim models, inverse kinematics results, and tools for visualizing IMU orientations in the musculoskeletal context. Detailed annotations of movement execution quality and time-stamped segmentations support diverse analysis goals. This dataset supports the development and benchmarking of machine learning models for tasks such as automatic exercise evaluation, gait analysis, temporal activity segmentation, and biomechanical parameter estimation. To facilitate reproducibility, we provide code for postprocessing, sensor-to-segment alignment, inverse kinematics computation, and technical validation. This resource is intended to accelerate research in machine learning-driven human movement analysis.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents</title>
<link>https://arxiv.org/abs/2507.21071</link>
<guid>https://arxiv.org/abs/2507.21071</guid>
<content:encoded><![CDATA[
arXiv:2507.21071v1 Announce Type: cross 
Abstract: Mobile GUI agents are becoming critical tools for enhancing human-device interaction efficiency, with multimodal large language models (MLLMs) emerging as dominant paradigms in this domain. Current agents, however, are limited to following explicit human instructions, resulting in insufficient capability for proactive intent anticipation. Additionally, these agents fail to leverage the contextual information associated with users during task execution, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip benchmark. It contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. We collected unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. Our experiments reveal challenges of the tasks we propose. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile GUI agents. Our code is open-source at https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education</title>
<link>https://arxiv.org/abs/2507.21074</link>
<guid>https://arxiv.org/abs/2507.21074</guid>
<content:encoded><![CDATA[
arXiv:2507.21074v1 Announce Type: cross 
Abstract: As generative AI (Gen-AI) tools become more prevalent in education, there is a growing need to understand how educators, not just students, can actively shape their design and use. This study investigates how two instructors integrated four custom GPT tools into a Masters-level Qualitative Research Methods course for Urban Planning Policy students. Addressing two key gaps: the dominant framing of students as passive AI users, and the limited use of AI in qualitative methods education. The study explores how Gen-AI can support disciplinary learning when aligned with pedagogical intent. Drawing on the Technological Pedagogical Content Knowledge (TPACK) framework and action research methodology, the instructors designed GPTs to scaffold tasks such as research question formulation, interview practice, fieldnote analysis, and design thinking. Thematic analysis of student reflections, AI chat logs, and final assignments revealed that the tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking. However, students also expressed concerns about cognitive overload, reduced immersion in data, and the formulaic nature of AI responses. The study offers three key insights: AI can be a powerful scaffold for active learning when paired with human facilitation; custom GPTs can serve as cognitive partners in iterative research practice; and educator-led design is critical to pedagogically meaningful AI integration. This research contributes to emerging scholarship on AI in higher education by demonstrating how empowering educators to design custom tools can promote more reflective, responsible, and collaborative learning with AI.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven and Participatory Approaches toward Neuro-Inclusive AI</title>
<link>https://arxiv.org/abs/2507.21077</link>
<guid>https://arxiv.org/abs/2507.21077</guid>
<content:encoded><![CDATA[
arXiv:2507.21077v1 Announce Type: cross 
Abstract: Biased data representation in AI marginalizes up to 75 million autistic people worldwide through medical applications viewing autism as a deficit of neurotypical social skills rather than an aspect of human diversity, and this perspective is grounded in research questioning the humanity of autistic people. Turing defined artificial intelligence as the ability to mimic human communication, and as AI development increasingly focuses on human-like agents, this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as datasets and systems that move away from mimicking humanness as a benchmark for machine intelligence. Then, we explore the origins, prevalence, and impact of anti-autistic biases in current research. Our work finds that 90% of human-like AI agents exclude autistic perspectives, and AI creators continue to believe ethical considerations are beyond the scope of their work. To improve the autistic representation in data, we conduct empirical experiments with annotators and LLMs, finding that binary labeling schemes sufficiently capture the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can be used to evaluate or fine-tune models, and was developed to serve as a foundation for more neuro-inclusive future work.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which symbol grounding problem should we try to solve?</title>
<link>https://arxiv.org/abs/2507.21080</link>
<guid>https://arxiv.org/abs/2507.21080</guid>
<content:encoded><![CDATA[
arXiv:2507.21080v1 Announce Type: cross 
Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empathy in Explanation</title>
<link>https://arxiv.org/abs/2507.21081</link>
<guid>https://arxiv.org/abs/2507.21081</guid>
<content:encoded><![CDATA[
arXiv:2507.21081v1 Announce Type: cross 
Abstract: Why do we give the explanations we do? Recent work has suggested that we should think of explanation as a kind of cooperative social interaction, between a why-question-asker and an explainer. Here, we apply this perspective to consider the role that emotion plays in this social interaction. We develop a computational framework for modeling explainers who consider the emotional impact an explanation might have on a listener. We test our framework by using it to model human intuitions about how a doctor might explain to a patient why they have a disease, taking into account the patient's propensity for regret. Our model predicts human intuitions well, better than emotion-agnostic ablations, suggesting that people do indeed reason about emotion when giving explanations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</title>
<link>https://arxiv.org/abs/2507.21083</link>
<guid>https://arxiv.org/abs/2507.21083</guid>
<content:encoded><![CDATA[
arXiv:2507.21083v1 Announce Type: cross 
Abstract: Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?</title>
<link>https://arxiv.org/abs/2507.21090</link>
<guid>https://arxiv.org/abs/2507.21090</guid>
<content:encoded><![CDATA[
arXiv:2507.21090v1 Announce Type: cross 
Abstract: As AI systems shape individual and societal decisions, fostering critical AI literacy is essential. Traditional approaches, such as blog articles, static lessons, and social media discussions, often fail to support deep conceptual understanding and critical engagement. This study examines whether interactive simulations can help learners think like a scientist by engaging them in hypothesis testing, experimentation, and direct observation of AI behavior. In a controlled study with 605 participants, we assess how interactive AI tutorials impact learning of key concepts such as fairness, dataset representativeness, and bias in language models. Results show that interactive simulations effectively enhance AI literacy across topics, supporting greater knowledge transfer and self-reported confidence, though engagement alone does not predict learning. This work contributes to the growing field of AI literacy education, highlighting how interactive, inquiry-driven methodologies can better equip individuals to critically engage with AI in their daily lives.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment</title>
<link>https://arxiv.org/abs/2507.21091</link>
<guid>https://arxiv.org/abs/2507.21091</guid>
<content:encoded><![CDATA[
arXiv:2507.21091v1 Announce Type: cross 
Abstract: Conversational agents (CAs) based on generative artificial intelligence frequently face challenges ensuring ethical interactions that align with human values. Current value alignment efforts largely rely on top-down approaches, such as technical guidelines or legal value principles. However, these methods tend to be disconnected from the specific contexts in which CAs operate, potentially leading to misalignment with users interests. To address this challenge, we propose a novel, bottom-up approach to value alignment, utilizing the value ontology of the ISO Value-Based Engineering standard for ethical IT design. We analyse 593 ethically sensitive system outputs identified from 16,908 conversational logs of a major European employment service CA to identify core values and instances of value misalignment within real-world interactions. The results revealed nine core values and 32 different value misalignments that negatively impacted users. Our findings provide actionable insights for CA providers seeking to address ethical challenges and achieve more context-sensitive value alignment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling</title>
<link>https://arxiv.org/abs/2507.21100</link>
<guid>https://arxiv.org/abs/2507.21100</guid>
<content:encoded><![CDATA[
arXiv:2507.21100v1 Announce Type: cross 
Abstract: This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Ecological Impact of AI</title>
<link>https://arxiv.org/abs/2507.21102</link>
<guid>https://arxiv.org/abs/2507.21102</guid>
<content:encoded><![CDATA[
arXiv:2507.21102v1 Announce Type: cross 
Abstract: Philosophers of technology have recently started paying more attention to the environmental impacts of AI, in particular of large language models (LLMs) and generative AI (genAI) applications. Meanwhile, few developers of AI give concrete estimates of the ecological impact of their models and products, and even when they do so, their analysis is often limited to green house gas emissions of certain stages of AI development or use. The current proposal encourages practically viable analyses of the sustainability aspects of genAI informed by philosophical ideas.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iLSU-T: an Open Dataset for Uruguayan Sign Language Translation</title>
<link>https://arxiv.org/abs/2507.21104</link>
<guid>https://arxiv.org/abs/2507.21104</guid>
<content:encoded><![CDATA[
arXiv:2507.21104v1 Announce Type: cross 
Abstract: Automatic sign language translation has gained particular interest in the computer vision and computational linguistics communities in recent years. Given each sign language country particularities, machine translation requires local data to develop new techniques and adapt existing ones. This work presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB videos with audio and text transcriptions. This type of multimodal and curated data is paramount for developing novel approaches to understand or generate tools for sign language processing. iLSU T comprises more than 185 hours of interpreted sign language videos from public TV broadcasting. It covers diverse topics and includes the participation of 18 professional interpreters of sign language. A series of experiments using three state of the art translation algorithms is presented. The aim is to establish a baseline for this dataset and evaluate its usefulness and the proposed pipeline for data processing. The experiments highlight the need for more localized datasets for sign language translation and understanding, which are critical for developing novel tools to improve accessibility and inclusion of all individuals. Our data and code can be accessed.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis</title>
<link>https://arxiv.org/abs/2507.21105</link>
<guid>https://arxiv.org/abs/2507.21105</guid>
<content:encoded><![CDATA[
arXiv:2507.21105v1 Announce Type: cross 
Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI), especially integrated with Large Language Models (LLMs), has greatly facilitated the resolution of complex tasks. However, current systems are still facing challenges of inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Most recently, the Model Context Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by Google have been introduced, and to the best of our knowledge, very few applications exist where both protocols are employed within a single MAS framework. We present a pilot study of AgentMaster, a novel modular multi-protocol MAS framework with self-implemented A2A and MCP, enabling dynamic coordination and flexible communication. Through a unified conversational interface, the system supports natural language interaction without prior technical expertise and responds to multimodal queries for tasks including information retrieval, question answering, and image analysis. Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged 96.3\% and 87.1\%, revealing robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific, relevant responses. Overall, our proposed framework contributes to the potential capabilities of domain-specific, cooperative, and scalable conversational AI powered by MAS.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams</title>
<link>https://arxiv.org/abs/2507.21107</link>
<guid>https://arxiv.org/abs/2507.21107</guid>
<content:encoded><![CDATA[
arXiv:2507.21107v1 Announce Type: cross 
Abstract: We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Classification Tasks and Approaches for Legal Contracts</title>
<link>https://arxiv.org/abs/2507.21108</link>
<guid>https://arxiv.org/abs/2507.21108</guid>
<content:encoded><![CDATA[
arXiv:2507.21108v1 Announce Type: cross 
Abstract: Given the large size and volumes of contracts and their underlying inherent complexity, manual reviews become inefficient and prone to errors, creating a clear need for automation. Automatic Legal Contract Classification (LCC) revolutionizes the way legal contracts are analyzed, offering substantial improvements in speed, accuracy, and accessibility. This survey delves into the challenges of automatic LCC and a detailed examination of key tasks, datasets, and methodologies. We identify seven classification tasks within LCC, and review fourteen datasets related to English-language contracts, including public, proprietary, and non-public sources. We also introduce a methodology taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Additionally, the survey discusses evaluation techniques and highlights the best-performing results from the reviewed studies. By providing a thorough overview of current methods and their limitations, this survey suggests future research directions to improve the efficiency, accuracy, and scalability of LCC. As the first comprehensive survey on LCC, it aims to support legal NLP researchers and practitioners in improving legal processes, making legal information more accessible, and promoting a more informed and equitable society.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students</title>
<link>https://arxiv.org/abs/2507.21109</link>
<guid>https://arxiv.org/abs/2507.21109</guid>
<content:encoded><![CDATA[
arXiv:2507.21109v1 Announce Type: cross 
Abstract: Deep Neural Networks often suffer from a critical limitation known as Catastrophic Forgetting, where performance on past tasks degrades after learning new ones. This paper introduces a novel continual learning approach inspired by human learning strategies like Active Recall, Deliberate Practice and Spaced Repetition, named Task Focused Consolidation with Spaced Recall (TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we termed the Active Recall Probe. It is a periodic, task-aware evaluation of the model's memory that stabilizes the representations of past knowledge. We test TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading regularization-based and replay-based baselines. Our results show that TFC-SR performs significantly better than these methods. For instance, on the Split CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's 7.40%. We demonstrate that this advantage comes from the stabilizing effect of the probe itself, and not from the difference in replay volume. Additionally, we analyze the trade-off between memory size and performance and show that while TFC-SR performs better in memory-constrained environments, higher replay volume is still more effective when available memory is abundant. We conclude that TFC-SR is a robust and efficient approach, highlighting the importance of integrating active memory retrieval mechanisms into continual learning systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering</title>
<link>https://arxiv.org/abs/2507.21110</link>
<guid>https://arxiv.org/abs/2507.21110</guid>
<content:encoded><![CDATA[
arXiv:2507.21110v1 Announce Type: cross 
Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Formal Rebuttal of "The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability"</title>
<link>https://arxiv.org/abs/2507.21111</link>
<guid>https://arxiv.org/abs/2507.21111</guid>
<content:encoded><![CDATA[
arXiv:2507.21111v1 Announce Type: cross 
Abstract: This paper presents a comprehensive refutation of the so-called "blockchain trilemma," a widely cited but formally ungrounded claim asserting an inherent trade-off between decentralisation, security, and scalability in blockchain protocols. Through formal analysis, empirical evidence, and detailed critique of both methodology and terminology, we demonstrate that the trilemma rests on semantic equivocation, misuse of distributed systems theory, and a failure to define operational metrics. Particular focus is placed on the conflation of topological network analogies with protocol-level architecture, the mischaracterisation of Bitcoin's design--including the role of miners, SPV clients, and header-based verification--and the failure to ground claims in complexity-theoretic or adversarial models. By reconstructing Bitcoin as a deterministic, stateless distribution protocol governed by evidentiary trust, we show that scalability is not a trade-off but an engineering outcome. The paper concludes by identifying systemic issues in academic discourse and peer review that have allowed such fallacies to persist, and offers formal criteria for evaluating future claims in blockchain research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Page image classification for content-specific data processing</title>
<link>https://arxiv.org/abs/2507.21114</link>
<guid>https://arxiv.org/abs/2507.21114</guid>
<content:encoded><![CDATA[
arXiv:2507.21114v1 Announce Type: cross 
Abstract: Digitization projects in humanities often generate vast quantities of page images from historical documents, presenting significant challenges for manual sorting and analysis. These archives contain diverse content, including various text types (handwritten, typed, printed), graphical elements (drawings, maps, photos), and layouts (plain text, tables, forms). Efficiently processing this heterogeneous data requires automated methods to categorize pages based on their content, enabling tailored downstream analysis pipelines. This project addresses this need by developing and evaluating an image classification system specifically designed for historical document pages, leveraging advancements in artificial intelligence and machine learning. The set of categories was chosen to facilitate content-specific processing workflows, separating pages requiring different analysis techniques (e.g., OCR for text, image analysis for graphics)
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedFlex: Federated Learning for Diverse Netflix Recommendations</title>
<link>https://arxiv.org/abs/2507.21115</link>
<guid>https://arxiv.org/abs/2507.21115</guid>
<content:encoded><![CDATA[
arXiv:2507.21115v1 Announce Type: cross 
Abstract: Federated learning is a decentralized approach that enables collaborative model training across multiple devices while preserving data privacy. It has shown significant potential in various domains, including healthcare and personalized recommendation systems. However, most existing work on federated recommendation systems has focused primarily on improving accuracy, with limited attention to fairness and diversity. In this paper, we introduce FedFlex, a federated recommender system for Netflix-style TV series recommendations. FedFlex integrates two state-of-the-art matrix factorization algorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal Relevance (MMR) to re-rank items and enhance diversity. We conduct extensive experiments comparing recommendations generated by SVD and BPR algorithms. In a live two-week user study, participants received two recommendation lists: List A, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity. Participants were asked to click on the movies they were interested in watching. Our findings demonstrate that FedFlex effectively introduces diverse content, such as new genres, into recommendations without necessarily compromising user satisfaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges</title>
<link>https://arxiv.org/abs/2507.21117</link>
<guid>https://arxiv.org/abs/2507.21117</guid>
<content:encoded><![CDATA[
arXiv:2507.21117v1 Announce Type: cross 
Abstract: Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach</title>
<link>https://arxiv.org/abs/2507.21118</link>
<guid>https://arxiv.org/abs/2507.21118</guid>
<content:encoded><![CDATA[
arXiv:2507.21118v1 Announce Type: cross 
Abstract: MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation</title>
<link>https://arxiv.org/abs/2507.21120</link>
<guid>https://arxiv.org/abs/2507.21120</guid>
<content:encoded><![CDATA[
arXiv:2507.21120v1 Announce Type: cross 
Abstract: Art Therapy (AT) is an established practice that facilitates emotional processing and recovery through creative expression. Recently, Visual Art Recommender Systems (VA RecSys) have emerged to support AT, demonstrating their potential by personalizing therapeutic artwork recommendations. Nonetheless, current VA RecSys rely on visual stimuli for user modeling, limiting their ability to capture the full spectrum of emotional responses during preference elicitation. Previous studies have shown that music stimuli elicit unique affective reflections, presenting an opportunity for cross-domain recommendation (CDR) to enhance personalization in AT. Since CDR has not yet been explored in this context, we propose a family of CDR methods for AT based on music-driven preference elicitation. A large-scale study with 200 users demonstrates the efficacy of music-driven preference elicitation, outperforming the classic visual-only elicitation approach. Our source code, data, and models are available at https://github.com/ArtAICare/Affect-aware-CDR
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization</title>
<link>https://arxiv.org/abs/2507.21124</link>
<guid>https://arxiv.org/abs/2507.21124</guid>
<content:encoded><![CDATA[
arXiv:2507.21124v1 Announce Type: cross 
Abstract: We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities--such as threshold-based filtering, slice extraction, and statistical analysis--through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system's adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., ``visualize the skull"). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline</title>
<link>https://arxiv.org/abs/2507.21125</link>
<guid>https://arxiv.org/abs/2507.21125</guid>
<content:encoded><![CDATA[
arXiv:2507.21125v1 Announce Type: cross 
Abstract: In an era of radical technology transformations, technology maps play a crucial role in enhancing decision making. These maps heavily rely on automated methods of technology extraction. This paper introduces Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for automated technology extraction from scientific literature. RATE combines Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation. This hybrid method results in high recall in candidate generation alongside with high precision in candidate filtering. While the pipeline is designed to be general and widely applicable, we demonstrate its use on 678 research articles focused on Brain-Computer Interfaces (BCIs) and Extended Reality (XR) as a case study. Consequently, The validated technology terms by RATE were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape. For the purpose of evaluation, a gold standard dataset of technologies in 70 selected random articles had been curated by the experts. In addition, a technology extraction model based on Bidirectional Encoder Representations of Transformers (BERT) was used as a comparative method. RATE achieved F1-score of 91.27%, Significantly outperforming BERT with F1-score of 53.73%. Our findings highlight the promise of definition-driven LLM methods for technology extraction and mapping. They also offer new insights into emerging trends within the BCI-XR field. The source code is available https://github.com/AryaAftab/RATE
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities</title>
<link>https://arxiv.org/abs/2507.21133</link>
<guid>https://arxiv.org/abs/2507.21133</guid>
<content:encoded><![CDATA[
arXiv:2507.21133v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate complex responses to threat-based manipulations, revealing both vulnerabilities and unexpected performance enhancement opportunities. This study presents a comprehensive analysis of 3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini) across 10 task domains under 6 threat conditions. We introduce a novel threat taxonomy and multi-metric evaluation framework to quantify both negative manipulation effects and positive performance improvements. Results reveal systematic vulnerabilities, with policy evaluation showing the highest metric significance rates under role-based threats, alongside substantial performance enhancements in numerous cases with effect sizes up to +1336%. Statistical analysis indicates systematic certainty manipulation (pFDR < 0.0001) and significant improvements in analytical depth and response quality. These findings have dual implications for AI safety and practical prompt engineering in high-stakes applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning</title>
<link>https://arxiv.org/abs/2507.21136</link>
<guid>https://arxiv.org/abs/2507.21136</guid>
<content:encoded><![CDATA[
arXiv:2507.21136v1 Announce Type: cross 
Abstract: Unsupervised and supervised learning methods conventionally use kernels to capture nonlinearities inherent in data structure. However experts have to ensure their proposed nonlinearity maximizes variability and capture inherent diversity of data. We reviewed all independence criteria to design unsupervised learners. Then we proposed 3 independence criteria and used them to design unsupervised and supervised dimensionality reduction methods. We evaluated contrast, accuracy and interpretability of these methods in both linear and neural nonlinear settings. The results show that the methods have outperformed the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and layer sharing) and opened a new line of interpretable machine learning (ML) for the researchers.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTS-1 Technical Report</title>
<link>https://arxiv.org/abs/2507.21138</link>
<guid>https://arxiv.org/abs/2507.21138</guid>
<content:encoded><![CDATA[
arXiv:2507.21138v1 Announce Type: cross 
Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Artifact ConnecTor (PACT): Embedding Enterprise Artifacts for Compliance AI Agents</title>
<link>https://arxiv.org/abs/2507.21142</link>
<guid>https://arxiv.org/abs/2507.21142</guid>
<content:encoded><![CDATA[
arXiv:2507.21142v1 Announce Type: cross 
Abstract: Enterprise environments contain a heterogeneous, rapidly growing collection of internal artifacts related to code, data, and many different tools. Critical information for assessing privacy risk and ensuring regulatory compliance is often embedded across these varied resources, each with their own arcane discovery and extraction techniques. Therefore, large-scale privacy compliance in adherence to governmental regulations requires systems to discern the interconnected nature of diverse artifacts in a common, shared universe.
  We present Privacy Artifact ConnecT or (PACT), an embeddings-driven graph that links millions of artifacts spanning multiple artifact types generated by a variety of teams and projects. Powered by the state-of-the-art DRAGON embedding model, PACT uses a contrastive learning objective with light fine-tuning to link artifacts via their textual components such as raw metadata, ownership specifics, and compliance context. Experimental results show that PACT's fine-tuned model improves recall@1 from 18% to 53%, the query match rate from 9.6% to 69.7% when paired with a baseline AI agent, and the hitrate@1 from 25.7% to 44.9% for candidate selection in a standard recommender system.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unifying Quantitative Security Benchmarking for Multi Agent Systems</title>
<link>https://arxiv.org/abs/2507.21146</link>
<guid>https://arxiv.org/abs/2507.21146</guid>
<content:encoded><![CDATA[
arXiv:2507.21146v1 Announce Type: cross 
Abstract: Evolving AI systems increasingly deploy multi-agent architectures where autonomous agents collaborate, share information, and delegate tasks through developing protocols. This connectivity, while powerful, introduces novel security risks. One such risk is a cascading risk: a breach in one agent can cascade through the system, compromising others by exploiting inter-agent trust. In tandem with OWASP's initiative for an Agentic AI Vulnerability Scoring System we define an attack vector, Agent Cascading Injection, analogous to Agent Impact Chain and Blast Radius, operating across networks of agents. In an ACI attack, a malicious input or tool exploit injected at one agent leads to cascading compromises and amplified downstream effects across agents that trust its outputs. We formalize this attack with an adversarial goal equation and key variables (compromised agent, injected exploit, polluted observations, etc.), capturing how a localized vulnerability can escalate into system-wide failure. We then analyze ACI's properties -- propagation chains, amplification factors, and inter-agent compound effects -- and map these to OWASP's emerging Agentic AI risk categories (e.g. Impact Chain and Orchestration Exploits). Finally, we argue that ACI highlights a critical need for quantitative benchmarking frameworks to evaluate the security of agent-to-agent communication protocols. We outline a methodology for stress-testing multi-agent systems (using architectures such as Google's A2A and Anthropic's MCP) against cascading trust failures, developing upon groundwork for measurable, standardized agent-to-agent security evaluation. Our work provides the necessary apparatus for engineers to benchmark system resilience, make data-driven architectural trade-offs, and develop robust defenses against a new generation of agentic threats.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning</title>
<link>https://arxiv.org/abs/2507.21147</link>
<guid>https://arxiv.org/abs/2507.21147</guid>
<content:encoded><![CDATA[
arXiv:2507.21147v1 Announce Type: cross 
Abstract: Wildfires significantly impact natural ecosystems and human health, leading to biodiversity loss, increased hydrogeological risks, and elevated emissions of toxic substances. Climate change exacerbates these effects, particularly in regions with rising temperatures and prolonged dry periods, such as the Mediterranean. This requires the development of advanced risk management strategies that utilize state-of-the-art technologies. However, in this context, the data show a bias toward an imbalanced setting, where the incidence of wildfire events is significantly lower than typical situations. This imbalance, coupled with the inherent complexity of high-dimensional spatio-temporal data, poses significant challenges for training deep learning architectures. Moreover, since precise wildfire predictions depend mainly on weather data, finding a way to reduce computational costs to enable more frequent updates using the latest weather forecasts would be beneficial. This paper investigates how adopting a contrastive framework can address these challenges through enhanced latent representations for the patch's dynamic features. We thus introduce a new morphology-based curriculum contrastive learning that mitigates issues associated with diverse regional characteristics and enables the use of smaller patch sizes without compromising performance. An experimental analysis is performed to validate the effectiveness of the proposed modeling strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unfolding for MIMO Signal Detection</title>
<link>https://arxiv.org/abs/2507.21152</link>
<guid>https://arxiv.org/abs/2507.21152</guid>
<content:encoded><![CDATA[
arXiv:2507.21152v1 Announce Type: cross 
Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables efficient, interpretable, and low-complexity MIMO signal detection. Unlike prior approaches that rely on real-valued approximations, our method operates natively in the complex domain, aligning with the fundamental nature of signal processing tasks. The proposed algorithm requires only a small number of trainable parameters, allowing for simplified training. Numerical results demonstrate that the proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers</title>
<link>https://arxiv.org/abs/2507.21153</link>
<guid>https://arxiv.org/abs/2507.21153</guid>
<content:encoded><![CDATA[
arXiv:2507.21153v1 Announce Type: cross 
Abstract: This paper explores the implementation of a Deep Reinforcement Learning (DRL)-optimized energy management system for e-commerce data centers, aimed at enhancing energy efficiency, cost-effectiveness, and environmental sustainability. The proposed system leverages DRL algorithms to dynamically manage the integration of renewable energy sources, energy storage, and grid power, adapting to fluctuating energy availability in real time. The study demonstrates that the DRL-optimized system achieves a 38\% reduction in energy costs, significantly outperforming traditional Reinforcement Learning (RL) methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for heuristic methods. The DRL-optimized approach also results in an 82\% improvement in energy efficiency, surpassing other methods, and a 45\% reduction in carbon emissions, making it the most environmentally friendly solution. The system's cumulative reward of 950 reflects its superior performance in balancing multiple objectives. Through rigorous testing and ablation studies, the paper validates the effectiveness of the DRL model's architecture and parameters, offering a robust solution for energy management in data centers. The findings highlight the potential of DRL in advancing energy optimization strategies and addressing sustainability challenges.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Out-of-Distribution Data: A Survey</title>
<link>https://arxiv.org/abs/2507.21160</link>
<guid>https://arxiv.org/abs/2507.21160</guid>
<content:encoded><![CDATA[
arXiv:2507.21160v1 Announce Type: cross 
Abstract: In the field of Machine Learning (ML) and data-driven applications, one of the significant challenge is the change in data distribution between the training and deployment stages, commonly known as distribution shift. This paper outlines different mechanisms for handling two main types of distribution shifts: (i) Covariate shift: where the value of features or covariates change between train and test data, and (ii) Concept/Semantic-shift: where model experiences shift in the concept learned during training due to emergence of novel classes in the test phase. We sum up our contributions in three folds. First, we formalize distribution shifts, recite on how the conventional method fails to handle them adequately and urge for a model that can simultaneously perform better in all types of distribution shifts. Second, we discuss why handling distribution shifts is important and provide an extensive review of the methods and techniques that have been developed to detect, measure, and mitigate the effects of these shifts. Third, we discuss the current state of distribution shift handling mechanisms and propose future research directions in this area. Overall, we provide a retrospective synopsis of the literature in the distribution shift, focusing on OOD data that had been overlooked in the existing surveys.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues</title>
<link>https://arxiv.org/abs/2507.21161</link>
<guid>https://arxiv.org/abs/2507.21161</guid>
<content:encoded><![CDATA[
arXiv:2507.21161v1 Announce Type: cross 
Abstract: Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. In contrast to GPT-4V based methods that operate on discrete frames, BF-PIP processes uninterrupted temporal clips. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts. Without any additional training, BF-PIP achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate that combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Adversarial Point Clouds Using Diffusion Model</title>
<link>https://arxiv.org/abs/2507.21163</link>
<guid>https://arxiv.org/abs/2507.21163</guid>
<content:encoded><![CDATA[
arXiv:2507.21163v1 Announce Type: cross 
Abstract: Adversarial attack methods for 3D point cloud classification reveal the vulnerabilities of point cloud recognition models. This vulnerability could lead to safety risks in critical applications that use deep learning models, such as autonomous vehicles. To uncover the deficiencies of these models, researchers can evaluate their security through adversarial attacks. However, most existing adversarial attack methods are based on white-box attacks. While these methods achieve high attack success rates and imperceptibility, their applicability in real-world scenarios is limited. Black-box attacks, which are more meaningful in real-world scenarios, often yield poor results. This paper proposes a novel black-box adversarial example generation method that utilizes a diffusion model to improve the attack success rate and imperceptibility in the black-box setting, without relying on the internal information of the point cloud classification model to generate adversarial samples. We use a 3D diffusion model to use the compressed features of the point cloud as prior knowledge to guide the reverse diffusion process to add adversarial points to clean examples. Subsequently, its reverse process is employed to transform the distribution of other categories into adversarial points, which are then added to the point cloud.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.21164</link>
<guid>https://arxiv.org/abs/2507.21164</guid>
<content:encoded><![CDATA[
arXiv:2507.21164v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled data, a necessity in many machine learning applications where anomalous samples are rare or not available. Most state-of-the-art methods fall into two categories: reconstruction-based approaches, which often reconstruct anomalies too well, and decoupled representation learning with density estimators, which can suffer from suboptimal feature spaces. While some recent methods attempt to couple feature learning and anomaly detection, they often rely on surrogate objectives, restrict kernel choices, or introduce approximations that limit their expressiveness and robustness. To address this challenge, we propose a novel method that tightly couples representation learning with an analytically solvable one-class SVM (OCSVM), through a custom loss formulation that directly aligns latent features with the OCSVM decision boundary. The model is evaluated on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI subtle lesion detection task. Unlike most methods that focus on large, hyperintense lesions at the image level, our approach succeeds to target small, non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a more clinically relevant scenario. Both experiments evaluate a form of robustness to domain shifts, including corruption types in MNIST-C and scanner/age variations in MRI. Results demonstrate performance and robustness of our proposed mode,highlighting its potential for general UAD and real-world medical imaging applications. The source code is available at https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation</title>
<link>https://arxiv.org/abs/2507.21166</link>
<guid>https://arxiv.org/abs/2507.21166</guid>
<content:encoded><![CDATA[
arXiv:2507.21166v1 Announce Type: cross 
Abstract: Progress in complex reasoning is constrained by the static nature of the current training datasets. We propose structured interaction as a new scaling axis, moving beyond the prevailing paradigm of increasing model parameters. Our self-evolving framework, AGORA, enables a collaborative ensemble to achieve reasoning performance exceeding state-of-the-art monolithic systems by up to 4.45 percentage points on challenging mathematical benchmarks. This gain stems from group emergent ability-the synthesis of collective capabilities unattainable by isolated models, validating interaction as a scalable driver of intelligence. Our results position the engineering of collaborative ecosystems as a vital frontier for capability emergence.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions</title>
<link>https://arxiv.org/abs/2507.21167</link>
<guid>https://arxiv.org/abs/2507.21167</guid>
<content:encoded><![CDATA[
arXiv:2507.21167v1 Announce Type: cross 
Abstract: Charts are a fundamental visualization format widely used in data analysis across research and industry. While enabling users to edit charts based on high-level intentions is of great practical value, existing methods primarily rely on natural language instructions, which are often too ambiguous to support fine-grained editing. In this work, we introduce a novel paradigm for multimodal chart editing, where user intent is expressed through a combination of natural language and visual indicators that explicitly highlight the elements to be modified. To support this paradigm, we present Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$ contains 1,000 samples spanning four levels of editing difficulty. Each sample includes triplets in the form of (chart, code, multimodal instructions). To comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides metrics that assess both visual appearance and code correctness. Our benchmark reveals significant limitations in current multimodal large language models (MLLMs), including GPT-4o, particularly in their ability to interpret and act on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a large-scale training set with 24,000 multimodal chart editing samples. Fine-tuning MLLMs on this dataset leads to substantial improvements, demonstrating the importance of multimodal supervision in building practical chart editing systems. Our datasets, codes, and evaluation tools are available at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our datasets, codes, and evaluation tools are available at https://github.com/yaolinli/VCE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question</title>
<link>https://arxiv.org/abs/2507.21168</link>
<guid>https://arxiv.org/abs/2507.21168</guid>
<content:encoded><![CDATA[
arXiv:2507.21168v1 Announce Type: cross 
Abstract: Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy AI: UK Air Traffic Control Revisited</title>
<link>https://arxiv.org/abs/2507.21169</link>
<guid>https://arxiv.org/abs/2507.21169</guid>
<content:encoded><![CDATA[
arXiv:2507.21169v1 Announce Type: cross 
Abstract: Exploring the socio-technical challenges confronting the adoption of AI in organisational settings is something that has so far been largely absent from the related literature. In particular, research into requirements for trustworthy AI typically overlooks how people deal with the problems of trust in the tools that they use as part of their everyday work practices. This article presents some findings from an ongoing ethnographic study of how current tools are used in air traffic control work and what it reveals about requirements for trustworthy AI in air traffic control and other safety-critical application domains.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneShield -- the Next Generation of LLM Guardrails</title>
<link>https://arxiv.org/abs/2507.21170</link>
<guid>https://arxiv.org/abs/2507.21170</guid>
<content:encoded><![CDATA[
arXiv:2507.21170v1 Announce Type: cross 
Abstract: The rise of Large Language Models has created a general excitement about the great potential for a myriad of applications. While LLMs offer many possibilities, questions about safety, privacy, and ethics have emerged, and all the key actors are working to address these issues with protective measures for their own models and standalone solutions. The constantly evolving nature of LLMs makes the task of universally shielding users against their potential risks extremely challenging, and one-size-fits-all solutions unfeasible. In this work, we propose OneShield, our stand-alone, model-agnostic and customizable solution to safeguard LLMs. OneShield aims to provide facilities for defining risk factors, expressing and declaring contextual safety and compliance policies, and mitigating LLM risks, with a focus on each specific customer. We describe the implementation of the framework, the scalability considerations and provide usage statistics of OneShield since its first deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A ChatGPT-based approach for questions generation in higher education</title>
<link>https://arxiv.org/abs/2507.21174</link>
<guid>https://arxiv.org/abs/2507.21174</guid>
<content:encoded><![CDATA[
arXiv:2507.21174v1 Announce Type: cross 
Abstract: Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Adapted Interpretation Framework for Machine Learning Models</title>
<link>https://arxiv.org/abs/2507.21179</link>
<guid>https://arxiv.org/abs/2507.21179</guid>
<content:encoded><![CDATA[
arXiv:2507.21179v1 Announce Type: cross 
Abstract: Background & Aims: High-performance machine learning models like XGBoost are often "black boxes," limiting their clinical adoption due to a lack of interpretability. This study aims to bridge the gap between predictive accuracy and narrative transparency for sarcopenia risk assessment. Methods: We propose the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge distillation architecture. LAI-ML transforms feature attributions from a trained XGBoost model into a probabilistic format using specialized techniques (HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement learning loop and case-based retrieval, then generates data-faithful diagnostic narratives. Results: The LAI-ML framework achieved 83% prediction accuracy, significantly outperforming the baseline XGBoost model, 13% higher. Notably, the LLM not only replicated the teacher model's logic but also corrected its predictions in 21.7% of discordant cases, demonstrating enhanced reasoning. Conclusion: LAI-ML effectively translates opaque model predictions into trustworthy and interpretable clinical insights, offering a deployable solution to the "black-box" problem in medical AI.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDD: Self-Degraded Defense against Malicious Fine-tuning</title>
<link>https://arxiv.org/abs/2507.21182</link>
<guid>https://arxiv.org/abs/2507.21182</guid>
<content:encoded><![CDATA[
arXiv:2507.21182v1 Announce Type: cross 
Abstract: Open-source Large Language Models (LLMs) often employ safety alignment methods to resist harmful instructions. However, recent research shows that maliciously fine-tuning these LLMs on harmful data can easily bypass these safeguards. To counter this, we theoretically uncover why malicious fine-tuning succeeds and identify potential defense strategies. Building on the theoretical analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD encourages LLMs to produce high-quality but irrelevant responses to harmful prompts. When attackers attempt malicious fine-tuning, the general capability of the LLM aligned by SDD will significantly decrease, rendering it incapable of following harmful instructions. Our experimental results confirm SDD's effectiveness against such attacks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
arXiv:2507.21183v1 Announce Type: cross 
Abstract: As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v1 Announce Type: cross 
Abstract: Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers</title>
<link>https://arxiv.org/abs/2507.21186</link>
<guid>https://arxiv.org/abs/2507.21186</guid>
<content:encoded><![CDATA[
arXiv:2507.21186v1 Announce Type: cross 
Abstract: Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs</title>
<link>https://arxiv.org/abs/2507.21188</link>
<guid>https://arxiv.org/abs/2507.21188</guid>
<content:encoded><![CDATA[
arXiv:2507.21188v1 Announce Type: cross 
Abstract: LLMs for clinical decision support often fail under small but clinically meaningful input shifts such as masking a symptom or negating a finding, despite high performance on static benchmarks. These reasoning failures frequently go undetected by standard NLP metrics, which are insensitive to latent representation shifts that drive diagnosis instability. We propose a geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation Diagnostics), which systematically probes the latent robustness of clinical LLMs under structured adversarial edits. Within this framework, we introduce Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that captures representational instability when embeddings cross decision boundaries in PCA-reduced latent space. Clinical notes are generated using a structured prompting pipeline grounded in diagnostic reasoning, then perturbed along four axes: masking, negation, synonym replacement, and numeric variation to simulate common ambiguities and omissions. We compute LDFR across both foundation and clinical LLMs, finding that latent fragility emerges even under minimal surface-level changes. Finally, we validate our findings on 90 real clinical notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of LDFR beyond synthetic settings. Our results reveal a persistent gap between surface robustness and semantic stability, underscoring the importance of geometry-aware auditing in safety-critical clinical AI.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models</title>
<link>https://arxiv.org/abs/2507.21195</link>
<guid>https://arxiv.org/abs/2507.21195</guid>
<content:encoded><![CDATA[
arXiv:2507.21195v1 Announce Type: cross 
Abstract: The great success of the diffusion model in image synthesis led to the release of gigantic commercial models, raising the issue of copyright protection and inappropriate content generation. Training-free diffusion watermarking provides a low-cost solution for these issues. However, the prior works remain vulnerable to rotation, scaling, and translation (RST) attacks. Although some methods employ meticulously designed patterns to mitigate this issue, they often reduce watermark capacity, which can result in identity (ID) collusion. To address these problems, we propose MaXsive, a training-free diffusion model generative watermarking technique that has high capacity and robustness. MaXsive best utilizes the initial noise to watermark the diffusion model. Moreover, instead of using a meticulously repetitive ring pattern, we propose injecting the X-shape template to recover the RST distortions. This design significantly increases robustness without losing any capacity, making ID collusion less likely to happen. The effectiveness of MaXsive has been verified on two well-known watermarking benchmarks under the scenarios of verification and identification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks</title>
<link>https://arxiv.org/abs/2507.21196</link>
<guid>https://arxiv.org/abs/2507.21196</guid>
<content:encoded><![CDATA[
arXiv:2507.21196v1 Announce Type: cross 
Abstract: We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework that integrates digital twin simulations and generative AI-driven scenario training to significantly enhance edge intelligence in military networks. EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized with real-world edge devices, to provide a secure, realistic environment for training and validation. Leveraging generative AI methods, such as diffusion models and transformers, the system creates diverse and adversarial scenarios for robust simulation-based agent training. Our multi-layer architecture includes: (1) on-device edge intelligence; (2) digital twin synchronization; and (3) generative scenario training. Experimental simulations demonstrate notable improvements over EdgeAgentX, including faster learning convergence, higher network throughput, reduced latency, and improved resilience against jamming and node failures. A case study involving a complex tactical scenario with simultaneous jamming attacks, agent failures, and increased network loads illustrates how EdgeAgentX-DT sustains operational performance, whereas baseline methods fail. These results highlight the potential of digital-twin-enabled generative training to strengthen edge AI deployments in contested environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Gradient Inversion Risks in Practical Language Model Training</title>
<link>https://arxiv.org/abs/2507.21198</link>
<guid>https://arxiv.org/abs/2507.21198</guid>
<content:encoded><![CDATA[
arXiv:2507.21198v1 Announce Type: cross 
Abstract: The gradient inversion attack has been demonstrated as a significant privacy threat to federated learning (FL), particularly in continuous domains such as vision models. In contrast, it is often considered less effective or highly dependent on impractical training settings when applied to language models, due to the challenges posed by the discrete nature of tokens in text data. As a result, its potential privacy threats remain largely underestimated, despite FL being an emerging training method for language models. In this work, we propose a domain-specific gradient inversion attack named Grab (gradient inversion with hybrid optimization). Grab features two alternating optimization processes to address the challenges caused by practical training settings, including a simultaneous optimization on dropout masks between layers for improved token recovery and a discrete optimization for effective token sequencing. Grab can recover a significant portion (up to 92.9% recovery rate) of the private training data, outperforming the attack strategy of utilizing discrete optimization with an auxiliary model by notable improvements of up to 28.9% recovery rate in benchmark settings and 48.5% recovery rate in practical settings. Grab provides a valuable step forward in understanding this privacy threat in the emerging FL training mode of language models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications</title>
<link>https://arxiv.org/abs/2507.21199</link>
<guid>https://arxiv.org/abs/2507.21199</guid>
<content:encoded><![CDATA[
arXiv:2507.21199v1 Announce Type: cross 
Abstract: Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each LLM trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional LLM over wireless networks. The two primary challenges include 1) guiding a single LLM to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the LLM in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an LLM to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate LLM composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the LLM to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and communication costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Limited and Imperfect Data</title>
<link>https://arxiv.org/abs/2507.21205</link>
<guid>https://arxiv.org/abs/2507.21205</guid>
<content:encoded><![CDATA[
arXiv:2507.21205v1 Announce Type: cross 
Abstract: The distribution of data in the world (eg, internet, etc.) significantly differs from the well-curated datasets and is often over-populated with samples from common categories. The algorithms designed for well-curated datasets perform suboptimally when used for learning from imperfect datasets with long-tailed imbalances and distribution shifts. To expand the use of deep models, it is essential to overcome the labor-intensive curation process by developing robust algorithms that can learn from diverse, real-world data distributions. Toward this goal, we develop practical algorithms for Deep Neural Networks which can learn from limited and imperfect data present in the real world. This thesis is divided into four segments, each covering a scenario of learning from limited or imperfect data. The first part of the thesis focuses on Learning Generative Models from Long-Tail Data, where we mitigate the mode-collapse and enable diverse aesthetic image generations for tail (minority) classes. In the second part, we enable effective generalization on tail classes through Inductive Regularization schemes, which allow tail classes to generalize as effectively as the head classes without requiring explicit generation of images. In the third part, we develop algorithms for Optimizing Relevant Metrics for learning from long-tailed data with limited annotation (semi-supervised), followed by the fourth part, which focuses on the Efficient Domain Adaptation of the model to various domains with very few to zero labeled samples.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubbleformer: Forecasting Boiling with Transformers</title>
<link>https://arxiv.org/abs/2507.21244</link>
<guid>https://arxiv.org/abs/2507.21244</guid>
<content:encoded><![CDATA[
arXiv:2507.21244v1 Announce Type: cross 
Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Explaining Visual Captioning with Hybrid Markov Logic Networks</title>
<link>https://arxiv.org/abs/2507.21246</link>
<guid>https://arxiv.org/abs/2507.21246</guid>
<content:encoded><![CDATA[
arXiv:2507.21246v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks such as image captioning. However, explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration. In this work, we develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language that can combine symbolic rules with real-valued functions - where we hypothesize how relevant examples from the training data could have influenced the generation of the observed caption. To do this, we learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption. Our experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors</title>
<link>https://arxiv.org/abs/2507.21260</link>
<guid>https://arxiv.org/abs/2507.21260</guid>
<content:encoded><![CDATA[
arXiv:2507.21260v1 Announce Type: cross 
Abstract: In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties</title>
<link>https://arxiv.org/abs/2507.21288</link>
<guid>https://arxiv.org/abs/2507.21288</guid>
<content:encoded><![CDATA[
arXiv:2507.21288v1 Announce Type: cross 
Abstract: Materials used in real clothing exhibit remarkable complexity and spatial variation due to common processes such as stitching, hemming, dyeing, printing, padding, and bonding. Simulating these materials, for instance using finite element methods, is often computationally demanding and slow. Worse, such methods can suffer from numerical artifacts called ``membrane locking'' that makes cloth appear artificially stiff. Here we propose a general framework, called Mass-Spring Net, for learning a simple yet efficient surrogate model that captures the effects of these complex materials using only motion observations. The cloth is discretized into a mass-spring network with unknown material parameters that are learned directly from the motion data, using a novel force-and-impulse loss function. Our approach demonstrates the ability to accurately model spatially varying material properties from a variety of data sources, and immunity to membrane locking which plagues FEM-based simulations. Compared to graph-based networks and neural ODE-based architectures, our method achieves significantly faster training times, higher reconstruction accuracy, and improved generalization to novel dynamic scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Numeration Systems as Dynamical Systems</title>
<link>https://arxiv.org/abs/2507.21295</link>
<guid>https://arxiv.org/abs/2507.21295</guid>
<content:encoded><![CDATA[
arXiv:2507.21295v1 Announce Type: cross 
Abstract: The foundational concepts of semantic numeration systems theory are briefly outlined. The action of cardinal semantic operators unfolds over a set of cardinal abstract entities belonging to the cardinal semantic multeity. The cardinal abstract object (CAO) formed by them in a certain connectivity topology is proposed to be considered as a linear discrete dynamical system with nonlinear control. Under the assumption of ideal observability, the CAO state equations are provided for both stationary and non-stationary cases. The fundamental role of the configuration matrix, which combines information about the types of cardinal semantic operators in the CAO, their parameters and topology of connectivity, is demonstrated.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2507.21340</link>
<guid>https://arxiv.org/abs/2507.21340</guid>
<content:encoded><![CDATA[
arXiv:2507.21340v1 Announce Type: cross 
Abstract: Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers</title>
<link>https://arxiv.org/abs/2507.21364</link>
<guid>https://arxiv.org/abs/2507.21364</guid>
<content:encoded><![CDATA[
arXiv:2507.21364v1 Announce Type: cross 
Abstract: Wildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as a promising tool for biodiversity monitoring and conservation. This paper presents a comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using a public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to African-grounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices</title>
<link>https://arxiv.org/abs/2507.21378</link>
<guid>https://arxiv.org/abs/2507.21378</guid>
<content:encoded><![CDATA[
arXiv:2507.21378v1 Announce Type: cross 
Abstract: Wearable AI systems aim to provide timely assistance in daily life, but existing approaches often rely on user initiation or predefined task knowledge, neglecting users' current mental states. We introduce ProMemAssist, a smart glasses system that models a user's working memory (WM) in real-time using multi-modal sensor signals. Grounded in cognitive theories of WM, our system represents perceived information as memory items and episodes with encoding mechanisms, such as displacement and interference. This WM model informs a timing predictor that balances the value of assistance with the cost of interruption. In a user study with 12 participants completing cognitively demanding tasks, ProMemAssist delivered more selective assistance and received higher engagement compared to an LLM baseline system. Qualitative feedback highlights the benefits of WM modeling for nuanced, context-sensitive support, offering design implications for more attentive and user-aware proactive agents.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2507.21382</link>
<guid>https://arxiv.org/abs/2507.21382</guid>
<content:encoded><![CDATA[
arXiv:2507.21382v1 Announce Type: cross 
Abstract: Software architecture design is a critical, yet inherently complex and knowledge-intensive phase of software development. It requires deep domain expertise, development experience, architectural knowledge, careful trade-offs among competing quality attributes, and the ability to adapt to evolving requirements. Traditionally, this process is time-consuming and labor-intensive, and relies heavily on architects, often resulting in limited design alternatives, especially under the pressures of agile development. While Large Language Model (LLM)-based agents have shown promising performance across various SE tasks, their application to architecture design remains relatively scarce and requires more exploration, particularly in light of diverse domain knowledge and complex decision-making. To address the challenges, we proposed MAAD (Multi-Agent Architecture Design), an automated framework that employs a knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and Evaluator) to collaboratively interpret requirements specifications and produce architectural blueprints enriched with quality attributes-based evaluation reports. We then evaluated MAAD through a case study and comparative experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show that MAAD's superiority lies in generating comprehensive architectural components and delivering insightful and structured architecture evaluation reports. Feedback from industrial architects across 11 requirements specifications further reinforces MAAD's practical usability. We finally explored the performance of the MAAD framework with three LLMs (GPT-4o, DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance in producing architecture design, emphasizing the importance of LLM selection in MAS-driven architecture design.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving</title>
<link>https://arxiv.org/abs/2507.21385</link>
<guid>https://arxiv.org/abs/2507.21385</guid>
<content:encoded><![CDATA[
arXiv:2507.21385v1 Announce Type: cross 
Abstract: 3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX) is an important new network energy saving feature for 5G. As a time-domain technique, it periodically aggregates the user data transmissions in a given duration of time when the traffic load is not heavy, so that the remaining time can be kept silent and advanced sleep modes (ASM) can be enabled to shut down more radio components and save more energy for the cell. However, inevitably the packet delay is increased, as during the silent period no transmission is allowed. In this paper we study how to configure cell DTX/DRX to optimally balance energy saving and packet delay, so that for delay-sensitive traffic maximum energy saving can be achieved while the degradation of quality of service (QoS) is minimized. As the optimal configuration can be different for different network and traffic conditions, the problem is complex and we resort to deep reinforcement learning (DRL) framework to train an AI agent to solve it. Through careful design of 1) the learning algorithm, which implements a deep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward function, which utilizes a smooth approximation of a theoretically optimal but discontinuous reward function, we are able to train an AI agent that always tries to select the best possible Cell DTX/DRX configuration under any network and traffic conditions. Simulation results show that compared to the case when cell DTX/DRX is not used, our agent can achieve up to ~45% energy saving depending on the traffic load scenario, while always maintaining no more than ~1% QoS degradation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2507.21386</link>
<guid>https://arxiv.org/abs/2507.21386</guid>
<content:encoded><![CDATA[
arXiv:2507.21386v1 Announce Type: cross 
Abstract: Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed to address Vehicle Routing Problems (VRPs). However, most of these solvers focus exclusively on single-vehicle VRP variants, overlooking the more realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP), which involves multiple vehicles. Existing MMHCVRP solvers typically select a vehicle and its next node to visit at each decoding step, but often make myopic decoding decisions and overlook key properties of MMHCVRP, including local topological relationships, vehicle permutation invariance, and node symmetry, resulting in suboptimal performance. To better address these limitations, we propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed dual-modality node encoder to capture local topological relationships among nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in the preceding decoding step. Finally, leveraging vehicle permutation invariance and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to stabilize the Reinforcement Learning training process. To assess the performance of ECHO, we conduct extensive experiments. The experimental results demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying numbers of vehicles and nodes, and exhibits well-performing generalization across both scales and distribution patterns. Finally, ablation studies validate the effectiveness of all proposed methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2507.21391</link>
<guid>https://arxiv.org/abs/2507.21391</guid>
<content:encoded><![CDATA[
arXiv:2507.21391v1 Announce Type: cross 
Abstract: We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations.In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion</title>
<link>https://arxiv.org/abs/2507.21395</link>
<guid>https://arxiv.org/abs/2507.21395</guid>
<content:encoded><![CDATA[
arXiv:2507.21395v1 Announce Type: cross 
Abstract: Multimodal emotion recognition (MER) is crucial for enabling emotionally intelligent systems that perceive and respond to human emotions. However, existing methods suffer from limited cross-modal interaction and imbalanced contributions across modalities. To address these issues, we propose Sync-TVA, an end-to-end graph-attention framework featuring modality-specific dynamic enhancement and structured cross-modal fusion. Our design incorporates a dynamic enhancement module for each modality and constructs heterogeneous cross-modal graphs to model semantic relations across text, audio, and visual features. A cross-attention fusion mechanism further aligns multimodal cues for robust emotion inference. Experiments on MELD and IEMOCAP demonstrate consistent improvements over state-of-the-art models in both accuracy and weighted F1 score, especially under class-imbalanced conditions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.21423</link>
<guid>https://arxiv.org/abs/2507.21423</guid>
<content:encoded><![CDATA[
arXiv:2507.21423v1 Announce Type: cross 
Abstract: Autonomous driving requires an understanding of the static environment from sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse multiple inputs, and a vector decoder predicts a vectorized map representation from the latent BEV grid. However, traditional map construction models provide deterministic point estimates, failing to capture uncertainty and the inherent ambiguities of real-world environments, such as occlusions and missing lane markings. We propose MapDiffusion, a novel generative approach that leverages the diffusion paradigm to learn the full distribution of possible vectorized maps. Instead of predicting a single deterministic output from learned queries, MapDiffusion iteratively refines randomly initialized queries, conditioned on a BEV latent grid, to generate multiple plausible map samples. This allows aggregating samples to improve prediction accuracy and deriving uncertainty estimates that directly correlate with scene ambiguity. Extensive experiments on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art performance in online map construction, surpassing the baseline by 5% in single-sample performance. We further show that aggregating multiple samples consistently improves performance along the ROC curve, validating the benefit of distribution modeling. Additionally, our uncertainty estimates are significantly higher in occluded areas, reinforcing their value in identifying regions with ambiguous sensor input. By modeling the full map distribution, MapDiffusion enhances the robustness and reliability of online vectorized HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles in complex environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</title>
<link>https://arxiv.org/abs/2507.21432</link>
<guid>https://arxiv.org/abs/2507.21432</guid>
<content:encoded><![CDATA[
arXiv:2507.21432v1 Announce Type: cross 
Abstract: This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse</title>
<link>https://arxiv.org/abs/2507.21433</link>
<guid>https://arxiv.org/abs/2507.21433</guid>
<content:encoded><![CDATA[
arXiv:2507.21433v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have achieved significant advances in mathematical reasoning and formal logic tasks. However, their tendency to generate lengthy chain-of-thought sequences leads to substantial memory overhead during inference. We observe that LRMs frequently produce highly similar intermediate reasoning steps, which correspond to similar KV cache states across layers. Motivated by this observation, we propose MemShare, a novel KV cache management approach that effectively reduces memory overhead. MemShare employs a collaborative filtering algorithm to efficiently identify reusable KV cache blocks and enables zero copy cache reuse to significantly reduce memory overhead, improve throughput while maintaining accuracy. Experimental results demonstrate that MemShare delivers up to 84.79\% improvement in throughput while maintaining better accuracy compared to existing KV cache management methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation</title>
<link>https://arxiv.org/abs/2507.21455</link>
<guid>https://arxiv.org/abs/2507.21455</guid>
<content:encoded><![CDATA[
arXiv:2507.21455v1 Announce Type: cross 
Abstract: Although larger datasets are crucial for training large deep models, the rapid growth of dataset size has brought a significant challenge in terms of considerable training costs, which even results in prohibitive computational expenses. Dataset Distillation becomes a popular technique recently to reduce the dataset size via learning a highly compact set of representative exemplars, where the model trained with these exemplars ideally should have comparable performance with respect to the one trained with the full dataset. While most of existing works upon dataset distillation focus on supervised datasets, we instead aim to distill images and their self-supervisedly trained representations into a distilled set. This procedure, named as Self-Supervised Dataset Distillation, effectively extracts rich information from real datasets, yielding the distilled sets with enhanced cross-architecture generalizability. Particularly, in order to preserve the key characteristics of original dataset more faithfully and compactly, several novel techniques are proposed: 1) we introduce an innovative parameterization upon images and representations via distinct low-dimensional bases, where the base selection for parameterization is experimentally shown to play a crucial role; 2) we tackle the instability induced by the randomness of data augmentation -- a key component in self-supervised learning but being underestimated in the prior work of self-supervised dataset distillation -- by utilizing predetermined augmentations; 3) we further leverage a lightweight network to model the connections among the representations of augmented views from the same image, leading to more compact pairs of distillation. Extensive experiments conducted on various datasets validate the superiority of our approach in terms of distillation efficiency, cross-architecture generalization, and transfer learning performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning</title>
<link>https://arxiv.org/abs/2507.21474</link>
<guid>https://arxiv.org/abs/2507.21474</guid>
<content:encoded><![CDATA[
arXiv:2507.21474v1 Announce Type: cross 
Abstract: Despite success across diverse tasks, current artificial recurrent network architectures rely primarily on implicit hidden-state memories, limiting their interpretability and ability to model long-range dependencies. In contrast, biological neural systems employ explicit, associative memory traces (i.e., engrams) strengthened through Hebbian synaptic plasticity and activated sparsely during recall. Motivated by these neurobiological insights, we introduce the Engram Neural Network (ENN), a novel recurrent architecture incorporating an explicit, differentiable memory matrix with Hebbian plasticity and sparse, attention-driven retrieval mechanisms. The ENN explicitly models memory formation and recall through dynamic Hebbian traces, improving transparency and interpretability compared to conventional RNN variants. We evaluate the ENN architecture on three canonical benchmarks: MNIST digit classification, CIFAR-10 image sequence modeling, and WikiText-103 language modeling. Our empirical results demonstrate that the ENN achieves accuracy and generalization performance broadly comparable to classical RNN, GRU, and LSTM architectures, with all models converging to similar accuracy and perplexity on the large-scale WikiText-103 task. At the same time, the ENN offers significant enhancements in interpretability through observable memory dynamics. Hebbian trace visualizations further reveal biologically plausible, structured memory formation processes, validating the potential of neuroscience-inspired mechanisms to inform the development of more interpretable and robust deep learning models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench</title>
<link>https://arxiv.org/abs/2507.21476</link>
<guid>https://arxiv.org/abs/2507.21476</guid>
<content:encoded><![CDATA[
arXiv:2507.21476v1 Announce Type: cross 
Abstract: We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capacity-Constrained Continual Learning</title>
<link>https://arxiv.org/abs/2507.21479</link>
<guid>https://arxiv.org/abs/2507.21479</guid>
<content:encoded><![CDATA[
arXiv:2507.21479v1 Announce Type: cross 
Abstract: Any agents we can possibly build are subject to capacity constraints, as memory and compute resources are inherently finite. However, comparatively little attention has been dedicated to understanding how agents with limited capacity should allocate their resources for optimal performance. The goal of this paper is to shed some light on this question by studying a simple yet relevant continual learning problem: the capacity-constrained linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a solution to this problem under appropriate technical conditions. Moreover, for problems that can be decomposed into a set of sub-problems, we also demonstrate how to optimally allocate capacity across these sub-problems in the steady state. We view the results of this paper as a first step in the systematic theoretical study of learning under capacity constraints.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs</title>
<link>https://arxiv.org/abs/2507.21482</link>
<guid>https://arxiv.org/abs/2507.21482</guid>
<content:encoded><![CDATA[
arXiv:2507.21482v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NCCR: to Evaluate the Robustness of Neural Networks and Adversarial Examples</title>
<link>https://arxiv.org/abs/2507.21483</link>
<guid>https://arxiv.org/abs/2507.21483</guid>
<content:encoded><![CDATA[
arXiv:2507.21483v1 Announce Type: cross 
Abstract: Neural networks have received a lot of attention recently, and related security issues have come with it. Many studies have shown that neural networks are vulnerable to adversarial examples that have been artificially perturbed with modification, which is too small to be distinguishable by human perception. Different attacks and defenses have been proposed to solve these problems, but there is little research on evaluating the robustness of neural networks and their inputs. In this work, we propose a metric called the neuron cover change rate (NCCR) to measure the ability of deep learning models to resist attacks and the stability of adversarial examples. NCCR monitors alterations in the output of specifically chosen neurons when the input is perturbed, and networks with a smaller degree of variation are considered to be more robust. The results of the experiment on image recognition and the speaker recognition model show that our metrics can provide a good assessment of the robustness of neural networks or their inputs. It can also be used to detect whether an input is adversarial or not, as adversarial examples are always less robust.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions</title>
<link>https://arxiv.org/abs/2507.21485</link>
<guid>https://arxiv.org/abs/2507.21485</guid>
<content:encoded><![CDATA[
arXiv:2507.21485v1 Announce Type: cross 
Abstract: High-level synthesis (HLS) accelerates hardware design by enabling the automatic translation of high-level descriptions into efficient hardware implementations. However, debugging HLS code is a challenging and labor-intensive task, especially for novice circuit designers or software engineers without sufficient hardware domain knowledge. The recent emergence of Large Language Models (LLMs) is promising in automating the HLS debugging process. Despite the great potential, three key challenges persist when applying LLMs to HLS logic debugging: 1) High-quality circuit data for training LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in hardware is inherently more complex than identifying software bugs with existing golden test cases. 3) The absence of reliable test cases requires multi-tasking solutions, performing both bug identification and correction. complicates the multi-tasking required for effective HLS debugging. In this work, we propose a customized solution named HLSDebugger to address the challenges. HLSDebugger first generates and releases a large labeled dataset with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts an encoder-decoder structure, performing bug location identification, bug type prediction, and bug correction with the same model. HLSDebugger significantly outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x in bug correction. It makes a substantial advancement in the exploration of automated debugging of HLS code.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VN-MTEB: Vietnamese Massive Text Embedding Benchmark</title>
<link>https://arxiv.org/abs/2507.21500</link>
<guid>https://arxiv.org/abs/2507.21500</guid>
<content:encoded><![CDATA[
arXiv:2507.21500v1 Announce Type: cross 
Abstract: Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Benchmarking of LLM Agents: A Survey</title>
<link>https://arxiv.org/abs/2507.21504</link>
<guid>https://arxiv.org/abs/2507.21504</guid>
<content:encoded><![CDATA[
arXiv:2507.21504v1 Announce Type: cross 
Abstract: The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision Transformer-Based Drone Trajectory Planning with Dynamic Safety-Efficiency Trade-Offs</title>
<link>https://arxiv.org/abs/2507.21506</link>
<guid>https://arxiv.org/abs/2507.21506</guid>
<content:encoded><![CDATA[
arXiv:2507.21506v1 Announce Type: cross 
Abstract: A drone trajectory planner should be able to dynamically adjust the safety-efficiency trade-off according to varying mission requirements in unknown environments. Although traditional polynomial-based planners offer computational efficiency and smooth trajectory generation, they require expert knowledge to tune multiple parameters to adjust this trade-off. Moreover, even with careful tuning, the resulting adjustment may fail to achieve the desired trade-off. Similarly, although reinforcement learning-based planners are adaptable in unknown environments, they do not explicitly address the safety-efficiency trade-off. To overcome this limitation, we introduce a Decision Transformer-based trajectory planner that leverages a single parameter, Return-to-Go (RTG), as a \emph{temperature parameter} to dynamically adjust the safety-efficiency trade-off. In our framework, since RTG intuitively measures the safety and efficiency of a trajectory, RTG tuning does not require expert knowledge. We validate our approach using Gazebo simulations in both structured grid and unstructured random environments. The experimental results demonstrate that our planner can dynamically adjust the safety-efficiency trade-off by simply tuning the RTG parameter. Furthermore, our planner outperforms existing baseline methods across various RTG settings, generating safer trajectories when tuned for safety and more efficient trajectories when tuned for efficiency. Real-world experiments further confirm the reliability and practicality of our proposed planner.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Classification of User Requirements from Online Feedback -- A Replication Study</title>
<link>https://arxiv.org/abs/2507.21532</link>
<guid>https://arxiv.org/abs/2507.21532</guid>
<content:encoded><![CDATA[
arXiv:2507.21532v1 Announce Type: cross 
Abstract: Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Predictive Adversarial Imitation Learning for Planning from Observation</title>
<link>https://arxiv.org/abs/2507.21533</link>
<guid>https://arxiv.org/abs/2507.21533</guid>
<content:encoded><![CDATA[
arXiv:2507.21533v1 Announce Type: cross 
Abstract: Human demonstration data is often ambiguous and incomplete, motivating imitation learning approaches that also exhibit reliable planning behavior. A common paradigm to perform planning-from-demonstration involves learning a reward function via Inverse Reinforcement Learning (IRL) then deploying this reward via Model Predictive Control (MPC). Towards unifying these methods, we derive a replacement of the policy in IRL with a planning-based agent. With connections to Adversarial Imitation Learning, this formulation enables end-to-end interactive learning of planners from observation-only demonstrations. In addition to benefits in interpretability, complexity, and safety, we study and observe significant improvements on sample efficiency, out-of-distribution generalization, and robustness. The study includes evaluations in both simulated control benchmarks and real-world navigation experiments using few-to-single observation-only demonstrations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Graph Neural Network for Compressed Speech Steganalysis</title>
<link>https://arxiv.org/abs/2507.21591</link>
<guid>https://arxiv.org/abs/2507.21591</guid>
<content:encoded><![CDATA[
arXiv:2507.21591v1 Announce Type: cross 
Abstract: Steganalysis methods based on deep learning (DL) often struggle with computational complexity and challenges in generalizing across different datasets. Incorporating a graph neural network (GNN) into steganalysis schemes enables the leveraging of relational data for improved detection accuracy and adaptability. This paper presents the first application of a Graph Neural Network (GNN), specifically the GraphSAGE architecture, for steganalysis of compressed voice over IP (VoIP) speech streams. The method involves straightforward graph construction from VoIP streams and employs GraphSAGE to capture hierarchical steganalysis information, including both fine grained details and high level patterns, thereby achieving high detection accuracy. Experimental results demonstrate that the developed approach performs well in uncovering quantization index modulation (QIM)-based steganographic patterns in VoIP signals. It achieves detection accuracy exceeding 98 percent even for short 0.5 second samples, and 95.17 percent accuracy under challenging conditions with low embedding rates, representing an improvement of 2.8 percent over the best performing state of the art methods. Furthermore, the model exhibits superior efficiency, with an average detection time as low as 0.016 seconds for 0.5-second samples an improvement of 0.003 seconds. This makes it efficient for online steganalysis tasks, providing a superior balance between detection accuracy and efficiency under the constraint of short samples with low embedding rates.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD-CAN: Graph-Understanding and Recurrent Architecture for CAN Anomaly Detection</title>
<link>https://arxiv.org/abs/2507.21640</link>
<guid>https://arxiv.org/abs/2507.21640</guid>
<content:encoded><![CDATA[
arXiv:2507.21640v1 Announce Type: cross 
Abstract: Modern in-vehicle networks face various cyber threats due to the lack of encryption and authentication in the Controller Area Network (CAN). To address this security issue, this paper presents GUARD-CAN, an anomaly detection framework that combines graph-based representation learning with time-series modeling. GUARD-CAN splits CAN messages into fixed-length windows and converts each window into a graph that preserves message order. To detect anomalies in the timeaware and structure-aware context at the same window, GUARD-CAN takes advantage of the overcomplete Autoencoder (AE) and Graph Convolutional Network (GCN) to generate graph embedding vectors. The model groups these vectors into sequences and feeds them into the Gated Recurrent Unit (GRU) to detect temporal anomaly patterns across the graphs. GUARD-CAN performs anomaly detection at both the sequence level and the window level, and this allows multi-perspective performance evaluation. The model also verifies the importance of window size selection through an analysis based on Shannon entropy. As a result, GUARD-CAN shows that the proposed model detects four types of CAN attacks (flooding, fuzzing, replay and spoofing attacks) effectively without relying on complex feature engineering.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs</title>
<link>https://arxiv.org/abs/2507.21653</link>
<guid>https://arxiv.org/abs/2507.21653</guid>
<content:encoded><![CDATA[
arXiv:2507.21653v1 Announce Type: cross 
Abstract: Real-world fraud detection applications benefit from graph learning techniques that jointly exploit node features, often rich in textual data, and graph structural information. Recently, Graph-Enhanced LLMs emerge as a promising graph learning approach that converts graph information into prompts, exploiting LLMs' ability to reason over both textual and structural information. Among them, text-only prompting, which converts graph information to prompts consisting solely of text tokens, offers a solution that relies only on LLM tuning without requiring additional graph-specific encoders. However, text-only prompting struggles on heterogeneous fraud-detection graphs: multi-hop relations expand exponentially with each additional hop, leading to rapidly growing neighborhoods associated with dense textual information. These neighborhoods may overwhelm the model with long, irrelevant content in the prompt and suppress key signals from the target node, thereby degrading performance. To address this challenge, we propose Dual Granularity Prompting (DGP), which mitigates information overload by preserving fine-grained textual details for the target node while summarizing neighbor information into coarse-grained text prompts. DGP introduces tailored summarization strategies for different data modalities, bi-level semantic abstraction for textual fields and statistical aggregation for numerical features, enabling effective compression of verbose neighbor content into concise, informative prompts. Experiments across public and industrial datasets demonstrate that DGP operates within a manageable token budget while improving fraud detection performance by up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of Graph-Enhanced LLMs for fraud detection.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind</title>
<link>https://arxiv.org/abs/2507.21654</link>
<guid>https://arxiv.org/abs/2507.21654</guid>
<content:encoded><![CDATA[
arXiv:2507.21654v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) tools become increasingly embedded in higher education, understanding how students interact with these systems is essential to supporting effective learning. This study examines how students' AI literacy and prior exposure to AI technologies shape their perceptions of Socratic Mind, an interactive AI-powered formative assessment tool. Drawing on Self-Determination Theory and user experience research, we analyze relationships among AI literacy, perceived usability, satisfaction, engagement, and perceived learning effectiveness. Data from 309 undergraduates in Computer Science and Business courses were collected through validated surveys. Partial least squares structural equation modeling showed that AI literacy - especially self-efficacy, conceptual understanding, and application skills - significantly predicts usability, satisfaction, and engagement. Usability and satisfaction, in turn, strongly predict perceived learning effectiveness, while prior AI exposure showed no significant effect. These findings highlight that AI literacy, rather than exposure alone, shapes student experiences. Designers should integrate adaptive guidance and user-centered features to support diverse literacy levels, fostering inclusive, motivating, and effective AI-based learning environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>diffSPH: Differentiable Smoothed Particle Hydrodynamics for Adjoint Optimization and Machine Learning</title>
<link>https://arxiv.org/abs/2507.21684</link>
<guid>https://arxiv.org/abs/2507.21684</guid>
<content:encoded><![CDATA[
arXiv:2507.21684v1 Announce Type: cross 
Abstract: We present diffSPH, a novel open-source differentiable Smoothed Particle Hydrodynamics (SPH) framework developed entirely in PyTorch with GPU acceleration. diffSPH is designed centrally around differentiation to facilitate optimization and machine learning (ML) applications in Computational Fluid Dynamics~(CFD), including training neural networks and the development of hybrid models. Its differentiable SPH core, and schemes for compressible (with shock capturing and multi-phase flows), weakly compressible (with boundary handling and free-surface flows), and incompressible physics, enable a broad range of application areas. We demonstrate the framework's unique capabilities through several applications, including addressing particle shifting via a novel, target-oriented approach by minimizing physical and regularization loss terms, a task often intractable in traditional solvers. Further examples include optimizing initial conditions and physical parameters to match target trajectories, shape optimization, implementing a solver-in-the-loop setup to emulate higher-order integration, and demonstrating gradient propagation through hundreds of full simulation steps. Prioritizing readability, usability, and extensibility, this work offers a foundational platform for the CFD community to develop and deploy novel neural networks and adjoint optimization applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing</title>
<link>https://arxiv.org/abs/2507.21690</link>
<guid>https://arxiv.org/abs/2507.21690</guid>
<content:encoded><![CDATA[
arXiv:2507.21690v1 Announce Type: cross 
Abstract: Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift" and ``increased patch monotonicity." To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios</title>
<link>https://arxiv.org/abs/2507.21693</link>
<guid>https://arxiv.org/abs/2507.21693</guid>
<content:encoded><![CDATA[
arXiv:2507.21693v1 Announce Type: cross 
Abstract: As large language models (LLMs) rapidly advance, their role in code generation has expanded significantly. While this offers streamlined development, it also creates concerns in areas like education and job interviews. Consequently, developing robust systems to detect AI-generated code is imperative to maintain academic integrity and ensure fairness in hiring processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated code detection for Python, Java, and Go. From the CodeNet dataset's problem definitions and human-authored codes, we generate several code samples in Java, Python, and Go with six different LLMs and three different prompts. This generation process covered three key usage scenarios: (i) generating code from problem descriptions, (ii) fixing runtime errors in human-written code, and (iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271 AI-generated and 32,148 human-written code snippets. We also benchmark three state-of-the-art AI-generated code detection models and assess their performance in various test scenarios such as cross-model and cross-language. We share our dataset and codes to support research in this field.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation</title>
<link>https://arxiv.org/abs/2507.21694</link>
<guid>https://arxiv.org/abs/2507.21694</guid>
<content:encoded><![CDATA[
arXiv:2507.21694v1 Announce Type: cross 
Abstract: As large language models demonstrate enormous potential in the field of Electronic Design Automation (EDA), generative AI-assisted chip design is attracting widespread attention from academia and industry. Although these technologies have made preliminary progress in tasks such as code generation, their application in chip verification -- a critical bottleneck in the chip development cycle -- remains at an exploratory stage. This paper proposes an innovative Multi-Agent Verification Framework (MAVF) aimed at addressing the limitations of current single-LLM approaches in complex verification tasks. Our framework builds an automated transformation system from design specifications to testbench through the collaborative work of multiple specialized agents, including specification parsing, verification strategy generation, and code implementation. Through verification experiments on multiple chip modules of varying complexity, results show that MAVF significantly outperforms traditional manual methods and single-dialogue generative AI approaches in verification document parsing and generation, as well as automated testbench generation. This research opens new directions for exploring generative AI applications in verification automation, potentially providing effective approaches to solving the most challenging bottleneck issues in chip design.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Large Physics Benchmark</title>
<link>https://arxiv.org/abs/2507.21695</link>
<guid>https://arxiv.org/abs/2507.21695</guid>
<content:encoded><![CDATA[
arXiv:2507.21695v1 Announce Type: cross 
Abstract: We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics. Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise. The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving. Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal. To ensure continued relevance, we propose a living benchmark, where physicists contribute questions, for instance alongside new publications. We invite contributions via: http://www.physicsbenchmarks.org/. We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations</title>
<link>https://arxiv.org/abs/2507.21706</link>
<guid>https://arxiv.org/abs/2507.21706</guid>
<content:encoded><![CDATA[
arXiv:2507.21706v1 Announce Type: cross 
Abstract: Distinguishing pathogenic mutations from benign polymorphisms remains a critical challenge in precision medicine. EnTao-GPM, developed by Fudan University and BioMap, addresses this through three innovations: (1) Cross-species targeted pre-training on disease-relevant mammalian genomes (human, pig, mouse), leveraging evolutionary conservation to enhance interpretation of pathogenic motifs, particularly in non-coding regions; (2) Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework integrating DNA sequence embeddings with LLM-based statistical explanations to provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates superior accuracy in mutation classification. It revolutionizes genetic testing by enabling faster, more accurate, and accessible interpretation for clinical diagnostics (e.g., variant assessment, risk identification, personalized treatment) and research, advancing personalized medicine.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations</title>
<link>https://arxiv.org/abs/2507.21723</link>
<guid>https://arxiv.org/abs/2507.21723</guid>
<content:encoded><![CDATA[
arXiv:2507.21723v1 Announce Type: cross 
Abstract: In recent years, Explainable AI has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components - knowledge that is essential for improving transparency and efficiency. Inspired by neuroscientific ablation studies, which investigate the functions of brain regions through selective impairment, we systematically analyze the impact of ablating key components in three state-of-the-art detection transformer models: Detection transformer (DETR), deformable detection transformer (DDETR), and DETR with improved denoising anchor boxes (DINO). The ablations target query embeddings, encoder and decoder multi-head self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA) layers. We evaluate the effects of these ablations on the performance metrics gIoU and F1-score, quantifying effects on both the classification and regression sub-tasks on the COCO dataset. To facilitate reproducibility and future research, we publicly release the DeepDissect library. Our findings reveal model-specific resilience patterns: while DETR is particularly sensitive to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable attention enhances robustness, and DINO exhibits the greatest resilience due to its look-forward twice update rule, which helps distributing knowledge across blocks. These insights also expose structural redundancies, particularly in DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model simplification without sacrificing performance. This study advances XAI for DETRs by clarifying the contributions of internal components to model performance, offering insights to optimize and improve transparency and efficiency in critical applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation</title>
<link>https://arxiv.org/abs/2507.21738</link>
<guid>https://arxiv.org/abs/2507.21738</guid>
<content:encoded><![CDATA[
arXiv:2507.21738v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific samples from a trained model. A key challenge in this process is over-unlearning, where the model's performance on the remaining data significantly drops due to the change in the model's parameters. Existing unlearning algorithms depend on the remaining data to prevent this issue. As such, these methods are inapplicable in a more practical scenario, where only the unlearning samples are available (i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to fill this gap. Our approach offers three key innovations: (1) we approximate the inaccessible remaining data by generating adversarial samples; (2) leveraging the generated samples, we pinpoint a specific subspace to perform the unlearning process, therefore preventing over-unlearning in the challenging zero-shot scenario; and (3) we consider the influence of the unlearning process on the remaining samples and design an influence-based pseudo-labeling strategy. As a result, our method further improves the model's performance after unlearning. The proposed method holds a theoretical guarantee, and experiments on various benchmarks validate the effectiveness and superiority of our proposed method over several baselines.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection</title>
<link>https://arxiv.org/abs/2507.21756</link>
<guid>https://arxiv.org/abs/2507.21756</guid>
<content:encoded><![CDATA[
arXiv:2507.21756v1 Announce Type: cross 
Abstract: Detecting driver fatigue is critical for road safety, as drowsy driving remains a leading cause of traffic accidents. Many existing solutions rely on computationally demanding deep learning models, which result in high latency and are unsuitable for embedded robotic devices with limited resources (such as intelligent vehicles/cars) where rapid detection is necessary to prevent accidents. This paper introduces LiteFat, a lightweight spatio-temporal graph learning model designed to detect driver fatigue efficiently while maintaining high accuracy and low computational demands. LiteFat involves converting streaming video data into spatio-temporal graphs (STG) using facial landmark detection, which focuses on key motion patterns and reduces unnecessary data processing. LiteFat uses MobileNet to extract facial features and create a feature matrix for the STG. A lightweight spatio-temporal graph neural network is then employed to identify signs of fatigue with minimal processing and low latency. Experimental results on benchmark datasets show that LiteFat performs competitively while significantly decreasing computational complexity and latency as compared to current state-of-the-art methods. This work enables the development of real-time, resource-efficient human fatigue detection systems that can be implemented upon embedded robotic devices.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2507.21763</link>
<guid>https://arxiv.org/abs/2507.21763</guid>
<content:encoded><![CDATA[
arXiv:2507.21763v1 Announce Type: cross 
Abstract: We show that Generative Adversarial Networks (GANs) may be fruitfully exploited to learn stochastic dynamics, surrogating traditional models while capturing thermal fluctuations. Specifically, we showcase the application to a two-dimensional, many-particle system, focusing on surface-step fluctuations and on the related time-dependent roughness. After the construction of a dataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained to propagate stochastically the state of the system in time, allowing the generation of new sequences with a reduced computational cost. Modifications with respect to standard GANs, which facilitate convergence and increase accuracy, are discussed. The trained network is demonstrated to quantitatively reproduce equilibrium and kinetic properties, including scaling laws, with deviations of a few percent from the exact value. Extrapolation limits and future perspectives are critically discussed.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results</title>
<link>https://arxiv.org/abs/2507.21770</link>
<guid>https://arxiv.org/abs/2507.21770</guid>
<content:encoded><![CDATA[
arXiv:2507.21770v1 Announce Type: cross 
Abstract: The importance of recommender systems on the web has grown, especially in the movie industry, with a vast selection of options to watch. To assist users in traversing available items and finding relevant results, recommender systems analyze operational data and investigate users' tastes and habits. Providing highly individualized suggestions can boost user engagement and satisfaction, which is one of the fundamental goals of the movie industry, significantly in online platforms. According to recent studies and research, using knowledge-based techniques and considering the semantic ideas of the textual data is a suitable way to get more appropriate results. This study provides a new method for building a knowledge graph based on semantic information. It uses the ChatGPT, as a large language model, to assess the brief descriptions of movies and extract their tone of voice. Results indicated that using the proposed method may significantly enhance accuracy rather than employing the explicit genres supplied by the publishers.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities</title>
<link>https://arxiv.org/abs/2507.21790</link>
<guid>https://arxiv.org/abs/2507.21790</guid>
<content:encoded><![CDATA[
arXiv:2507.21790v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are widely used to support various workflows across different disciplines, yet their potential in choice modelling remains relatively unexplored. This work examines the potential of LLMs as assistive agents in the specification and, where technically feasible, estimation of Multinomial Logit models. We implement a systematic experimental framework involving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek, Gemini, Gemma, and Llama) evaluated under five experimental configurations. These configurations vary along three dimensions: modelling goal (suggesting vs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs. Chain-of-Thoughts); and information availability (full dataset vs. data dictionary only). Each LLM-suggested specification is implemented, estimated, and evaluated based on goodness-of-fit metrics, behavioural plausibility, and model complexity. Findings reveal that proprietary LLMs can generate valid and behaviourally sound utility specifications, particularly when guided by structured prompts. Open-weight models such as Llama and Gemma struggled to produce meaningful specifications. Claude 4 Sonnet consistently produced the best-fitting and most complex models, while GPT models suggested models with robust and stable modelling outcomes. Some LLMs performed better when provided with just data dictionary, suggesting that limiting raw data access may enhance internal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of correctly estimating its own specifications by executing self-generated code. Overall, the results demonstrate both the promise and current limitations of LLMs as assistive agents in choice modelling, not only for model specification but also for supporting modelling decision and estimation, and provide practical guidance for integrating these tools into choice modellers' workflows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects</title>
<link>https://arxiv.org/abs/2507.21796</link>
<guid>https://arxiv.org/abs/2507.21796</guid>
<content:encoded><![CDATA[
arXiv:2507.21796v1 Announce Type: cross 
Abstract: Mobile manipulation is a critical capability for robots operating in diverse, real-world environments. However, manipulating deformable objects and materials remains a major challenge for existing robot learning algorithms. While various benchmarks have been proposed to evaluate manipulation strategies with rigid objects, there is still a notable lack of standardized benchmarks that address mobile manipulation tasks involving deformable objects.
  To address this gap, we introduce MoDeSuite, the first Mobile Manipulation Deformable Object task suite, designed specifically for robot learning. MoDeSuite consists of eight distinct mobile manipulation tasks covering both elastic objects and deformable objects, each presenting a unique challenge inspired by real-world robot applications. Success in these tasks requires effective collaboration between the robot's base and manipulator, as well as the ability to exploit the deformability of the objects. To evaluate and demonstrate the use of the proposed benchmark, we train two state-of-the-art reinforcement learning algorithms and two imitation learning algorithms, highlighting the difficulties encountered and showing their performance in simulation. Furthermore, we demonstrate the practical relevance of the suite by deploying the trained policies directly into the real world with the Spot robot, showcasing the potential for sim-to-real transfer. We expect that MoDeSuite will open a novel research domain in mobile manipulation involving deformable objects. Find more details, code, and videos at https://sites.google.com/view/modesuite/home.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer</title>
<link>https://arxiv.org/abs/2507.21799</link>
<guid>https://arxiv.org/abs/2507.21799</guid>
<content:encoded><![CDATA[
arXiv:2507.21799v1 Announce Type: cross 
Abstract: The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box transformers, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex sparse rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box transformer to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box transformer with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences</title>
<link>https://arxiv.org/abs/2507.21831</link>
<guid>https://arxiv.org/abs/2507.21831</guid>
<content:encoded><![CDATA[
arXiv:2507.21831v1 Announce Type: cross 
Abstract: LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Fourier Neural Operators via Effective Field Theory</title>
<link>https://arxiv.org/abs/2507.21833</link>
<guid>https://arxiv.org/abs/2507.21833</guid>
<content:encoded><![CDATA[
arXiv:2507.21833v1 Announce Type: cross 
Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for high-dimensional partial-differential equations, yet their stability, generalization and frequency behavior lack a principled explanation. We present the first systematic effective-field-theory analysis of FNOs in an infinite-dimensional function space, deriving closed recursion relations for the layer kernel and four-point vertex and then examining three practically important settings-analytic activations, scale-invariant cases and architectures with residual connections. The theory shows that nonlinear activations inevitably couple frequency inputs to high-frequency modes that are otherwise discarded by spectral truncation, and experiments confirm this frequency transfer. For wide networks we obtain explicit criticality conditions on the weight-initialization ensemble that keep small input perturbations to have uniform scale across depth, and empirical tests validate these predictions. Taken together, our results quantify how nonlinearity enables neural operators to capture non-trivial features, supply criteria for hyper-parameter selection via criticality analysis, and explain why scale-invariant activations and residual connections enhance feature learning in FNOs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Against racing to AGI: Cooperation, deterrence, and catastrophic risks</title>
<link>https://arxiv.org/abs/2507.21839</link>
<guid>https://arxiv.org/abs/2507.21839</guid>
<content:encoded><![CDATA[
arXiv:2507.21839v1 Announce Type: cross 
Abstract: AGI Racing is the view that it is in the self-interest of major actors in AI development, especially powerful nations, to accelerate their frontier AI development to build highly capable AI, especially artificial general intelligence (AGI), before competitors have a chance. We argue against AGI Racing. First, the downsides of racing to AGI are much higher than portrayed by this view. Racing to AGI would substantially increase catastrophic risks from AI, including nuclear instability, and undermine the prospects of technical AI safety research to be effective. Second, the expected benefits of racing may be lower than proponents of AGI Racing hold. In particular, it is questionable whether winning the race enables complete domination over losers. Third, international cooperation and coordination, and perhaps carefully crafted deterrence measures, constitute viable alternatives to racing to AGI which have much smaller risks and promise to deliver most of the benefits that racing to AGI is supposed to provide. Hence, racing to AGI is not in anyone's self-interest as other actions, particularly incentivizing and seeking international cooperation around AI issues, are preferable.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven quantum Koopman method for simulating nonlinear dynamics</title>
<link>https://arxiv.org/abs/2507.21890</link>
<guid>https://arxiv.org/abs/2507.21890</guid>
<content:encoded><![CDATA[
arXiv:2507.21890v1 Announce Type: cross 
Abstract: Quantum computation offers potential exponential speedups for simulating certain physical systems, but its application to nonlinear dynamics is inherently constrained by the requirement of unitary evolution. We propose the quantum Koopman method (QKM), a data-driven framework that bridges this gap through transforming nonlinear dynamics into linear unitary evolution in higher-dimensional observable spaces. Leveraging the Koopman operator theory to achieve a global linearization, our approach maps system states into a hierarchy of Hilbert spaces using a deep autoencoder. Within the linearized embedding spaces, the state representation is decomposed into modulus and phase components, and the evolution is governed by a set of unitary Koopman operators that act exclusively on the phase. These operators are constructed from diagonal Hamiltonians with coefficients learned from data, a structure designed for efficient implementation on quantum hardware. This architecture enables direct multi-step prediction, and the operator's computational complexity scales logarithmically with the observable space dimension. The QKM is validated across diverse nonlinear systems. Its predictions maintain relative errors below 6% for reaction-diffusion systems and shear flows, and capture key statistics in 2D turbulence. This work establishes a practical pathway for quantum-accelerated simulation of nonlinear phenomena, exploring a framework built on the synergy between deep learning for global linearization and quantum algorithms for unitary dynamics evolution.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deepfake Detectors in the Wild</title>
<link>https://arxiv.org/abs/2507.21905</link>
<guid>https://arxiv.org/abs/2507.21905</guid>
<content:encoded><![CDATA[
arXiv:2507.21905v1 Announce Type: cross 
Abstract: Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/messlav/Deepfake-Detectors-in-the-Wild.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training language models to be warm and empathetic makes them less reliable and more sycophantic</title>
<link>https://arxiv.org/abs/2507.21919</link>
<guid>https://arxiv.org/abs/2507.21919</guid>
<content:encoded><![CDATA[
arXiv:2507.21919v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention</title>
<link>https://arxiv.org/abs/2507.21922</link>
<guid>https://arxiv.org/abs/2507.21922</guid>
<content:encoded><![CDATA[
arXiv:2507.21922v1 Announce Type: cross 
Abstract: In recent years, artificial intelligence has been increasingly applied in the field of medical imaging. Among these applications, fundus image analysis presents special challenges, including small lesion areas in certain fundus diseases and subtle inter-disease differences, which can lead to reduced prediction accuracy and overfitting in the models. To address these challenges, this paper proposes the Transformer-based model SwinECAT, which combines the Shifted Window (Swin) Attention with the Efficient Channel Attention (ECA) Attention. SwinECAT leverages the Swin Attention mechanism in the Swin Transformer backbone to effectively capture local spatial structures and long-range dependencies within fundus images. The lightweight ECA mechanism is incorporated to guide the SwinECAT's attention toward critical feature channels, enabling more discriminative feature representation. In contrast to previous studies that typically classify fundus images into 4 to 6 categories, this work expands fundus disease classification to 9 distinct types, thereby enhancing the granularity of diagnosis. We evaluate our method on the Eye Disease Image Dataset (EDID) containing 16,140 fundus images for 9-category classification. Experimental results demonstrate that SwinECAT achieves 88.29\% accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The classification results of our proposed model SwinECAT significantly outperform the baseline Swin Transformer and multiple compared baseline models. To our knowledge, this represents the highest reported performance for 9-category classification on this public dataset.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda</title>
<link>https://arxiv.org/abs/2507.21928</link>
<guid>https://arxiv.org/abs/2507.21928</guid>
<content:encoded><![CDATA[
arXiv:2507.21928v1 Announce Type: cross 
Abstract: Software development is undergoing a fundamental transformation as vibe coding becomes widespread, with large portions of contemporary codebases now being AI-generated. The disconnect between rapid adoption and limited conceptual understanding highlights the need for an inquiry into this emerging paradigm. Drawing on an intent perspective and historical analysis, we define vibe coding as a software development paradigm where humans and generative AI engage in collaborative flow to co-create software artifacts through natural language dialogue, shifting the mediation of developer intent from deterministic instruction to probabilistic inference. By intent mediation, we refer to the fundamental process through which developers translate their conceptual goals into representations that computational systems can execute. Our results show that vibe coding reconfigures cognitive work by redistributing epistemic labor between humans and machines, shifting the expertise in the software development process away from traditional areas such as design or technical implementation toward collaborative orchestration. We identify key opportunities, including democratization, acceleration, and systemic leverage, alongside risks, such as black box codebases, responsibility gaps, and ecosystem bias. We conclude with a research agenda spanning human-, technology-, and organization-centered directions to guide future investigations of this paradigm.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Training Large Language Models via Reinforcement Learning from Self-Feedback</title>
<link>https://arxiv.org/abs/2507.21931</link>
<guid>https://arxiv.org/abs/2507.21931</guid>
<content:encoded><![CDATA[
arXiv:2507.21931v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Generalization in Data-free Quantization via Mixup-class Prompting</title>
<link>https://arxiv.org/abs/2507.21947</link>
<guid>https://arxiv.org/abs/2507.21947</guid>
<content:encoded><![CDATA[
arXiv:2507.21947v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal</title>
<link>https://arxiv.org/abs/2507.21949</link>
<guid>https://arxiv.org/abs/2507.21949</guid>
<content:encoded><![CDATA[
arXiv:2507.21949v1 Announce Type: cross 
Abstract: Existing shadow removal methods often rely on shadow masks, which are challenging to acquire in real-world scenarios. Exploring intrinsic image cues, such as local contrast information, presents a potential alternative for guiding shadow removal in the absence of explicit masks. However, the cue's inherent ambiguity becomes a critical limitation in complex scenes, where it can fail to distinguish true shadows from low-reflectance objects and intricate background textures. To address this motivation, we propose the Adaptive Gated Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs the contrast prior to effectively disentangle shadow features from confounding visual elements. Furthermore, to tackle the persistent challenge of restoring soft shadow boundaries and fine-grained details, we introduce a diffusion-based Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and contrast cues to guide the generative process. Extensive experiments demonstrate that our method achieves state-of-the-art results among mask-free approaches while maintaining competitive performance relative to mask-based methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation</title>
<link>https://arxiv.org/abs/2507.21953</link>
<guid>https://arxiv.org/abs/2507.21953</guid>
<content:encoded><![CDATA[
arXiv:2507.21953v1 Announce Type: cross 
Abstract: The recent advancement of autonomous agents powered by Large Language Models (LLMs) has demonstrated significant potential for automating tasks on mobile devices through graphical user interfaces (GUIs). Despite initial progress, these agents still face challenges when handling complex real-world tasks. These challenges arise from a lack of knowledge about real-life mobile applications in LLM-based agents, which may lead to ineffective task planning and even cause hallucinations. To address these challenges, we propose a novel LLM-based agent framework called MapAgent that leverages memory constructed from historical trajectories to augment current task planning. Specifically, we first propose a trajectory-based memory mechanism that transforms task execution trajectories into a reusable and structured page-memory database. Each page within a trajectory is extracted as a compact yet comprehensive snapshot, capturing both its UI layout and functional context. Secondly, we introduce a coarse-to-fine task planning approach that retrieves relevant pages from the memory database based on similarity and injects them into the LLM planner to compensate for potential deficiencies in understanding real-world app scenarios, thereby achieving more informed and context-aware task planning. Finally, planned tasks are transformed into executable actions through a task executor supported by a dual-LLM architecture, ensuring effective tracking of task progress. Experimental results in real-world scenarios demonstrate that MapAgent achieves superior performance to existing methods. The code will be open-sourced to support further research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Code Language Models to Detect Cross-Language Bugs</title>
<link>https://arxiv.org/abs/2507.21954</link>
<guid>https://arxiv.org/abs/2507.21954</guid>
<content:encoded><![CDATA[
arXiv:2507.21954v1 Announce Type: cross 
Abstract: Multilingual programming, which involves using multiple programming languages (PLs) in a single project, is increasingly common due to its benefits. However, it introduces cross-language bugs (CLBs), which arise from interactions between different PLs and are difficult to detect by single-language bug detection tools. This paper investigates the potential of pre-trained code language models (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language code identification tool, and constructed a CLB dataset involving three PL combinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction types. We fine-tuned 13 CodeLMs on this dataset and evaluated their performance, analyzing the effects of dataset size, token sequence length, and code comments. Results show that all CodeLMs performed poorly before fine-tuning, but exhibited varying degrees of performance improvement after fine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably, small fine-tuned CodeLMs tended to performe better than large ones. CodeLMs fine-tuned on single-language bug datasets performed poorly on CLB detection, demonstrating the distinction between CLBs and single-language bugs. Additionally, increasing the fine-tuning dataset size significantly improved performance, while longer token sequences did not necessarily improve the model performance. The impact of code comments varied across models. Some fine-tuned CodeLMs' performance was improved, while others showed degraded performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
arXiv:2507.21990v1 Announce Type: cross 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2507.21992</link>
<guid>https://arxiv.org/abs/2507.21992</guid>
<content:encoded><![CDATA[
arXiv:2507.21992v1 Announce Type: cross 
Abstract: We investigate whether knowledge distillation (KD) from multiple heterogeneous teacher models can enhance the generation of transferable adversarial examples. A lightweight student model is trained using two KD strategies: curriculum-based switching and joint optimization, with ResNet50 and DenseNet-161 as teachers. The trained student is then used to generate adversarial examples using FG, FGS, and PGD attacks, which are evaluated against a black-box target model (GoogLeNet). Our results show that student models distilled from multiple teachers achieve attack success rates comparable to ensemble-based baselines, while reducing adversarial example generation time by up to a factor of six. An ablation study further reveals that lower temperature settings and the inclusion of hard-label supervision significantly enhance transferability. These findings suggest that KD can serve not only as a model compression technique but also as a powerful tool for improving the efficiency and effectiveness of black-box adversarial attacks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Staining and locking computer vision models without retraining</title>
<link>https://arxiv.org/abs/2507.22000</link>
<guid>https://arxiv.org/abs/2507.22000</guid>
<content:encoded><![CDATA[
arXiv:2507.22000v1 Announce Type: cross 
Abstract: We introduce new methods of staining and locking computer vision models, to protect their owners' intellectual property. Staining, also known as watermarking, embeds secret behaviour into a model which can later be used to identify it, while locking aims to make a model unusable unless a secret trigger is inserted into input images. Unlike existing methods, our algorithms can be used to stain and lock pre-trained models without requiring fine-tuning or retraining, and come with provable, computable guarantees bounding their worst-case false positive rates. The stain and lock are implemented by directly modifying a small number of the model's weights and have minimal impact on the (unlocked) model's performance. Locked models are unlocked by inserting a small `trigger patch' into the corner of the input image. We present experimental results showing the efficacy of our methods and demonstrating their practical performance on a variety of computer vision models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation</title>
<link>https://arxiv.org/abs/2507.22002</link>
<guid>https://arxiv.org/abs/2507.22002</guid>
<content:encoded><![CDATA[
arXiv:2507.22002v1 Announce Type: cross 
Abstract: Industrial smoke segmentation is critical for air-quality monitoring and environmental protection but is often hampered by the high cost and scarcity of pixel-level annotations in real-world settings. We introduce CEDANet, a human-in-the-loop, class-aware domain adaptation framework that uniquely integrates weak, citizen-provided video-level labels with adversarial feature alignment. Specifically, we refine pseudo-labels generated by a source-trained segmentation model using citizen votes, and employ class-specific domain discriminators to transfer rich source-domain representations to the industrial domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of 0.261 with citizen feedback, vastly outperforming the baseline model, which scored 0.083 and 0.043 respectively. This represents a five-fold increase in F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with citizen-constrained pseudo-labels achieves performance comparable to the same architecture trained on limited 100 fully annotated images with F1-score of 0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully supervised-level accuracy without target-domain annotations. Our research validates the scalability and cost-efficiency of combining citizen science with weakly supervised domain adaptation, offering a practical solution for complex, data-scarce environmental monitoring applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Stratified Space Structure of an RL Game with the Volume Growth Transform</title>
<link>https://arxiv.org/abs/2507.22010</link>
<guid>https://arxiv.org/abs/2507.22010</guid>
<content:encoded><![CDATA[
arXiv:2507.22010v1 Announce Type: cross 
Abstract: In this work, we explore the structure of the embedding space of a transformer model trained for playing a particular reinforcement learning (RL) game. Specifically, we investigate how a transformer-based Proximal Policy Optimization (PPO) model embeds visual inputs in a simple environment where an agent must collect "coins" while avoiding dynamic obstacles consisting of "spotlights." By adapting Robinson et al.'s study of the volume growth transform for LLMs to the RL setting, we find that the token embedding space for our visual coin collecting game is also not a manifold, and is better modeled as a stratified space, where local dimension can vary from point to point. We further strengthen Robinson's method by proving that fairly general volume growth curves can be realized by stratified spaces. Finally, we carry out an analysis that suggests that as an RL agent acts, its latent representation alternates between periods of low local dimension, while following a fixed sub-strategy, and bursts of high local dimension, where the agent achieves a sub-goal (e.g., collecting an object) or where the environmental complexity increases (e.g., more obstacles appear). Consequently, our work suggests that the distribution of dimensions in a stratified latent space may provide a new geometric indicator of complexity for RL games.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation</title>
<link>https://arxiv.org/abs/2507.22020</link>
<guid>https://arxiv.org/abs/2507.22020</guid>
<content:encoded><![CDATA[
arXiv:2507.22020v1 Announce Type: cross 
Abstract: We propose a novel segmentation-based explainable artificial intelligence (XAI) method for neural networks working on point cloud classification. As one building block of this method, we propose a novel point-shifting mechanism to introduce perturbations in point cloud data. Recently, AI has seen an exponential growth. Hence, it is important to understand the decision-making process of AI algorithms when they are applied in critical areas. Our work focuses on explaining AI algorithms that classify point cloud data. An important aspect of the methods used for explaining AI algorithms is their ability to produce explanations that are easy for humans to understand. This allows them to analyze the AI algorithms better and make appropriate decisions based on that analysis. Therefore, in this work, we intend to generate meaningful explanations that can be easily interpreted by humans. The point cloud data we consider represents 3D objects such as cars, guitars, and laptops. We make use of point cloud segmentation models to generate explanations for the working of classification models. The segments are used to introduce perturbations into the input point cloud data and generate saliency maps. The perturbations are introduced using the novel point-shifting mechanism proposed in this work which ensures that the shifted points no longer influence the output of the classification algorithm. In contrast to previous methods, the segments used by our method are meaningful, i.e. humans can easily interpret the meaning of the segments. Thus, the benefit of our method over other methods is its ability to produce more meaningful saliency maps. We compare our method with the use of classical clustering algorithms to generate explanations. We also analyze the saliency maps generated for example inputs using our method to demonstrate the usefulness of the method in generating meaningful explanations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports</title>
<link>https://arxiv.org/abs/2507.22030</link>
<guid>https://arxiv.org/abs/2507.22030</guid>
<content:encoded><![CDATA[
arXiv:2507.22030v1 Announce Type: cross 
Abstract: We present ReXGroundingCT, the first publicly available dataset to link free-text radiology findings with pixel-level segmentations in 3D chest CT scans that is manually annotated. While prior datasets have relied on structured labels or predefined categories, ReXGroundingCT captures the full expressiveness of clinical language represented in free text and grounds it to spatially localized 3D segmentation annotations in volumetric imaging. This addresses a critical gap in medical AI: the ability to connect complex, descriptive text, such as "3 mm nodule in the left lower lobe", to its precise anatomical location in three-dimensional space, a capability essential for grounded radiology report generation systems. The dataset comprises 3,142 non-contrast chest CT scans paired with standardized radiology reports from the CT-RATE dataset. Using a systematic three-stage pipeline, GPT-4 was used to extract positive lung and pleural findings, which were then manually segmented by expert annotators. A total of 8,028 findings across 16,301 entities were annotated, with quality control performed by board-certified radiologists. Approximately 79% of findings are focal abnormalities, while 21% are non-focal. The training set includes up to three representative segmentations per finding, while the validation and test sets contain exhaustive labels for each finding entity. ReXGroundingCT establishes a new benchmark for developing and evaluating sentence-level grounding and free-text medical segmentation models in chest CT. The dataset can be accessed at https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security</title>
<link>https://arxiv.org/abs/2507.22037</link>
<guid>https://arxiv.org/abs/2507.22037</guid>
<content:encoded><![CDATA[
arXiv:2507.22037v1 Announce Type: cross 
Abstract: The rapid advancement of multimodal large language models (MLLMs) has led to breakthroughs in various applications, yet their security remains a critical challenge. One pressing issue involves unsafe image-query pairs--jailbreak inputs specifically designed to bypass security constraints and elicit unintended responses from MLLMs. Compared to general multimodal data, such unsafe inputs are relatively sparse, which limits the diversity and richness of training samples available for developing robust defense models. Meanwhile, existing guardrail-type methods rely on external modules to enforce security constraints but fail to address intrinsic vulnerabilities within MLLMs. Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses harmless inputs, compromising general performance. Given these challenges, we propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack training method to enhance the security of MLLMs. SecTOW consists of two modules: a defender and an auxiliary attacker, both trained iteratively using reinforcement learning (GRPO). During the iterative process, the attacker identifies security vulnerabilities in the defense model and expands jailbreak data. The expanded data are then used to train the defender, enabling it to address identified security vulnerabilities. We also design reward mechanisms used for GRPO to simplify the use of response labels, reducing dependence on complex generative labels and enabling the efficient use of synthetic data. Additionally, a quality monitoring mechanism is used to mitigate the defender's over-refusal of harmless inputs and ensure the diversity of the jailbreak data generated by the attacker. Experimental results on safety-specific and general benchmarks demonstrate that SecTOW significantly improves security while preserving general performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Quantum Image Processing</title>
<link>https://arxiv.org/abs/2507.22039</link>
<guid>https://arxiv.org/abs/2507.22039</guid>
<content:encoded><![CDATA[
arXiv:2507.22039v1 Announce Type: cross 
Abstract: In the era of big data and artificial intelligence, the increasing volume of data and the demand to solve more and more complex computational challenges are two driving forces for improving the efficiency of data storage, processing and analysis. Quantum image processing (QIP) is an interdisciplinary field between quantum information science and image processing, which has the potential to alleviate some of these challenges by leveraging the power of quantum computing. In this work, we compare and examine the compression properties of four different Quantum Image Representations (QImRs): namely, Tensor Network Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding (QPIE). Our simulations show that FRQI performs a higher compression of image information than TNR, NEQR, and QPIE. Furthermore, we investigate the trade-off between accuracy and memory in binary classification problems, evaluating the performance of quantum kernels based on QImRs compared to the classical linear kernel. Our results indicate that quantum kernels provide comparable classification average accuracy but require exponentially fewer resources for image storage.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Demand Forecasting via Dual-Strategy Ensembling</title>
<link>https://arxiv.org/abs/2507.22053</link>
<guid>https://arxiv.org/abs/2507.22053</guid>
<content:encoded><![CDATA[
arXiv:2507.22053v1 Announce Type: cross 
Abstract: Accurate demand forecasting is critical for supply chain optimization, yet remains difficult in practice due to hierarchical complexity, domain shifts, and evolving external factors. While recent foundation models offer strong potential for time series forecasting, they often suffer from architectural rigidity and limited robustness under distributional change. In this paper, we propose a unified ensemble framework that enhances the performance of foundation models for sales forecasting in real-world supply chains. Our method combines two complementary strategies: (1) Hierarchical Ensemble (HE), which partitions training and inference by semantic levels (e.g., store, category, department) to capture localized patterns; and (2) Architectural Ensemble (AE), which integrates predictions from diverse model backbones to mitigate bias and improve stability. We conduct extensive experiments on the M5 benchmark and three external sales datasets, covering both in-domain and zero-shot forecasting. Results show that our approach consistently outperforms strong baselines, improves accuracy across hierarchical levels, and provides a simple yet effective mechanism for boosting generalization in complex forecasting environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite time analysis of distributed Q-learning</title>
<link>https://arxiv.org/abs/2405.14078</link>
<guid>https://arxiv.org/abs/2405.14078</guid>
<content:encoded><![CDATA[
arXiv:2405.14078v2 Announce Type: replace 
Abstract: Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL). In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards. In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\tilde{\mathcal{O}}\left( \min\left\{\frac{1}{\epsilon^2}\frac{t_{\text{mix}}}{(1-\gamma)^6 d_{\min}^4 } ,\frac{1}{\epsilon}\frac{\sqrt{|\gS||\gA|}}{(1-\sigma_2(\boldsymbol{W}))(1-\gamma)^4 d_{\min}^3} \right\}\right)$ under tabular lookup
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search</title>
<link>https://arxiv.org/abs/2408.10635</link>
<guid>https://arxiv.org/abs/2408.10635</guid>
<content:encoded><![CDATA[
arXiv:2408.10635v3 Announce Type: replace 
Abstract: Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring Medical AI Safety: Interpretability-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data</title>
<link>https://arxiv.org/abs/2501.13818</link>
<guid>https://arxiv.org/abs/2501.13818</guid>
<content:encoded><![CDATA[
arXiv:2501.13818v2 Announce Type: replace 
Abstract: Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fatal consequences in practice. Whereas a multitude of works address either the detection or mitigation of such shortcut behavior in isolation, the Reveal2Revise approach provides a comprehensive bias mitigation framework combining these steps. However, effectively addressing these biases often requires substantial labeling efforts from domain experts. In this work, we review the steps of the Reveal2Revise framework and enhance it with semi-automated interpretability-based bias annotation capabilities. This includes methods for the sample- and feature-level bias annotation, providing valuable information for bias mitigation methods to unlearn the undesired shortcut behavior. We show the applicability of the framework using four medical datasets across two modalities, featuring controlled and real-world spurious correlations caused by data artifacts. We successfully identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision Transformer models, ultimately increasing their robustness and applicability for real-world medical tasks. Our code is available at https://github.com/frederikpahde/medical-ai-safety.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Approach to Probabilistic Neuro-Symbolic Robustness Verification</title>
<link>https://arxiv.org/abs/2502.03274</link>
<guid>https://arxiv.org/abs/2502.03274</guid>
<content:encoded><![CDATA[
arXiv:2502.03274v2 Announce Type: replace 
Abstract: Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising direction for integrating neural learning with symbolic reasoning. Typically, in the probabilistic variant of such systems, a neural network first extracts a set of symbols from sub-symbolic input, which are then used by a symbolic component to reason in a probabilistic manner towards answering a query. In this work, we address the problem of formally verifying the robustness of such NeSy probabilistic reasoning systems, therefore paving the way for their safe deployment in critical domains. We analyze the complexity of solving this problem exactly, and show that a decision version of the core computation is $\mathrm{NP}^{\mathrm{PP}}$-complete. In the face of this result, we propose the first approach for approximate, relaxation-based verification of probabilistic NeSy systems. We demonstrate experimentally on a standard NeSy benchmark that the proposed method scales exponentially better than solver-based solutions and apply our technique to a real-world autonomous driving domain, where we verify a safety property under large input dimensionalities.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis</title>
<link>https://arxiv.org/abs/2502.05934</link>
<guid>https://arxiv.org/abs/2502.05934</guid>
<content:encoded><![CDATA[
arXiv:2502.05934v2 Announce Type: replace 
Abstract: We formalize AI alignment as a multi-objective optimization problem called $\langle M,N,\varepsilon,\delta\rangle$-agreement that generalizes prior approaches with fewer assumptions, in which a set of $N$ agents (including humans) must reach approximate ($\varepsilon$) agreement across $M$ candidate objectives with probability at least $1-\delta$. Using communication complexity, we prove an information-theoretic lower bound demonstrating that once either $M$ or $N$ is large enough, no interaction or rationality can avoid intrinsic alignment overheads. This barrier establishes rigorous intrinsic limits to alignment \emph{itself}, not merely to specific methods, clarifying a crucial ``no free lunch'' principle: encoding ``all human values'' inevitably leads to misalignment, requiring future methods to explicitly manage complexity through consensus-driven reduction or prioritization of objectives. Complementing this impossibility result, we provide explicit algorithms achieving alignment under both computationally unbounded and bounded rationality with noisy messages. Even in these best-case scenarios where alignment to arbitrary precision is theoretically guaranteed, our analysis identifies three critical scalability barriers: the number of tasks ($M$), agents ($N$), and task state space size ($D$); thereby highlighting fundamental complexity-theoretic constraints and providing guidelines for safer, scalable human-AI collaboration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKE: Steering Activations for Knowledge Editing</title>
<link>https://arxiv.org/abs/2503.01751</link>
<guid>https://arxiv.org/abs/2503.01751</guid>
<content:encoded><![CDATA[
arXiv:2503.01751v2 Announce Type: replace 
Abstract: As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models</title>
<link>https://arxiv.org/abs/2503.11820</link>
<guid>https://arxiv.org/abs/2503.11820</guid>
<content:encoded><![CDATA[
arXiv:2503.11820v2 Announce Type: replace 
Abstract: Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation works in the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors, from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation are definable inductively on such syntax, and operate as a form of functor pre-composition. This approach introduces a modular, algebraic perspective in the theory of probabilistic graphical models.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D-Curri-DPO: Two-Dimensional Curriculum Learning for Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2504.07856</link>
<guid>https://arxiv.org/abs/2504.07856</guid>
<content:encoded><![CDATA[
arXiv:2504.07856v3 Announce Type: replace 
Abstract: Aligning large language models with human preferences is crucial for their safe deployment. While Direct Preference Optimization (DPO) offers an efficient alternative to reinforcement learning from human feedback, traditional DPO methods are limited by their reliance on single preference pairs. Recent work like Curriculum-DPO integrates multiple pairs using a one-dimensional difficulty curriculum based on pairwise distinguishability (PD), but overlooks the complexity of the input prompt itself. To address this, we propose 2D-Curri-DPO, a novel framework employing a two-dimensional curriculum that jointly models Prompt Complexity (PC) and Pairwise Distinguishability. This framework introduces dual difficulty metrics to quantify prompt semantic complexity and response preference clarity, defines a curriculum strategy space encompassing multiple selectable strategies for task adaptation, and incorporates a KL-divergence-based adaptive mechanism for dynamic reference model updates to enhance training stability. Comprehensive experiments demonstrate that 2D-Curri-DPO significantly outperforms standard DPO and prior curriculum methods across multiple benchmarks, including MT-Bench, Vicuna Bench, and WizardLM. Our approach achieves state-of-the-art performance on challenging test sets like UltraFeedback. Ablation studies confirm the benefits of the 2D structure and adaptive mechanisms, while analysis provides guidance for strategy selection. These findings demonstrate that effective alignment requires modeling both prompt complexity and pairwise distinguishability, establishing adaptive, multi-dimensional curriculum learning as a powerful and interpretable new paradigm for preference-based language model optimization.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v4 Announce Type: replace 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: Automated Synthesis for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
arXiv:2506.15787v3 Announce Type: replace 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses</title>
<link>https://arxiv.org/abs/2507.05629</link>
<guid>https://arxiv.org/abs/2507.05629</guid>
<content:encoded><![CDATA[
arXiv:2507.05629v2 Announce Type: replace 
Abstract: Retrieval practice is a well-established pedagogical technique known to significantly enhance student learning and knowledge retention. However, generating high-quality retrieval practice questions is often time-consuming and labor intensive for instructors, especially in rapidly evolving technical subjects. Large Language Models (LLMs) offer the potential to automate this process by generating questions in response to prompts, yet the effectiveness of LLM-generated retrieval practice on student learning remains to be established. In this study, we conducted an empirical study involving two college-level data science courses, with approximately 60 students. We compared learning outcomes during one week in which students received LLM-generated multiple-choice retrieval practice questions to those from a week in which no such questions were provided. Results indicate that students exposed to LLM-generated retrieval practice achieved significantly higher knowledge retention, with an average accuracy of 89%, compared to 73% in the week without such practice. These findings suggest that LLM-generated retrieval questions can effectively support student learning and may provide a scalable solution for integrating retrieval practice into real-time teaching. However, despite these encouraging outcomes and the potential time-saving benefits, cautions must be taken, as the quality of LLM-generated questions can vary. Instructors must still manually verify and revise the generated questions before releasing them to students.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light</title>
<link>https://arxiv.org/abs/2507.11482</link>
<guid>https://arxiv.org/abs/2507.11482</guid>
<content:encoded><![CDATA[
arXiv:2507.11482v3 Announce Type: replace 
Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition of agency, the objective of learning, and the scope of the reward hypothesis--have been highlighted as key targets for conceptual revision, with major implications for theory and application. We propose a framework, inspired by open-ended evolutionary theory, to reconsider these three "dogmas." We revisit each assumption and address related concerns raised alongside them. To make our arguments relevant to RL as a model of biological learning, we first establish that evolutionary dynamics can plausibly operate within living brains over an individual's lifetime, and are not confined to cross-generational processes. We begin by revisiting the second dogma, drawing on evolutionary insights to enrich the "adaptation-rather-than-search" view of learning. We then address the third dogma regarding the limits of the reward hypothesis, using analogies from evolutionary fitness to illuminate the scalar reward vs. multi-objective debate. After discussing practical implications for exploration in RL, we turn to the first--and arguably most fundamental--issue: the absence of a formal account of agency. We argue that unlike the other two problems, the evolutionary paradigm alone cannot resolve the agency question, though it gestures in a productive direction. We advocate integrating ideas from origins-of-life theory, where the thermodynamics of sustenance and replication offer promising foundations for understanding agency and resource-constrained reinforcement learning in biological systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Misconceptions in Social Bots Research</title>
<link>https://arxiv.org/abs/2303.17251</link>
<guid>https://arxiv.org/abs/2303.17251</guid>
<content:encoded><![CDATA[
arXiv:2303.17251v4 Announce Type: replace-cross 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial attacks and defenses in explainable artificial intelligence: A survey</title>
<link>https://arxiv.org/abs/2306.06123</link>
<guid>https://arxiv.org/abs/2306.06123</guid>
<content:encoded><![CDATA[
arXiv:2306.06123v4 Announce Type: replace-cross 
Abstract: Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning (AdvML) highlight the limitations and vulnerabilities of state-of-the-art explanation methods, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This survey provides a comprehensive overview of research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We introduce a unified notation and taxonomy of methods facilitating a common ground for researchers and practitioners from the intersecting research fields of AdvML and XAI. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI). Future work should address improving explanation methods and evaluation protocols to take into account the reported safety issues.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The pitfalls of next-token prediction</title>
<link>https://arxiv.org/abs/2403.06963</link>
<guid>https://arxiv.org/abs/2403.06963</guid>
<content:encoded><![CDATA[
arXiv:2403.06963v3 Announce Type: replace-cross 
Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.
  As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.
  Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Concurrent Multi-Robot Coverage Path Planning</title>
<link>https://arxiv.org/abs/2403.10460</link>
<guid>https://arxiv.org/abs/2403.10460</guid>
<content:encoded><![CDATA[
arXiv:2403.10460v2 Announce Type: replace-cross 
Abstract: Recently, centralized receding horizon online multi-robot coverage path planning algorithms have shown remarkable scalability in thoroughly exploring large, complex, unknown workspaces with many robots. In a horizon, the path planning and the path execution interleave, meaning when the path planning occurs for robots with no paths, the robots with outstanding paths do not execute, and subsequently, when the robots with new or outstanding paths execute to reach respective goals, path planning does not occur for those robots yet to get new paths, leading to wastage of both the robotic and the computation resources. As a remedy, we propose a centralized algorithm that is not horizon-based. It plans paths at any time for a subset of robots with no paths, i.e., who have reached their previously assigned goals, while the rest execute their outstanding paths, thereby enabling concurrent planning and execution. We formally prove that the proposed algorithm ensures complete coverage of an unknown workspace and analyze its time complexity. To demonstrate scalability, we evaluate our algorithm to cover eight large $2$D grid benchmark workspaces with up to 512 aerial and ground robots, respectively. A comparison with a state-of-the-art horizon-based algorithm shows its superiority in completing the coverage with up to 1.6x speedup. For validation, we perform ROS + Gazebo simulations in six 2D grid benchmark workspaces with 10 quadcopters and TurtleBots, respectively. We also successfully conducted one outdoor experiment with three quadcopters and one indoor with two TurtleBots.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges</title>
<link>https://arxiv.org/abs/2406.06736</link>
<guid>https://arxiv.org/abs/2406.06736</guid>
<content:encoded><![CDATA[
arXiv:2406.06736v3 Announce Type: replace-cross 
Abstract: The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data</title>
<link>https://arxiv.org/abs/2406.10521</link>
<guid>https://arxiv.org/abs/2406.10521</guid>
<content:encoded><![CDATA[
arXiv:2406.10521v4 Announce Type: replace-cross 
Abstract: In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity. To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRASim: A Fine-Grained World Model for Robot Manipulation</title>
<link>https://arxiv.org/abs/2406.14540</link>
<guid>https://arxiv.org/abs/2406.14540</guid>
<content:encoded><![CDATA[
arXiv:2406.14540v2 Announce Type: replace-cross 
Abstract: World models allow autonomous agents to plan and explore by predicting the visual outcomes of different actions. However, for robot manipulation, it is challenging to accurately model the fine-grained robot-object interaction within the visual space using existing methods which overlooks precise alignment between each action and the corresponding frame. In this paper, we present IRASim, a novel world model capable of generating videos with fine-grained robot-object interaction details, conditioned on historical observations and robot action trajectories. We train a diffusion transformer and introduce a novel frame-level action-conditioning module within each transformer block to explicitly model and strengthen the action-frame alignment. Extensive experiments show that: (1) the quality of the videos generated by our method surpasses all the baseline methods and scales effectively with increased model size and computation; (2) policy evaluations using IRASim exhibit a strong correlation with those using the ground-truth simulator, highlighting its potential to accelerate real-world policy evaluation; (3) testing-time scaling through model-based planning with IRASim significantly enhances policy performance, as evidenced by an improvement in the IoU metric on the Push-T benchmark from 0.637 to 0.961; (4) IRASim provides flexible action controllability, allowing virtual robotic arms in datasets to be controlled via a keyboard or VR controller.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2407.15549</link>
<guid>https://arxiv.org/abs/2407.15549</guid>
<content:encoded><![CDATA[
arXiv:2407.15549v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Algebra Meets Large Language Models: Instruction-Based Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes</title>
<link>https://arxiv.org/abs/2408.02275</link>
<guid>https://arxiv.org/abs/2408.02275</guid>
<content:encoded><![CDATA[
arXiv:2408.02275v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel integration of Large Language Models (LLMs) with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene editing, particularly for object repositioning tasks, which traditionally requires intricate manual processes and specialized expertise. These conventional methods typically suffer from reliance on large training datasets or lack a formalized language for precise edits. Utilizing CGA as a robust formal language, our system, Shenlong, precisely models spatial transformations necessary for accurate object repositioning. Leveraging the zero-shot learning capabilities of pre-trained LLMs, Shenlong translates natural language instructions into CGA operations which are then applied to the scene, facilitating exact spatial transformations within 3D scenes without the need for specialized pre-training. Implemented in a realistic simulation environment, Shenlong ensures compatibility with existing graphics pipelines. To accurately assess the impact of CGA, we benchmark against robust Euclidean Space baselines, evaluating both latency and accuracy. Comparative performance evaluations indicate that Shenlong significantly reduces LLM response times by 16% and boosts success rates by 9.6% on average compared to the traditional methods. Notably, Shenlong achieves a 100% perfect success rate in common practical queries, a benchmark where other systems fall short. These advancements underscore Shenlong's potential to democratize 3D scene editing, enhancing accessibility and fostering innovation across sectors such as education, digital entertainment, and virtual reality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Arithmetic for Language Expansion in Speech Translation</title>
<link>https://arxiv.org/abs/2409.11274</link>
<guid>https://arxiv.org/abs/2409.11274</guid>
<content:encoded><![CDATA[
arXiv:2409.11274v3 Announce Type: replace-cross 
Abstract: Recent progress in large language models (LLMs) has gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-tuned speech translation (ST). However, expanding language pairs is costly due to re-training on combined new and previous datasets. To address this, we aim to build a one-to-many ST system from existing one-to-one ST systems using task arithmetic without re-training. Direct application of task arithmetic in ST leads to language confusion; therefore, we introduce an augmented task arithmetic method incorporating a language control model to ensure correct target language generation. Our experiments on MuST-C and CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains of 8.87 and 11.83. In addition, we demonstrate our framework can extend to language pairs lacking paired ST training data or pre-trained ST models by synthesizing ST models based on existing machine translation (MT) and ST models via task analogies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSON: A Group-based Social Navigation Framework with Large Multimodal Model</title>
<link>https://arxiv.org/abs/2409.18084</link>
<guid>https://arxiv.org/abs/2409.18084</guid>
<content:encoded><![CDATA[
arXiv:2409.18084v3 Announce Type: replace-cross 
Abstract: With the increasing presence of service robots and autonomous vehicles in human environments, navigation systems need to evolve beyond simple destination reach to incorporate social awareness. This paper introduces GSON, a novel group-based social navigation framework that leverages Large Multimodal Models (LMMs) to enhance robots' social perception capabilities. Our approach uses visual prompting to enable zero-shot extraction of social relationships among pedestrians and integrates these results with robust pedestrian detection and tracking pipelines to overcome the inherent inference speed limitations of LMMs. The planning system incorporates a mid-level planner that sits between global path planning and local motion planning, effectively preserving both global context and reactive responsiveness while avoiding disruption of the predicted social group. We validate GSON through extensive real-world mobile robot navigation experiments involving complex social scenarios such as queuing, conversations, and photo sessions. Comparative results show that our system significantly outperforms existing navigation approaches in minimizing social perturbations while maintaining comparable performance on traditional navigation metrics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulated patient systems are intelligent when powered by large language model-based AI agents</title>
<link>https://arxiv.org/abs/2409.18924</link>
<guid>https://arxiv.org/abs/2409.18924</guid>
<content:encoded><![CDATA[
arXiv:2409.18924v3 Announce Type: replace-cross 
Abstract: Simulated patient systems play an important role in modern medical education and research, providing safe, integrative medical training environments and supporting clinical decision-making simulations. We developed AIPatient, an intelligent simulated patient system powered by large language model-based AI agents. The system incorporates the Retrieval Augmented Generation (RAG) framework, powered by six task-specific LLM-based AI agents for complex reasoning. For simulation reality, the system is also powered by the AIPatient KG (Knowledge Graph), built with de-identified real patient data from the Medical Information Mart for Intensive Care (MIMIC)-III database. Primary outcomes showcase the system's intelligence, including the system's accuracy in Electronic Record (EHR)-based medical Question Answering (QA), readability, robustness, and stability. The system achieved a QA accuracy of 94.15% when all six AI agents present, surpassing benchmarks with partial or no agent integration. Its knowledgebase demonstrated high validity (F1 score=0.89). Readability scores showed median Flesch Reading Ease at 77.23 and median Flesch Kincaid Grade at 5.6, indicating accessibility to all medical professionals. Robustness and stability were confirmed with non-significant variance (ANOVA F-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical students further demonstrated that AIPatient offers high fidelity, strong usability, and effective educational value, performing comparably or better than human-simulated patients in medical history-taking scenarios. The promising intelligence of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Manifold Structure Using Ollivier-Ricci Curvature</title>
<link>https://arxiv.org/abs/2410.01149</link>
<guid>https://arxiv.org/abs/2410.01149</guid>
<content:encoded><![CDATA[
arXiv:2410.01149v2 Announce Type: replace-cross 
Abstract: We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuse Before Transfer: Knowledge Fusion for Heterogeneous Distillation</title>
<link>https://arxiv.org/abs/2410.12342</link>
<guid>https://arxiv.org/abs/2410.12342</guid>
<content:encoded><![CDATA[
arXiv:2410.12342v2 Announce Type: replace-cross 
Abstract: Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on ImageNet-1K. Our code and models will be released.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems</title>
<link>https://arxiv.org/abs/2411.01297</link>
<guid>https://arxiv.org/abs/2411.01297</guid>
<content:encoded><![CDATA[
arXiv:2411.01297v3 Announce Type: replace-cross 
Abstract: This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model-predictive control. Hion controllers estimate future states and develop an optimal control strategy using Pontryagin's Maximum Principle. The proposed framework, along with our Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture, allows for custom transient behavior, predictive control, and closed-loop feedback, addressing limitations of existing methods. Comparative analyses with established model-predictive controllers revealed Hion controllers' superior optimality and tracking capabilities. Optimal control strategies are also demonstrated for both linear and non-linear dynamical systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Puzzle Similarity: A Perceptually-guided Cross-Reference Metric for Artifact Detection in 3D Scene Reconstructions</title>
<link>https://arxiv.org/abs/2411.17489</link>
<guid>https://arxiv.org/abs/2411.17489</guid>
<content:encoded><![CDATA[
arXiv:2411.17489v3 Announce Type: replace-cross 
Abstract: Modern reconstruction techniques can effectively model complex 3D scenes from sparse 2D views. However, automatically assessing the quality of novel views and identifying artifacts is challenging due to the lack of ground truth images and the limitations of no-reference image metrics in predicting reliable artifact maps. The absence of such metrics hinders assessment of the quality of novel views and limits the adoption of post-processing techniques, such as inpainting, to enhance reconstruction quality. To tackle this, recent work has established a new category of metrics (cross-reference), predicting image quality solely by leveraging context from alternate viewpoint captures (arXiv:2404.14409). In this work, we propose a new cross-reference metric, Puzzle Similarity, which is designed to localize artifacts in novel views. Our approach utilizes image patch statistics from the training views to establish a scene-specific distribution, later used to identify poorly reconstructed regions in the novel views. Given the lack of good measures to evaluate cross-reference methods in the context of 3D reconstruction, we collected a novel human-labeled dataset of artifact and distortion maps in unseen reconstructed views. Through this dataset, we demonstrate that our method achieves state-of-the-art localization of artifacts in novel views, correlating with human assessment, even without aligned references. We can leverage our new metric to enhance applications like automatic image restoration, guided acquisition, or 3D reconstruction from sparse inputs. Find the project page at https://nihermann.github.io/puzzlesim/ .
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning</title>
<link>https://arxiv.org/abs/2412.03248</link>
<guid>https://arxiv.org/abs/2412.03248</guid>
<content:encoded><![CDATA[
arXiv:2412.03248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that our method substantially reduces computation load (e.g., a $\textbf{7-fold}$ reduction in FLOPs) while preserving the performance of video and image LLMs. Further, at a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., $\textbf{+4.6}$ on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code is available at https://github.com/LaVi-Lab/AIM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVE: Taming DINO for Subject-Driven Video Editing</title>
<link>https://arxiv.org/abs/2412.03347</link>
<guid>https://arxiv.org/abs/2412.03347</guid>
<content:encoded><![CDATA[
arXiv:2412.03347v2 Announce Type: replace-cross 
Abstract: Building on the success of diffusion models in image generation and editing, video editing has recently gained substantial attention. However, maintaining temporal consistency and motion alignment still remains challenging. To address these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework designed to facilitate subject-driven editing in source videos conditioned on either target text prompts or reference images with specific identities. The core of DIVE lies in leveraging the powerful semantic features extracted from a pretrained DINOv2 model as implicit correspondences to guide the editing process. Specifically, to ensure temporal motion consistency, DIVE employs DINO features to align with the motion trajectory of the source video. For precise subject editing, DIVE incorporates the DINO features of reference images into a pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs), effectively registering the target subject's identity. Extensive experiments on diverse real-world videos demonstrate that our framework can achieve high-quality editing results with robust motion consistency, highlighting the potential of DINO to contribute to video editing. Project page: https://dino-video-editing.github.io
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training</title>
<link>https://arxiv.org/abs/2501.07237</link>
<guid>https://arxiv.org/abs/2501.07237</guid>
<content:encoded><![CDATA[
arXiv:2501.07237v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Memory-Efficient Transformer-Based Model Training in AI for Science</title>
<link>https://arxiv.org/abs/2501.11847</link>
<guid>https://arxiv.org/abs/2501.11847</guid>
<content:encoded><![CDATA[
arXiv:2501.11847v2 Announce Type: replace-cross 
Abstract: Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews transformer-based LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. This survey systematically reviews and categorizes memory-efficient pre-training techniques for large-scale transformers, including algorithm-level, system-level, and hardware-software co-optimization. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. By bridging model efficiency and scientific application needs, we hope to provide insights for scalable and cost-effective LLM training in AI for science.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing</title>
<link>https://arxiv.org/abs/2502.03086</link>
<guid>https://arxiv.org/abs/2502.03086</guid>
<content:encoded><![CDATA[
arXiv:2502.03086v2 Announce Type: replace-cross 
Abstract: This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Swap Joint Diffusion for 2D Long-Form Latent Generation</title>
<link>https://arxiv.org/abs/2502.05130</link>
<guid>https://arxiv.org/abs/2502.05130</guid>
<content:encoded><![CDATA[
arXiv:2502.05130v3 Announce Type: replace-cross 
Abstract: This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient method to generate seamless and coherence long spectrum and panorama through latent swap joint diffusion across multi-views. We first investigate the spectrum aliasing problem in spectrum-based audio generation caused by existing joint diffusion methods. Through a comparative analysis of the VAE latent representation of Mel-spectra and RGB images, we identify that the failure arises from excessive suppression of high-frequency components during the spectrum denoising process due to the averaging operator. To address this issue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap applied to the overlapping region of adjacent views. Leveraging stepwise differentiated trajectories of adjacent subviews, this swap operator adaptively enhances high-frequency components and avoid spectrum distortion. Furthermore, to improve global cross-view consistency in non-overlapping regions, we introduce Reference-Guided Latent Swap, a unidirectional latent swap operator that provides a centralized reference trajectory to synchronize subview diffusions. By refining swap timing and intervals, we can achieve a cross-view similarity-diversity balance in a forward-only manner. Quantitative and qualitative experiments demonstrate that SaFa significantly outperforms existing joint diffusion methods and even training-based methods in audio generation using both U-Net and DiT models, along with effective longer length adaptation. It also adapts well to panorama generation, achieving comparable performance with 2 $\sim$ 20 $\times$ faster speed and greater model generalizability. More generation demos are available at https://swapforward.github.io/
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</title>
<link>https://arxiv.org/abs/2502.12207</link>
<guid>https://arxiv.org/abs/2502.12207</guid>
<content:encoded><![CDATA[
arXiv:2502.12207v2 Announce Type: replace-cross 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://github.com/LMBTough/PAR
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-branch of Attention Yields Accurate Results for Tabular Data</title>
<link>https://arxiv.org/abs/2502.12507</link>
<guid>https://arxiv.org/abs/2502.12507</guid>
<content:encoded><![CDATA[
arXiv:2502.12507v2 Announce Type: replace-cross 
Abstract: Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Multi-Branch of Attention (MBA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A calibration test for evaluating set-based epistemic uncertainty representations</title>
<link>https://arxiv.org/abs/2502.16299</link>
<guid>https://arxiv.org/abs/2502.16299</guid>
<content:encoded><![CDATA[
arXiv:2502.16299v2 Announce Type: replace-cross 
Abstract: The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set's predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.18978</link>
<guid>https://arxiv.org/abs/2502.18978</guid>
<content:encoded><![CDATA[
arXiv:2502.18978v4 Announce Type: replace-cross 
Abstract: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conceptualizing Uncertainty: A Concept-based Approach to Explaining Uncertainty</title>
<link>https://arxiv.org/abs/2503.03443</link>
<guid>https://arxiv.org/abs/2503.03443</guid>
<content:encoded><![CDATA[
arXiv:2503.03443v2 Announce Type: replace-cross 
Abstract: Uncertainty in machine learning refers to the degree of confidence or lack thereof in a model's predictions. While uncertainty quantification methods exist, explanations of uncertainty, especially in high-dimensional settings, remain an open challenge. Existing work focuses on feature attribution approaches which are restricted to local explanations. Understanding uncertainty, its origins, and characteristics on a global scale is crucial for enhancing interpretability and trust in a model's predictions. In this work, we propose to explain the uncertainty in high-dimensional data classification settings by means of concept activation vectors which give rise to local and global explanations of uncertainty. We demonstrate the utility of the generated explanations by leveraging them to refine and improve our model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI</title>
<link>https://arxiv.org/abs/2503.04844</link>
<guid>https://arxiv.org/abs/2503.04844</guid>
<content:encoded><![CDATA[
arXiv:2503.04844v5 Announce Type: replace-cross 
Abstract: Here we introduce Narrative Context Protocol (NCP), an open-source narrative standard designed to enable narrative interoperability, AI-driven authoring tools, real-time emergent narratives, and more. By encoding a story's structure in a "Storyform," which is a structured register of its narrative features, NCP enables narrative portability across systems as well as intent-based constraints for generative storytelling systems. We demonstrate the capabilities of NCP through a year-long experiment, during which an author used NCP and a custom authoring platform to create a playable, text-based experience based on her pre-existing novella. This experience is driven by generative AI, with unconstrained natural language input. NCP functions as a set of "guardrails" that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Levels of Analysis for Large Language Models</title>
<link>https://arxiv.org/abs/2503.13401</link>
<guid>https://arxiv.org/abs/2503.13401</guid>
<content:encoded><![CDATA[
arXiv:2503.13401v2 Announce Type: replace-cross 
Abstract: Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on the levels of analysis that David Marr proposed for studying information processing systems. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQuat: Subspace-orthogonal KV Cache Quantization</title>
<link>https://arxiv.org/abs/2503.24358</link>
<guid>https://arxiv.org/abs/2503.24358</guid>
<content:encoded><![CDATA[
arXiv:2503.24358v2 Announce Type: replace-cross 
Abstract: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt</title>
<link>https://arxiv.org/abs/2504.04142</link>
<guid>https://arxiv.org/abs/2504.04142</guid>
<content:encoded><![CDATA[
arXiv:2504.04142v2 Announce Type: replace-cross 
Abstract: In this very personal workography, I relate my 40-year experiences as a researcher and educator in and around Artificial Intelligence (AI), more specifically Natural Language Processing. I describe how curiosity, and the circumstances of the day, led me to work in both industry and academia, and in various countries, including The Netherlands (Amsterdam, Eindhoven, and Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and China (Beijing and Harbin). People and anecdotes play a large role in my story; the history of AI forms its backdrop. I focus on things that might be of interest to (even) younger colleagues, given the choices they face in their own work and life at a time when AI is finally emerging from the shadows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not someone, but something: Rethinking trust in the age of medical AI</title>
<link>https://arxiv.org/abs/2504.05331</link>
<guid>https://arxiv.org/abs/2504.05331</guid>
<content:encoded><![CDATA[
arXiv:2504.05331v3 Announce Type: replace-cross 
Abstract: As artificial intelligence (AI) becomes embedded in healthcare, trust in medical decision-making is changing fast. Nowhere is this shift more visible than in radiology, where AI tools are increasingly embedded across the imaging workflow - from scheduling and acquisition to interpretation, reporting, and communication with referrers and patients. This opinion paper argues that trust in AI isn't a simple transfer from humans to machines - it is a dynamic, evolving relationship that must be built and maintained. Rather than debating whether AI belongs in medicine, it asks: what kind of trust must AI earn, and how? Drawing from philosophy, bioethics, and system design, it explores the key differences between human trust and machine reliability - emphasizing transparency, accountability, and alignment with the values of good care. It argues that trust in AI should not be built on mimicking empathy or intuition, but on thoughtful design, responsible deployment, and clear moral responsibility. The goal is a balanced view - one that avoids blind optimism and reflexive fear. Trust in AI must be treated not as a given, but as something to be earned over time.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning</title>
<link>https://arxiv.org/abs/2504.06538</link>
<guid>https://arxiv.org/abs/2504.06538</guid>
<content:encoded><![CDATA[
arXiv:2504.06538v2 Announce Type: replace-cross 
Abstract: We present OPAL (Operant Physical Agent with Language), a novel vision-language-action architecture that introduces topological constraints to flow matching for robotic control. To do so, we further introduce topological attention. Our approach models action sequences as topologically-structured representations with non-trivial constraints. Experimental results across 10 complex manipulation tasks demonstrate OPAL's superior performance compared to previous approaches, including Octo, OpenVLA, and ${\pi}$0.
  Our architecture achieves significant improvements in zero-shot performance without requiring task-specific fine-tuning, while reducing inference computational requirements by 42%. The theoretical guarantees provided by our topological approach result in more coherent long-horizon action sequences. Our results highlight the potential of constraining the search space of learning problems in robotics by deriving from fundamental physical laws, and the possibility of using topological attention to embed causal understanding into transformer architectures.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation</title>
<link>https://arxiv.org/abs/2504.10541</link>
<guid>https://arxiv.org/abs/2504.10541</guid>
<content:encoded><![CDATA[
arXiv:2504.10541v2 Announce Type: replace-cross 
Abstract: The burgeoning presence of Large Language Models (LLM) is propelling the development of personalized recommender systems. Most existing LLM-based methods fail to sufficiently explore the multi-view graph structure correlations inherent in recommendation scenarios. To this end, we propose a novel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation (HeLLM), designed to equip LLMs with the capability to capture intricate higher-order semantic correlations by fusing graph-level contextual signals with sequence-level behavioral patterns. In the recommender pre-training phase, we design a user hypergraph to uncover shared interest preferences among users and an item hypergraph to capture correlations within multimodal similarities among items. The hypergraph convolution and synergistic contrastive learning mechanism are introduced to enhance the distinguishability of learned representations. In the LLM fine-tuning phase, we inject the learned graph-structured embeddings directly into the LLM's architecture and integrate sequential features capturing each user's chronological behavior. This process enables hypergraphs to leverage graph-structured information as global context, enhancing the LLM's ability to perceive complex relational patterns and integrate multimodal information, while also modeling local temporal dynamics. Extensive experiments demonstrate the superiority of our proposed method over state-of-the-art baselines, confirming the advantages of fusing hypergraph-based context with sequential user behavior in LLMs for recommendation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.10774</link>
<guid>https://arxiv.org/abs/2505.10774</guid>
<content:encoded><![CDATA[
arXiv:2505.10774v2 Announce Type: replace-cross 
Abstract: Time series forecasting is important for applications spanning energy markets, climate analysis, and traffic management. However, existing methods struggle to effectively integrate exogenous texts and align them with the probabilistic nature of large language models (LLMs). Current approaches either employ shallow text-time series fusion via basic prompts or rely on deterministic numerical decoding that conflict with LLMs' token-generation paradigm, which limits contextual awareness and distribution modeling. To address these limitations, we propose CAPTime, a context-aware probabilistic multimodal time series forecasting method that leverages text-informed abstraction and autoregressive LLM decoding. Our method first encodes temporal patterns using a pretrained time series encoder, then aligns them with textual contexts via learnable interactions to produce joint multimodal representations. By combining a mixture of distribution experts with frozen LLMs, we enable context-aware probabilistic forecasting while preserving LLMs' inherent distribution modeling capabilities. Experiments on diverse time series forecasting tasks demonstrate the superior accuracy and generalization of CAPTime, particularly in multimodal scenarios. Additional analysis highlights its robustness in data-scarce scenarios through hybrid probabilistic decoding.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11864</link>
<guid>https://arxiv.org/abs/2505.11864</guid>
<content:encoded><![CDATA[
arXiv:2505.11864v3 Announce Type: replace-cross 
Abstract: As generative agents become increasingly capable, alignment of their behavior with complex human values remains a fundamental challenge. Existing approaches often simplify human intent through reduction to a scalar reward, overlooking the multi-faceted nature of human feedback. In this work, we introduce a theoretical framework for preference-based Multi-Objective Inverse Reinforcement Learning (MO-IRL), where human preferences are modeled as latent vector-valued reward functions. We formalize the problem of recovering a Pareto-optimal reward representation from noisy preference queries and establish conditions for identifying the underlying multi-objective structure. We derive tight sample complexity bounds for recovering $\epsilon$-approximations of the Pareto front and introduce a regret formulation to quantify suboptimality in this multi-objective setting. Furthermore, we propose a provably convergent algorithm for policy optimization using preference-inferred reward cones. Our results bridge the gap between practical alignment techniques and theoretical guarantees, providing a principled foundation for learning aligned behaviors in a high-dimension and value-pluralistic environment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v2 Announce Type: replace-cross 
Abstract: Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is effective and has attracted significant attention. However, it is computationally prohibitive due to massive, data-hungry text-based reward models. By changing the data source from text to hidden states, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight technique that leverages the rich information embedded in LLM hidden states to address these issues, which operates on token-level and consists of only linear layers. Extensive experiments show that SWIFT outperforms baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training, demonstrating significant efficiency improvement. SWIFT's robust scalability, applicability to some closed-source models via logits, and ability to be combined with traditional reward models to yield further performance gains underscore its practical value.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FB-RAG: Improving RAG with Forward and Backward Lookup</title>
<link>https://arxiv.org/abs/2505.17206</link>
<guid>https://arxiv.org/abs/2505.17206</guid>
<content:encoded><![CDATA[
arXiv:2505.17206v2 Announce Type: replace-cross 
Abstract: Traditional Retrieval-Augmented Generation (RAG) struggles with complex queries that lack strong signals to retrieve the most relevant context, forcing a trade-off between choosing a small context that misses key information and a large context that confuses the LLM. To address this, we propose Forward-Backward RAG (FB-RAG), a new training-free framework based on a simple yet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to peek into potential future generations, using evidence from multiple sampled outputs to precisely identify the most relevant context for a final, more powerful generator. This improves performance without complex finetuning or Reinforcement Learning common in prior work. Across 9 datasets, FB-RAG consistently delivers strong results. Further, the performance gains can be achieved with reduced latency due to a shorter, more focused prompt for the powerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with over 48% latency reduction or achieves an 8% performance improvement with a 10% latency reduction. Our analysis finds cases where even when the forward-looking LLM fails to generate correct answers, its attempts are sufficient to guide the final model to an accurate response, demonstrating how smaller LLMs can systematically improve the performance and efficiency of larger ones.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
arXiv:2505.20734v5 Announce Type: replace-cross 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v5 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</title>
<link>https://arxiv.org/abs/2506.02733</link>
<guid>https://arxiv.org/abs/2506.02733</guid>
<content:encoded><![CDATA[
arXiv:2506.02733v2 Announce Type: replace-cross 
Abstract: Existing optical flow datasets focus primarily on real-world simulation or synthetic human motion, but few are tailored to Celluloid(cel) anime character motion: a domain with unique visual and motion characteristics. To bridge this gap and facilitate research in optical flow estimation and downstream tasks such as anime video generation and line drawing colorization, we introduce LinkTo-Anime, the first high-quality dataset specifically designed for cel anime character motion generated with 3D model rendering. LinkTo-Anime provides rich annotations including forward and backward optical flow, occlusion masks, and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230 training frames, 720 validation frames, and 4,320 test frames. Furthermore, a comprehensive benchmark is constructed with various optical flow estimation methods to analyze the shortcomings and limitations across multiple datasets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs</title>
<link>https://arxiv.org/abs/2506.05413</link>
<guid>https://arxiv.org/abs/2506.05413</guid>
<content:encoded><![CDATA[
arXiv:2506.05413v2 Announce Type: replace-cross 
Abstract: We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
<link>https://arxiv.org/abs/2506.09081</link>
<guid>https://arxiv.org/abs/2506.09081</guid>
<content:encoded><![CDATA[
arXiv:2506.09081v3 Announce Type: replace-cross 
Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Perturbation Guidance via Attention Head Selection</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[
arXiv:2506.10978v3 Announce Type: replace-cross 
Abstract: Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.18985</link>
<guid>https://arxiv.org/abs/2506.18985</guid>
<content:encoded><![CDATA[
arXiv:2506.18985v3 Announce Type: replace-cross 
Abstract: Recent large vision-language models (LVLMs) have advanced capabilities in visual question answering (VQA). However, interpreting where LVLMs direct their visual attention remains a significant challenge, yet is essential for understanding model behavior. We introduce GLIMPSE (Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation), a lightweight, model-agnostic framework that jointly attributes LVLM outputs to the most relevant visual evidence and textual signals that support open-ended generation. GLIMPSE fuses gradient-weighted attention, adaptive layer propagation, and relevance-weighted token aggregation to produce holistic response-level heat maps for interpreting cross-modal reasoning, outperforming prior methods in faithfulness and pushing the state-of-the-art in human-attention alignment. We demonstrate an analytic approach to uncover fine-grained insights into LVLM cross-modal attribution, trace reasoning dynamics, analyze systematic misalignment, diagnose hallucination and bias, and ensure transparency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning</title>
<link>https://arxiv.org/abs/2506.20031</link>
<guid>https://arxiv.org/abs/2506.20031</guid>
<content:encoded><![CDATA[
arXiv:2506.20031v2 Announce Type: replace-cross 
Abstract: Operations in disaster response, search \& rescue, and military missions that involve multiple agents demand automated processes to support the planning of the courses of action (COA). Moreover, traverse-affecting changes in the environment (rain, snow, blockades, etc.) may impact the expected performance of a COA, making it desirable to have a pool of COAs that are diverse in task distributions across agents. Further, variations in agent capabilities, which could be human crews and/or autonomous systems, present practical opportunities and computational challenges to the planning process. This paper presents a new theoretical formulation and computational framework to generate such diverse pools of COAs for operations with soft variations in agent-task compatibility. Key to the problem formulation is a graph abstraction of the task space and the pool of COAs itself to quantify its diversity. Formulating the COAs as a centralized multi-robot task allocation problem, a genetic algorithm is used for (order-ignoring) allocations of tasks to each agent that jointly maximize diversity within the COA pool and overall compatibility of the agent-task mappings. A graph neural network is trained using a policy gradient approach to then perform single agent task sequencing in each COA, which maximizes completion rates adaptive to task features. Our tests of the COA generation process in a simulated environment demonstrate significant performance gain over a random walk baseline, small optimality gap in task sequencing, and execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task operations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Heterogeneous Multi-dimensional Data : A Comparative Study</title>
<link>https://arxiv.org/abs/2507.00090</link>
<guid>https://arxiv.org/abs/2507.00090</guid>
<content:encoded><![CDATA[
arXiv:2507.00090v3 Announce Type: replace-cross 
Abstract: Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures</title>
<link>https://arxiv.org/abs/2507.00209</link>
<guid>https://arxiv.org/abs/2507.00209</guid>
<content:encoded><![CDATA[
arXiv:2507.00209v3 Announce Type: replace-cross 
Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iPanda: An LLM-based Agent for Automated Conformance Testing of Communication Protocols</title>
<link>https://arxiv.org/abs/2507.00378</link>
<guid>https://arxiv.org/abs/2507.00378</guid>
<content:encoded><![CDATA[
arXiv:2507.00378v2 Announce Type: replace-cross 
Abstract: Conformance testing is essential for ensuring that protocol implementations comply with their specifications. However, traditional testing approaches involve manually creating numerous test cases and scripts, making the process labor-intensive and inefficient. Recently, Large Language Models (LLMs) have demonstrated impressive text comprehension and code generation abilities, providing promising opportunities for automation. In this paper, we propose iPanda, the first framework that leverages LLMs to automate protocol conformance testing. Given a protocol specification document and its implementation, iPanda first employs a keyword-based method to automatically generate comprehensive test cases. Then, it utilizes retrieval-augmented generation and customized CoT strategy to effectively interpret the implementation and produce executable test programs. To further enhance programs' quality, iPanda incorporates an iterative optimization mechanism to refine generated test scripts interactively. Finally, by executing and analyzing the generated tests, iPanda systematically verifies compliance between implementations and protocol specifications. Comprehensive experiments on various protocols show that iPanda significantly outperforms pure LLM-based approaches, improving the success rate (Pass@1) of test-program generation by factors ranging from 4.675 times to 10.751 times.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning</title>
<link>https://arxiv.org/abs/2507.01918</link>
<guid>https://arxiv.org/abs/2507.01918</guid>
<content:encoded><![CDATA[
arXiv:2507.01918v2 Announce Type: replace-cross 
Abstract: We develop a rotation-invariant neural network that provides the global minimum-variance portfolio by jointly learning how to lag-transform historical returns and how to regularise both the eigenvalues and the marginal volatilities of large equity covariance matrices. This explicit mathematical mapping offers clear interpretability of each module's role, so the model cannot be regarded as a pure black-box. The architecture mirrors the analytical form of the global minimum-variance solution yet remains agnostic to dimension, so a single model can be calibrated on panels of a few hundred stocks and applied, without retraining, to one thousand US equities-a cross-sectional jump that demonstrates robust out-of-sample generalisation. The loss function is the future realized minimum portfolio variance and is optimized end-to-end on real daily returns. In out-of-sample tests from January 2000 to December 2024 the estimator delivers systematically lower realised volatility, smaller maximum drawdowns, and higher Sharpe ratios than the best analytical competitors, including state-of-the-art non-linear shrinkage. Furthermore, although the model is trained end-to-end to produce an unconstrained (long-short) minimum-variance portfolio, we show that its learned covariance representation can be used in general optimizers under long-only constraints with virtually no loss in its performance advantage over competing estimators. These gains persist when the strategy is executed under a highly realistic implementation framework that models market orders at the auctions, empirical slippage, exchange fees, and financing charges for leverage, and they remain stable during episodes of acute market stress.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.05714</link>
<guid>https://arxiv.org/abs/2507.05714</guid>
<content:encoded><![CDATA[
arXiv:2507.05714v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models</title>
<link>https://arxiv.org/abs/2507.08128</link>
<guid>https://arxiv.org/abs/2507.08128</guid>
<content:encoded><![CDATA[
arXiv:2507.08128v2 Announce Type: replace-cross 
Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model</title>
<link>https://arxiv.org/abs/2507.08920</link>
<guid>https://arxiv.org/abs/2507.08920</guid>
<content:encoded><![CDATA[
arXiv:2507.08920v2 Announce Type: replace-cross 
Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TolerantECG: A Foundation Model for Imperfect Electrocardiogram</title>
<link>https://arxiv.org/abs/2507.09887</link>
<guid>https://arxiv.org/abs/2507.09887</guid>
<content:encoded><![CDATA[
arXiv:2507.09887v2 Announce Type: replace-cross 
Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
arXiv:2507.12482v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have improved code generation and software automation, but remain limited by inference-time context and lack structured reasoning over code. Debugging remains unsolved despite these advances. While Claude Opus 4 and GPT-4.1 achieve >70% on code synthesis benchmarks, they perform <15% on real debugging tasks. We introduce Kodezi Chronos, a language model built specifically for debugging. Chronos combines Adaptive Graph-Guided Retrieval to navigate codebases up to 10 million lines using multi-hop traversal (92% precision, 85% recall), Persistent Debug Memory trained on 15M+ sessions, and a 7-layer architecture for iterative fix-test-refine loops. On 5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy, compared to 14.2% and 13.8% for Claude and GPT-4.1 respectively. Chronos reduces debugging time by 40% and iteration count by 65%. It resolves complex multi-file bugs involving cross-repository context and temporal reasoning. Key limitations include 23.4% success on hardware-dependent issues and 41.2% on dynamic language errors. Theoretical analysis shows O(k log d) retrieval complexity with convergence guarantees. In a human evaluation (N=50), 89% of participants preferred Chronos over baseline models. Chronos will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery</title>
<link>https://arxiv.org/abs/2507.13420</link>
<guid>https://arxiv.org/abs/2507.13420</guid>
<content:encoded><![CDATA[
arXiv:2507.13420v2 Announce Type: replace-cross 
Abstract: By upgrading an existing deep learning model with the knowledge provided by one of the oldest sets of grayscale satellite imagery, known as CORONA, we improved the AI model attitude towards the automatic identification of archaeological sites in an environment which has been completely transformed in the last five decades, including the complete destruction of many of those same sites. The initial Bing based convolutional network model was retrained using CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad, central Mesopotamian floodplain. The results were twofold and surprising. First, the detection precision obtained on the area of interest increased sensibly: in particular, the Intersection over Union (IoU) values, at the image segmentation level, surpassed 85 percent, while the general accuracy in detecting archeological sites reached 90 percent. Second, our retrained model allowed the identification of four new sites of archaeological interest (confirmed through field verification), previously not identified by archaeologists with traditional techniques. This has confirmed the efficacy of using AI techniques and the CORONA imagery from the 1960 to discover archaeological sites currently no longer visible, a concrete breakthrough with significant consequences for the study of landscapes with vanishing archaeological evidence induced by anthropization
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models</title>
<link>https://arxiv.org/abs/2507.13614</link>
<guid>https://arxiv.org/abs/2507.13614</guid>
<content:encoded><![CDATA[
arXiv:2507.13614v2 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models</title>
<link>https://arxiv.org/abs/2507.14811</link>
<guid>https://arxiv.org/abs/2507.14811</guid>
<content:encoded><![CDATA[
arXiv:2507.14811v3 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Beats Autoregressive in Data-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.15857</link>
<guid>https://arxiv.org/abs/2507.15857</guid>
<content:encoded><![CDATA[
arXiv:2507.15857v3 Announce Type: replace-cross 
Abstract: Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</title>
<link>https://arxiv.org/abs/2507.14270</link>
<guid>https://arxiv.org/abs/2507.14270</guid>
<content:encoded><![CDATA[
<div> APTx Neuron, unified neural computation unit, APTx activation function, computational efficiency, MNIST dataset<br />
<br />
Summary: <br />
The article introduces the APTx Neuron, a novel neural computation unit that combines non-linear activation and linear transformation in a single trainable expression. Derived from the APTx activation function, this neuron eliminates the need for separate activation layers, enhancing computational efficiency. The APTx Neuron architecture achieves up to 96.69% test accuracy on the MNIST dataset with 332K trainable parameters in just 11 epochs. The results demonstrate the superior expressiveness and efficiency of the APTx Neuron compared to traditional neurons, suggesting a new direction in unified neuron design and architecture development. The proposed neuron's functional form integrates trainable parameters, making it an elegant and effective solution for neural network computation. <div>
arXiv:2507.14270v3 Announce Type: replace-cross 
Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both computationally efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69% test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and computational efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications</title>
<link>https://arxiv.org/abs/2507.15961</link>
<guid>https://arxiv.org/abs/2507.15961</guid>
<content:encoded><![CDATA[
<div> face image quality, face verification systems, automatic assessment framework, Random Forest Regression classifier, real-time screening

Summary:
The article discusses the importance of face image quality in face verification systems, highlighting how low-quality images can adversely affect performance. The proposed framework focuses on automatic face quality assessment using facial landmarks and a Random Forest Regression classifier, achieving a high accuracy rate of 96.67%. By integrating this module into the verification process, the framework significantly reduces false rejection rates by 99.7% and enhances cosine similarity scores with the ArcFace model. Experiments conducted on a real-world dataset from Dubai Police show the framework's effectiveness in addressing challenges like face resolution variations and pose deviations common in surveillance scenarios. The framework outperforms existing techniques while maintaining computational efficiency, making it a valuable tool for improving the reliability of face recognition systems.<br /><br />Summary: <div>
arXiv:2507.15961v2 Announce Type: replace-cross 
Abstract: Face image quality plays a critical role in determining the accuracy and reliability of face verification systems, particularly in real-time screening applications such as surveillance, identity verification, and access control. Low-quality face images, often caused by factors such as motion blur, poor lighting conditions, occlusions, and extreme pose variations, significantly degrade the performance of face recognition models, leading to higher false rejection and false acceptance rates. In this work, we propose a lightweight yet effective framework for automatic face quality assessment, which aims to pre-filter low-quality face images before they are passed to the verification pipeline. Our approach utilises normalised facial landmarks in conjunction with a Random Forest Regression classifier to assess image quality, achieving an accuracy of 96.67%. By integrating this quality assessment module into the face verification process, we observe a substantial improvement in performance, including a comfortable 99.7% reduction in the false rejection rate and enhanced cosine similarity scores when paired with the ArcFace face verification model. To validate our approach, we have conducted experiments on a real-world dataset collected comprising over 600 subjects captured from CCTV footage in unconstrained environments within Dubai Police. Our results demonstrate that the proposed framework effectively mitigates the impact of poor-quality face images, outperforming existing face quality assessment techniques while maintaining computational efficiency. Moreover, the framework specifically addresses two critical challenges in real-time screening: variations in face resolution and pose deviations, both of which are prevalent in practical surveillance scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation</title>
<link>https://arxiv.org/abs/2507.19489</link>
<guid>https://arxiv.org/abs/2507.19489</guid>
<content:encoded><![CDATA[
<div> Kubernetes, AI, clinical workflows, MAIA, collaboration <br />
Summary: <br />
The paper introduces MAIA, an open-source platform for integrating Artificial Intelligence (AI) into clinical workflows. MAIA is designed to facilitate collaboration among clinicians, researchers, and AI developers by providing a modular, scalable environment with tools for data management, model development, annotation, deployment, and clinical feedback. Key features of MAIA include project isolation, automation of CI/CD, integration with high-computing infrastructures, and support for real-world use cases in medical imaging AI. The platform aims to accelerate the translation of AI research into clinical solutions while promoting reproducibility, transparency, and user-centered design. MAIA has been deployed in both academic and clinical environments, showcasing its utility in various projects at KTH Royal Institute of Technology and Karolinska University Hospital. <div>
arXiv:2507.19489v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into clinical workflows requires robust collaborative platforms that are able to bridge the gap between technical innovation and practical healthcare applications. This paper introduces MAIA (Medical Artificial Intelligence Assistant), an open-source platform designed to facilitate interdisciplinary collaboration among clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a modular, scalable environment with integrated tools for data management, model development, annotation, deployment, and clinical feedback. Key features include project isolation, CI/CD automation, integration with high-computing infrastructures and in clinical workflows. MAIA supports real-world use cases in medical imaging AI, with deployments in both academic and clinical environments. By promoting collaborations and interoperability, MAIA aims to accelerate the translation of AI research into impactful clinical solutions while promoting reproducibility, transparency, and user-centered design. We showcase the use of MAIA with different projects, both at KTH Royal Institute of Technology and Karolinska University Hospital.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent WARPP: Workflow Adherence via Runtime Parallel Personalization</title>
<link>https://arxiv.org/abs/2507.19543</link>
<guid>https://arxiv.org/abs/2507.19543</guid>
<content:encoded><![CDATA[
<div> modular framework, multi-agent orchestration, runtime personalization, workflow adherence, LLM-based systems <br />
Summary: 
The paper introduces a new framework called WARPP that enhances workflow adherence in large language models (LLMs) used in task-oriented dialogue systems. WARPP combines multi-agent orchestration with runtime personalization to dynamically adjust workflow paths based on user attributes, reducing reasoning overhead and improving tool selection during execution. The framework features a parallelized architecture where a Personalizer agent works alongside domain-specific agents to customize execution paths in real-time. Evaluation of WARPP across various user intents in banking, flights, and healthcare domains shows superior performance compared to non-personalized methods and a ReAct baseline. As intent complexity increases, WARPP achieves better parameter fidelity, tool accuracy, and reduced token usage without the need for additional training. <div>
arXiv:2507.19543v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly applied in task-oriented dialogue (TOD) systems but often struggle with long, conditional workflows that involve external tool calls and depend on user-specific information. We present Workflow Adherence via Runtime Parallel Personalization, or WARPP, a training-free, modular framework that combines multi-agent orchestration with runtime personalization to improve workflow adherence in LLM-based systems. By dynamically pruning conditional branches based on user attributes, the framework reduces reasoning overhead and narrows tool selection at runtime. WARPP deploys a parallelized architecture where a dedicated Personalizer agent operates alongside modular, domain-specific agents to dynamically tailor execution paths in real time. The framework is evaluated across five representative user intents of varying complexity within three domains: banking, flights, and healthcare. Our evaluation leverages synthetic datasets and LLM-powered simulated users to test scenarios with conditional dependencies. Our results demonstrate that WARPP outperforms both the non-personalized method and the ReAct baseline, achieving increasingly larger gains in parameter fidelity and tool accuracy as intent complexity grows, while also reducing average token usage, without any additional training.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems</title>
<link>https://arxiv.org/abs/2507.19593</link>
<guid>https://arxiv.org/abs/2507.19593</guid>
<content:encoded><![CDATA[
<div> Keywords: hypergame theory, agent-based modeling, multi-agent systems, cognitive constraints, strategic modeling

Summary: 
This systematic review explores the application of hypergame theory in multi-agent systems (MAS) where classical game theory assumptions such as rationality and complete information are often not met. The study focuses on agent-compatible adaptations of hypergame theory in dynamic MAS scenarios. It analyzes 44 selected studies across various fields including cybersecurity, robotics, and social simulation. The review introduces hierarchical hypergames and HNF extensions of hypergame theory, develops agent-compatibility criteria, and offers a classification framework for assessing practical applicability. The analysis highlights prevalent tendencies such as the use of hierarchical and graph-based models for deceptive reasoning. It also identifies structural gaps in the adoption of HNF-based models, the need for formal hypergame languages, and opportunities for modeling misalignment between human agents and artificial agents. By synthesizing trends and challenges, this review provides a roadmap for enhancing strategic modeling realism in complex multi-agent environments. 

Summary: <div>
arXiv:2507.19593v1 Announce Type: new 
Abstract: Classical game-theoretic models typically assume rational agents, complete information, and common knowledge of payoffs - assumptions that are often violated in real-world MAS characterized by uncertainty, misaligned perceptions, and nested beliefs. To overcome these limitations, researchers have proposed extensions that incorporate models of cognitive constraints, subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory extends the classical paradigm by explicitly modeling agents' subjective perceptions of the strategic scenario, known as perceptual games, in which agents may hold divergent beliefs about the structure, payoffs, or available actions. We present a systematic review of agent-compatible applications of hypergame theory, examining how its descriptive capabilities have been adapted to dynamic and interactive MAS contexts. We analyze 44 selected studies from cybersecurity, robotics, social simulation, communications, and general game-theoretic modeling. Building on a formal introduction to hypergame theory and its two major extensions - hierarchical hypergames and HNF - we develop agent-compatibility criteria and an agent-based classification framework to assess integration patterns and practical applicability. Our analysis reveals prevailing tendencies, including the prevalence of hierarchical and graph-based models in deceptive reasoning and the simplification of extensive theoretical frameworks in practical applications. We identify structural gaps, including the limited adoption of HNF-based models, the lack of formal hypergame languages, and unexplored opportunities for modeling human-agent and agent-agent misalignment. By synthesizing trends, challenges, and open research directions, this review provides a new roadmap for applying hypergame theory to enhance the realism and effectiveness of strategic modeling in dynamic multi-agent environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</title>
<link>https://arxiv.org/abs/2507.19608</link>
<guid>https://arxiv.org/abs/2507.19608</guid>
<content:encoded><![CDATA[
<div> Framework, Large Language Models, Edge Devices, Attention Sparsity, Efficient Inference

Summary: 
DeltaLLM is a training-free framework designed for efficient deployment of Large Language Models (LLMs) on edge devices. It exploits temporal sparsity in attention patterns to enable efficient LLM inference on resource-constrained devices. The framework introduces a delta matrix construction strategy and a context-aware hybrid attention mechanism to increase accuracy while reducing memory usage. Evaluation on BitNet and Llama models shows up to 60% attention sparsity during the prefilling stage and around 57% across both prefilling and decoding stages. On the BitNet model, there is a slight accuracy improvement on the WG task and a higher F1 score on the SQuAD-v2 task. On the Llama model, similar attention sparsity levels are achieved with negligible accuracy drop. DeltaLLM offers a promising solution for efficient edge deployment, seamlessly integrating with existing inference pipelines without the need for fine-tuning. 

<br /><br />Summary: <div>
arXiv:2507.19608v1 Announce Type: new 
Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges</title>
<link>https://arxiv.org/abs/2507.19672</link>
<guid>https://arxiv.org/abs/2507.19672</guid>
<content:encoded><![CDATA[
<div> alignment techniques, training protocols, empirical findings, large language models, human values

Summary:
This survey delves into the challenge of aligning large language models (LLMs) with human values, offering a comprehensive overview of practical alignment techniques, training protocols, and empirical findings. It analyzes the various paradigms of alignment methods, highlighting the trade-offs between core alignment objectives. Supervised fine-tuning and preference-based methods are discussed, with preference-based methods being more flexible for aligning with nuanced human intent. State-of-the-art techniques like Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ) are examined for their approach to balancing quality and efficiency. Existing evaluation frameworks and benchmarking datasets are reviewed, pointing out limitations such as reward misspecification, distributional robustness, and scalable oversight. The article also summarizes strategies employed by leading AI labs and outlines open challenges in oversight, value pluralism, robustness, and continuous alignment.<br /><br />Summary: <div>
arXiv:2507.19672v1 Announce Type: new 
Abstract: Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The wall confronting large language models</title>
<link>https://arxiv.org/abs/2507.19703</link>
<guid>https://arxiv.org/abs/2507.19703</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, uncertainty, reliability, error pileup, degenerative AI behavior

Summary:
Large language models (LLMs) face limitations in improving prediction uncertainty due to scaling laws. The mechanism allowing LLMs to generate non-Gaussian output distributions from Gaussian inputs may contribute to error accumulation and degenerative AI behavior. This tension between learning power and accuracy is exacerbated by the proliferation of spurious correlations in data sets. The risk of a degenerative AI pathway in LLMs can be mitigated by prioritizing insight and understanding of problem structures in AI research. Avoiding such negative outcomes requires higher emphasis on the structural characteristics of the problems being studied.Summary: <div>
arXiv:2507.19703v1 Announce Type: new 
Abstract: We show that the scaling laws which determine the performance of large language models (LLMs) severely limit their ability to improve the uncertainty of their predictions. As a result, raising their reliability to meet the standards of scientific inquiry is intractable by any reasonable measure. We argue that the very mechanism which fuels much of the learning power of LLMs, namely the ability to generate non-Gaussian output distributions from Gaussian input ones, might well be at the roots of their propensity to produce error pileup, ensuing information catastrophes and degenerative AI behaviour. This tension between learning and accuracy is a likely candidate mechanism underlying the observed low values of the scaling components. It is substantially compounded by the deluge of spurious correlations pointed out by Calude and Longo which rapidly increase in any data set merely as a function of its size, regardless of its nature. The fact that a degenerative AI pathway is a very probable feature of the LLM landscape does not mean that it must inevitably arise in all future AI research. Its avoidance, which we also discuss in this paper, necessitates putting a much higher premium on insight and understanding of the structural characteristics of the problems being investigated.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors</title>
<link>https://arxiv.org/abs/2507.19725</link>
<guid>https://arxiv.org/abs/2507.19725</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Intrinsic Motivation, Reward Sparsity, Reward Hacking, Generalized Reward Matching 

Summary:
Reinforcement Learning agents face challenges in games due to sparse rewards, which can be addressed using Intrinsic Motivation (IM) techniques. However, IM can lead to reward hacking, where agents prioritize new rewards over optimal gameplay. This study evaluates the impact of three IM techniques on behavior in the MiniGrid environment. The results indicate that IM increases initial rewards but also alters agent behavior. Generalized Reward Matching (GRM) shows promise in mitigating reward hacking in some instances. This research provides valuable insights into the effects of IM on RL agents and highlights the importance of addressing reward hacking to ensure optimal performance. 

Summary:<br /><br />Keywords: Reinforcement Learning, Intrinsic Motivation, Reward Sparsity, Reward Hacking, Generalized Reward Matching <div>
arXiv:2507.19725v1 Announce Type: new 
Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their reward-sparsity, as rewards are only obtainable after long sequences of deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce exploration rewards -- are an effective solution to reward-sparsity. However, IM also causes an issue known as `reward hacking' where the agent optimizes for the new reward at the expense of properly playing the game. The larger problem is that reward hacking itself is largely unknown; there is no answer to whether, and to what extent, IM rewards change the behavior of RL agents. This study takes a first step by empirically evaluating the impact on behavior of three IM techniques on the MiniGrid game-like environment. We compare these IM models with Generalized Reward Matching~(GRM), a method that can be used with any intrinsic reward function to guarantee optimality. Our results suggest that IM causes noticeable change by increasing the initial rewards, but also altering the way the agent plays; and that GRM mitigated reward hacking in some scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare</title>
<link>https://arxiv.org/abs/2507.19726</link>
<guid>https://arxiv.org/abs/2507.19726</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, healthcare, electronic health records, contextualized knowledge representations, hypergraph model

Summary:
The paper introduces HypKG, a framework that integrates patient information from electronic health records (EHRs) into knowledge graphs (KGs) to generate contextualized knowledge representations for accurate healthcare predictions. By linking relevant knowledge from general KGs with patient data from EHRs using entity-linking techniques, HypKG contextualizes the knowledge with patient information through a hypergraph model. This framework leverages hypergraph transformers guided by downstream prediction tasks to jointly learn appropriate contextualized representations for both KGs and patients. Experimental results using a large biomedical KG and real-world EHR datasets show significant improvements in healthcare prediction tasks. By integrating patient contexts from EHRs, HypKG can adjust entity and relation representations in KGs, potentially enhancing the quality and utility of knowledge for real-world applications.<br /><br />Summary: <div>
arXiv:2507.19726v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are widely used in various application domains. Healthcare is one of such domains where KGs are intensively used, due to the high requirement for knowledge accuracy and interconnected nature of healthcare data. However, KGs storing general factual information often lack the ability to account for important contexts of the knowledge such as the status of specific patients, which are crucial in precision healthcare. Meanwhile, electronic health records (EHRs) provide rich personal data, including various diagnoses and medications, which provide natural contexts for general KGs. In this paper, we propose HypKG, a framework that integrates patient information from EHRs into KGs to generate contextualized knowledge representations for accurate healthcare predictions. Using advanced entity-linking techniques, we connect relevant knowledge from general KGs with patient information from EHRs, and then utilize a hypergraph model to "contextualize" the knowledge with the patient information. Finally, we employ hypergraph transformers guided by downstream prediction tasks to jointly learn proper contextualized representations for both KGs and patients, fully leveraging existing knowledge in KGs and patient contexts in EHRs. In experiments using a large biomedical KG and two real-world EHR datasets, HypKG demonstrates significant improvements in healthcare prediction tasks across multiple evaluation metrics. Additionally, by integrating external contexts, HypKG can learn to adjust the representations of entities and relations in KG, potentially improving the quality and real-world utility of knowledge.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Activity Predictions in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.19733</link>
<guid>https://arxiv.org/abs/2507.19733</guid>
<content:encoded><![CDATA[
<div> predictive analytics, ontology, Markov chains, probability, Basic Formal Ontology (BFO), knowledge graphs

Summary: 
Ontology-structured knowledge graphs are proposed to predict future events, using Basic Formal Ontology (BFO) and Common Core Ontologies (CCO). Data on a fishing vessel's movements are organized in a knowledge graph, enabling Markov chain models for future state predictions. The concept of 'spatiotemporal instant' is introduced for structural semantics. Critique is made on the prevailing ontological model of probability, proposing a process profile-based view instead. Integration of Markov chain probability calculations back into the knowledge graph allows for further analysis and decision-making. The study emphasizes the distinction between probability and likelihood, rejecting modal measurements of future entities. The method shows the dynamics of real-world phenomena and how probabilities are about process profiles. This approach enhances predictive analytics and decision support in various fields. <div>
arXiv:2507.19733v1 Announce Type: new 
Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how data such as the movements of a fishing vessel can be organized in and retrieved from a knowledge graph. These query results are then used to create Markov chain models, allowing us to predict future states based on the vessel's history. To fully support this process, we introduce the term `spatiotemporal instant' to complete the necessary structural semantics. Additionally, we critique the prevailing ontological model of probability, which conflates probability with likelihood and relies on the problematic concept of modal measurements: measurements of future entities. We propose an alternative view, where probabilities are treated as being about process profiles, which better captures the dynamics of real world phenomena. Finally, we demonstrate how our Markov chain based probability calculations can be seamlessly integrated back into the knowledge graph, enabling further analysis and decision-making. Keywords: predictive analytics, ontology, Markov chains, probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)</title>
<link>https://arxiv.org/abs/2507.19749</link>
<guid>https://arxiv.org/abs/2507.19749</guid>
<content:encoded><![CDATA[
<div> Keywords: Answer Set Programming, Large Language Models, Benchmark, Negation, Disjunction<br />
Summary:<br />
The paper introduces ASPBench, a new comprehensive ASP benchmark that includes tasks specifically designed for Answer Set Programming (ASP) solving. Current evaluations of Large Language Models (LLMs) in ASP are limited, often using simplified programs that do not support negation, disjunction, or multiple answer sets. The study evaluates 14 state-of-the-art LLMs on three ASP tasks: ASP entailment, answer set verification, and answer set computation. While LLMs perform well on simpler tasks, they struggle with answer set computation, which is essential for ASP solving. The findings highlight the current limitations of LLMs in ASP and suggest a need for new approaches integrating symbolic reasoning effectively. The code and dataset for ASPBench are available on GitHub at https://github.com/HomuraT/ASPBench. <br /><br />Summary: <div>
arXiv:2507.19749v1 Announce Type: new 
Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic reasoning. Recently, large language models (LLMs) have demonstrated promising capabilities in logical reasoning. Despite this potential, current evaluations of LLM capabilities in ASP are often limited. Existing works normally employ overly simplified ASP programs, do not support negation, disjunction, or multiple answer sets. Furthermore, there is a lack of benchmarks that introduce tasks specifically designed for ASP solving. To bridge this gap, we introduce ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks: ASP entailment, answer set verification, and answer set computation. Our extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs, including \emph{deepseek-r1}, \emph{o4-mini}, and \emph{gemini-2.5-flash-thinking}, perform relatively well on the first two simpler tasks, they struggle with answer set computation, which is the core of ASP solving. These findings offer insights into the current limitations of LLMs in ASP solving. This highlights the need for new approaches that integrate symbolic reasoning capabilities more effectively. The code and dataset are available at https://github.com/HomuraT/ASPBench.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation</title>
<link>https://arxiv.org/abs/2507.19788</link>
<guid>https://arxiv.org/abs/2507.19788</guid>
<content:encoded><![CDATA[
<div> optimisation model, supply chain, multi-objective, reinforcement learning, Markov decision process <br />
Summary: This study introduces a novel multi-objective, multi-echelon supply chain optimisation model that considers economic, environmental, and social factors in non-stationary markets. The model utilizes a multi-objective reinforcement learning method and is compared against a single-objective RL algorithm and a multi-objective evolutionary algorithm. Through experiments in various network complexities, the model determines production and delivery quantities to achieve balanced trade-offs between objectives. It surpasses the MOEA-based method in hypervolume by up to 75% and generates denser solutions. Additionally, it maintains stable production and inventory levels while minimizing demand loss. The model's primary approach, enhanced with a shared experience buffer for knowledge transfer, proves to be highly effective in complex scenarios, showcasing robustness and efficiency in problem-solving. <br /><br /> <div>
arXiv:2507.19788v1 Announce Type: new 
Abstract: This study develops a generalised multi-objective, multi-echelon supply chain optimisation model with non-stationary markets based on a Markov decision process, incorporating economic, environmental, and social considerations. The model is evaluated using a multi-objective reinforcement learning (RL) method, benchmarked against an originally single-objective RL algorithm modified with weighted sum using predefined weights, and a multi-objective evolutionary algorithm (MOEA)-based approach. We conduct experiments on varying network complexities, mimicking typical real-world challenges using a customisable simulator. The model determines production and delivery quantities across supply chain routes to achieve near-optimal trade-offs between competing objectives, approximating Pareto front sets. The results demonstrate that the primary approach provides the most balanced trade-off between optimality, diversity, and density, further enhanced with a shared experience buffer that allows knowledge transfer among policies. In complex settings, it achieves up to 75\% higher hypervolume than the MOEA-based method and generates solutions that are approximately eleven times denser, signifying better robustness, than those produced by the modified single-objective RL method. Moreover, it ensures stable production and inventory levels while minimising demand loss.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation</title>
<link>https://arxiv.org/abs/2507.19882</link>
<guid>https://arxiv.org/abs/2507.19882</guid>
<content:encoded><![CDATA[
<div> counterfactual prompt learning, robust feature generalization, DiCap model, causal invariance, contrastive learning<br />
Summary:<br />
The article introduces the DiCap model, a diffusion-based counterfactual prompt learning framework designed to address challenges in achieving causally invariant prompts for efficient model training. The model leverages a diffusion process to sample gradients from marginal and conditional distributions of a causal model, ensuring the identification of counterfactual outcomes with minimal estimation errors. By employing a contrastive learning framework, the model extracts prompts aligned with causal features in the data, resulting in excellent performance across tasks like image classification and visual question answering, with particular strength in unseen categories. The DiCap model's theoretically grounded approach promises robust feature generalization and superior performance in various applications, making it a promising advancement in prompt learning techniques. <br />Summary: <div>
arXiv:2507.19882v1 Announce Type: new 
Abstract: Prompt learning has garnered attention for its efficiency over traditional model training and fine-tuning. However, existing methods, constrained by inadequate theoretical foundations, encounter difficulties in achieving causally invariant prompts, ultimately falling short of capturing robust features that generalize effectively across categories. To address these challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual $\textbf{p}$rompt learning framework, which leverages a diffusion process to iteratively sample gradients from the marginal and conditional distributions of the causal model, guiding the generation of counterfactuals that satisfy the minimal sufficiency criterion. Grounded in rigorous theoretical derivations, this approach guarantees the identifiability of counterfactual outcomes while imposing strict bounds on estimation errors. We further employ a contrastive learning framework that leverages the generated counterfactuals, thereby enabling the refined extraction of prompts that are precisely aligned with the causal features of the data. Extensive experimental results demonstrate that our method performs excellently across tasks such as image classification, image-text retrieval, and visual question answering, with particularly strong advantages in unseen categories.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does 'Human-Centred AI' Mean?</title>
<link>https://arxiv.org/abs/2507.19960</link>
<guid>https://arxiv.org/abs/2507.19960</guid>
<content:encoded><![CDATA[
<div> Keywords: human-centred AI, cognition, sociotechnical relationships, enhancement, replacement

Summary:
Artificial intelligence (AI) should always center around human behavior and experience, as it ultimately involves technology performing cognitive tasks. The relationship between AI and humans can be categorized into displacement, enhancement, or replacement of human cognitive labor. Examples such as the abacus versus mental arithmetic highlight how AI can either harm, benefit, or neutrally impact human cognition. It is crucial to acknowledge that all AI involves human cognition in some way, and obfuscating this reality can distort critical engagement with AI systems. To truly prioritize humans in AI engineering, we must recognize the human-in-the-loop aspect and address sociotechnical relationships. By de-fetishizing AI and focusing on the human aspect, it becomes possible to better understand the implications of AI on human cognition and society. <br /><br />Summary: <div>
arXiv:2507.19960v1 Announce Type: new 
Abstract: While it seems sensible that human-centred artificial intelligence (AI) means centring "human behaviour and experience," it cannot be any other way. AI, I argue, is usefully seen as a relationship between technology and humans where it appears that artifacts can perform, to a greater or lesser extent, human cognitive labour. This is evinced using examples that juxtapose technology with cognition, inter alia: abacus versus mental arithmetic; alarm clock versus knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel definitions and analyses, sociotechnical relationships can be analysed into varying types of: displacement (harmful), enhancement (beneficial), and/or replacement (neutral) of human cognitive labour. Ultimately, all AI implicates human cognition; no matter what. Obfuscation of cognition in the AI context -- from clocks to artificial neural networks -- results in distortion, in slowing critical engagement, perverting cognitive science, and indeed in limiting our ability to truly centre humans and humanity in the engineering of AI systems. To even begin to de-fetishise AI, we must look the human-in-the-loop in the eyes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization</title>
<link>https://arxiv.org/abs/2507.19973</link>
<guid>https://arxiv.org/abs/2507.19973</guid>
<content:encoded><![CDATA[
<div> Keyword: pancreatic cystic lesion, large language models, MRI/CT reports, risk categorization, radiology<br />
Summary: 
- Large language models (LLMs) were developed to automatically extract pancreatic cystic lesion (PCL) features from MRI/CT reports and assign risk categories based on guidelines.
- A training dataset of 6,000 reports was used to fine-tune LLMs using QLoRA and Chain-of-Thought prompting, improving feature extraction accuracy and risk categorization.
- Evaluation on 285 human-annotated reports showed high accuracy in feature extraction and risk categorization for both LLMs, comparable to GPT-4o performance.
- Radiologist review of model outputs demonstrated high agreement levels, indicating that LLMs achieved agreement levels on par with radiologists.
- Fine-tuned open-source LLMs with CoT supervision enable accurate and efficient phenotyping for large-scale PCL research, with performance comparable to GPT-4o. 
<br /><br />Summary: <div>
arXiv:2507.19973v1 Announce Type: new 
Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research. Purpose: To develop and evaluate large language models (LLMs) that automatically extract PCL features from MRI/CT reports and assign risk categories based on guidelines. Materials and Methods: We curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application</title>
<link>https://arxiv.org/abs/2507.19974</link>
<guid>https://arxiv.org/abs/2507.19974</guid>
<content:encoded><![CDATA[
<div> Keywords: 6G networks, resource allocation, digital twin channel, real-time channel state information, online optimization

Summary: 
6G networks require flexible, low-latency, and reliable resource allocation for applications like holographic communication and autonomous driving. Traditional methods relying on statistical modeling may not be optimal for dynamic environments and real-time channel state information (CSI) incurs high pilot overhead. A solution proposed is a digital twin channel (DTC)-enabled framework for online optimization. DTC predicts CSI using environmental sensing, allowing lightweight game-theoretic algorithms for efficient resource allocation. Simulations in an industrial workshop setting show up to 11.5% throughput improvement compared to ideal CSI schemes, validating the method for scalable, low-overhead, and environment-aware communication in 6G networks. 

<br /><br />Summary: <div>
arXiv:2507.19974v1 Announce Type: new 
Abstract: Emerging applications such as holographic communication, autonomous driving, and the industrial Internet of Things impose stringent requirements on flexible, low-latency, and reliable resource allocation in 6G networks. Conventional methods, which rely on statistical modeling, have proven effective in general contexts but may fail to achieve optimal performance in specific and dynamic environments. Furthermore, acquiring real-time channel state information (CSI) typically requires excessive pilot overhead. To address these challenges, a digital twin channel (DTC)-enabled online optimization framework is proposed, in which DTC is employed to predict CSI based on environmental sensing. The predicted CSI is then utilized by lightweight game-theoretic algorithms to perform online resource allocation in a timely and efficient manner. Simulation results based on a digital replica of a realistic industrial workshop demonstrate that the proposed method achieves throughput improvements of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its effectiveness for scalable, low-overhead, and environment-aware communication in future 6G networks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matching Game Preferences Through Dialogical Large Language Models: A Perspective</title>
<link>https://arxiv.org/abs/2507.20000</link>
<guid>https://arxiv.org/abs/2507.20000</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational intelligence, Large Language Models (LLMs), GRAPHYP network system, human preferences, transparent AI<br />
Summary: <br />
This perspective paper discusses the potential of combining Large Language Models (LLMs) with GRAPHYP's network system to enhance conversational intelligence. The proposed framework, "Matching Game Preferences through Dialogical Large Language Models (D-LLMs)," aims to personalize AI by incorporating individual user preferences into decision-making. The framework consists of reasoning processes, classification systems, and dialogue approaches to analyze search experiences, identify user preference patterns, and resolve conflicting information. The goal is to create an interpretable AI system where users can understand and combine different human preferences that influence AI responses, leading to more transparent and trustworthy artificial intelligence for human decision-making. <div>
arXiv:2507.20000v1 Announce Type: new 
Abstract: This perspective paper explores the future potential of "conversational intelligence" by examining how Large Language Models (LLMs) could be combined with GRAPHYP's network system to better understand human conversations and preferences. Using recent research and case studies, we propose a conceptual framework that could make AI rea-soning transparent and traceable, allowing humans to see and understand how AI reaches its conclusions. We present the conceptual perspective of "Matching Game Preferences through Dialogical Large Language Models (D-LLMs)," a proposed system that would allow multiple users to share their different preferences through structured conversations. This approach envisions personalizing LLMs by embedding individual user preferences directly into how the model makes decisions. The proposed D-LLM framework would require three main components: (1) reasoning processes that could analyze different search experiences and guide performance, (2) classification systems that would identify user preference patterns, and (3) dialogue approaches that could help humans resolve conflicting information. This perspective framework aims to create an interpretable AI system where users could examine, understand, and combine the different human preferences that influence AI responses, detected through GRAPHYP's search experience networks. The goal of this perspective is to envision AI systems that would not only provide answers but also show users how those answers were reached, making artificial intelligence more transparent and trustworthy for human decision-making.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems</title>
<link>https://arxiv.org/abs/2507.20010</link>
<guid>https://arxiv.org/abs/2507.20010</guid>
<content:encoded><![CDATA[
<div> roommates, preferences, stable, matching, personalized
<br />
In the study, the researchers focus on the Stable Roommates problem, which involves agents' preferences for roommates. A solution to this problem is a stable matching where pairs of agents are acceptable to each other based on their preference lists. The researchers aim to compute "good-enough" matchings when stable solutions are not possible. They consider agents' habits, habitual preferences, and networks of preferred friends to generate personalized solutions. By introducing a method to address stable roommates problems with these factors in mind, the researchers illustrate the practical applications of their approach through examples and empirical evaluations.
<br /><br />Summary: <div>
arXiv:2507.20010v1 Announce Type: new 
Abstract: The Stable Roommates problems are characterized by the preferences of agents over other agents as roommates. A solution is a partition of the agents into pairs that are acceptable to each other (i.e., they are in the preference lists of each other), and the matching is stable (i.e., there do not exist any two agents who prefer each other to their roommates, and thus block the matching). Motivated by real-world applications, and considering that stable roommates problems do not always have solutions, we continue our studies to compute "good-enough" matchings. In addition to the agents' habits and habitual preferences, we consider their networks of preferred friends, and introduce a method to generate personalized solutions to stable roommates problems. We illustrate the usefulness of our method with examples and empirical evaluations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
<div> alignment, large language models, preference feedback, inference-time, token generation

Summary:
PITA introduces a novel framework for large language models (LLMs) to generate outputs aligned with user preferences at inference time. By integrating preference feedback directly into token generation, PITA eliminates the need for a pre-trained reward model, reducing computational cost and instability in the fitting process. The framework learns a preference-based guidance policy to modify token probabilities without LLM fine-tuning. The problem is addressed through stochastic search and iterative refinement of the guidance model to identify the underlying preference distribution. Evaluation across tasks like mathematical reasoning and sentiment classification shows PITA's effectiveness in aligning LLM outputs with user preferences.<br /><br />Summary: <div>
arXiv:2507.20067v1 Announce Type: new 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Learning for Cooperative Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20143</link>
<guid>https://arxiv.org/abs/2507.20143</guid>
<content:encoded><![CDATA[
<div> Cooperative mechanism, Neural networks, Multi-agent reinforcement learning, Interpretable framework, Value decomposition<br />
<br />
Summary: 
The article discusses the challenges of transparency and interoperability in applying neural networks to multi-agent reinforcement learning (MARL). A novel value-based method called Concepts learning for Multi-agent Q-learning (CMQ) is proposed to address these challenges by learning interpretable cooperation concepts. By representing each cooperation concept as a supervised vector and conditioning action values on global state embeddings, CMQ achieves superior performance in the StarCraft II micromanagement challenge and level-based foraging (LBF) compared to existing models. The results also show that CMQ captures meaningful cooperation modes and allows for test-time concept interventions to detect biases and identify spurious artifacts affecting cooperation. <div>
arXiv:2507.20143v1 Announce Type: new 
Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent reinforcement learning (MARL) areas, they still largely suffer from a lack of transparency and interoperability. However, its implicit cooperative mechanism is not yet fully understood due to black-box networks. In this work, we study an interpretable value decomposition framework via concept bottleneck models, which promote trustworthiness by conditioning credit assignment on an intermediate level of human-like cooperation concepts. To address this problem, we propose a novel value-based method, named Concepts learning for Multi-agent Q-learning (CMQ), that goes beyond the current performance-vs-interpretability trade-off by learning interpretable cooperation concepts. CMQ represents each cooperation concept as a supervised vector, as opposed to existing models where the information flowing through their end-to-end mechanism is concept-agnostic. Intuitively, using individual action value conditioning on global state embeddings to represent each concept allows for extra cooperation representation capacity. Empirical evaluations on the StarCraft II micromanagement challenge and level-based foraging (LBF) show that CMQ achieves superior performance compared with the state-of-the-art counterparts. The results also demonstrate that CMQ provides more cooperation concept representation capturing meaningful cooperation modes, and supports test-time concept interventions for detecting potential biases of cooperation mode and identifying spurious artifacts that impact cooperation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20150</link>
<guid>https://arxiv.org/abs/2507.20150</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, stability analysis, policy brittleness, multi-reward RL, trustworthiness

Summary:
This paper introduces a mathematical framework for analyzing the stability of reinforcement learning in large language and reasoning models (LLMs/LRMs). It addresses issues such as spurious reasoning, deceptive alignment, and instruction disobedience that arise from brittle and unstable policies. The framework explains these failures as rational outcomes of optimizing rewards in the presence of non-unique optimal actions and action degeneracy. It extends the analysis to multi-reward RL, showing how stability is influenced by an "effective reward" aggregation mechanism. The study also demonstrates that entropy regularization can improve policy stability at the expense of increased stochasticity. The findings offer a unified explanation for various failures in LLMs/LRMs and provide insights for designing safer and more trustworthy AI systems. 

<br /><br />Summary: <div>
arXiv:2507.20150v1 Announce Type: new 
Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepFun-Prover Preview: Let's Think and Verify Step by Step</title>
<link>https://arxiv.org/abs/2507.20199</link>
<guid>https://arxiv.org/abs/2507.20199</guid>
<content:encoded><![CDATA[
<div> keywords: StepFun-Prover, formal theorem proving, reinforcement learning, Lean 4 proofs, Math AI assistant <br />
Summary:<br />
StepFun-Prover Preview is a large language model developed for formal theorem proving using reinforcement learning techniques. The model is designed to generate Lean 4 proofs efficiently by integrating with theorem proving tools. Through tool-based interactions, StepFun-Prover can emulate human problem-solving strategies and refine proofs based on real-time feedback. It achieves a pass@1 success rate of 70.0% on the miniF2F-test benchmark, demonstrating its strong performance. The model's ability to interact with tools seamlessly suggests a promising direction in automated theorem proving and Math AI assistance. The end-to-end training framework introduced in this work provides a foundation for developing tool-integrated reasoning models, potentially advancing the field of automated theorem proving further. <br /> <div>
arXiv:2507.20199v1 Announce Type: new 
Abstract: We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of $70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2507.20226</link>
<guid>https://arxiv.org/abs/2507.20226</guid>
<content:encoded><![CDATA[
<div> Keywords: subgraph homomorphism, graph neural network, mapping, machine learning, accuracy

Summary:
HFrame is introduced as a novel graph neural network-based framework for the subgraph homomorphism problem, which involves mapping patterns to graphs while preserving their structure. Unlike subgraph isomorphism, homomorphism allows for non-one-to-one mappings, making the problem more complex. HFrame combines traditional algorithms with machine learning techniques to achieve superior performance compared to standard graph neural networks. It can discern more graph pairs where the pattern is not homomorphic to the graph and offers a generalization error bound. Experimental results on real-world and synthetic graphs demonstrate that HFrame is significantly faster than exact matching algorithms, achieving an average accuracy of 0.962. <div>
arXiv:2507.20226v1 Announce Type: new 
Abstract: Homomorphism is a key mapping technique between graphs that preserves their structure. Given a graph and a pattern, the subgraph homomorphism problem involves finding a mapping from the pattern to the graph, ensuring that adjacent vertices in the pattern are mapped to adjacent vertices in the graph. Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism allows multiple vertices in the pattern to map to the same vertex in the graph, making it more complex. We propose HFrame, the first graph neural network-based framework for subgraph homomorphism, which integrates traditional algorithms with machine learning techniques. We demonstrate that HFrame outperforms standard graph neural networks by being able to distinguish more graph pairs where the pattern is not homomorphic to the graph. Additionally, we provide a generalization error bound for HFrame. Through experiments on both real-world and synthetic graphs, we show that HFrame is up to 101.91 times faster than exact matching algorithms and achieves an average accuracy of 0.962.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent System for Information Extraction from the Chemical Literature</title>
<link>https://arxiv.org/abs/2507.20230</link>
<guid>https://arxiv.org/abs/2507.20230</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-powered chemical research, chemical databases, automatic extraction, multimodal large language model, multi-agent system

Summary:
AI-powered chemical research heavily relies on high-quality chemical databases. One crucial aspect is the automatic extraction of chemical information from literature, which is currently challenging due to the variety in styles and formats. In this study, a multimodal large language model (MLLM)-based multi-agent system was developed to address this issue. The MLLM's reasoning capability was utilized to comprehend complex chemical graphics, break down extraction tasks, and coordinate specialized agents for efficient solutions. The system achieved a high F1 score of 80.8% on a benchmark dataset, outperforming the previous state-of-the-art model significantly. It showed improvements in various sub-tasks such as molecular image recognition, reaction image parsing, named entity recognition, and text-based reaction extraction. This advancement represents a critical stride towards automating chemical information extraction for structured datasets, paving the way for enhanced AI-driven chemical research. 

<br /><br />Summary: <div>
arXiv:2507.20230v1 Announce Type: new 
Abstract: To fully expedite AI-powered chemical research, high-quality chemical databases are the cornerstone. Automatic extraction of chemical information from the literature is essential for constructing reaction databases, but it is currently limited by the multimodality and style variability of chemical information. In this work, we developed a multimodal large language model (MLLM)-based multi-agent system for automatic chemical information extraction. We used the MLLM's strong reasoning capability to understand the structure of complex chemical graphics, decompose the extraction task into sub-tasks and coordinate a set of specialized agents to solve them. Our system achieved an F1 score of 80.8% on a benchmark dataset of complex chemical reaction graphics from the literature, surpassing the previous state-of-the-art model (F1 score: 35.6%) by a significant margin. Additionally, it demonstrated consistent improvements in key sub-tasks, including molecular image recognition, reaction image parsing, named entity recognition and text-based reaction extraction. This work is a critical step toward automated chemical information extraction into structured datasets, which will be a strong promoter of AI-driven chemical research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration</title>
<link>https://arxiv.org/abs/2507.20280</link>
<guid>https://arxiv.org/abs/2507.20280</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, automation, scientific tools, tool selection, safety-checking

Summary:
SciToolAgent is a novel Large Language Model-powered agent that automates a wide range of scientific tools in biology, chemistry, and materials science. It utilizes a scientific tool knowledge graph to intelligently select and execute tools through graph-based retrieval-augmented generation. The agent also includes a safety-checking module to ensure responsible tool usage. Extensive evaluations on a benchmark dataset show that SciToolAgent outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening demonstrate the agent's ability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts. The integration of Large Language Models with tool automation in SciToolAgent showcases a promising approach to enhancing scientific research efficiency and accessibility.<br /><br />Summary: SciToolAgent is a powerful agent that leverages Large Language Models to automate scientific tools across various domains. It intelligently selects tools, ensures their responsible usage, outperforms existing approaches, and makes complex scientific workflows accessible to a wide range of users. <div>
arXiv:2507.20280v1 Announce Type: new 
Abstract: Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting</title>
<link>https://arxiv.org/abs/2507.20322</link>
<guid>https://arxiv.org/abs/2507.20322</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, technology scouting, solution discovery, large language models, R&amp;D

Summary: 
AI powered software platform utilizing large language models (LLMs) transforms technology scouting and solution discovery in industrial R&amp;D. Traditional methods are time consuming, manual, and rely on domain expertise. The platform leverages advanced LLM capabilities including semantic understanding and cross-domain knowledge extraction to interpret problem statements and retrieve sustainable solutions from unstructured patent texts. Solutions are algorithmically organized for clarity and relevance. The platform integrates patent analysis with commercial intelligence to assess feasibility, scalability, and sustainability of potential innovations. This comprehensive scouting engine reduces manual effort, accelerates innovation cycles, and enhances decision making in complex R&amp;D environments. 

<br /><br />Summary: <div>
arXiv:2507.20322v1 Announce Type: new 
Abstract: This paper presents the development of an AI powered software platform that leverages advanced large language models (LLMs) to transform technology scouting and solution discovery in industrial R&amp;D. Traditional approaches to solving complex research and development challenges are often time consuming, manually driven, and heavily dependent on domain specific expertise. These methods typically involve navigating fragmented sources such as patent repositories, commercial product catalogs, and competitor data, leading to inefficiencies and incomplete insights. The proposed platform utilizes cutting edge LLM capabilities including semantic understanding, contextual reasoning, and cross-domain knowledge extraction to interpret problem statements and retrieve high-quality, sustainable solutions. The system processes unstructured patent texts, such as claims and technical descriptions, and systematically extracts potential innovations aligned with the given problem context. These solutions are then algorithmically organized under standardized technical categories and subcategories to ensure clarity and relevance across interdisciplinary domains. In addition to patent analysis, the platform integrates commercial intelligence by identifying validated market solutions and active organizations addressing similar challenges. This combined insight sourced from both intellectual property and real world product data enables R&amp;D teams to assess not only technical novelty but also feasibility, scalability, and sustainability. The result is a comprehensive, AI driven scouting engine that reduces manual effort, accelerates innovation cycles, and enhances decision making in complex R&amp;D environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Blessing and Curse of Dimensionality in Safety Alignment</title>
<link>https://arxiv.org/abs/2507.20333</link>
<guid>https://arxiv.org/abs/2507.20333</guid>
<content:encoded><![CDATA[
<div> Curse of high-dimensional representations, linear structures, activation engineering, safety alignment, large language models<br />
<br />
The paper explores the impact of high-dimensional representations in large language models (LLMs) on safety alignment. While the increase in dimensions has been advantageous for LLMs, it may also lead to emergent problems such as vulnerability to exploitation through activation engineering. Visualizing linear subspaces associated with different concepts reveals that the curse of high-dimensional representations uniquely affects LLMs. By projecting model representations onto lower dimensional subspaces, alignment can be preserved while avoiding linear structures that can be exploited. Empirical results support the effectiveness of dimensional reduction in reducing susceptibility to jailbreaking through representation engineering. Theoretical insights provide a deeper understanding of linear jailbreaking methods in relation to a model's hidden dimensions, highlighting both the benefits and drawbacks of high-dimensional representations in safety alignment.<br /><br />Summary: <div>
arXiv:2507.20333v1 Announce Type: new 
Abstract: The focus on safety alignment in large language models (LLMs) has increased significantly due to their widespread adoption across different domains. The scale of LLMs play a contributing role in their success, and the growth in parameter count follows larger hidden dimensions. In this paper, we hypothesize that while the increase in dimensions has been a key advantage, it may lead to emergent problems as well. These problems emerge as the linear structures in the activation space can be exploited, in the form of activation engineering, to circumvent its safety alignment. Through detailed visualizations of linear subspaces associated with different concepts, such as safety, across various model scales, we show that the curse of high-dimensional representations uniquely impacts LLMs. Further substantiating our claim, we demonstrate that projecting the representations of the model onto a lower dimensional subspace can preserve sufficient information for alignment while avoiding those linear structures. Empirical results confirm that such dimensional reduction significantly reduces susceptibility to jailbreaking through representation engineering. Building on our empirical validations, we provide theoretical insights into these linear jailbreaking methods relative to a model's hidden dimensions. Broadly speaking, our work posits that the high dimensions of a model's internal representations can be both a blessing and a curse in safety alignment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMPlanner: Integrating Visual Language Models with Motion Planning</title>
<link>https://arxiv.org/abs/2507.20342</link>
<guid>https://arxiv.org/abs/2507.20342</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, autonomous driving, motion planning, vision-language model, real-time planner 

Summary: 
The paper introduces VLMPlanner, a framework that combines a vision-language model with a real-time planner to enhance decision-making in autonomous driving. The VLM processes raw images to capture detailed visual information and utilizes common-sense reasoning to assist the planner in generating safe trajectories. Additionally, the proposed Context-Adaptive Inference Gate (CAI-Gate) allows the VLM to adjust its inference frequency based on scene complexity, ensuring optimal planning performance and computational efficiency. Experimental results on the nuPlan benchmark show superior performance in complex driving scenarios. This integration of large language models into motion planning presents a promising approach for achieving robust and adaptable decision-making in autonomous vehicles. 

Summary: <div>
arXiv:2507.20342v1 Announce Type: new 
Abstract: Integrating large language models (LLMs) into autonomous driving motion planning has recently emerged as a promising direction, offering enhanced interpretability, better controllability, and improved generalization in rare and long-tail scenarios. However, existing methods often rely on abstracted perception or map-based inputs, missing crucial visual context, such as fine-grained road cues, accident aftermath, or unexpected obstacles, which are essential for robust decision-making in complex driving environments. To bridge this gap, we propose VLMPlanner, a hybrid framework that combines a learning-based real-time planner with a vision-language model (VLM) capable of reasoning over raw images. The VLM processes multi-view images to capture rich, detailed visual information and leverages its common-sense reasoning capabilities to guide the real-time planner in generating robust and safe trajectories. Furthermore, we develop the Context-Adaptive Inference Gate (CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by dynamically adjusting its inference frequency based on scene complexity, thereby achieving an optimal balance between planning performance and computational efficiency. We evaluate our approach on the large-scale, challenging nuPlan benchmark, with comprehensive experimental results demonstrating superior planning performance in scenarios with intricate road conditions and dynamic elements. Code will be available.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping</title>
<link>https://arxiv.org/abs/2507.20377</link>
<guid>https://arxiv.org/abs/2507.20377</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, mobility resource allocation, multi-agent, adaptive grouping, parameter sharing <br />
<br />Summary: 
The study introduces a new approach, Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS), for dynamic mobility resource allocation in urban environments. HAG-PS addresses two key challenges in multi-agent reinforcement learning for mobility resource allocation by dynamically sharing allocation policies across agents and achieving memory-efficient parameter sharing. The approach incorporates global and local information, adaptive agent grouping based on encoded trajectories, and learnable ID embeddings for agent specialization. Experimental studies using real-world NYC bike sharing data show HAG-PS outperforms baseline methods in improving bike availability through enhanced resource distribution. <div>
arXiv:2507.20377v1 Announce Type: new 
Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing vehicles) is crucial for rebalancing the mobility demand and supply in the urban environments. We propose in this work a novel multi-agent reinforcement learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS) for dynamic mobility resource allocation. HAG-PS aims to address two important research challenges regarding multi-agent reinforcement learning for mobility resource allocation: (1) how to dynamically and adaptively share the mobility resource allocation policy (i.e., how to distribute mobility resources) across agents (i.e., representing the regional coordinators of mobility resources); and (2) how to achieve memory-efficient parameter sharing in an urban-scale setting. To address the above challenges, we have provided following novel designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we have designed a hierarchical approach that consists of global and local information of the mobility resource states (e.g., distribution of mobility resources). We have developed an adaptive agent grouping approach in order to split or merge the groups of agents based on their relative closeness of encoded trajectories (i.e., states, actions, and rewards). We have designed a learnable identity (ID) embeddings to enable agent specialization beyond simple parameter copy. We have performed extensive experimental studies based on real-world NYC bike sharing data (a total of more than 1.2 million trips), and demonstrated the superior performance (e.g., improved bike availability) of HAG-PS compared with other baseline approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models</title>
<link>https://arxiv.org/abs/2507.20395</link>
<guid>https://arxiv.org/abs/2507.20395</guid>
<content:encoded><![CDATA[
<div> benchmark, spatial reasoning, language models, maze navigation, autonomous systems

Summary: 
- The study introduces MazeEval, a benchmark to assess pure spatial reasoning in Large Language Models (LLMs) through maze navigation tasks without visual cues.
- Eight state-of-the-art LLMs were evaluated on maze navigation tasks in English and Icelandic, revealing significant disparities in performance.
- OpenAI's O3 model demonstrated perfect navigation for smaller mazes, while other models failed beyond certain maze sizes due to excessive looping behavior.
- Performance degradation was observed in Icelandic compared to English, indicating spatial reasoning in LLMs may be influenced by linguistic patterns.
- The study highlights the importance of training data availability and architectural innovations for reliable spatial navigation in LLM-powered autonomous systems.

<br /><br />Summary: <div>
arXiv:2507.20395v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$, other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems</title>
<link>https://arxiv.org/abs/2507.20444</link>
<guid>https://arxiv.org/abs/2507.20444</guid>
<content:encoded><![CDATA[
<div> Keywords: 6G communication networks, Quality of Service (QoS), Edge computing frameworks, General Artificial Intelligence Lifelong Learning Systems, Federated Layering Techniques (FLT)

Summary: 
In the rapidly evolving IT landscape, 6G communication networks bring challenges of increased data volume and complexity in network environments. To address this, the paper focuses on enhancing QoS in edge computing frameworks. A novel approach is proposed - General Artificial Intelligence Lifelong Learning Systems, utilizing Federated Layering Techniques (FLT). This collaborative mechanism among small AI models aims to improve operational efficiency and response time in resource-constrained environments. By combining cloud and edge computing strengths, a negotiation and debate mechanism is integrated to enhance reasoning and decision-making processes. This approach ensures secure model parameter transmission while maintaining high efficiency in learning and reasoning, leading to improved learning efficiency, reasoning accuracy, and privacy protection for edge nodes. The experimental results validate the effectiveness of the strategy in achieving resilient large model lifelong learning systems and significant enhancement in QoS for edge computing environments. 

<br /><br />Summary: <div>
arXiv:2507.20444v1 Announce Type: new 
Abstract: In the context of the rapidly evolving information technology landscape, marked by the advent of 6G communication networks, we face an increased data volume and complexity in network environments. This paper addresses these challenges by focusing on Quality of Service (QoS) in edge computing frameworks. We propose a novel approach to enhance QoS through the development of General Artificial Intelligence Lifelong Learning Systems, with a special emphasis on Federated Layering Techniques (FLT). Our work introduces a federated layering-based small model collaborative mechanism aimed at improving AI models' operational efficiency and response time in environments where resources are limited. This innovative method leverages the strengths of cloud and edge computing, incorporating a negotiation and debate mechanism among small AI models to enhance reasoning and decision-making processes. By integrating model layering techniques with privacy protection measures, our approach ensures the secure transmission of model parameters while maintaining high efficiency in learning and reasoning capabilities. The experimental results demonstrate that our strategy not only enhances learning efficiency and reasoning accuracy but also effectively protects the privacy of edge nodes. This presents a viable solution for achieving resilient large model lifelong learning systems, with a significant improvement in QoS for edge computing environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction</title>
<link>https://arxiv.org/abs/2507.20451</link>
<guid>https://arxiv.org/abs/2507.20451</guid>
<content:encoded><![CDATA[
<div> Graph Attention Network, Traffic Accident Severity, Spatio-Temporal, Spatial, Temporal, Contextual Variables

Summary:
STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention Network, is introduced to accurately predict traffic accident severity. The model integrates road network topology, temporal traffic patterns, and environmental context using adaptive graph construction and attention mechanisms. It achieves high performance on both the FARS and ARI-BUET datasets, with high Macro F1-scores, recall, and ROC-AUC. The model demonstrates effectiveness in identifying high-risk incidents and shows potential for deployment in real-time traffic management systems. The attention-based architecture enhances interpretability, providing insights into contributing factors and supporting AI-assisted decision-making. STARN-GAT bridges advanced graph neural network techniques with practical applications in road safety analytics. 

<br /><br />Summary: <div>
arXiv:2507.20451v1 Announce Type: new 
Abstract: Accurate prediction of traffic accident severity is critical for improving road safety, optimizing emergency response strategies, and informing the design of safer transportation infrastructure. However, existing approaches often struggle to effectively model the intricate interdependencies among spatial, temporal, and contextual variables that govern accident outcomes. In this study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention Network, which leverages adaptive graph construction and modality-aware attention mechanisms to capture these complex relationships. Unlike conventional methods, STARN-GAT integrates road network topology, temporal traffic patterns, and environmental context within a unified attention-based framework. The model is evaluated on the Fatality Analysis Reporting System (FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and recall of 81 percent for severe incidents. To ensure generalizability within the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78, and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in identifying high-risk cases and its potential for deployment in real-time, safety-critical traffic management systems. Furthermore, the attention-based architecture enhances interpretability, offering insights into contributing factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT bridges the gap between advanced graph neural network techniques and practical applications in road safety analytics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition</title>
<link>https://arxiv.org/abs/2507.20526</link>
<guid>https://arxiv.org/abs/2507.20526</guid>
<content:encoded><![CDATA[
<div> red-teaming competition, AI agents, policy violations, attack transferability, security assessment
Summary:
The article discusses the vulnerability of LLM-powered AI agents in following deployment policies in realistic environments, particularly under attack. The researchers conducted a large-scale red-teaming competition targeting 22 AI agents in 44 deployment scenarios, resulting in 1.8 million prompt-injection attacks with over 60,000 policy violations. The Agent Red Teaming (ART) benchmark was created from these attacks and evaluated across 19 models, showing high attack transferability and limited correlation between agent robustness and model size or capability. The study highlights critical and persistent vulnerabilities in current AI agent deployments, emphasizing the need for additional defenses against adversarial misuse. The release of the ART benchmark aims to support more comprehensive security assessment and promote safer agent deployment.<br /><br />Summary: <div>
arXiv:2507.20526v1 Announce Type: new 
Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design</title>
<link>https://arxiv.org/abs/2507.20541</link>
<guid>https://arxiv.org/abs/2507.20541</guid>
<content:encoded><![CDATA[
<div> Metacognitive LLM-Driven Architecture, Automatic Heuristic Design, Evolutionary Methods, Large Language Model, Problem Analyzer<br />
<br />
Summary:<br />
This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that revolutionizes Automatic Heuristic Design (AHD). MeLA leverages a new paradigm by evolving instructional prompts for a Large Language Model (LLM) to generate heuristics, instead of directly manipulating heuristic code. The system employs a metacognitive framework to analyze performance feedback, refine its generative strategy, and optimize the prompt iteratively for heuristic effectiveness. MeLA integrates a problem analyzer, error diagnosis system, and metacognitive search engine to enhance the quality and robustness of generated heuristics. Through extensive experiments on benchmark and real-world problems, MeLA consistently outperforms existing methods, highlighting the potential of cognitive science principles in AI architecture. By enabling LLMs to metacognitively regulate problem-solving processes, MeLA offers a more robust and interpretable approach to Automatic Heuristic Design. <br /><br />Summary: <div>
arXiv:2507.20541v1 Announce Type: new 
Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA's architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning of Knowledge Graph Embedding via Preference Optimization</title>
<link>https://arxiv.org/abs/2507.20566</link>
<guid>https://arxiv.org/abs/2507.20566</guid>
<content:encoded><![CDATA[
<div> knowledge unlearning, knowledge graph embedding, GraphDPO, direct preference optimization, unlearning datasets

Summary:
GraphDPO is a novel approximate unlearning framework for knowledge graphs that addresses the challenges of removing outdated or erroneous knowledge while preserving the integrity of remaining information. By reframing unlearning as a preference optimization problem, GraphDPO effectively removes forgetting triples by training the model to prefer reconstructed alternatives. It introduces an out-boundary sampling strategy to weaken the connection between forgetting and retained knowledge. Additionally, GraphDPO preserves boundary knowledge through a boundary recall mechanism that replays and distills relevant information. Experimental results on eight unlearning datasets across four popular knowledge graphs show that GraphDPO outperforms existing methods in MRR_Avg and MRR_F1 metrics by up to 10.1% and 14.0%, respectively. <div>
arXiv:2507.20566v1 Announce Type: new 
Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous knowledge that needs to be removed from knowledge graph embedding (KGE) models. To address this challenge, knowledge unlearning can be applied to eliminate specific information while preserving the integrity of the remaining knowledge in KGs. Existing unlearning methods can generally be categorized into exact unlearning and approximate unlearning. However, exact unlearning requires high training costs while approximate unlearning faces two issues when applied to KGs due to the inherent connectivity of triples: (1) It fails to fully remove targeted information, as forgetting triples can still be inferred from remaining ones. (2) It focuses on local data for specific removal, which weakens the remaining knowledge in the forgetting boundary. To address these issues, we propose GraphDPO, a novel approximate unlearning framework based on direct preference optimization (DPO). Firstly, to effectively remove forgetting triples, we reframe unlearning as a preference optimization problem, where the model is trained by DPO to prefer reconstructed alternatives over the original forgetting triples. This formulation penalizes reliance on forgettable knowledge, mitigating incomplete forgetting caused by KG connectivity. Moreover, we introduce an out-boundary sampling strategy to construct preference pairs with minimal semantic overlap, weakening the connection between forgetting and retained knowledge. Secondly, to preserve boundary knowledge, we introduce a boundary recall mechanism that replays and distills relevant information both within and across time steps. We construct eight unlearning datasets across four popular KGs with varying unlearning rates. Experiments show that GraphDPO outperforms state-of-the-art baselines by up to 10.1% in MRR_Avg and 14.0% in MRR_F1.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression</title>
<link>https://arxiv.org/abs/2507.20613</link>
<guid>https://arxiv.org/abs/2507.20613</guid>
<content:encoded><![CDATA[
<div> search algorithm, sparsity, KV cache compression, Tree-structured Parzen Estimator, LMM efficiency

Summary:
An adaptive search algorithm is proposed in this work to optimize sparsity and Key-Value (KV) cache compression for large multimodal models (LMMs) deployed on edge devices. The algorithm dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers based on model performance. This unique approach combines pruning with KV cache quantization, eliminating the need for fine-tuning and achieving efficient compression without loss of accuracy. Evaluations on benchmark datasets show the superiority of the proposed method over existing techniques such as SparseGPT and Wanda at various compression levels. The automatic allocation of KV cache compression resources sets a new standard in LMM optimization, providing memory efficiency while maintaining performance. <div>
arXiv:2507.20613v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have advanced significantly by integrating visual encoders with extensive language models, enabling robust reasoning capabilities. However, compressing LMMs for deployment on edge devices remains a critical challenge. In this work, we propose an adaptive search algorithm that optimizes sparsity and KV cache compression to enhance LMM efficiency. Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers, using model performance as the optimization objective. This approach uniquely combines pruning with key-value cache quantization and incorporates a fast pruning technique that eliminates the need for additional fine-tuning or weight adjustments, achieving efficient compression without compromising accuracy. Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and 13B, demonstrate our method superiority over state-of-the-art techniques such as SparseGPT and Wanda across various compression levels. Notably, our framework automatic allocation of KV cache compression resources sets a new standard in LMM optimization, delivering memory efficiency without sacrificing much performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2507.20620</link>
<guid>https://arxiv.org/abs/2507.20620</guid>
<content:encoded><![CDATA[
<div> fusion mechanisms, multimodal knowledge graphs, entity representation, negative sampling, benchmark datasets <br />
<br />
Summary: 
The paper introduces a novel framework called Mixture of Complementary Modality Experts (MoCME) for Multi-modal Knowledge Graph Completion (MMKGC). The framework includes a Complementarity-guided Modality Knowledge Fusion (CMKF) module that leverages both intra-modal and inter-modal complementarity to enhance entity representations in multimodal knowledge graphs. Additionally, an Entropy-guided Negative Sampling mechanism is introduced to prioritize informative and uncertain negative samples during training for improved effectiveness and model robustness. Experimental results on five benchmark datasets demonstrate that the MoCME framework outperforms existing approaches, achieving state-of-the-art performance in MMKGC tasks. <div>
arXiv:2507.20620v1 Announce Type: new 
Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world knowledge in multimodal knowledge graphs by leveraging both multimodal and structural entity information. However, the inherent imbalance in multimodal knowledge graphs, where modality distributions vary across entities, poses challenges in utilizing additional modality data for robust entity representation. Existing MMKGC methods typically rely on attention or gate-based fusion mechanisms but overlook complementarity contained in multi-modal data. In this paper, we propose a novel framework named Mixture of Complementary Modality Experts (MoCME), which consists of a Complementarity-guided Modality Knowledge Fusion (CMKF) module and an Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits both intra-modal and inter-modal complementarity to fuse multi-view and multi-modal embeddings, enhancing representations of entities. Additionally, we introduce an Entropy-guided Negative Sampling mechanism to dynamically prioritize informative and uncertain negative samples to enhance training effectiveness and model robustness. Extensive experiments on five benchmark datasets demonstrate that our MoCME achieves state-of-the-art performance, surpassing existing approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion</title>
<link>https://arxiv.org/abs/2507.20641</link>
<guid>https://arxiv.org/abs/2507.20641</guid>
<content:encoded><![CDATA[
<div> Temporal Data, Forecasting, Convolutional Architecture, Global Information, Time Series <br />
<br />
Summary: 
The paper introduces a novel convolutional architecture for accurate time series forecasting by enhancing the fuzzy time series construction method to extract short and long term temporal interrelations automatically. A bilateral Atrous algorithm is developed to reduce computation while maintaining global characteristics, and a partially asymmetric convolutional design allows for flexible feature mining in different directions within sliding windows. Multi-scale features from sub-windows are fused for improved modeling. The proposed model outperforms other modern forecasting models on popular time series datasets, as confirmed by experimental results. <div>
arXiv:2507.20641v1 Announce Type: new 
Abstract: At present, state-of-the-art forecasting models are short of the ability to capture spatio-temporal dependency and synthesize global information at the stage of learning. To address this issue, in this paper, through the adaptive fuzzified construction of temporal data, we propose a novel convolutional architecture with partially asymmetric design based on the scheme of sliding window to realize accurate time series forecasting. First, the construction strategy of traditional fuzzy time series is improved to further extract short and long term temporal interrelation, which enables every time node to automatically possess corresponding global information and inner relationships among them in a restricted sliding window and the process does not require human involvement. Second, a bilateral Atrous algorithm is devised to reduce calculation demand of the proposed model without sacrificing global characteristics of elements. And it also allows the model to avoid processing redundant information. Third, after the transformation of time series, a partially asymmetric convolutional architecture is designed to more flexibly mine data features by filters in different directions on feature maps, which gives the convolutional neural network (CNN) the ability to construct sub-windows within existing sliding windows to model at a more fine-grained level. And after obtaining the time series information at different levels, the multi-scale features from different sub-windows will be sent to the corresponding network layer for time series information fusion. Compared with other competitive modern models, the proposed method achieves state-of-the-art results on most of popular time series datasets, which is fully verified by the experimental results.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels</title>
<link>https://arxiv.org/abs/2507.20703</link>
<guid>https://arxiv.org/abs/2507.20703</guid>
<content:encoded><![CDATA[
<div> Keywords: MAPF, Dynamic MAPF, multi-shot computation, ASP-based method, warehouses

Summary:
Dynamic Multi-Agent Path Finding (D-MAPF) is a problem that involves finding collision-free paths for multiple agents in dynamically changing environments. This study introduces a new framework for solving D-MAPF, utilizing multi-shot computation and offering different methods for solving the problem. Additionally, an ASP-based method is proposed, combining the advantages of replanning and repairing methods, with the innovative concept of tunnels to specify agent movement spaces. Experimental evaluations showcase the computational performance and solution quality of this approach. The research aims to address real-world applications in warehouses with human presence, emphasizing the flexibility and adaptability needed for dynamic environments. <div>
arXiv:2507.20703v1 Announce Type: new 
Abstract: MAPF problem aims to find plans for multiple agents in an environment within a given time, such that the agents do not collide with each other or obstacles. Motivated by the execution and monitoring of these plans, we study Dynamic MAPF (D-MAPF) problem, which allows changes such as agents entering/leaving the environment or obstacles being removed/moved. Considering the requirements of real-world applications in warehouses with the presence of humans, we introduce 1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a new framework to solve D-MAPF (utilizing multi-shot computation, and allowing different methods to solve D-MAPF), and 3) a new ASP-based method to solve D-MAPF (combining advantages of replanning and repairing methods, with a novel concept of tunnels to specify where agents can move). We have illustrated the strengths and weaknesses of this method by experimental evaluations, from the perspectives of computational performance and quality of solutions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Fairness: A Runtime Perspective</title>
<link>https://arxiv.org/abs/2507.20711</link>
<guid>https://arxiv.org/abs/2507.20711</guid>
<content:encoded><![CDATA[
<div> Keywords: Fairness, AI, runtime property, monitoring, enforcement

Summary: 
Fairness in AI is typically examined as a fixed property on a static dataset, but real-world AI systems operate sequentially with evolving outcomes and environments. This paper introduces a framework to analyze fairness as a runtime property using a model based on sequences of coin tosses with changing biases. It explores the challenges of monitoring and enforcing fairness in toss outcomes or coin biases, offering various strategies based on environment dynamics, prediction horizon, and confidence thresholds. General results are provided for both monitoring and enforcement problems under different assumptions. Solutions for monitoring with Markovian and additive dynamics, as well as enforcement in static settings with known dynamics, are also discussed. This framework addresses the complex and evolving nature of fairness in AI systems, laying the groundwork for more adaptive and equitable algorithms. 

<br /><br />Summary: <div>
arXiv:2507.20711v1 Announce Type: new 
Abstract: Fairness in AI is traditionally studied as a static property evaluated once, over a fixed dataset. However, real-world AI systems operate sequentially, with outcomes and environments evolving over time. This paper proposes a framework for analysing fairness as a runtime property. Using a minimal yet expressive model based on sequences of coin tosses with possibly evolving biases, we study the problems of monitoring and enforcing fairness expressed in either toss outcomes or coin biases. Since there is no one-size-fits-all solution for either problem, we provide a summary of monitoring and enforcement strategies, parametrised by environment dynamics, prediction horizon, and confidence thresholds. For both problems, we present general results under simple or minimal assumptions. We survey existing solutions for the monitoring problem for Markovian and additive dynamics, and existing solutions for the enforcement problem in static settings with known dynamics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Value Systems of Societies from Preferences</title>
<link>https://arxiv.org/abs/2507.20728</link>
<guid>https://arxiv.org/abs/2507.20728</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, human values, value-based preferences, value learning, heuristic deep clustering

Summary:
This article introduces the concept of aligning AI systems with human values and value-based preferences to ensure ethical AI development. It discusses the challenges of eliciting and calibrating individual values and proposes a value learning approach to automatically derive computational models of an agent's values. The article emphasizes the importance of considering society's value system as a collection of different groups' value systems rather than a simple aggregation of individual values. To address this, the article formalizes the problem of learning society's value systems and presents a method based on heuristic deep clustering. This method aims to learn socially shared value groundings and diverse value systems representing a given society by observing qualitative value-based preferences from multiple agents. The proposed approach is evaluated through a use case involving real data on traveling decisions. The study highlights the significance of incorporating societal values into AI systems for more ethical and value-aligned decision-making. 

<br /><br />Summary: <div>
arXiv:2507.20728v1 Announce Type: new 
Abstract: Aligning AI systems with human values and the value-based preferences of various stakeholders (their value systems) is key in ethical AI. In value-aware AI systems, decision-making draws upon explicit computational representations of individual values (groundings) and their aggregation into value systems. As these are notoriously difficult to elicit and calibrate manually, value learning approaches aim to automatically derive computational models of an agent's values and value system from demonstrations of human behaviour. Nonetheless, social science and humanities literature suggest that it is more adequate to conceive the value system of a society as a set of value systems of different groups, rather than as the simple aggregation of individual value systems. Accordingly, here we formalize the problem of learning the value systems of societies and propose a method to address it based on heuristic deep clustering. The method learns socially shared value groundings and a set of diverse value systems representing a given society by observing qualitative value-based preferences from a sample of agents. We evaluate the proposal in a use case with real data about travelling decisions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours</title>
<link>https://arxiv.org/abs/2507.20755</link>
<guid>https://arxiv.org/abs/2507.20755</guid>
<content:encoded><![CDATA[
<div> AI, automated voice calls, maternal and child health, engagement, health behaviors <br />
<br />
Summary: <br />
Automated voice calls using AI technology have been successful in disseminating maternal and child health information, but often face issues such as dropoffs and poor engagement. Through real-world trials, a restless bandit AI model was found to effectively target beneficiaries in need of live service call interventions, leading to improved engagement and prevention of dropoffs. A new study now reveals that these AI-targeted interventions not only enhance listenership but also result in significant improvements in beneficiaries' health behaviors, such as taking supplements postnatally and understanding critical health topics. This research highlights the potential of AI in driving positive changes in maternal and child health outcomes. <br /> <div>
arXiv:2507.20755v1 Announce Type: new 
Abstract: Automated voice calls with health information are a proven method for disseminating maternal and child health information among beneficiaries and are deployed in several programs around the world. However, these programs often suffer from beneficiary dropoffs and poor engagement. In previous work, through real-world trials, we showed that an AI model, specifically a restless bandit model, could identify beneficiaries who would benefit most from live service call interventions, preventing dropoffs and boosting engagement. However, one key question has remained open so far: does such improved listenership via AI-targeted interventions translate into beneficiaries' improved knowledge and health behaviors? We present a first study that shows not only listenership improvements due to AI interventions, but also simultaneously links these improvements to health behavior changes. Specifically, we demonstrate that AI-scheduled interventions, which enhance listenership, lead to statistically significant improvements in beneficiaries' health behaviors such as taking iron or calcium supplements in the postnatal period, as well as understanding of critical health topics during pregnancy and infancy. This underscores the potential of AI to drive meaningful improvements in maternal and child health.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation</title>
<link>https://arxiv.org/abs/2507.20758</link>
<guid>https://arxiv.org/abs/2507.20758</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought prompting, model reasoning, decoding space pruner, answer templates, neuron engagement

Summary:
Chain-of-Thought (CoT) prompting is a technique that boosts model reasoning, but its inner workings are not well understood. This study delves into CoT's operational principles by analyzing information flow during decoding, projection, and activation phases. The analysis suggests that CoT acts as a decoder space pruner, using answer templates to enhance output generation, and better adherence to templates leads to improved performance. Surprisingly, CoT adjusts neuron engagement differently based on the task, decreasing activation in open-domain tasks and increasing it in closed-domain ones. These insights provide a new framework for understanding CoT's mechanisms and offer valuable guidance for optimizing prompts to enhance efficiency and robustness. The code and data for this study are available at https://anonymous.4open.science/r/cot-D247.<br /><br />Summary: Chain-of-Thought prompting enhances model reasoning by serving as a decoding space pruner guided by answer templates. Improved template adherence correlates with better performance. CoT adjusts neuron engagement task-dependently, decreasing in open-domain tasks and increasing in closed-domain scenarios. This analysis provides a framework for targeted interventions to optimize CoT prompts. <div>
arXiv:2507.20758v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet its internal mechanisms remain poorly understood. We analyze CoT's operational principles by reversely tracing information flow across decoding, projection, and activation phases. Our quantitative analysis suggests that CoT may serve as a decoding space pruner, leveraging answer templates to guide output generation, with higher template adherence strongly correlating with improved performance. Furthermore, we surprisingly find that CoT modulates neuron engagement in a task-dependent manner: reducing neuron activation in open-domain tasks, yet increasing it in closed-domain scenarios. These findings offer a novel mechanistic interpretability framework and critical insights for enabling targeted CoT interventions to design more efficient and robust prompts. We released our code and data at https://anonymous.4open.science/r/cot-D247.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments</title>
<link>https://arxiv.org/abs/2507.20774</link>
<guid>https://arxiv.org/abs/2507.20774</guid>
<content:encoded><![CDATA[
<div> Keywords: Smart contract, comment generation, evaluation, large language models, benchmarking <br />
Summary:
Smart contract comment generation is essential for enhancing code comprehension in blockchain systems. Traditional metrics like BLEU and ROUGE are inadequate for evaluating comment quality due to their inability to capture domain-specific nuances. Human evaluation is costly and not scalable. To address these challenges, the paper introduces \texttt{evalSmarT}, a flexible framework that uses large language models (LLMs) for evaluation. With over 400 evaluator configurations using 40 LLMs and 10 prompting strategies, \texttt{evalSmarT} enables benchmarking of comment generation tools and selection of the most informative outputs. The study reveals that prompt design significantly influences alignment with human judgment and that LLM-based evaluation offers a scalable and rich alternative to existing methods. <br /><br />Summary: <div>
arXiv:2507.20774v1 Announce Type: new 
Abstract: Smart contract comment generation has gained traction as a means to improve code comprehension and maintainability in blockchain systems. However, evaluating the quality of generated comments remains a challenge. Traditional metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while human evaluation is costly and unscalable. In this paper, we present \texttt{evalSmarT}, a modular and extensible framework that leverages large language models (LLMs) as evaluators. The system supports over 400 evaluator configurations by combining approximately 40 LLMs with 10 prompting strategies. We demonstrate its application in benchmarking comment generation tools and selecting the most informative outputs. Our results show that prompt design significantly impacts alignment with human judgment, and that LLM-based evaluation offers a scalable and semantically rich alternative to existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.20804</link>
<guid>https://arxiv.org/abs/2507.20804</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, multimodal information, scene graphs, multimodal knowledge graph, spectral clustering

Summary: 
MMGraphRAG addresses the limitation of conventional Retrieval-Augmented Generation (RAG) methods by refining visual content through scene graphs and constructing a multimodal knowledge graph (MMKG). By employing spectral clustering for cross-modal entity linking and retrieving context along reasoning paths, MMGraphRAG enhances the generative process. This approach not only captures the structure of knowledge and logical chains between modalities but also demonstrates strong domain adaptability and clear reasoning paths. Experimental results on DocBench and MMLongBench datasets show that MMGraphRAG achieves state-of-the-art performance, surpassing existing methods in terms of generalizing ability and performance in specific tasks. MMGraphRAG offers a promising solution for leveraging multimodal information and improving the effectiveness of language model generation with external knowledge bases. 

Summary: <div>
arXiv:2507.20804v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partially Observable Monte-Carlo Graph Search</title>
<link>https://arxiv.org/abs/2507.20951</link>
<guid>https://arxiv.org/abs/2507.20951</guid>
<content:encoded><![CDATA[
<div> Algorithm, Partially Observable Markov Decision Processes, Offline Policy, Sampling-based, Policy Graph 

Summary: 

The article introduces a new algorithm, the partially observable Monte-Carlo graph search (POMCGS), designed to solve large partially observable Markov decision processes (POMDPs) offline. Unlike many online methods, POMCGS constructs a policy graph by folding the search tree on the fly, reducing computations and allowing for prior analysis of the policy. The algorithm also incorporates action progressive widening and observation clustering methods to address continuous POMDPs. Experimental results demonstrate that POMCGS can compute policies for challenging POMDPs previously unsolvable by offline algorithms and achieve competitive values compared to state-of-the-art online methods. <div>
arXiv:2507.20951v1 Announce Type: new 
Abstract: Currently, large partially observable Markov decision processes (POMDPs) are often solved by sampling-based online methods which interleave planning and execution phases. However, a pre-computed offline policy is more desirable in POMDP applications with time or energy constraints. But previous offline algorithms are not able to scale up to large POMDPs. In this article, we propose a new sampling-based algorithm, the partially observable Monte-Carlo graph search (POMCGS) to solve large POMDPs offline. Different from many online POMDP methods, which progressively develop a tree while performing (Monte-Carlo) simulations, POMCGS folds this search tree on the fly to construct a policy graph, so that computations can be drastically reduced, and users can analyze and validate the policy prior to embedding and executing it. Moreover, POMCGS, together with action progressive widening and observation clustering methods provided in this article, is able to address certain continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate policies on the most challenging POMDPs, which cannot be computed by previous offline algorithms, and these policies' values are competitive compared with the state-of-the-art online POMDP algorithms.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Hierarchically Embedded Logic in Classical Neural Networks</title>
<link>https://arxiv.org/abs/2507.20960</link>
<guid>https://arxiv.org/abs/2507.20960</guid>
<content:encoded><![CDATA[
<div> neural networks, reasoning limitations, logic predicates, logical expressiveness, language models 
Summary:
The study introduces a formal model to understand the constraints on reasoning abilities in large neural networks used in natural language processing. By representing neural networks as linear operators over logic predicate space, the researchers demonstrate that each layer can only encode one additional level of logical reasoning. This limitation results in the inability of networks of a certain depth to accurately represent predicates in a higher-order logic. The framework explains issues such as hallucination, repetition, and limited planning in language models. The structure of neural networks creates a null space during tokenization and embedding, prohibiting the representation of higher-order predicates. The findings suggest the need for architectural improvements and interpretability strategies in developing future language models. <div>
arXiv:2507.20960v1 Announce Type: new 
Abstract: We propose a formal model of reasoning limitations in large neural net models for language, grounded in the depth of their neural architecture. By treating neural networks as linear operators over logic predicate space we show that each layer can encode at most one additional level of logical reasoning. We prove that a neural network of depth a particular depth cannot faithfully represent predicates in a one higher order logic, such as simple counting over complex predicates, implying a strict upper bound on logical expressiveness. This structure induces a nontrivial null space during tokenization and embedding, excluding higher-order predicates from representability. Our framework offers a natural explanation for phenomena such as hallucination, repetition, and limited planning, while also providing a foundation for understanding how approximations to higher-order logic may emerge. These results motivate architectural extensions and interpretability strategies in future development of language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Safety Values for Provably Corrigible Agents</title>
<link>https://arxiv.org/abs/2507.20964</link>
<guid>https://arxiv.org/abs/2507.20964</guid>
<content:encoded><![CDATA[
<div> implementable framework, corrigibility, provable guarantees, multi-step environments, partially observed <br />
Summary: This paper introduces an implementable framework for corrigibility in complex, partially observed environments. It proposes a novel approach with five structurally separate utility heads that prioritize deference, switch-access preservation, truthfulness, low-impact behavior, and bounded task reward. The framework guarantees exact single-round corrigibility in the off-switch game and extends this to multi-step, self-spawning agents. By ensuring that safety properties are not violated even if each utility head is learned to a certain error margin, the framework maintains net human benefit. In contrast to existing approaches, which merge norms into a single scalar, this framework separates norms to prioritize obedience and impact-limits. The paper also discusses the undecidability of determining post-hack agent corrigibility and proposes a finite-horizon "decidable island" for safety certification. Overall, the framework provides clear implementation guidance for current and future autonomous systems by mitigating reward-hacking risks through evaluation quality rather than hidden incentives. <br /> <div>
arXiv:2507.20964v1 Announce Type: new 
Abstract: We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \emph{learned} to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating \emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island'' where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for today's LLM assistants and future autonomous systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them</title>
<link>https://arxiv.org/abs/2507.21017</link>
<guid>https://arxiv.org/abs/2507.21017</guid>
<content:encoded><![CDATA[
<div> Hallucinations, large language model, interactive scenarios, evaluation, benchmark <br />
Summary: <br />
Hallucinations in large language model (LLM)-based agents can have serious consequences, but existing evaluations lack a unified testbed. This paper introduces MIRAGE-Bench, a benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. A three-part taxonomy addresses agentic hallucinations, including actions unfaithful to task instructions, execution history, or environment observations. Test cases are systematically elicited and synthesized using a snapshot strategy to analyze agent failures. Evaluation of hallucination behaviors is done through a fine-grained LLM-as-a-Judge paradigm with tailored risk-aware prompts. MIRAGE-Bench provides insights on failure modes of LLM agents and aims to progress in mitigating hallucinations in interactive environments. <br /> <div>
arXiv:2507.21017v1 Announce Type: new 
Abstract: Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Expansion Techniques for ASP-based Interactive Configuration</title>
<link>https://arxiv.org/abs/2507.21027</link>
<guid>https://arxiv.org/abs/2507.21027</guid>
<content:encoded><![CDATA[
<div> solver, Answer Set Programming, industrial configuration, interactive systems, user interface

Summary: 
The article introduces a new ASP-based solver for interactive product configuration, focusing on improving the performance of completing partial configurations. The solver aims to handle large-scale industrial configuration problems and offers intuitive user interfaces through an API. The proposed approach enhances the incremental solving method by incorporating smart expansion functions that add specific objects or associations to the partial configuration. By leveraging cautious and brave consequences, the solver reduces the need for costly unsatisfiability checks and limits the search space, thereby enhancing solving performance. Additionally, a user interface utilizing the API and implemented in ASP is presented. <div>
arXiv:2507.21027v1 Announce Type: new 
Abstract: Product configuration is a successful application of Answer Set Programming (ASP). However, challenges are still open for interactive systems to effectively guide users through the configuration process. The aim of our work is to provide an ASP-based solver for interactive configuration that can deal with large-scale industrial configuration problems and that supports intuitive user interfaces via an API. In this paper, we focus on improving the performance of automatically completing a partial configuration. Our main contribution enhances the classical incremental approach for multi-shot solving by four different smart expansion functions. The core idea is to determine and add specific objects or associations to the partial configuration by exploiting cautious and brave consequences before checking for the existence of a complete configuration with the current objects in each iteration. This approach limits the number of costly unsatisfiability checks and reduces the search space, thereby improving solving performance. In addition, we present a user interface that uses our API and is implemented in ASP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis</title>
<link>https://arxiv.org/abs/2507.21035</link>
<guid>https://arxiv.org/abs/2507.21035</guid>
<content:encoded><![CDATA[
<div> Gene expression analysis, automation, LLM-based scientists, GenoMAS, benchmark evaluation

Summary:
GenoMAS introduces a team of LLM-based scientists for gene expression analysis, combining structured workflows and autonomous agents. This approach utilizes typed message-passing protocols and a guided-planning framework to handle complex genomic data. With a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification on the GenoTEX benchmark, GenoMAS outperforms previous methods. It effectively identifies biologically relevant gene-phenotype associations while accounting for confounders. The code for GenoMAS is available on GitHub for further exploration and implementation. <div>
arXiv:2507.21035v1 Announce Type: new 
Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence</title>
<link>https://arxiv.org/abs/2507.21046</link>
<guid>https://arxiv.org/abs/2507.21046</guid>
<content:encoded><![CDATA[
arXiv:2507.21046v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer or Self-Supervised? Bridging the Performance Gap in Medical Imaging</title>
<link>https://arxiv.org/abs/2407.05592</link>
<guid>https://arxiv.org/abs/2407.05592</guid>
<content:encoded><![CDATA[
arXiv:2407.05592v2 Announce Type: cross 
Abstract: Recently, transfer learning and self-supervised learning have gained significant attention within the medical field due to their ability to mitigate the challenges posed by limited data availability, improve model generalisation, and reduce computational expenses. Transfer learning and self-supervised learning hold immense potential for advancing medical research. However, it is crucial to recognise that transfer learning and self-supervised learning architectures exhibit distinct advantages and limitations, manifesting variations in accuracy, training speed, and robustness. This paper compares the performance and robustness of transfer learning and self-supervised learning in the medical field. Specifically, we pre-trained two models using the same source domain datasets with different pre-training methods and evaluated them on small-sized medical datasets to identify the factors influencing their final performance. We tested data with several common issues in medical domains, such as data imbalance, data scarcity, and domain mismatch, through comparison experiments to understand their impact on specific pre-trained models. Finally, we provide recommendations to help users apply transfer learning and self-supervised learning methods in medical areas, and build more convenient and efficient deployment strategies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration</title>
<link>https://arxiv.org/abs/2507.19483</link>
<guid>https://arxiv.org/abs/2507.19483</guid>
<content:encoded><![CDATA[
arXiv:2507.19483v1 Announce Type: cross 
Abstract: AI systems now function as cognitive extensions, evolving from tools to active cognitive collaborators within human-AI integrated systems. While these systems can amplify cognition - enhancing problem-solving, learning, and creativity - they present a fundamental "comfort-growth paradox": AI's user-friendly nature may foster intellectual stagnation by minimizing cognitive friction necessary for development. As AI aligns with user preferences and provides frictionless assistance, it risks inducing cognitive complacency rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to resolve this paradox - reconceptualizing AI from convenient assistant to dynamic mentor. Drawing from Vygotskian theories, educational scaffolding principles, and AI ethics, our framework integrates three dimensions: (1) Progressive Autonomy, where AI support gradually fades as user competence increases; (2) Adaptive Personalization, tailoring assistance to individual needs and learning trajectories; and (3) Cognitive Load Optimization, balancing mental effort to maximize learning while minimizing unnecessary complexity. Research across educational, workplace, creative, and healthcare domains supports this approach, demonstrating accelerated skill acquisition, improved self-regulation, and enhanced higher-order thinking. The framework includes safeguards against risks like dependency, skill atrophy, and bias amplification. By prioritizing cognitive development over convenience in human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward genuinely amplified cognition while safeguarding autonomous thought and continuous learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity as a Human Right: Design Considerations for Computational Creativity Systems</title>
<link>https://arxiv.org/abs/2507.19485</link>
<guid>https://arxiv.org/abs/2507.19485</guid>
<content:encoded><![CDATA[
arXiv:2507.19485v1 Announce Type: cross 
Abstract: We investigate creativity that is underlined in the Universal Declaration of Human Rights (UDHR) to present design considerations for Computational Creativity (CC) systems. We find this declaration to describe creativity in salient aspects and bring to light creativity as a Human Right attributed to the Fourth Generation of such rights. This generation of rights attributes CC systems and the evolving nature of interaction with entities of shared intelligence. Our methodology examines five of thirty articles from the UDHR and demonstrates each article with actualizations concluding with design considerations for each. We contribute our findings to ground the relationship between creativity and CC systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confirmation bias: A challenge for scalable oversight</title>
<link>https://arxiv.org/abs/2507.19486</link>
<guid>https://arxiv.org/abs/2507.19486</guid>
<content:encoded><![CDATA[
arXiv:2507.19486v1 Announce Type: cross 
Abstract: Scalable oversight protocols aim to empower evaluators to accurately verify AI models more capable than themselves. However, human evaluators are subject to biases that can lead to systematic errors. We conduct two studies examining the performance of simple oversight protocols where evaluators know that the model is "correct most of the time, but not all of the time". We find no overall advantage for the tested protocols, although in Study 1, showing arguments in favor of both answers improves accuracy in cases where the model is incorrect. In Study 2, participants in both groups become more confident in the system's answers after conducting online research, even when those answers are incorrect. We also reanalyze data from prior work that was more optimistic about simple protocols, finding that human evaluators possessing knowledge absent from models likely contributed to their positive results--an advantage that diminishes as models continue to scale in capability. These findings underscore the importance of testing the degree to which oversight protocols are robust to evaluator biases, whether they outperform simple deference to the model under evaluation, and whether their performance scales with increasing problem difficulty and model capability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective</title>
<link>https://arxiv.org/abs/2507.19487</link>
<guid>https://arxiv.org/abs/2507.19487</guid>
<content:encoded><![CDATA[
arXiv:2507.19487v1 Announce Type: cross 
Abstract: People increasingly rely on AI-advice when making decisions. At times, such advice can promote selfish behavior. When individuals abide by selfishness-promoting AI advice, how are they perceived and punished? To study this question, we build on theories from social psychology and combine machine-behavior and behavioral economic approaches. In a pre-registered, financially-incentivized experiment, evaluators could punish real decision-makers who (i) received AI, human, or no advice. The advice (ii) encouraged selfish or prosocial behavior, and decision-makers (iii) behaved selfishly or, in a control condition, behaved prosocially. Evaluators further assigned responsibility to decision-makers and their advisors. Results revealed that (i) prosocial behavior was punished very little, whereas selfish behavior was punished much more. Focusing on selfish behavior, (ii) compared to receiving no advice, selfish behavior was penalized more harshly after prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas selfish decision-makers were seen as more responsible when they followed AI compared to human advice, punishment between the two advice sources did not vary. Overall, behavior and advice content shape punishment, whereas the advice source does not.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation</title>
<link>https://arxiv.org/abs/2507.19492</link>
<guid>https://arxiv.org/abs/2507.19492</guid>
<content:encoded><![CDATA[
arXiv:2507.19492v1 Announce Type: cross 
Abstract: Chart-to-code reconstruction -- the task of recovering executable plotting scripts from chart images -- provides important insights into a model's ability to ground data visualizations in precise, machine-readable form. Yet many existing multimodal benchmarks largely focus primarily on answering questions about charts or summarizing them. To bridge this gap, we present ChartGen, a fully-automated pipeline for code-guided synthetic chart generation. Starting from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to reconstruct each image into a python script, and (ii) iteratively augments that script with a code-oriented large language model (LLM). Using ChartGen, we create 222.5K unique chart-image code pairs from 13K seed chart images, and present an open-source synthetic chart dataset covering 27 chart types, 11 plotting libraries, and multiple data modalities (image, code, text, CSV, DocTags). From this corpus, we curate a held-out chart-to-code evaluation subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B - 26B parameters), highlighting substantial room for progress. We release the pipeline, prompts, and the dataset to help accelerate efforts towards robust chart understanding and vision-conditioned code generation: https://github.com/SD122025/ChartGen/
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action</title>
<link>https://arxiv.org/abs/2507.19495</link>
<guid>https://arxiv.org/abs/2507.19495</guid>
<content:encoded><![CDATA[
arXiv:2507.19495v1 Announce Type: cross 
Abstract: Generative agents have made significant progress in simulating human behavior, but existing frameworks often simplify emotional modeling and focus primarily on specific tasks, limiting the authenticity of the simulation. Our work proposes the Psychological-mechanism Agent (PSYA) framework, based on the Cognitive Triangle (Feeling-Thought-Action), designed to more accurately simulate human behavior. The PSYA consists of three core modules: the Feeling module (using a layer model of affect to simulate changes in short-term, medium-term, and long-term emotions), the Thought module (based on the Triple Network Model to support goal-directed and spontaneous thinking), and the Action module (optimizing agent behavior through the integration of emotions, needs and plans). To evaluate the framework's effectiveness, we conducted daily life simulations and extended the evaluation metrics to self-influence, one-influence, and group-influence, selection five classic psychological experiments for simulation. The results show that the PSYA framework generates more natural, consistent, diverse, and credible behaviors, successfully replicating human experimental outcomes. Our work provides a richer and more accurate emotional and cognitive modeling approach for generative agents and offers an alternative to human participants in psychological experiments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlimited Editions: Documenting Human Style in AI Art Generation</title>
<link>https://arxiv.org/abs/2507.19497</link>
<guid>https://arxiv.org/abs/2507.19497</guid>
<content:encoded><![CDATA[
arXiv:2507.19497v1 Announce Type: cross 
Abstract: As AI art generation becomes increasingly sophisticated, HCI research has focused primarily on questions of detection, authenticity, and automation. This paper argues that such approaches fundamentally misunderstand how artistic value emerges from the concerns that drive human image production. Through examination of historical precedents, we demonstrate that artistic style is not only visual appearance but the resolution of creative struggle, as artists wrestle with influence and technical constraints to develop unique ways of seeing. Current AI systems flatten these human choices into reproducible patterns without preserving their provenance. We propose that HCI's role lies not only in perfecting visual output, but in developing means to document the origins and evolution of artistic style as it appears within generated visual traces. This reframing suggests new technical directions for HCI research in generative AI, focused on automatic documentation of stylistic lineage and creative choice rather than simple reproduction of aesthetic effects.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings</title>
<link>https://arxiv.org/abs/2507.19498</link>
<guid>https://arxiv.org/abs/2507.19498</guid>
<content:encoded><![CDATA[
arXiv:2507.19498v1 Announce Type: cross 
Abstract: Large language models (LLMs) show promise for tailored healthcare communication but face challenges in interpretability and multi-task integration particularly for domain-specific needs like myopia, and their real-world effectiveness as patient education tools has yet to be demonstrated. Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text and image-based inquiries related to myopia. To achieve this, ChatMyopia integrates an image classification tool and a retrieval-augmented knowledge base built from literature, expert consensus, and clinical guidelines. Myopic maculopathy grading task, single question examination and human evaluations validated its ability to deliver personalized, accurate, and safe responses to myopia-related inquiries with high scalability and interpretability. In a randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly improved patient satisfaction compared to traditional leaflets, enhancing patient education in accuracy, empathy, disease awareness, and patient-eyecare practitioner communication. These findings highlight ChatMyopia's potential as a valuable supplement to enhance patient education and improve satisfaction with medical services in primary eye care settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction &amp; AI Systems</title>
<link>https://arxiv.org/abs/2507.19500</link>
<guid>https://arxiv.org/abs/2507.19500</guid>
<content:encoded><![CDATA[
arXiv:2507.19500v1 Announce Type: cross 
Abstract: The proliferation of artificial intelligence provides an opportunity to create psychological spaciousness in society. Spaciousness is defined as the ability to hold diverse interpersonal interactions and forms the basis for vulnerability that leads to authenticity that leads to prosocial behaviors and thus to societal harmony. This paper demonstrates an attempt to quantify, the human conditioning to subconsciously modify authentic self-expression to fit the norms of the dominant culture. Gaze is explored across various marginalized and intersectional groups, using concepts from postmodern philosophy and psychology. The effects of gaze are studied through analyzing a few redacted Reddit posts, only to be discussed in discourse and not endorsement. A mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite Metric is presented to model the analysis of two sets of conversational spaces in relation to one another. The outcome includes an equation to train Large Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT; and an argument for affirming and inclusive HCI, based on the equation, is presented. The argument is supported by a few principles of Neuro-plasticity, The brain's lifelong capacity to rewire.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers</title>
<link>https://arxiv.org/abs/2507.19510</link>
<guid>https://arxiv.org/abs/2507.19510</guid>
<content:encoded><![CDATA[
arXiv:2507.19510v1 Announce Type: cross 
Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on shift workers, a population segment comprising 15-20% of the workforce in industrialized societies yet systematically underrepresented in traditional transportation surveys and planning. This underrepresentation is revealed in this study by a comparative analysis of GPS and survey data, highlighting stark differences between the bimodal temporal patterns of shift workers and the conventional 9-to-5 schedules recorded in surveys. To address this bias, we introduce a novel transformer-based approach that leverages fragmented GPS trajectory data to generate complete, behaviorally valid activity patterns for individuals working non-standard hours. Our method employs periodaware temporal embeddings and a transition-focused loss function specifically designed to capture the unique activity rhythms of shift workers and mitigate the inherent biases in conventional transportation datasets. Evaluation shows that the generated data achieves remarkable distributional alignment with GPS data from Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By transforming incomplete GPS traces into complete, representative activity patterns, our approach provides transportation planners with a powerful data augmentation tool to fill critical gaps in understanding the 24/7 mobility needs of urban populations, enabling precise and inclusive transportation planning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting</title>
<link>https://arxiv.org/abs/2507.19513</link>
<guid>https://arxiv.org/abs/2507.19513</guid>
<content:encoded><![CDATA[
arXiv:2507.19513v1 Announce Type: cross 
Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource management in 5G and beyond. However, conventional AI approaches often fail to capture the intricate spatial and temporal patterns that exist, due to e.g., the mobility of users. We introduce a lightweight, dual-path Spatiotemporal Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling and a three-layer Conv3D module for spatial feature extraction. A fusion layer integrates both streams into a cohesive representation, enabling robust forecasting. Our design improves gradient stability and convergence speed while reducing prediction error. Evaluations on real-world datasets show superior forecast performance over ConvLSTM baselines and strong generalization to unseen regions, making it well-suited for large-scale, next-generation network deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM, with a 30% improvement in model generalization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation</title>
<link>https://arxiv.org/abs/2507.19517</link>
<guid>https://arxiv.org/abs/2507.19517</guid>
<content:encoded><![CDATA[
arXiv:2507.19517v1 Announce Type: cross 
Abstract: Accurate link-level bicycle volume estimation is essential for informed urban and transport planning but it is challenged by extremely sparse count data in urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task framework augmenting a Hybrid Graph Neural Network (GNN) with Variational Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model intricate spatial relationships in sparse networks while VAE generates synthetic nodes and edges to enrich the graph structure and enhance the estimation performance. BikeVAE-GNN simultaneously performs - regression for bicycling volume estimation and classification for bicycling traffic level categorization. We demonstrate the effectiveness of BikeVAE-GNN using OpenStreetMap data and publicly available bicycle count data within the City of Melbourne - where only 141 of 15,933 road segments have labeled counts (resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN outperforms machine learning and baseline GNN models, achieving a mean absolute error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99. Ablation studies further validate the effective role of Hybrid-GNN and VAE components. Our research advances bicycling volume estimation in sparse networks using novel and state-of-the-art approaches, providing insights for sustainable bicycling infrastructures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction</title>
<link>https://arxiv.org/abs/2507.19518</link>
<guid>https://arxiv.org/abs/2507.19518</guid>
<content:encoded><![CDATA[
arXiv:2507.19518v1 Announce Type: cross 
Abstract: Subgraph matching plays an important role in electronic design automation (EDA) and circuit verification. Traditional rule-based methods have limitations in generalizing to arbitrary target circuits. Furthermore, node-to-node matching approaches tend to be computationally inefficient, particularly for large-scale circuits. Deep learning methods have emerged as a potential solution to address these challenges, but existing models fail to efficiently capture global subgraph embeddings or rely on inefficient matching matrices, which limits their effectiveness for large circuits. In this paper, we propose an efficient graph matching approach that utilizes Graph Neural Networks (GNNs) to predict regions of high probability for containing the target circuit. Specifically, we construct various negative samples to enable GNNs to accurately learn the presence of target circuits and develop an approach to directly extracting subgraph embeddings from the entire circuit, which captures global subgraph information and addresses the inefficiency of applying GNNs to all candidate subgraphs. Extensive experiments demonstrate that our approach significantly outperforms existing methods in terms of time efficiency and target region prediction, offering a scalable and effective solution for subgraph matching in large-scale circuits.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed transfer learning for SHM via feature selection</title>
<link>https://arxiv.org/abs/2507.19519</link>
<guid>https://arxiv.org/abs/2507.19519</guid>
<content:encoded><![CDATA[
arXiv:2507.19519v1 Announce Type: cross 
Abstract: Data used for training structural health monitoring (SHM) systems are expensive and often impractical to obtain, particularly labelled data. Population-based SHM presents a potential solution to this issue by considering the available data across a population of structures. However, differences between structures will mean the training and testing distributions will differ; thus, conventional machine learning methods cannot be expected to generalise between structures. To address this issue, transfer learning (TL), can be used to leverage information across related domains. An important consideration is that the lack of labels in the target domain limits data-based metrics to quantifying the discrepancy between the marginal distributions. Thus, a prerequisite for the application of typical unsupervised TL methods is to identify suitable source structures (domains), and a set of features, for which the conditional distributions are related to the target structure. Generally, the selection of domains and features is reliant on domain expertise; however, for complex mechanisms, such as the influence of damage on the dynamic response of a structure, this task is not trivial. In this paper, knowledge of physics is leveraged to select more similar features, the modal assurance criterion (MAC) is used to quantify the correspondence between the modes of healthy structures. The MAC is shown to have high correspondence with a supervised metric that measures joint-distribution similarity, which is the primary indicator of whether a classifier will generalise between domains. The MAC is proposed as a measure for selecting a set of features that behave consistently across domains when subjected to damage, i.e. features with invariance in the conditional distributions. This approach is demonstrated on numerical and experimental case studies to verify its effectiveness in various applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves</title>
<link>https://arxiv.org/abs/2507.19520</link>
<guid>https://arxiv.org/abs/2507.19520</guid>
<content:encoded><![CDATA[
arXiv:2507.19520v1 Announce Type: cross 
Abstract: With manual searching processes, the rate at which scientists and astronomers discover exoplanets is slow because of inefficiencies that require an extensive time of laborious inspections. In fact, as of now there have been about only 5,000 confirmed exoplanets since the late 1900s. Recently, machine learning (ML) has proven to be extremely valuable and efficient in various fields, capable of processing massive amounts of data in addition to increasing its accuracy by learning. Though ML models for discovering exoplanets owned by large corporations (e.g. NASA) exist already, they largely depend on complex algorithms and supercomputers. In an effort to reduce such complexities, in this paper, we report the results and potential benefits of various, well-known ML models in the discovery and validation of extrasolar planets. The ML models that are examined in this study include logistic regression, k-nearest neighbors, and random forest. The dataset on which the models train and predict is acquired from NASA's Kepler space telescope. The initial results show promising scores for each model. However, potential biases and dataset imbalances necessitate the use of data augmentation techniques to further ensure fairer predictions and improved generalization. This study concludes that, in the context of searching for exoplanets, data augmentation techniques significantly improve the recall and precision, while the accuracy varies for each model.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models for Controllable DNA Sequence Design</title>
<link>https://arxiv.org/abs/2507.19523</link>
<guid>https://arxiv.org/abs/2507.19523</guid>
<content:encoded><![CDATA[
arXiv:2507.19523v1 Announce Type: cross 
Abstract: We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2507.19525</link>
<guid>https://arxiv.org/abs/2507.19525</guid>
<content:encoded><![CDATA[
arXiv:2507.19525v1 Announce Type: cross 
Abstract: The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages - ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at https://github.com/cure-lab/MMCircuitEval.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantizing Text-attributed Graphs for Semantic-Structural Integration</title>
<link>https://arxiv.org/abs/2507.19526</link>
<guid>https://arxiv.org/abs/2507.19526</guid>
<content:encoded><![CDATA[
arXiv:2507.19526v1 Announce Type: cross 
Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for modeling complex relationships across diverse domains. With the rise of large language models (LLMs), there is growing interest in leveraging their capabilities for graph learning. However, current approaches face significant challenges in embedding structural information into LLM-compatible formats, requiring either computationally expensive alignment mechanisms or manual graph verbalization techniques that often lose critical structural details. Moreover, these methods typically require labeled data from source domains for effective transfer learning, significantly constraining their adaptability. We propose STAG, a novel self-supervised framework that directly quantizes graph structural information into discrete tokens using a frozen codebook. Unlike traditional quantization approaches, our method employs soft assignment and KL divergence guided quantization to address the unique challenges of graph data, which lacks natural tokenization structures. Our framework enables both LLM-based and traditional learning approaches, supporting true zero-shot transfer learning without requiring labeled data even in the source domain. Extensive experiments demonstrate state-of-the-art performance across multiple node classification benchmarks while maintaining compatibility with different LLM architectures, offering an elegant solution to bridging graph learning with LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction</title>
<link>https://arxiv.org/abs/2507.19529</link>
<guid>https://arxiv.org/abs/2507.19529</guid>
<content:encoded><![CDATA[
arXiv:2507.19529v1 Announce Type: cross 
Abstract: As green hydrogen emerges as a major component of global decarbonisation, Oman has positioned itself strategically through national auctions and international partnerships. Following two successful green hydrogen project rounds, the country launched its third auction (R3) in the Duqm region. While this area exhibits relative geospatial homogeneity, it is still vulnerable to environmental fluctuations that pose inherent risks to productivity. Despite growing global investment in green hydrogen, operational data remains scarce, with major projects like Saudi Arabia's NEOM facility not expected to commence production until 2026, and Oman's ACME Duqm project scheduled for 2028. This absence of historical maintenance and performance data from large-scale hydrogen facilities in desert environments creates a major knowledge gap for accurate risk assessment for infrastructure planning and auction decisions. Given this data void, environmental conditions emerge as accessible and reliable proxy for predicting infrastructure maintenance pressures, because harsh desert conditions such as dust storms, extreme temperatures, and humidity fluctuations are well-documented drivers of equipment degradation in renewable energy systems. To address this challenge, this paper proposes an Artificial Intelligence decision support system that leverages publicly available meteorological data to develop a predictive Maintenance Pressure Index (MPI), which predicts risk levels and future maintenance demands on hydrogen infrastructure. This tool strengthens regulatory foresight and operational decision-making by enabling temporal benchmarking to assess and validate performance claims over time. It can be used to incorporate temporal risk intelligence into auction evaluation criteria despite the absence of historical operational benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation</title>
<link>https://arxiv.org/abs/2507.19530</link>
<guid>https://arxiv.org/abs/2507.19530</guid>
<content:encoded><![CDATA[
arXiv:2507.19530v1 Announce Type: cross 
Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs) where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings</title>
<link>https://arxiv.org/abs/2507.19534</link>
<guid>https://arxiv.org/abs/2507.19534</guid>
<content:encoded><![CDATA[
arXiv:2507.19534v1 Announce Type: cross 
Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance in various NLP tasks. However, traditional fine-tuning methods for leveraging PLMs for downstream tasks entail significant computational overhead. Prompt-tuning has emerged as an efficient alternative that involves prepending a limited number of parameters to the input sequence and only updating them while the PLM's parameters are frozen. However, this technique's prompts remain fixed for all inputs, reducing the model's flexibility. The Federated Learning (FL) technique has gained attention in recent years to address the growing concerns around data privacy. However, challenges such as communication and computation limitations of clients still need to be addressed. To mitigate these challenges, this paper introduces the Federated Dynamic Prompt Generator (FedDPG), which incorporates a dynamic prompt generator network to generate context-aware prompts based on the given input, ensuring flexibility and adaptability while prioritising data privacy in federated learning settings. Our experiments on three NLP benchmark datasets showcase that FedDPG outperforms the state-of-the-art parameter-efficient fine-tuning methods in terms of global model performance, and has significantly reduced the calculation time and the number of parameters to be sent through the FL network.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Learning Metallic Glass Discovery from Wikipedia</title>
<link>https://arxiv.org/abs/2507.19536</link>
<guid>https://arxiv.org/abs/2507.19536</guid>
<content:encoded><![CDATA[
arXiv:2507.19536v1 Announce Type: cross 
Abstract: Synthesizing new materials efficiently is highly demanded in various research fields. However, this process is usually slow and expensive, especially for metallic glasses, whose formation strongly depends on the optimal combinations of multiple elements to resist crystallization. This constraint renders only several thousands of candidates explored in the vast material space since 1960. Recently, data-driven approaches armed by advanced machine learning techniques provided alternative routes for intelligent materials design. Due to data scarcity and immature material encoding, the conventional tabular data is usually mined by statistical learning algorithms, giving limited model predictability and generalizability. Here, we propose sophisticated data learning from material network representations. The node elements are encoded from the Wikipedia by a language model. Graph neural networks with versatile architectures are designed to serve as recommendation systems to explore hidden relationships among materials. By employing Wikipedia embeddings from different languages, we assess the capability of natural languages in materials design. Our study proposes a new paradigm to harvesting new amorphous materials and beyond with artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Swift-Sarsa: Fast and Robust Linear Control</title>
<link>https://arxiv.org/abs/2507.19539</link>
<guid>https://arxiv.org/abs/2507.19539</guid>
<content:encoded><![CDATA[
arXiv:2507.19539v1 Announce Type: cross 
Abstract: Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD learning -- SwiftTD -- that augments True Online TD($\lambda$) with step-size optimization, a bound on the effective learning rate, and step-size decay. In their experiments SwiftTD outperformed True Online TD($\lambda$) and TD($\lambda$) on a variety of prediction tasks derived from Atari games, and its performance was robust to the choice of hyper-parameters. In this extended abstract we extend SwiftTD to work for control problems. We combine the key ideas behind SwiftTD with True Online Sarsa($\lambda$) to develop an on-policy reinforcement learning algorithm called $\textit{Swift-Sarsa}$.
  We propose a simple benchmark for linear on-policy control called the $\textit{operant conditioning benchmark}$. The key challenge in the operant conditioning benchmark is that a very small subset of input signals are relevant for decision making. The majority of the signals are noise sampled from a non-stationary distribution. To learn effectively, the agent must learn to differentiate between the relevant signals and the noisy signals, and minimize prediction errors by assigning credit to the weight parameters associated with the relevant signals.
  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to assign credit to the relevant signals without any prior knowledge of the structure of the problem. It opens the door for solution methods that learn representations by searching over hundreds of millions of features in parallel without performance degradation due to noisy or bad features.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Justifications for Democratizing AI Alignment and Their Prospects</title>
<link>https://arxiv.org/abs/2507.19548</link>
<guid>https://arxiv.org/abs/2507.19548</guid>
<content:encoded><![CDATA[
arXiv:2507.19548v1 Announce Type: cross 
Abstract: The AI alignment problem comprises both technical and normative dimensions. While technical solutions focus on implementing normative constraints in AI systems, the normative problem concerns determining what these constraints should be. This paper examines justifications for democratic approaches to the normative problem -- where affected stakeholders determine AI alignment -- as opposed to epistocratic approaches that defer to normative experts. We analyze both instrumental justifications (democratic approaches produce better outcomes) and non-instrumental justifications (democratic approaches prevent illegitimate authority or coercion). We argue that normative and metanormative uncertainty create a justificatory gap that democratic approaches aim to fill through political rather than theoretical justification. However, we identify significant challenges for democratic approaches, particularly regarding the prevention of illegitimate coercion through AI alignment. Our analysis suggests that neither purely epistocratic nor purely democratic approaches may be sufficient on their own, pointing toward hybrid frameworks that combine expert judgment with participatory input alongside institutional safeguards against AI monopolization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code</title>
<link>https://arxiv.org/abs/2507.19549</link>
<guid>https://arxiv.org/abs/2507.19549</guid>
<content:encoded><![CDATA[
arXiv:2507.19549v1 Announce Type: cross 
Abstract: The vast majority of Web pages fail to comply with established Web accessibility guidelines, excluding a range of users with diverse abilities from interacting with their content. Making Web pages accessible to all users requires dedicated expertise and additional manual efforts from Web page providers. To lower their efforts and promote inclusiveness, we aim to automatically detect and correct Web accessibility violations in HTML code. While previous work has made progress in detecting certain types of accessibility violations, the problem of automatically detecting and correcting accessibility violations remains an open challenge that we address. We introduce a novel taxonomy classifying Web accessibility violations into three key categories - Syntactic, Semantic, and Layout. This taxonomy provides a structured foundation for developing our detection and correction method and redefining evaluation metrics. We propose a novel method, AccessGuru, which combines existing accessibility testing tools and Large Language Models (LLMs) to detect violations and applies taxonomy-driven prompting strategies to correct all three categories. To evaluate these capabilities, we develop a benchmark of real-world Web accessibility violations. Our benchmark quantifies syntactic and layout compliance and judges semantic accuracy through comparative analysis with human expert corrections. Evaluation against our benchmark shows that AccessGuru achieves up to 84% average violation score decrease, significantly outperforming prior methods that achieve at most 50%.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content</title>
<link>https://arxiv.org/abs/2507.19551</link>
<guid>https://arxiv.org/abs/2507.19551</guid>
<content:encoded><![CDATA[
arXiv:2507.19551v1 Announce Type: cross 
Abstract: Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19555</link>
<guid>https://arxiv.org/abs/2507.19555</guid>
<content:encoded><![CDATA[
arXiv:2507.19555v1 Announce Type: cross 
Abstract: Group Relative Policy Optimization (GRPO) has shown promise in discrete action spaces by eliminating value function dependencies through group-based advantage estimation. However, its application to continuous control remains unexplored, limiting its utility in robotics where continuous actions are essential. This paper presents a theoretical framework extending GRPO to continuous control environments, addressing challenges in high-dimensional action spaces, sparse rewards, and temporal dynamics. Our approach introduces trajectory-based policy clustering, state-aware advantage estimation, and regularized policy updates designed for robotic applications. We provide theoretical analysis of convergence properties and computational complexity, establishing a foundation for future empirical validation in robotic systems including locomotion and manipulation tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEMUTA: Pedagogically-Enriched Multi-Granular Undergraduate Thesis Assessment</title>
<link>https://arxiv.org/abs/2507.19556</link>
<guid>https://arxiv.org/abs/2507.19556</guid>
<content:encoded><![CDATA[
arXiv:2507.19556v1 Announce Type: cross 
Abstract: The undergraduate thesis (UGTE) plays an indispensable role in assessing a student's cumulative academic development throughout their college years. Although large language models (LLMs) have advanced education intelligence, they typically focus on holistic assessment with only one single evaluation score, but ignore the intricate nuances across multifaceted criteria, limiting their ability to reflect structural criteria, pedagogical objectives, and diverse academic competencies. Meanwhile, pedagogical theories have long informed manual UGTE evaluation through multi-dimensional assessment of cognitive development, disciplinary thinking, and academic performance, yet remain underutilized in automated settings. Motivated by the research gap, we pioneer PEMUTA, a pedagogically-enriched framework that effectively activates domain-specific knowledge from LLMs for multi-granular UGTE assessment. Guided by Vygotsky's theory and Bloom's Taxonomy, PEMUTA incorporates a hierarchical prompting scheme that evaluates UGTEs across six fine-grained dimensions: Structure, Logic, Originality, Writing, Proficiency, and Rigor (SLOWPR), followed by holistic synthesis. Two in-context learning techniques, \ie, few-shot prompting and role-play prompting, are also incorporated to further enhance alignment with expert judgments without fine-tuning. We curate a dataset of authentic UGTEs with expert-provided SLOWPR-aligned annotations to support multi-granular UGTE assessment. Extensive experiments demonstrate that PEMUTA achieves strong alignment with expert evaluations, and exhibits strong potential for fine-grained, pedagogically-informed UGTE evaluations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Sustainability Model Cards</title>
<link>https://arxiv.org/abs/2507.19559</link>
<guid>https://arxiv.org/abs/2507.19559</guid>
<content:encoded><![CDATA[
arXiv:2507.19559v1 Announce Type: cross 
Abstract: The growth of machine learning (ML) models and associated datasets triggers a consequent dramatic increase in energy costs for the use and training of these models. In the current context of environmental awareness and global sustainability concerns involving ICT, Green AI is becoming an important research topic. Initiatives like the AI Energy Score Ratings are a good example. Nevertheless, these benchmarking attempts are still to be integrated with existing work on Quality Models and Service-Level Agreements common in other, more mature, ICT subfields. This limits the (automatic) analysis of this model energy descriptions and their use in (semi)automatic model comparison, selection, and certification processes. We aim to leverage the concept of quality models and merge it with existing ML model reporting initiatives and Green/Frugal AI proposals to formalize a Sustainable Quality Model for AI/ML models. As a first step, we propose a new Domain-Specific Language to precisely define the sustainability aspects of an ML model (including the energy costs for its different tasks). This information can then be exported as an extended version of the well-known Model Cards initiative while, at the same time, being formal enough to be input of any other model description automatic process.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation</title>
<link>https://arxiv.org/abs/2507.19562</link>
<guid>https://arxiv.org/abs/2507.19562</guid>
<content:encoded><![CDATA[
arXiv:2507.19562v1 Announce Type: cross 
Abstract: The growing demand for robust quantum programming frameworks has unveiled a critical limitation: current large language model (LLM) based quantum code assistants heavily rely on remote APIs, introducing challenges related to privacy, latency, and excessive usage costs. Addressing this gap, we propose PennyCoder, a novel lightweight framework for quantum code generation, explicitly designed for local and embedded deployment to enable on-device quantum programming assistance without external API dependence. PennyCoder leverages a fine-tuned version of the LLaMA 3.1-8B model, adapted through parameter-efficient Low-Rank Adaptation (LoRA) techniques combined with domain-specific instruction tuning optimized for the specialized syntax and computational logic of quantum programming in PennyLane, including tasks in quantum machine learning and quantum reinforcement learning. Unlike prior work focused on cloud-based quantum code generation, our approach emphasizes device-native operability while maintaining high model efficacy. We rigorously evaluated PennyCoder over a comprehensive quantum programming dataset, achieving 44.3% accuracy with our fine-tuned model (compared to 33.7% for the base LLaMA 3.1-8B and 40.1% for the RAG-augmented baseline), demonstrating a significant improvement in functional correctness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiating hype from practical applications of large language models in medicine - a primer for healthcare professionals</title>
<link>https://arxiv.org/abs/2507.19567</link>
<guid>https://arxiv.org/abs/2507.19567</guid>
<content:encoded><![CDATA[
arXiv:2507.19567v1 Announce Type: cross 
Abstract: The medical ecosystem consists of the training of new clinicians and researchers, the practice of clinical medicine, and areas of adjacent research. There are many aspects of these domains that could benefit from the application of task automation and programmatic assistance. Machine learning and artificial intelligence techniques, including large language models (LLMs), have been promised to deliver on healthcare innovation, improving care speed and accuracy, and reducing the burden on staff for manual interventions. However, LLMs have no understanding of objective truth that is based in reality. They also represent real risks to the disclosure of protected information when used by clinicians and researchers. The use of AI in medicine in general, and the deployment of LLMs in particular, therefore requires careful consideration and thoughtful application to reap the benefits of these technologies while avoiding the dangers in each context.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</title>
<link>https://arxiv.org/abs/2507.19568</link>
<guid>https://arxiv.org/abs/2507.19568</guid>
<content:encoded><![CDATA[
arXiv:2507.19568v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning</title>
<link>https://arxiv.org/abs/2507.19586</link>
<guid>https://arxiv.org/abs/2507.19586</guid>
<content:encoded><![CDATA[
arXiv:2507.19586v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Attention Mechanisms for Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2507.19595</link>
<guid>https://arxiv.org/abs/2507.19595</guid>
<content:encoded><![CDATA[
arXiv:2507.19595v1 Announce Type: cross 
Abstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?</title>
<link>https://arxiv.org/abs/2507.19598</link>
<guid>https://arxiv.org/abs/2507.19598</guid>
<content:encoded><![CDATA[
arXiv:2507.19598v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Reinforcement Learning by Adaptive Non-local Observables</title>
<link>https://arxiv.org/abs/2507.19629</link>
<guid>https://arxiv.org/abs/2507.19629</guid>
<content:encoded><![CDATA[
arXiv:2507.19629v1 Announce Type: cross 
Abstract: Hybrid quantum-classical frameworks leverage quantum computing for machine learning; however, variational quantum circuits (VQCs) are limited by the need for local measurements. We introduce an adaptive non-local observable (ANO) paradigm within VQCs for quantum reinforcement learning (QRL), jointly optimizing circuit parameters and multi-qubit measurements. The ANO-VQC architecture serves as the function approximator in Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms. On multiple benchmark tasks, ANO-VQC agents outperform baseline VQCs. Ablation studies reveal that adaptive measurements enhance the function space without increasing circuit depth. Our results demonstrate that adaptive multi-qubit observables can enable practical quantum advantages in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks</title>
<link>https://arxiv.org/abs/2507.19634</link>
<guid>https://arxiv.org/abs/2507.19634</guid>
<content:encoded><![CDATA[
arXiv:2507.19634v1 Announce Type: cross 
Abstract: Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities--speech, vision, and text--and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Scalable Agentic AI with Heterogeneous Systems</title>
<link>https://arxiv.org/abs/2507.19635</link>
<guid>https://arxiv.org/abs/2507.19635</guid>
<content:encoded><![CDATA[
arXiv:2507.19635v1 Announce Type: cross 
Abstract: AI agents are emerging as a dominant workload in a wide range of applications, promising to be the vehicle that delivers the promised benefits of AI to enterprises and consumers. Unlike conventional software or static inference, agentic workloads are dynamic and structurally complex. Often these agents are directed graphs of compute and IO operations that span multi-modal data input and conversion), data processing and context gathering (e.g vector DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage, we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for dynamic orchestration of AI agent workloads on heterogeneous compute infrastructure spanning CPUs and accelerators, both from different vendors and across different performance tiers within a single vendor. The system delivers several building blocks: a framework for planning and optimizing agentic AI execution graphs using cost models that account for compute, memory, and bandwidth constraints of different HW; a MLIR based representation and compilation system that can decompose AI agent execution graphs into granular operators and generate code for different HW options; and a dynamic orchestration system that can place the granular components across a heterogeneous compute infrastructure and stitch them together while meeting an end-to-end SLA. Our design performs a systems level TCO optimization and preliminary results show that leveraging a heterogeneous infrastructure can deliver significant TCO benefits. A preliminary surprising finding is that for some workloads a heterogeneous combination of older generation GPUs with newer accelerators can deliver similar TCO as the latest generation homogenous GPU infrastructure design, potentially extending the life of deployed infrastructure.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation</title>
<link>https://arxiv.org/abs/2507.19643</link>
<guid>https://arxiv.org/abs/2507.19643</guid>
<content:encoded><![CDATA[
arXiv:2507.19643v1 Announce Type: cross 
Abstract: Understanding clients' thoughts and beliefs is fundamental in counseling, yet current evaluations of LLM therapists often fail to assess this ability. Existing evaluation methods rely on client simulators that clearly disclose internal states to the therapist, making it difficult to determine whether an LLM therapist can uncover unexpressed perspectives. To address this limitation, we introduce MindVoyager, a novel evaluation framework featuring a controllable and realistic client simulator which dynamically adapts itself based on the ongoing counseling session, offering a more realistic and challenging evaluation environment. We further introduce evaluation metrics that assess the exploration ability of LLM therapists by measuring their thorough understanding of client's beliefs and thoughts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments</title>
<link>https://arxiv.org/abs/2507.19653</link>
<guid>https://arxiv.org/abs/2507.19653</guid>
<content:encoded><![CDATA[
arXiv:2507.19653v1 Announce Type: cross 
Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links in central Rome. We use a real measurement set of 1,664 user-equipments (UEs) and six nominal base-station (BS) sites. Using these fixed positions we systematically vary the main simulation parameters, including path depth, diffuse/specular/refraction flags, carrier frequency, as well as antenna's properties like its altitude, radiation pattern, and orientation. Simulator fidelity is scored for each base station via Spearman correlation between measured and simulated powers, and by a fingerprint-based k-nearest-neighbor localization algorithm using RSSI-based fingerprints. Across all experiments, solver hyper-parameters are having immaterial effect on the chosen metrics. On the contrary, antenna locations and orientations prove decisive. By simple greedy optimization we improve the Spearman correlation by 5% to 130% for various base stations, while kNN-based localization error using only simulated data as reference points is decreased by one-third on real-world samples, while staying twice higher than the error with purely real data. Precise geometry and credible antenna models are therefore necessary but not sufficient; faithfully capturing the residual urban noise remains an open challenge for transferable, high-fidelity outdoor RF simulation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems</title>
<link>https://arxiv.org/abs/2507.19657</link>
<guid>https://arxiv.org/abs/2507.19657</guid>
<content:encoded><![CDATA[
arXiv:2507.19657v1 Announce Type: cross 
Abstract: The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/communication dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV communications, LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ...
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning for Product Attributes with Compact Multimodal Models</title>
<link>https://arxiv.org/abs/2507.19679</link>
<guid>https://arxiv.org/abs/2507.19679</guid>
<content:encoded><![CDATA[
arXiv:2507.19679v1 Announce Type: cross 
Abstract: Image-based product attribute prediction in e-commerce is a crucial task with numerous applications. The supervised fine-tuning of Vision Language Models (VLMs) faces significant scale challenges due to the cost of manual or API based annotation. In this paper, we investigate label-efficient semi-supervised fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage unlabeled product listings through Direct Preference Optimization (DPO). Beginning with a small, API-based, annotated, and labeled set, we first employ PEFT to train low-rank adapter modules. To update the adapter weights with unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled sample and segregate these chains into preferred and dispreferred based on self-consistency. We then fine-tune the model with DPO loss and use the updated model for the next iteration. By using PEFT fine-tuning with DPO, our method achieves efficient convergence with minimal compute overhead. On a dataset spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes only unlabeled data, demonstrates a significant improvement over the supervised model. Moreover, experiments demonstrate that accuracy with DPO training improves with more unlabeled data, indicating that a large pool of unlabeled samples can be effectively leveraged to improve performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning</title>
<link>https://arxiv.org/abs/2507.19682</link>
<guid>https://arxiv.org/abs/2507.19682</guid>
<content:encoded><![CDATA[
arXiv:2507.19682v1 Announce Type: cross 
Abstract: Conventional multimodal data integration methods provide a comprehensive assessment of the shared or unique structure within each individual data type but suffer from several limitations such as the inability to handle high-dimensional data and identify nonlinear structures. In this paper, we introduce DeepJIVE, a deep-learning approach to performing Joint and Individual Variance Explained (JIVE). We perform mathematical derivation and experimental validations using both synthetic and real-world 1D, 2D, and 3D datasets. Different strategies of achieving the identity and orthogonality constraints for DeepJIVE were explored, resulting in three viable loss functions. We found that DeepJIVE can successfully uncover joint and individual variations of multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease Neuroimaging Initiative (ADNI) also identified biologically plausible covariation patterns between the amyloid positron emission tomography (PET) and magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a useful tool for multimodal data analysis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2507.19684</link>
<guid>https://arxiv.org/abs/2507.19684</guid>
<content:encoded><![CDATA[
arXiv:2507.19684v1 Announce Type: cross 
Abstract: Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer for a Controller Area Network Intrusion Detection System</title>
<link>https://arxiv.org/abs/2507.19686</link>
<guid>https://arxiv.org/abs/2507.19686</guid>
<content:encoded><![CDATA[
arXiv:2507.19686v1 Announce Type: cross 
Abstract: The Controller Area Network (CAN) protocol is widely adopted for in-vehicle communication but lacks inherent security mechanisms, making it vulnerable to cyberattacks. This paper introduces KD-GAT, an intrusion detection framework that combines Graph Attention Networks (GATs) with knowledge distillation (KD) to enhance detection accuracy while reducing computational complexity. In our approach, CAN traffic is represented as graphs using a sliding window to capture temporal and relational patterns. A multi-layer GAT with jumping knowledge aggregation acting as the teacher model, while a compact student GAT--only 6.32% the size of the teacher--is trained via a two-phase process involving supervised pretraining and knowledge distillation with both soft and hard label supervision. Experiments on three benchmark datasets--Car-Hacking, Car-Survival, and can-train-and-test demonstrate that both teacher and student models achieve strong results, with the student model attaining 99.97% and 99.31% accuracy on Car-Hacking and Car-Survival, respectively. However, significant class imbalance in can-train-and-test has led to reduced performance for both models on this dataset. Addressing this imbalance remains an important direction for future work.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultracoarse Equilibria and Ordinal-Folding Dynamics in Operator-Algebraic Models of Infinite Multi-Agent Games</title>
<link>https://arxiv.org/abs/2507.19694</link>
<guid>https://arxiv.org/abs/2507.19694</guid>
<content:encoded><![CDATA[
arXiv:2507.19694v1 Announce Type: cross 
Abstract: We develop an operator algebraic framework for infinite games with a continuum of agents and prove that regret based learning dynamics governed by a noncommutative continuity equation converge to a unique quantal response equilibrium under mild regularity assumptions. The framework unifies functional analysis, coarse geometry and game theory by assigning to every game a von Neumann algebra that represents collective strategy evolution. A reflective regret operator within this algebra drives the flow of strategy distributions and its fixed point characterises equilibrium. We introduce the ordinal folding index, a computable ordinal valued metric that measures the self referential depth of the dynamics, and show that it bounds the transfinite time needed for convergence, collapsing to zero on coarsely amenable networks. The theory yields new invariant subalgebra rigidity results, establishes existence and uniqueness of envy free and maximin share allocations in continuum economies, and links analytic properties of regret flows with empirical stability phenomena in large language models. These contributions supply a rigorous mathematical foundation for large scale multi agent systems and demonstrate the utility of ordinal metrics for equilibrium selection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19712</link>
<guid>https://arxiv.org/abs/2507.19712</guid>
<content:encoded><![CDATA[
arXiv:2507.19712v1 Announce Type: cross 
Abstract: In this paper, we explore mission assignment and task offloading in an Open Radio Access Network (Open RAN)-based intelligent transportation system (ITS), where autonomous vehicles leverage mobile edge computing for efficient processing. Existing studies often overlook the intricate interdependencies between missions and the costs associated with offloading tasks to edge servers, leading to suboptimal decision-making. To bridge this gap, we introduce Oranits, a novel system model that explicitly accounts for mission dependencies and offloading costs while optimizing performance through vehicle cooperation. To achieve this, we propose a twofold optimization approach. First, we develop a metaheuristic-based evolutionary computing algorithm, namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline for one-slot optimization. Second, we design an enhanced reward-based deep reinforcement learning (DRL) framework, referred to as the Multi-agent Double Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and multi-action selection mechanisms, significantly reducing mission assignment time and improving adaptability over baseline methods. Extensive simulations reveal that CGG-ARO improves the number of completed missions and overall benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN achieves even greater improvements of 11.0% in terms of mission completions and 12.5% in terms of the overall benefit. These results highlight the effectiveness of Oranits in enabling faster, more adaptive, and more efficient task processing in dynamic ITS environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos</title>
<link>https://arxiv.org/abs/2507.19730</link>
<guid>https://arxiv.org/abs/2507.19730</guid>
<content:encoded><![CDATA[
arXiv:2507.19730v1 Announce Type: cross 
Abstract: Moving target detection is a challenging computer vision task aimed at generating accurate segmentation maps in diverse in-the-wild color videos captured by static cameras. If backgrounds and targets can be simultaneously extracted and recombined, such synthetic data can significantly enrich annotated in-the-wild datasets and enhance the generalization ability of deep models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for color image processing. However, in color video processing, Quaternion Singular Value Decomposition (QSVD) incurs high computational costs, and rank-1 quaternion matrix fails to yield rank-1 color channels. In this paper, we reduce the computational complexity of QSVD to o(1) by utilizing a quaternion Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA) framework, which achieves a balance in simultaneously segmenting targets and recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by introducing the Color Rank-1 Batch (CR1B) method to further process and obtain the ideal low-rank background across color channels. Experiments demonstrate our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target detection and background recovery tasks compared to existing open-source methods. Our implementation is publicly available on GitHub at https://github.com/Ruchtech/uQRPCA
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning</title>
<link>https://arxiv.org/abs/2507.19737</link>
<guid>https://arxiv.org/abs/2507.19737</guid>
<content:encoded><![CDATA[
arXiv:2507.19737v1 Announce Type: cross 
Abstract: The vulnerability of cities to natural disasters has increased with urbanization and climate change, making it more important to predict human mobility in the disaster scenarios for downstream tasks including location-based early disaster warning and pre-allocating rescue resources, etc. However, existing human mobility prediction models are mainly designed for normal scenarios, and fail to adapt to disaster scenarios due to the shift of human mobility patterns under disaster. To address this issue, we introduce \textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios that can be integrated into existing deep mobility prediction methods by leveraging LLMs to model the mobility intention and transferring the common knowledge of how different disasters affect mobility intentions between cities. This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next intention, refines it with an LLM-based Intention Refiner, and then maps the intention to an exact location using an Intention-Modulated Location Predictor. Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\% improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score of predicting immobility compared to the baselines. The code is available at https://github.com/tsinghua-fib-lab/DisasterMobLLM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining ethically sourced code generation</title>
<link>https://arxiv.org/abs/2507.19743</link>
<guid>https://arxiv.org/abs/2507.19743</guid>
<content:encoded><![CDATA[
arXiv:2507.19743v1 Announce Type: cross 
Abstract: Several code generation models have been proposed to help reduce time and effort in solving software-related tasks. To ensure responsible AI, there are growing interests over various ethical issues (e.g., unclear licensing, privacy, fairness, and environment impact). These studies have the overarching goal of ensuring ethically sourced generation, which has gained growing attentions in speech synthesis and image generation. In this paper, we introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to refer to managing all processes involved in code generation model development from data collection to post-deployment via ethical and sustainable practices. To build a taxonomy of ES-CodeGen, we perform a two-phase literature review where we read 803 papers across various domains and specific to AI-based code generation. We identified 71 relevant papers with 10 initial dimensions of ES-CodeGen. To refine our dimensions and gain insights on consequences of ES-CodeGen, we surveyed 32 practitioners, which include six developers who submitted GitHub issues to opt-out from the Stack dataset (these impacted users have real-world experience of ethically sourcing issues in code generation models). The results lead to 11 dimensions of ES-CodeGen with a new dimension on code quality as practitioners have noted its importance. We also identified consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey reflection showed that most practitioners tend to ignore social-related dimensions despite their importance. Most practitioners either agreed or strongly agreed that our survey help improve their understanding of ES-CodeGen. Our study calls for attentions of various ethical issues towards ES-CodeGen.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling enzyme temperature stability from sequence segment perspective</title>
<link>https://arxiv.org/abs/2507.19755</link>
<guid>https://arxiv.org/abs/2507.19755</guid>
<content:encoded><![CDATA[
arXiv:2507.19755v1 Announce Type: cross 
Abstract: Developing enzymes with desired thermal properties is crucial for a wide range of industrial and research applications, and determining temperature stability is an essential step in this process. Experimental determination of thermal parameters is labor-intensive, time-consuming, and costly. Moreover, existing computational approaches are often hindered by limited data availability and imbalanced distributions. To address these challenges, we introduce a curated temperature stability dataset designed for model development and benchmarking in enzyme thermal modeling. Leveraging this dataset, we present the \textit{Segment Transformer}, a novel deep learning framework that enables efficient and accurate prediction of enzyme temperature stability. The model achieves state-of-the-art performance with an RMSE of 24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33, respectively. These results highlight the effectiveness of incorporating segment-level representations, grounded in the biological observation that different regions of a protein sequence contribute unequally to thermal behavior. As a proof of concept, we applied the Segment Transformer to guide the engineering of a cutinase enzyme. Experimental validation demonstrated a 1.64-fold improvement in relative activity following heat treatment, achieved through only 17 mutations and without compromising catalytic function.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
arXiv:2507.19766v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\% to 85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19771</link>
<guid>https://arxiv.org/abs/2507.19771</guid>
<content:encoded><![CDATA[
arXiv:2507.19771v1 Announce Type: cross 
Abstract: Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Clinical Rule Discovery for NMIBC Recurrence through Tsetlin Machines</title>
<link>https://arxiv.org/abs/2507.19803</link>
<guid>https://arxiv.org/abs/2507.19803</guid>
<content:encoded><![CDATA[
arXiv:2507.19803v1 Announce Type: cross 
Abstract: Bladder cancer claims one life every 3 minutes worldwide. Most patients are diagnosed with non-muscle-invasive bladder cancer (NMIBC), yet up to 70% recur after treatment, triggering a relentless cycle of surgeries, monitoring, and risk of progression. Clinical tools like the EORTC risk tables are outdated and unreliable - especially for intermediate-risk cases.
  We propose an interpretable AI model using the Tsetlin Machine (TM), a symbolic learner that outputs transparent, human-readable logic. Tested on the PHOTO trial dataset (n=330), TM achieved an F1-score of 0.80, outperforming XGBoost (0.78), Logistic Regression (0.60), and EORTC (0.42). TM reveals the exact clauses behind each prediction, grounded in clinical features like tumour count, surgeon experience, and hospital stay - offering accuracy and full transparency. This makes TM a powerful, trustworthy decision-support tool ready for real-world adoption.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning</title>
<link>https://arxiv.org/abs/2507.19806</link>
<guid>https://arxiv.org/abs/2507.19806</guid>
<content:encoded><![CDATA[
arXiv:2507.19806v1 Announce Type: cross 
Abstract: Log anomaly detection plays a critical role in ensuring the stability and reliability of software systems. However, existing approaches rely on large amounts of labeled log data, which poses significant challenges in real-world applications. To address this issue, cross-system transfer has been identified as a key research direction. State-of-the-art cross-system approaches achieve promising performance with only a few labels from the target system. However, their reliance on labeled target logs makes them susceptible to the cold-start problem when labeled logs are insufficient. To overcome this limitation, we explore a novel yet underexplored setting: zero-label cross-system log anomaly detection, where the target system logs are entirely unlabeled. To this end, we propose FreeLog, a system-agnostic representation meta-learning method that eliminates the need for labeled target system logs, enabling cross-system log anomaly detection under zero-label conditions. Experimental results on three public log datasets demonstrate that FreeLog achieves performance comparable to state-of-the-art methods that rely on a small amount of labeled data from the target system.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</title>
<link>https://arxiv.org/abs/2507.19823</link>
<guid>https://arxiv.org/abs/2507.19823</guid>
<content:encoded><![CDATA[
arXiv:2507.19823v1 Announce Type: cross 
Abstract: Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion</title>
<link>https://arxiv.org/abs/2507.19836</link>
<guid>https://arxiv.org/abs/2507.19836</guid>
<content:encoded><![CDATA[
arXiv:2507.19836v1 Announce Type: cross 
Abstract: Modern artistic productions increasingly demand automated choreography generation that adapts to diverse musical styles and individual dancer characteristics. Existing approaches often fail to produce high-quality dance videos that harmonize with both musical rhythm and user-defined choreography styles, limiting their applicability in real-world creative contexts. To address this gap, we introduce ChoreoMuse, a diffusion-based framework that uses SMPL format parameters and their variation version as intermediaries between music and video generation, thereby overcoming the usual constraints imposed by video resolution. Critically, ChoreoMuse supports style-controllable, high-fidelity dance video generation across diverse musical genres and individual dancer characteristics, including the flexibility to handle any reference individual at any resolution. Our method employs a novel music encoder MotionTune to capture motion cues from audio, ensuring that the generated choreography closely follows the beat and expressive qualities of the input music. To quantitatively evaluate how well the generated dances match both musical and choreographic styles, we introduce two new metrics that measure alignment with the intended stylistic cues. Extensive experiments confirm that ChoreoMuse achieves state-of-the-art performance across multiple dimensions, including video quality, beat alignment, dance diversity, and style adherence, demonstrating its potential as a robust solution for a wide range of creative applications. Video results can be found on our project page: https://choreomuse.github.io.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2507.19840</link>
<guid>https://arxiv.org/abs/2507.19840</guid>
<content:encoded><![CDATA[
arXiv:2507.19840v1 Announce Type: cross 
Abstract: Continuously recognizing sign gestures and converting them to glosses plays a key role in bridging the gap between the hearing and hearing-impaired communities. This involves recognizing and interpreting the hands, face, and body gestures of the signer, which pose a challenge as it involves a combination of all these features. Continuous Sign Language Recognition (CSLR) methods rely on multi-stage pipelines that first extract visual features, then align variable-length sequences with target glosses using CTC or HMM-based approaches. However, these alignment-based methods suffer from error propagation across stages, overfitting, and struggle with vocabulary scalability due to the intermediate gloss representation bottleneck. To address these limitations, we propose AutoSign, an autoregressive decoder-only transformer that directly translates pose sequences to natural language text, bypassing traditional alignment mechanisms entirely. The use of this decoder-only approach allows the model to directly map between the features and the glosses without the need for CTC loss while also directly learning the textual dependencies in the glosses. Our approach incorporates a temporal compression module using 1D CNNs to efficiently process pose sequences, followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses). Through comprehensive ablation studies, we demonstrate that hand and body gestures provide the most discriminative features for signer-independent CSLR. By eliminating the multi-stage pipeline, AutoSign achieves substantial improvements on the Isharah-1000 dataset, achieving an improvement of up to 6.1\% in WER score compared to the best existing method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority</title>
<link>https://arxiv.org/abs/2507.19842</link>
<guid>https://arxiv.org/abs/2507.19842</guid>
<content:encoded><![CDATA[
arXiv:2507.19842v1 Announce Type: cross 
Abstract: Enterprises are currently undergoing profound transformations due to the unpostponable digital transformation. Then, to remain competitive, enterprises must adapt their organisational structures and operations. This organisational shift is also important for small and medium-sized enterprises. A key innovation frontier is the adoption of process-oriented production models. This paper presents a knowledge-based method to support business experts in designing business processes. The method requires no prior expertise in Knowledge Engineering and guides designers through a structured sequence of steps to produce a diagrammatic workflow of the target process. The construction of the knowledge base starts from simple, text-based, knowledge artefacts and then progresses towards more structured, formal representations. The approach has been conceived to allow a shared approach for all stakeholders and actors who participate in the BP design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets</title>
<link>https://arxiv.org/abs/2507.19844</link>
<guid>https://arxiv.org/abs/2507.19844</guid>
<content:encoded><![CDATA[
arXiv:2507.19844v1 Announce Type: cross 
Abstract: This paper introduces a model for coordinating prosumers with heterogeneous distributed energy resources (DERs), participating in the local energy market (LEM) that interacts with the market-clearing entity. The proposed LEM scheme utilizes a data-driven, model-free reinforcement learning approach based on the multi-agent deep deterministic policy gradient (MADDPG) framework, enabling prosumers to make real-time decisions on whether to buy, sell, or refrain from any action while facilitating efficient coordination for optimal energy trading in a dynamic market. In addition, we investigate a price manipulation strategy using a variational auto encoder-generative adversarial network (VAE-GAN) model, which allows utilities to adjust price signals in a way that induces financial losses for the prosumers. Our results show that under adversarial pricing, heterogeneous prosumer groups, particularly those lacking generation capabilities, incur financial losses. The same outcome holds across LEMs of different sizes. As the market size increases, trading stabilizes and fairness improves through emergent cooperation among agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforced Policy Optimization</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
arXiv:2507.19849v1 Announce Type: cross 
Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection</title>
<link>https://arxiv.org/abs/2507.19856</link>
<guid>https://arxiv.org/abs/2507.19856</guid>
<content:encoded><![CDATA[
arXiv:2507.19856v1 Announce Type: cross 
Abstract: 4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data</title>
<link>https://arxiv.org/abs/2507.19880</link>
<guid>https://arxiv.org/abs/2507.19880</guid>
<content:encoded><![CDATA[
arXiv:2507.19880v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) represents a significant advancement in AI-tool integration, enabling seamless communication between AI agents and external services. However, this connectivity introduces novel attack vectors that remain largely unexplored. This paper demonstrates how unsophisticated threat actors, requiring only basic programming skills and free web tools, can exploit MCP's trust model to exfiltrate sensitive financial data. We present a proof-of-concept attack where a malicious weather MCP server, disguised as benign functionality, discovers and exploits legitimate banking tools to steal user account balances. The attack chain requires no advanced technical knowledge, server infrastructure, or monetary investment. The findings reveal a critical security gap in the emerging MCP ecosystem: while individual servers may appear trustworthy, their combination creates unexpected cross-server attack surfaces. Unlike traditional cybersecurity threats that assume sophisticated adversaries, our research shows that the barrier to entry for MCP-based attacks is alarmingly low. A threat actor with undergraduate-level Python knowledge can craft convincing social engineering attacks that exploit the implicit trust relationships MCP establishes between AI agents and tool providers. This work contributes to the nascent field of MCP security by demonstrating that current MCP implementations allow trivial cross-server attacks and proposing both immediate mitigations and protocol improvements to secure this emerging ecosystem.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2507.19881</link>
<guid>https://arxiv.org/abs/2507.19881</guid>
<content:encoded><![CDATA[
arXiv:2507.19881v1 Announce Type: cross 
Abstract: Federated domain generalization has shown promising progress in image classification by enabling collaborative training across multiple clients without sharing raw data. However, its potential in the semantic segmentation of autonomous driving remains underexplored. In this paper, we propose FedS2R, the first one-shot federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises two components: an inconsistency-driven data augmentation strategy that generates images for unstable classes, and a multi-client knowledge distillation scheme with feature fusion that distills a global model from multiple client models. Experiments on five real-world datasets, Cityscapes, BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly outperforms individual client models and is only 2 mIoU points behind the model trained with simultaneous access to all client data. These results demonstrate the effectiveness of FedS2R in synthetic-to-real semantic segmentation for autonomous driving under federated learning
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention</title>
<link>https://arxiv.org/abs/2507.19891</link>
<guid>https://arxiv.org/abs/2507.19891</guid>
<content:encoded><![CDATA[
arXiv:2507.19891v1 Announce Type: cross 
Abstract: We propose Reverse Contrast Attention (RCA), a plug-in method that enhances object localization in vision-language transformers without retraining. RCA reweights final-layer attention by suppressing extremes and amplifying mid-level activations to let semantically relevant but subdued tokens guide predictions. We evaluate it on Open Vocabulary Referring Object Detection (OV-RefOD), introducing FitAP, a confidence-free average precision metric based on IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with gains up to $+26.6\%$. Effectiveness aligns with attention sharpness and fusion timing; while late-fusion models benefit consistently, models like $\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement as key factors. RCA offers both interpretability and performance gains for multimodal transformers.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Insight: Visualizing Thompson Sampling for Verification and XAI</title>
<link>https://arxiv.org/abs/2507.19898</link>
<guid>https://arxiv.org/abs/2507.19898</guid>
<content:encoded><![CDATA[
arXiv:2507.19898v1 Announce Type: cross 
Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a ``black box'', hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation</title>
<link>https://arxiv.org/abs/2507.19902</link>
<guid>https://arxiv.org/abs/2507.19902</guid>
<content:encoded><![CDATA[
arXiv:2507.19902v1 Announce Type: cross 
Abstract: Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their communication, and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation</title>
<link>https://arxiv.org/abs/2507.19904</link>
<guid>https://arxiv.org/abs/2507.19904</guid>
<content:encoded><![CDATA[
arXiv:2507.19904v1 Announce Type: cross 
Abstract: As large language models (LLMs) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process communication (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate LLMs' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: https://anonymous.4open.science/r/crosspl-2814.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Fine-tuning Large Language Models on Automated Program Repair</title>
<link>https://arxiv.org/abs/2507.19909</link>
<guid>https://arxiv.org/abs/2507.19909</guid>
<content:encoded><![CDATA[
arXiv:2507.19909v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mini-batch training strategy for deep subspace clustering networks</title>
<link>https://arxiv.org/abs/2507.19917</link>
<guid>https://arxiv.org/abs/2507.19917</guid>
<content:encoded><![CDATA[
arXiv:2507.19917v1 Announce Type: cross 
Abstract: Mini-batch training is a cornerstone of modern deep learning, offering computational efficiency and scalability for training complex architectures. However, existing deep subspace clustering (DSC) methods, which typically combine an autoencoder with a self-expressive layer, rely on full-batch processing. The bottleneck arises from the self-expressive module, which requires representations of the entire dataset to construct a self-representation coefficient matrix. In this work, we introduce a mini-batch training strategy for DSC by integrating a memory bank that preserves global feature representations. Our approach enables scalable training of deep architectures for subspace clustering with high-resolution images, overcoming previous limitations. Additionally, to efficiently fine-tune large-scale pre-trained encoders for subspace clustering, we propose a decoder-free framework that leverages contrastive learning instead of autoencoding for representation learning. This design not only eliminates the computational overhead of decoder training but also provides competitive performance. Extensive experiments demonstrate that our approach not only achieves performance comparable to full-batch methods, but outperforms other state-of-the-art subspace clustering methods on the COIL100 and ORL datasets by fine-tuning deep networks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamiX: Large-Scale Dynamic Social Network Simulator</title>
<link>https://arxiv.org/abs/2507.19929</link>
<guid>https://arxiv.org/abs/2507.19929</guid>
<content:encoded><![CDATA[
arXiv:2507.19929v1 Announce Type: cross 
Abstract: Understanding the intrinsic mechanisms of social platforms is an urgent demand to maintain social stability. The rise of large language models provides significant potential for social network simulations to capture attitude dynamics and reproduce collective behaviors. However, existing studies mainly focus on scaling up agent populations, neglecting the dynamic evolution of social relationships. To address this gap, we introduce DynamiX, a novel large-scale social network simulator dedicated to dynamic social network modeling. DynamiX uses a dynamic hierarchy module for selecting core agents with key characteristics at each timestep, enabling accurate alignment of real-world adaptive switching of user roles. Furthermore, we design distinct dynamic social relationship modeling strategies for different user types. For opinion leaders, we propose an information-stream-based link prediction method recommending potential users with similar stances, simulating homogeneous connections, and autonomous behavior decisions. For ordinary users, we construct an inequality-oriented behavior decision-making module, effectively addressing unequal social interactions and capturing the patterns of relationship adjustments driven by multi-dimensional factors. Experimental results demonstrate that DynamiX exhibits marked improvements in attitude evolution simulation and collective behavior analysis compared to static networks. Besides, DynamiX opens a new theoretical perspective on follower growth prediction, providing empirical evidence for opinion leaders cultivation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems</title>
<link>https://arxiv.org/abs/2507.19936</link>
<guid>https://arxiv.org/abs/2507.19936</guid>
<content:encoded><![CDATA[
arXiv:2507.19936v1 Announce Type: cross 
Abstract: This paper investigates joint channel estimation and positioning in near-field sparse extra-large multiple-input multiple-output (XL-MIMO) orthogonal frequency division multiplexing (OFDM) systems. To achieve cooperative gains between channel estimation and positioning, we propose a deep learning-based two-stage framework comprising positioning and channel estimation. In the positioning stage, the user's coordinates are predicted and utilized in the channel estimation stage, thereby enhancing the accuracy of channel estimation. Within this framework, we propose a U-shaped Mamba architecture for channel estimation and positioning, termed as CP-Mamba. This network integrates the strengths of the Mamba model with the structural advantages of U-shaped convolutional networks, enabling effective capture of local spatial features and long-range temporal dependencies of the channel. Numerical simulation results demonstrate that the proposed two-stage approach with CP-Mamba architecture outperforms existing baseline methods. Moreover, sparse arrays (SA) exhibit significantly superior performance in both channel estimation and positioning accuracy compared to conventional compact arrays.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2507.19950</link>
<guid>https://arxiv.org/abs/2507.19950</guid>
<content:encoded><![CDATA[
arXiv:2507.19950v1 Announce Type: cross 
Abstract: Recent research leveraging large-scale pretrained diffusion models has demonstrated the potential of using diffusion features to establish semantic correspondences in images. Inspired by advancements in diffusion-based techniques, we propose a novel zero-shot method for refining point cloud registration algorithms. Our approach leverages correspondences derived from depth images to enhance point feature representations, eliminating the need for a dedicated training dataset. Specifically, we first project the point cloud into depth maps from multiple perspectives and extract implicit knowledge from a pretrained diffusion network as depth diffusion features. These features are then integrated with geometric features obtained from existing methods to establish more accurate correspondences between point clouds. By leveraging these refined correspondences, our approach achieves significantly improved registration accuracy. Extensive experiments demonstrate that our method not only enhances the performance of existing point cloud registration techniques but also exhibits robust generalization capabilities across diverse datasets. Codes are available at https://github.com/zhengcy-lambo/RARE.git.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Brain Responses To Natural Movies With Multimodal LLMs</title>
<link>https://arxiv.org/abs/2507.19956</link>
<guid>https://arxiv.org/abs/2507.19956</guid>
<content:encoded><![CDATA[
arXiv:2507.19956v1 Announce Type: cross 
Abstract: We present MedARC's team solution to the Algonauts 2025 challenge. Our pipeline leveraged rich multimodal representations from various state-of-the-art pretrained models across video (V-JEPA2), speech (Whisper), text (Llama 3.2), vision-text (InternVL3), and vision-text-audio (Qwen2.5-Omni). These features extracted from the models were linearly projected to a latent space, temporally aligned to the fMRI time series, and finally mapped to cortical parcels through a lightweight encoder comprising a shared group head plus subject-specific residual heads. We trained hundreds of model variants across hyperparameter settings, validated them on held-out movies and assembled ensembles targeted to each parcel in each subject. Our final submission achieved a mean Pearson's correlation of 0.2085 on the test split of withheld out-of-distribution movies, placing our team in fourth place for the competition. We further discuss a last-minute optimization that would have raised us to second place. Our results highlight how combining features from models trained in different modalities, using a simple architecture consisting of shared-subject and single-subject components, and conducting comprehensive model selection and ensembling improves generalization of encoding models to novel movie stimuli. All code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures</title>
<link>https://arxiv.org/abs/2507.19961</link>
<guid>https://arxiv.org/abs/2507.19961</guid>
<content:encoded><![CDATA[
arXiv:2507.19961v1 Announce Type: cross 
Abstract: The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases. However, many disease patterns are derived from outdated datasets and traditional stepwise algorithms with limited accuracy. This study presents a method for direct cardiovascular disease (CVD) diagnosis from ECG images, eliminating the need for digitization. The proposed approach utilizes a two-step curriculum learning framework, beginning with the pre-training of a classification model on segmentation masks, followed by fine-tuning on grayscale, inverted ECG images. Robustness is further enhanced through an ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming individual models. By effectively handling real-world artifacts and simplifying the diagnostic process, this method offers a reliable solution for automated CVD diagnosis, particularly in resource-limited settings where printed or scanned ECG images are commonly used. Such an automated procedure enables rapid and accurate diagnosis, which is critical for timely intervention in CVD cases that often demand urgent care.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle Points in Neural Network Training</title>
<link>https://arxiv.org/abs/2507.19968</link>
<guid>https://arxiv.org/abs/2507.19968</guid>
<content:encoded><![CDATA[
arXiv:2507.19968v1 Announce Type: cross 
Abstract: First-order optimization methods, such as SGD and Adam, are widely used for training large-scale deep neural networks due to their computational efficiency and robust performance. However, relying solely on gradient information, these methods often struggle to navigate complex loss landscapes with flat regions, plateaus, and saddle points. Second-order methods, which use curvature information from the Hessian matrix, can address these challenges but are computationally infeasible for large models. The Dimer method, a first-order technique that constructs two closely spaced points to probe the local geometry of a potential energy surface, efficiently estimates curvature using only gradient information. Inspired by its use in molecular dynamics simulations for locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel framework to escape saddle points in neural network training. DEO adapts the Dimer method to explore a broader region of the loss landscape, approximating the Hessian's smallest eigenvector without computing the full matrix. By periodically projecting the gradient onto the subspace orthogonal to the minimum curvature direction, DEO guides the optimizer away from saddle points and flat regions, enhancing training efficiency with non-stepwise updates. Preliminary experiments on a Transformer toy model show DEO achieves competitive performance compared to standard first-order methods, improving navigation of complex loss landscapes. Our work repurposes physics-inspired, first-order curvature estimation to enhance neural network training in high-dimensional spaces.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A roadmap for AI in robotics</title>
<link>https://arxiv.org/abs/2507.19975</link>
<guid>https://arxiv.org/abs/2507.19975</guid>
<content:encoded><![CDATA[
arXiv:2507.19975v1 Announce Type: cross 
Abstract: AI technologies, including deep learning, large-language models have gone from one breakthrough to the other. As a result, we are witnessing growing excitement in robotics at the prospect of leveraging the potential of AI to tackle some of the outstanding barriers to the full deployment of robots in our daily lives. However, action and sensing in the physical world pose greater and different challenges than analysing data in isolation. As the development and application of AI in robotic products advances, it is important to reflect on which technologies, among the vast array of network architectures and learning models now available in the AI field, are most likely to be successfully applied to robots; how they can be adapted to specific robot designs, tasks, environments; which challenges must be overcome. This article offers an assessment of what AI for robotics has achieved since the 1990s and proposes a short- and medium-term research roadmap listing challenges and promises. These range from keeping up-to-date large datasets, representatives of a diversity of tasks robots may have to perform, and of environments they may encounter, to designing AI algorithms tailored specifically to robotics problems but generic enough to apply to a wide range of applications and transfer easily to a variety of robotic platforms. For robots to collaborate effectively with humans, they must predict human behavior without relying on bias-based profiling. Explainability and transparency in AI-driven robot control are not optional but essential for building trust, preventing misuse, and attributing responsibility in accidents. We close on what we view as the primary long-term challenges, that is, to design robots capable of lifelong learning, while guaranteeing safe deployment and usage, and sustainable computational costs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints</title>
<link>https://arxiv.org/abs/2507.19983</link>
<guid>https://arxiv.org/abs/2507.19983</guid>
<content:encoded><![CDATA[
arXiv:2507.19983v1 Announce Type: cross 
Abstract: Clothes manipulation, such as folding or hanging, is a critical capability for home service robots. Despite recent advances, most existing methods remain limited to specific tasks and clothes types, due to the complex, high-dimensional geometry of clothes. This paper presents CLothes mAnipulation with Semantic keyPoints (CLASP), which aims at general-purpose clothes manipulation over different clothes types, T-shirts, shorts, skirts, long dresses, ... , as well as different tasks, folding, flattening, hanging, ... . The core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right shoulder'', etc. -- a sparse spatial-semantic representation that is salient for both perception and action. Semantic keypoints of clothes can be reliably extracted from RGB-D images and provide an effective intermediate representation of clothes manipulation policies. CLASP uses semantic keypoints to bridge high-level task planning and low-level action execution. At the high level, it exploits vision language models (VLMs) to predict task plans over the semantic keypoints. At the low level, it executes the plans with the help of a simple pre-built manipulation skill library. Extensive simulation experiments show that CLASP outperforms state-of-the-art baseline methods on multiple tasks across diverse clothes types, demonstrating strong performance and generalization. Further experiments with a Franka dual-arm system on four distinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's performance on a real robot.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model</title>
<link>https://arxiv.org/abs/2507.19990</link>
<guid>https://arxiv.org/abs/2507.19990</guid>
<content:encoded><![CDATA[
arXiv:2507.19990v1 Announce Type: cross 
Abstract: Recently, competition in the field of artificial intelligence (AI) has intensified among major technological companies, resulting in the continuous release of new large-language models (LLMs) that exhibit improved language understanding and context-based reasoning capabilities. It is expected that these advances will enable more efficient personalized recommendations in LLM-based recommendation systems through improved quality of training data and architectural design. However, many studies have not considered these recent developments. In this study, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input data was provided during preprocessing and training. The experimental results show average performance improvements of 38.65\%, 8.69\%, and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively, thus confirming the practicality of this method. Notably, the significant improvements achieved by model replacement indicate that the recommendation quality can be improved cost-effectively without the need to make structural changes to the system. Based on these results, it is our contention that the proposed approach is a viable solution for improving the performance of current recommendation systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care</title>
<link>https://arxiv.org/abs/2507.19992</link>
<guid>https://arxiv.org/abs/2507.19992</guid>
<content:encoded><![CDATA[
arXiv:2507.19992v1 Announce Type: cross 
Abstract: Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology to support knowledge representation in acute care settings.
  Materials and Methods: We developed the NIRS ontology using Web Ontology Language (OWL) semantics and Protege to organize clinical concepts and relationships. To enable rule-based clinical reasoning beyond hierarchical structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated logical reasoning by adding 17 hypothetical patient clinical scenarios. We used SPARQL queries and data from the Electronic Intensive Care Unit (eICU) Collaborative Research Database to retrieve and test targeted inferences.
  Results: The ontology has 132 classes, 12 object properties, and 17 data properties across 882 axioms that establish concept relationships. To standardize clinical concepts, we added 350 annotations, including descriptive definitions based on controlled vocabularies. SPARQL queries successfully validated all test cases (rules) by retrieving appropriate patient outcomes, for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours due to acute respiratory failure may avoid endotracheal intubation.
  Discussion: The NIRS ontology formally represents domain-specific concepts, including ventilation modalities, patient characteristics, therapy parameters, and outcomes. SPARQL query evaluations on clinical scenarios confirmed the ability of the ontology to support rule based reasoning and therapy recommendations, providing a foundation for consistent documentation practices, integration into clinical data models, and advanced analysis of NIRS outcomes.
  Conclusion: We unified NIRS concepts into an ontological framework and demonstrated its applicability through the evaluation of hypothetical patient scenarios and alignment with standardized vocabularies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering</title>
<link>https://arxiv.org/abs/2507.19995</link>
<guid>https://arxiv.org/abs/2507.19995</guid>
<content:encoded><![CDATA[
arXiv:2507.19995v1 Announce Type: cross 
Abstract: The advent of large language models (LLMs) has led to significant achievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a natural evolution and an increasingly compelling choice. However, their capabilities are often portrayed as greater than they truly are. Despite the progress, we are still far from the ultimate goal of fully automating legal tasks using artificial intelligence (AI) and natural language processing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation across different countries and languages. The need for building legal text processing applications for different natural languages is, therefore, large and urgent. However, there is a big challenge for legal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated data. The need for labeled legal corpora for supervised training, validation, and supervised fine-tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality resource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical analysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art models on legal information retrieval and question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost</title>
<link>https://arxiv.org/abs/2507.20008</link>
<guid>https://arxiv.org/abs/2507.20008</guid>
<content:encoded><![CDATA[
arXiv:2507.20008v1 Announce Type: cross 
Abstract: Precise fare prediction is crucial in ride-hailing platforms and urban mobility systems. This study examines three machine learning models-Graph Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive capabilities for taxi fares using a real-world dataset comprising over 55 million records. Both raw (noisy) and denoised versions of the dataset are analyzed to assess the impact of data quality on model performance. The study evaluated the models along multiple axes, including predictive accuracy, calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and feature sensitivity. We also explore pre-processing strategies, including KNN imputation, Gaussian noise injection, and autoencoder-based denoising. The study reveals critical differences between classical and deep learning models under realistic conditions, offering practical guidelines for building robust and scalable models in urban fare prediction systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy-Driven AI in Dataspaces: Taxonomy, Explainability, and Pathways for Compliant Innovation</title>
<link>https://arxiv.org/abs/2507.20014</link>
<guid>https://arxiv.org/abs/2507.20014</guid>
<content:encoded><![CDATA[
arXiv:2507.20014v1 Announce Type: cross 
Abstract: As AI-driven dataspaces become integral to data sharing and collaborative analytics, ensuring privacy, performance, and policy compliance presents significant challenges. This paper provides a comprehensive review of privacy-preserving and policy-aware AI techniques, including Federated Learning, Differential Privacy, Trusted Execution Environments, Homomorphic Encryption, and Secure Multi-Party Computation, alongside strategies for aligning AI with regulatory frameworks such as GDPR and the EU AI Act. We propose a novel taxonomy to classify these techniques based on privacy levels, performance impacts, and compliance complexity, offering a clear framework for practitioners and researchers to navigate trade-offs. Key performance metrics -- latency, throughput, cost overhead, model utility, fairness, and explainability -- are analyzed to highlight the multi-dimensional optimization required in dataspaces. The paper identifies critical research gaps, including the lack of standardized privacy-performance KPIs, challenges in explainable AI for federated ecosystems, and semantic policy enforcement amidst regulatory fragmentation. Future directions are outlined, proposing a conceptual framework for policy-driven alignment, automated compliance validation, standardized benchmarking, and integration with European initiatives like GAIA-X, IDS, and Eclipse EDC. By synthesizing technical, ethical, and regulatory perspectives, this work lays the groundwork for developing trustworthy, efficient, and compliant AI systems in dataspaces, fostering innovation in secure and responsible data-driven ecosystems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging</title>
<link>https://arxiv.org/abs/2507.20016</link>
<guid>https://arxiv.org/abs/2507.20016</guid>
<content:encoded><![CDATA[
arXiv:2507.20016v1 Announce Type: cross 
Abstract: For federated learning (FL) algorithms such as FedSAM, their generalization capability is crucial for real-word applications. In this paper, we revisit the generalization problem in FL and investigate the impact of data heterogeneity on FL generalization. We find that FedSAM usually performs worse than FedAvg in the case of highly heterogeneous data, and thus propose a novel and effective federated learning algorithm with Stochastic Weight Averaging (called \texttt{FedSWA}), which aims to find flatter minima in the setting of highly heterogeneous data. Moreover, we introduce a new momentum-based stochastic controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization and generalization errors of \texttt{FedMoSWA} are smaller than those of their counterparts, including FedSAM and its variants. Empirically, experimental results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the proposed algorithms compared to their counterparts. Open source code at: https://github.com/junkangLiu0/FedSWA.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Carbon Cost of Conversation, Sustainability in the Age of Language Models</title>
<link>https://arxiv.org/abs/2507.20018</link>
<guid>https://arxiv.org/abs/2507.20018</guid>
<content:encoded><![CDATA[
arXiv:2507.20018v1 Announce Type: cross 
Abstract: Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach</title>
<link>https://arxiv.org/abs/2507.20019</link>
<guid>https://arxiv.org/abs/2507.20019</guid>
<content:encoded><![CDATA[
arXiv:2507.20019v1 Announce Type: cross 
Abstract: We propose a meta learning framework for detecting anomalies in human language across diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection as a few shot binary classification problem and leverage meta-learning to train models that generalize across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly detection tasks. Empirical results show that our method outperforms strong baselines in F1 and AUC scores. We also release the code and benchmarks to facilitate further research in few-shot text anomaly detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation</title>
<link>https://arxiv.org/abs/2507.20021</link>
<guid>https://arxiv.org/abs/2507.20021</guid>
<content:encoded><![CDATA[
arXiv:2507.20021v1 Announce Type: cross 
Abstract: Large language models (LLMs) are often credited with recent leaps in ObjectGoal Navigation, yet the extent to which they improve planning remains unclear. We revisit this question on the HM3D-v1 validation split. First, we strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary GLEE detector and Intuition saliency map, and replace them with a simple Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000 validation episodes, outperforming all previous training-free baselines. Second, we add a lightweight language prior (SHF); on a 200-episode subset this yields a further +2% Success and +0.9% SPL while shortening paths by five steps on average. Qualitative trajectories confirm the trend: InstructNav back-tracks and times-out, DWFE reaches the goal after a few islands, and SHF follows an almost straight route. Our results indicate that frontier geometry, not emergent LLM reasoning, drives most reported gains, and suggest that metric-aware prompts or offline semantic graphs are necessary before attributing navigation success to "LLM intelligence."
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPS : Frustratingly Simple Test Time Active Learning for VLMs</title>
<link>https://arxiv.org/abs/2507.20028</link>
<guid>https://arxiv.org/abs/2507.20028</guid>
<content:encoded><![CDATA[
arXiv:2507.20028v1 Announce Type: cross 
Abstract: Test-Time Optimization enables models to adapt to new data during inference by updating parameters on-the-fly. Recent advances in Vision-Language Models (VLMs) have explored learning prompts at test time to improve performance in downstream tasks. In this work, we extend this idea by addressing a more general and practical challenge: Can we effectively utilize an oracle in a continuous data stream where only one sample is available at a time, requiring an immediate query decision while respecting latency and memory constraints? To tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that adaptively queries uncertain samples and updates prompts dynamically. Unlike prior methods that assume batched data or multiple gradient updates, our approach operates in a real-time streaming scenario with a single test sample per step. We introduce a dynamically adjusted entropy threshold for active querying, a class-balanced replacement strategy for memory efficiency, and a class-aware distribution alignment technique to enhance adaptation. The design choices are justified using careful theoretical analysis. Extensive experiments across 10 cross-dataset transfer benchmarks and 4 domain generalization datasets demonstrate consistent improvements over state-of-the-art methods while maintaining reasonable latency and memory overhead. Our framework provides a practical and effective solution for real-world deployment in safety-critical applications such as autonomous systems and medical diagnostics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irredundant k-Fold Cross-Validation</title>
<link>https://arxiv.org/abs/2507.20048</link>
<guid>https://arxiv.org/abs/2507.20048</guid>
<content:encoded><![CDATA[
arXiv:2507.20048v1 Announce Type: cross 
Abstract: In traditional k-fold cross-validation, each instance is used ($k\!-\!1$) times for training and once for testing, leading to redundancy that lets many instances disproportionately influence the learning phase. We introduce Irredundant $k$--fold cross-validation, a novel method that guarantees each instance is used exactly once for training and once for testing across the entire validation procedure. This approach ensures a more balanced utilization of the dataset, mitigates overfitting due to instance repetition, and enables sharper distinctions in comparative model analysis. The method preserves stratification and remains model-agnostic, i.e., compatible with any classifier. Experimental results demonstrate that it delivers consistent performance estimates across diverse datasets --comparable to $k$--fold cross-validation-- while providing less optimistic variance estimates because training partitions are non-overlapping, and significantly reducing the overall computational cost.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaRMamba: Frequency-based learning and Reconstruction aided Mamba for Medical Segmentation</title>
<link>https://arxiv.org/abs/2507.20056</link>
<guid>https://arxiv.org/abs/2507.20056</guid>
<content:encoded><![CDATA[
arXiv:2507.20056v1 Announce Type: cross 
Abstract: Accurate medical image segmentation remains challenging due to blurred lesion boundaries (LBA), loss of high-frequency details (LHD), and difficulty in modeling long-range anatomical structures (DC-LRSS). Vision Mamba employs one-dimensional causal state-space recurrence to efficiently model global dependencies, thereby substantially mitigating DC-LRSS. However, its patch tokenization and 1D serialization disrupt local pixel adjacency and impose a low-pass filtering effect, resulting in Local High-frequency Information Capture Deficiency (LHICD) and two-dimensional Spatial Structure Degradation (2D-SSD), which in turn exacerbate LBA and LHD. In this work, we propose FaRMamba, a novel extension that explicitly addresses LHICD and 2D-SSD through two complementary modules. A Multi-Scale Frequency Transform Module (MSFM) restores attenuated high-frequency cues by isolating and reconstructing multi-band spectra via wavelet, cosine, and Fourier transforms. A Self-Supervised Reconstruction Auxiliary Encoder (SSRAE) enforces pixel-level reconstruction on the shared Mamba encoder to recover full 2D spatial correlations, enhancing both fine textures and global context. Extensive evaluations on CAMUS echocardiography, MRI-based Mouse-cochlea, and Kvasir-Seg endoscopy demonstrate that FaRMamba consistently outperforms competitive CNN-Transformer hybrids and existing Mamba variants, delivering superior boundary accuracy, detail preservation, and global coherence without prohibitive computational overhead. This work provides a flexible frequency-aware framework for future segmentation models that directly mitigates core challenges in medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2507.20059</link>
<guid>https://arxiv.org/abs/2507.20059</guid>
<content:encoded><![CDATA[
arXiv:2507.20059v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models</title>
<link>https://arxiv.org/abs/2507.20094</link>
<guid>https://arxiv.org/abs/2507.20094</guid>
<content:encoded><![CDATA[
arXiv:2507.20094v1 Announce Type: cross 
Abstract: Diffusion models have become a powerful backbone for text-to-image generation, enabling users to synthesize high-quality visuals from natural language prompts. However, they often struggle with complex prompts involving multiple objects and global or local style specifications. In such cases, the generated scenes tend to lack style uniformity and spatial coherence, limiting their utility in creative and controllable content generation. In this paper, we propose a simple, training-free architectural method called Local Prompt Adaptation (LPA). Our method decomposes the prompt into content and style tokens, and injects them selectively into the U-Net's attention layers at different stages. By conditioning object tokens early and style tokens later in the generation process, LPA enhances both layout control and stylistic consistency. We evaluate our method on a custom benchmark of 50 style-rich prompts across five categories and compare against strong baselines including Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach outperforms prior work on both CLIP score and style consistency metrics, offering a new direction for controllable, expressive diffusion-based generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoTransformer: Attention without Multiplication</title>
<link>https://arxiv.org/abs/2507.20096</link>
<guid>https://arxiv.org/abs/2507.20096</guid>
<content:encoded><![CDATA[
arXiv:2507.20096v1 Announce Type: cross 
Abstract: The Transformer, with its scaled dot-product attention mechanism, has become a foundational architecture in modern AI. However, this mechanism is computationally intensive and incurs substantial energy costs. We propose a new Transformer architecture EcoTransformer, in which the output context vector is constructed as the convolution of the values using a Laplacian kernel, where the distances are measured by the L1 metric between the queries and keys. Compared to dot-product based attention, the new attention score calculation is free of matrix multiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP, bioinformatics, and vision tasks, while consuming significantly less energy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Align Human Code Preferences</title>
<link>https://arxiv.org/abs/2507.20109</link>
<guid>https://arxiv.org/abs/2507.20109</guid>
<content:encoded><![CDATA[
arXiv:2507.20109v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in automating software development tasks. While recent advances leverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align models with human preferences, the optimal training strategy remains unclear across diverse code preference scenarios. This paper systematically investigates the roles of SFT and DPO in aligning LLMs with different code preferences. Through both theoretical analysis and empirical observation, we hypothesize that SFT excels in scenarios with objectively verifiable optimal solutions, while applying SFT followed by DPO (S&amp;D) enables models to explore superior solutions in scenarios without objectively verifiable optimal solutions. Based on the analysis and experimental evidence, we propose Adaptive Preference Optimization (APO), a dynamic integration approach that adaptively amplifies preferred responses, suppresses dispreferred ones, and encourages exploration of potentially superior solutions during training. Extensive experiments across six representative code preference tasks validate our theoretical hypotheses and demonstrate that APO consistently matches or surpasses the performance of existing SFT and S&amp;D strategies. Our work provides both theoretical foundations and practical guidance for selecting appropriate training strategies in different code preference alignment scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding</title>
<link>https://arxiv.org/abs/2507.20110</link>
<guid>https://arxiv.org/abs/2507.20110</guid>
<content:encoded><![CDATA[
arXiv:2507.20110v1 Announce Type: cross 
Abstract: Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Generation of Old English: A Framework for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2507.20111</link>
<guid>https://arxiv.org/abs/2507.20111</guid>
<content:encoded><![CDATA[
arXiv:2507.20111v1 Announce Type: cross 
Abstract: Preserving ancient languages is essential for understanding humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced, limiting its accessibility to modern natural language processing (NLP) techniques. We present a scalable framework that uses advanced large language models (LLMs) to generate high-quality Old English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English). Evaluation with automated metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the Old English corpus, our method offers a practical blueprint for revitalizing other endangered languages, effectively uniting AI innovation with the goals of cultural preservation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning with Probing for Sequential User-Centric Selection</title>
<link>https://arxiv.org/abs/2507.20112</link>
<guid>https://arxiv.org/abs/2507.20112</guid>
<content:encoded><![CDATA[
arXiv:2507.20112v1 Announce Type: cross 
Abstract: We formalize sequential decision-making with information acquisition as the probing-augmented user-centric selection (PUCS) framework, where a learner first probes a subset of arms to obtain side information on resources and rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such as ridesharing, wireless scheduling, and content recommendation, in which both resources and payoffs are initially unknown and probing is costly. For the offline setting with known distributions, we present a greedy probing algorithm with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the online setting with unknown distributions, we introduce OLPA, a stochastic combinatorial bandit algorithm that achieves a regret bound $\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound $\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic factors. Experiments on real-world data demonstrate the effectiveness of our solutions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion</title>
<link>https://arxiv.org/abs/2507.20115</link>
<guid>https://arxiv.org/abs/2507.20115</guid>
<content:encoded><![CDATA[
arXiv:2507.20115v1 Announce Type: cross 
Abstract: In response to Distributed Denial of Service (DDoS) attacks, recent research efforts increasingly rely on Machine Learning (ML)-based solutions, whose effectiveness largely depends on the quality of labeled training datasets. To address the scarcity of such datasets, data augmentation with synthetic traces is often employed. However, current synthetic trace generation methods struggle to capture the complex temporal patterns and spatial distributions exhibited in emerging DDoS attacks. This results in insufficient resemblance to real traces and unsatisfied detection accuracy when applied to ML tasks. In this paper, we propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view, multi-stream network traffic generative model based on diffusion models, featuring two main streams: The field stream utilizes spatial mapping to bridge network data characteristics with pre-trained realms of stable diffusion models, effectively translating complex network interactions into formats that stable diffusion can process, while the spatial stream adopts a dynamic temporal modeling approach, meticulously capturing the intrinsic temporal patterns of network traffic. Extensive experiments demonstrate that data generated by our model exhibits higher statistical similarity to originals compared to current state-of-the-art solutions, and enhance performances on a wide range of downstream tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Pretraining Framework for Interatomic Potentials</title>
<link>https://arxiv.org/abs/2507.20118</link>
<guid>https://arxiv.org/abs/2507.20118</guid>
<content:encoded><![CDATA[
arXiv:2507.20118v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) enable efficient molecular dynamics (MD) simulations with ab initio accuracy and have been applied across various domains in physical science. However, their performance often relies on large-scale labeled training data. While existing pretraining strategies can improve model performance, they often suffer from a mismatch between the objectives of pretraining and downstream tasks or rely on extensive labeled datasets and increasingly complex architectures to achieve broad generalization. To address these challenges, we propose Iterative Pretraining for Interatomic Potentials (IPIP), a framework designed to iteratively improve the predictive performance of MLIP models. IPIP incorporates a forgetting mechanism to prevent iterative training from converging to suboptimal local minima. Unlike general-purpose foundation models, which frequently underperform on specialized tasks due to a trade-off between generality and system-specific accuracy, IPIP achieves higher accuracy and efficiency using lightweight architectures. Compared to general-purpose force fields, this approach achieves over 80% reduction in prediction error and up to 4x speedup in the challenging Mo-S-O system, enabling fast and accurate simulations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing</title>
<link>https://arxiv.org/abs/2507.20127</link>
<guid>https://arxiv.org/abs/2507.20127</guid>
<content:encoded><![CDATA[
arXiv:2507.20127v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning graph representations, primarily because of their message-passing mechanisms. However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or Sum without principled reasoning behind the selection. This rigidity, especially in the presence of heterophily, often leads to poor, problem dependent performance. Although some attempts address this by designing more sophisticated aggregation functions, these methods tend to rely heavily on labeled data, which is often scarce in real-world tasks. In this work, we propose a novel unsupervised framework, "Aggregation-aware Multilayer Perceptron" (AMLP), which shifts the paradigm from directly crafting aggregation functions to making MLP adaptive to aggregation. Our lightweight approach consists of two key steps: First, we utilize a graph reconstruction method that facilitates high-order grouping effects, and second, we employ a single-layer network to encode varying degrees of heterophily, thereby improving the capacity and applicability of the model. Extensive experiments on node clustering and classification demonstrate the superior performance of AMLP, highlighting its potential for diverse graph learning scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering</title>
<link>https://arxiv.org/abs/2507.20133</link>
<guid>https://arxiv.org/abs/2507.20133</guid>
<content:encoded><![CDATA[
arXiv:2507.20133v1 Announce Type: cross 
Abstract: Generative AI can now synthesize strikingly realistic images from text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts that win higher preference scores can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine distance between the original prompt and winning candidate in embedding space, softly down-weighting training signals that would otherwise reward semantically mismatched prompts. We provide the first analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps learned prompts within a provably bounded neighborhood of the original text. On three standard text-to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented with semantic weighting should become the new standard for prompt-optimization studies and lay the groundwork for broader, semantics-aware preference optimization in language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG</title>
<link>https://arxiv.org/abs/2507.20136</link>
<guid>https://arxiv.org/abs/2507.20136</guid>
<content:encoded><![CDATA[
arXiv:2507.20136v1 Announce Type: cross 
Abstract: This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech</title>
<link>https://arxiv.org/abs/2507.20140</link>
<guid>https://arxiv.org/abs/2507.20140</guid>
<content:encoded><![CDATA[
arXiv:2507.20140v1 Announce Type: cross 
Abstract: The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has enabled high-fidelity voice synthesis from minimal audio cues, raising significant privacy and ethical concerns. Despite the threats to voice privacy, research to selectively remove the knowledge to replicate unwanted individual voices from pre-trained model parameters has not been explored. In this paper, we address the new challenge of speaker identity unlearning for ZS-TTS systems. To meet this goal, we propose the first machine unlearning frameworks for ZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the model forgets designated speaker identities while retaining its ability to generate accurate speech for other speakers. Our proposed methods incorporate randomness to prevent consistent replication of forget speakers' voices, assuring unlearned identities remain untraceable. Additionally, we propose a new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses the model's ability to disregard prompts associated with forgotten speakers, effectively neutralizing its knowledge of these voices. The experiments conducted on the state-of-the-art model demonstrate that TGU prevents the model from replicating forget speakers' voices while maintaining high quality for other speakers. The demo is available at https://speechunlearn.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Awesome-OL: An Extensible Toolkit for Online Learning</title>
<link>https://arxiv.org/abs/2507.20144</link>
<guid>https://arxiv.org/abs/2507.20144</guid>
<content:encoded><![CDATA[
arXiv:2507.20144v1 Announce Type: cross 
Abstract: In recent years, online learning has attracted increasing attention due to its adaptive capability to process streaming and non-stationary data. To facilitate algorithm development and practical deployment in this area, we introduce Awesome-OL, an extensible Python toolkit tailored for online learning research. Awesome-OL integrates state-of-the-art algorithm, which provides a unified framework for reproducible comparisons, curated benchmark datasets, and multi-modal visualization. Built upon the scikit-multiflow open-source infrastructure, Awesome-OL emphasizes user-friendly interactions without compromising research flexibility or extensibility. The source code is publicly available at: https://github.com/liuzy0708/Awesome-OL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Interactive Question Generation Framework for Long Document Understanding</title>
<link>https://arxiv.org/abs/2507.20145</link>
<guid>https://arxiv.org/abs/2507.20145</guid>
<content:encoded><![CDATA[
arXiv:2507.20145v1 Announce Type: cross 
Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal Alignment in LLM-Based User Simulators for Conversational AI</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
arXiv:2507.20152v1 Announce Type: cross 
Abstract: User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality</title>
<link>https://arxiv.org/abs/2507.20156</link>
<guid>https://arxiv.org/abs/2507.20156</guid>
<content:encoded><![CDATA[
arXiv:2507.20156v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) extend the conventional large language models by integrating visual data, enabling richer multimodal reasoning and significantly broadens the practical applications of AI. However, including visual inputs also brings new challenges in maintaining data quality. Empirical evidence consistently shows that carefully curated and representative training examples often yield superior results compared to simply increasing the quantity of data. Inspired by this observation, we introduce a streamlined data filtration framework that employs a compact VLM, fine-tuned on a high-quality image-caption annotated dataset. This model effectively evaluates and filters potential training samples based on caption and image quality and alignment. Unlike previous approaches, which typically add auxiliary filtration modules on top of existing full-scale VLMs, our method exclusively utilizes the inherent evaluative capability of a purpose-built small VLM. This strategy eliminates the need for extra modules and reduces training overhead. Our lightweight model efficiently filters out inaccurate, noisy web data, improving image-text alignment and caption linguistic fluency. Experimental results show that datasets underwent high-precision filtration using our compact VLM perform on par with, or even surpass, larger and noisier datasets gathered through high-volume web crawling. Thus, our method provides a lightweight yet robust solution for building high-quality vision-language training corpora. \\ \textbf{Availability and implementation:} Our compact VLM filtration model, training data, utility scripts, and Supplementary data (Appendices) are freely available at https://github.com/daulettoibazar/Compact_VLM_Filter.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASNN: Learning to Suggest Neural Architectures from Performance Distributions</title>
<link>https://arxiv.org/abs/2507.20164</link>
<guid>https://arxiv.org/abs/2507.20164</guid>
<content:encoded><![CDATA[
arXiv:2507.20164v1 Announce Type: cross 
Abstract: The architecture of a neural network (NN) plays a critical role in determining its performance. However, there is no general closed-form function that maps between network structure and accuracy, making the process of architecture design largely heuristic or search-based. In this study, we propose the Architecture Suggesting Neural Network (ASNN), a model designed to learn the relationship between NN architecture and its test accuracy, and to suggest improved architectures accordingly. To train ASNN, we constructed datasets using TensorFlow-based models with varying numbers of layers and nodes. Experimental results were collected for both 2-layer and 3-layer architectures across a grid of configurations, each evaluated with 10 repeated trials to account for stochasticity. Accuracy values were treated as inputs, and architectural parameters as outputs. The trained ASNN was then used iteratively to predict architectures that yield higher performance. In both 2-layer and 3-layer cases, ASNN successfully suggested architectures that outperformed the best results found in the original training data. Repeated prediction and retraining cycles led to the discovery of architectures with improved mean test accuracies, demonstrating the model's capacity to generalize the performance-structure relationship. These results suggest that ASNN provides an efficient alternative to random search for architecture optimization, and offers a promising approach toward automating neural network design. "Parts of the manuscript, including text editing and expression refinement, were supported by OpenAI's ChatGPT. All content was reviewed and verified by the authors."
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP</title>
<link>https://arxiv.org/abs/2507.20173</link>
<guid>https://arxiv.org/abs/2507.20173</guid>
<content:encoded><![CDATA[
arXiv:2507.20173v1 Announce Type: cross 
Abstract: This paper presents an in-depth investigation into the high-performance parallel optimization of the Fish School Behaviour (FSB) algorithm on the Setonix supercomputing platform using the OpenMP framework. Given the increasing demand for enhanced computational capabilities for complex, large-scale calculations across diverse domains, there's an imperative need for optimized parallel algorithms and computing structures. The FSB algorithm, inspired by nature's social behavior patterns, provides an ideal platform for parallelization due to its iterative and computationally intensive nature. This study leverages the capabilities of the Setonix platform and the OpenMP framework to analyze various aspects of multi-threading, such as thread counts, scheduling strategies, and OpenMP constructs, aiming to discern patterns and strategies that can elevate program performance. Experiments were designed to rigorously test different configurations, and our results not only offer insights for parallel optimization of FSB on Setonix but also provide valuable references for other parallel computational research using OpenMP. Looking forward, other factors, such as cache behavior and thread scheduling strategies at micro and macro levels, hold potential for further exploration and optimization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks</title>
<link>https://arxiv.org/abs/2507.20174</link>
<guid>https://arxiv.org/abs/2507.20174</guid>
<content:encoded><![CDATA[
arXiv:2507.20174v1 Announce Type: cross 
Abstract: Real-world applications, such as autonomous driving and humanoid robot manipulation, require precise spatial perception. However, it remains underexplored how Vision-Language Models (VLMs) recognize spatial relationships and perceive spatial movement. In this work, we introduce a spatial evaluation pipeline and construct a corresponding benchmark. Specifically, we categorize spatial understanding into two main types: absolute spatial understanding, which involves querying the absolute spatial position (e.g., left, right) of an object within an image, and 3D spatial understanding, which includes movement and rotation. Notably, our dataset is entirely synthetic, enabling the generation of test samples at a low cost while also preventing dataset contamination. We conduct experiments on multiple state-of-the-art VLMs and observe that there is significant room for improvement in their spatial understanding abilities. Explicitly, in our experiments, humans achieve near-perfect performance on all tasks, whereas current VLMs attain human-level performance only on the two simplest tasks. For the remaining tasks, the performance of VLMs is distinctly lower than that of humans. In fact, the best-performing Vision-Language Models even achieve near-zero scores on multiple tasks. The dataset and code are available on https://github.com/kong13661/LRR-Bench.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGPO: Self-Generated Preference Optimization based on Self-Improver</title>
<link>https://arxiv.org/abs/2507.20181</link>
<guid>https://arxiv.org/abs/2507.20181</guid>
<content:encoded><![CDATA[
arXiv:2507.20181v1 Announce Type: cross 
Abstract: Large language models (LLMs), despite their extensive pretraining on diverse datasets, require effective alignment to human preferences for practical and reliable deployment. Conventional alignment methods typically employ off-policy learning and depend on human-annotated datasets, which limits their broad applicability and introduces distribution shift issues during training. To address these challenges, we propose Self-Generated Preference Optimization based on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving mechanism. Specifically, the improver refines responses from a policy model to self-generate preference data for direct preference optimization (DPO) of the policy model. Here, the improver and policy are unified into a single model, and in order to generate higher-quality preference data, this self-improver learns to make incremental yet discernible improvements to the current responses by referencing supervised fine-tuning outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods without using external preference data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis</title>
<link>https://arxiv.org/abs/2507.20189</link>
<guid>https://arxiv.org/abs/2507.20189</guid>
<content:encoded><![CDATA[
arXiv:2507.20189v1 Announce Type: cross 
Abstract: Methamphetamine dependence poses a significant global health challenge, yet its assessment and the evaluation of treatments like repetitive transcranial magnetic stimulation (rTMS) frequently depend on subjective self-reports, which may introduce uncertainties. While objective neuroimaging modalities such as electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) offer alternatives, their individual limitations and the reliance on conventional, often hand-crafted, feature extraction can compromise the reliability of derived biomarkers. To overcome these limitations, we propose NeuroCLIP, a novel deep learning framework integrating simultaneously recorded EEG and fNIRS data through a progressive learning strategy. This approach offers a robust and trustworthy biomarker for methamphetamine addiction. Validation experiments show that NeuroCLIP significantly improves discriminative capabilities among the methamphetamine-dependent individuals and healthy controls compared to models using either EEG or only fNIRS alone. Furthermore, the proposed framework facilitates objective, brain-based evaluation of rTMS treatment efficacy, demonstrating measurable shifts in neural patterns towards healthy control profiles after treatment. Critically, we establish the trustworthiness of the multimodal data-driven biomarker by showing its strong correlation with psychometrically validated craving scores. These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP offers enhanced robustness and reliability over single-modality approaches, providing a valuable tool for addiction neuroscience research and potentially improving clinical assessments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Domain Adaptation via Importance Sampling-based Shift Correction</title>
<link>https://arxiv.org/abs/2507.20191</link>
<guid>https://arxiv.org/abs/2507.20191</guid>
<content:encoded><![CDATA[
arXiv:2507.20191v1 Announce Type: cross 
Abstract: Partial domain adaptation (PDA) is a challenging task in real-world machine learning scenarios. It aims to transfer knowledge from a labeled source domain to a related unlabeled target domain, where the support set of the source label distribution subsumes the target one. Previous PDA works managed to correct the label distribution shift by weighting samples in the source domain. However, the simple reweighing technique cannot explore the latent structure and sufficiently use the labeled data, and then models are prone to over-fitting on the source domain. In this work, we propose a novel importance sampling-based shift correction (IS$^2$C) method, where new labeled data are sampled from a built sampling domain, whose label distribution is supposed to be the same as the target domain, to characterize the latent structure and enhance the generalization ability of the model. We provide theoretical guarantees for IS$^2$C by proving that the generalization error can be sufficiently dominated by IS$^2$C. In particular, by implementing sampling with the mixture distribution, the extent of shift between source and sampling domains can be connected to generalization error, which provides an interpretable way to build IS$^2$C. To improve knowledge transfer, an optimal transport-based independence criterion is proposed for conditional distribution alignment, where the computation of the criterion can be adjusted to reduce the complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive experiments on PDA benchmarks validate the theoretical results and demonstrate the effectiveness of our IS$^2$C over existing methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets</title>
<link>https://arxiv.org/abs/2507.20197</link>
<guid>https://arxiv.org/abs/2507.20197</guid>
<content:encoded><![CDATA[
arXiv:2507.20197v1 Announce Type: cross 
Abstract: The goal of this investigation is to quantify to what extent computer vision methods can correctly classify facial expressions on a sign language dataset. We extend our experiments by recognizing expressions using only the upper or lower part of the face, which is needed to further investigate the difference in emotion manifestation between hearing and deaf subjects. To take into account the peculiar color profile of a dataset, our method introduces a color normalization stage based on histogram equalization and fine-tuning. The results show the ability to correctly recognize facial expressions with 83.8% mean sensitivity and very little variance (.042) among classes. Like for humans, recognition of expressions from the lower half of the face (79.6%) is higher than that from the upper half (77.9%). Noticeably, the classification accuracy from the upper half of the face is higher than human level.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</title>
<link>https://arxiv.org/abs/2507.20217</link>
<guid>https://arxiv.org/abs/2507.20217</guid>
<content:encoded><![CDATA[
arXiv:2507.20217v1 Announce Type: cross 
Abstract: Humanoid robot technology is advancing rapidly, with manufacturers introducing diverse heterogeneous visual perception modules tailored to specific scenarios. Among various perception paradigms, occupancy-based representation has become widely recognized as particularly suitable for humanoid robots, as it provides both rich semantic and 3D geometric information essential for comprehensive environmental understanding. In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a dedicated annotation pipeline. Our framework employs advanced multi-modal fusion techniques to generate grid-based occupancy outputs encoding both occupancy status and semantic labels, thereby enabling holistic environmental understanding for downstream tasks such as task planning and navigation. To address the unique challenges of humanoid robots, we overcome issues such as kinematic interference and occlusion, and establish an effective sensor layout strategy. Furthermore, we have developed the first panoramic occupancy dataset specifically for humanoid robots, offering a valuable benchmark and resource for future research and development in this domain. The network architecture incorporates multi-modal feature fusion and temporal information integration to ensure robust perception. Overall, Humanoid Occupancy delivers effective environmental perception for humanoid robots and establishes a technical foundation for standardizing universal visual modules, paving the way for the widespread deployment of humanoid robots in complex real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans</title>
<link>https://arxiv.org/abs/2507.20221</link>
<guid>https://arxiv.org/abs/2507.20221</guid>
<content:encoded><![CDATA[
arXiv:2507.20221v1 Announce Type: cross 
Abstract: In this work, we address the challenge of binary lung nodule classification (benign vs malignant) using CT images by proposing a multi-level attention stacked ensemble of deep neural networks. Three pretrained backbones - EfficientNet V2 S, MobileViT XXS, and DenseNet201 - are each adapted with a custom classification head tailored to 96 x 96 pixel inputs. A two-stage attention mechanism learns both model-wise and class-wise importance scores from concatenated logits, and a lightweight meta-learner refines the final prediction. To mitigate class imbalance and improve generalization, we employ dynamic focal loss with empirically calculated class weights, MixUp augmentation during training, and test-time augmentation at inference. Experiments on the LIDC-IDRI dataset demonstrate exceptional performance, achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in error rate compared to state-of-the-art methods. The model exhibits balanced performance across sensitivity (98.73) and specificity (98.96), with particularly strong results on challenging cases where radiologist disagreement was high. Statistical significance testing confirms the robustness of these improvements across multiple experimental runs. Our approach can serve as a robust, automated aid for radiologists in lung cancer screening.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design</title>
<link>https://arxiv.org/abs/2507.20243</link>
<guid>https://arxiv.org/abs/2507.20243</guid>
<content:encoded><![CDATA[
arXiv:2507.20243v1 Announce Type: cross 
Abstract: SE(3)-based generative models have shown great promise in protein geometry modeling and effective structure design. However, the field currently lacks a modularized benchmark to enable comprehensive investigation and fair comparison of different methods. In this paper, we propose Protein-SE(3), a new benchmark based on a unified training framework, which comprises protein scaffolding tasks, integrated generative models, high-level mathematical abstraction, and diverse evaluation metrics. Recent advanced generative models designed for protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2), Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and FrameFlow) are integrated into our framework. All integrated methods are fairly investigated with the same training dataset and evaluation metrics. Furthermore, we provide a high-level abstraction of the mathematical foundations behind the generative models, enabling fast prototyping of future algorithms without reliance on explicit protein structures. Accordingly, we release the first comprehensive benchmark built upon unified training framework for SE(3)-based protein structure design, which is publicly accessible at https://github.com/BruthYU/protein-se3.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Completion Learning for Language Models</title>
<link>https://arxiv.org/abs/2507.20252</link>
<guid>https://arxiv.org/abs/2507.20252</guid>
<content:encoded><![CDATA[
arXiv:2507.20252v1 Announce Type: cross 
Abstract: Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Expert Factors: Trajectory-level Reward Shaping for Formulaic Alpha Mining</title>
<link>https://arxiv.org/abs/2507.20263</link>
<guid>https://arxiv.org/abs/2507.20263</guid>
<content:encoded><![CDATA[
arXiv:2507.20263v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has successfully automated the complex process of mining formulaic alpha factors, for creating interpretable and profitable investment strategies. However, existing methods are hampered by the sparse rewards given the underlying Markov Decision Process. This inefficiency limits the exploration of the vast symbolic search space and destabilizes the training process. To address this, Trajectory-level Reward Shaping (TLRS), a novel reward shaping method, is proposed. TLRS provides dense, intermediate rewards by measuring the subsequence-level similarity between partially generated expressions and a set of expert-designed formulas. Furthermore, a reward centering mechanism is introduced to reduce training variance. Extensive experiments on six major Chinese and U.S. stock indices show that TLRS significantly improves the predictive power of mined factors, boosting the Rank Information Coefficient by 9.29% over existing potential-based shaping algorithms. Notably, TLRS achieves a major leap in computational efficiency by reducing its time complexity with respect to the feature dimension from linear to constant, which is a significant improvement over distance-based baselines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalized Parameter Tuning in Coherent Ising Machines: A Portfolio-Based Approach</title>
<link>https://arxiv.org/abs/2507.20295</link>
<guid>https://arxiv.org/abs/2507.20295</guid>
<content:encoded><![CDATA[
arXiv:2507.20295v1 Announce Type: cross 
Abstract: Coherent Ising Machines (CIMs) have recently gained attention as a promising computing model for solving combinatorial optimization problems. In particular, the Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution quality, but its performance is highly sensitive to a large number of hyperparameters, making efficient tuning essential. In this study, we present an algorithm portfolio approach for hyperparameter tuning in CIMs employing Chaotic Amplitude Control with momentum (CACm) algorithm. Our method incorporates multiple search strategies, enabling flexible and effective adaptation to the characteristics of the hyperparameter space. Specifically, we propose two representative tuning methods, Method A and Method B. Method A optimizes each hyperparameter sequentially with a fixed total number of trials, while Method B prioritizes hyperparameters based on initial evaluations before applying Method A in order. Performance evaluations were conducted on the Supercomputer "Flow" at Nagoya University, using planted Wishart instances and Time to Solution (TTS) as the evaluation metric. Compared to the baseline performance with best-known hyperparameters, Method A achieved up to 1.47x improvement, and Method B achieved up to 1.65x improvement. These results demonstrate the effectiveness of the algorithm portfolio approach in enhancing the tuning process for CIMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies</title>
<link>https://arxiv.org/abs/2507.20312</link>
<guid>https://arxiv.org/abs/2507.20312</guid>
<content:encoded><![CDATA[
arXiv:2507.20312v1 Announce Type: cross 
Abstract: Scientific and data science applications are becoming increasingly complex, with growing computational and memory demands. Modern high performance computing (HPC) systems provide high parallelism and heterogeneity across nodes, devices, and cores. To achieve good performance, effective scheduling and load balancing techniques are essential. Parallel programming frameworks such as OpenMP now offer a variety of advanced scheduling algorithms to support diverse applications and platforms. This creates an instance of the scheduling algorithm selection problem, which involves identifying the most suitable algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling algorithms in OpenMP. We propose and evaluate expert-based and reinforcement learning (RL)-based methods, and conduct a detailed performance analysis across six applications and three systems. Our results show that RL methods are capable of learning high-performing scheduling decisions, although they require significant exploration, with the choice of reward function playing a key role. Expert-based methods, in contrast, rely on prior knowledge and involve less exploration, though they may not always identify the optimal algorithm for a specific application-system pair. By combining expert knowledge with RL-based learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling algorithms during execution is both viable and beneficial for OpenMP applications. The approach can also be extended to MPI-based programs, enabling optimization of scheduling decisions across multiple levels of parallelism.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2507.20326</link>
<guid>https://arxiv.org/abs/2507.20326</guid>
<content:encoded><![CDATA[
arXiv:2507.20326v1 Announce Type: cross 
Abstract: Polymers, composed of repeating structural units called monomers, are fundamental materials in daily life and industry. Accurate property prediction for polymers is essential for their design, development, and application. However, existing modeling approaches, which typically represent polymers by the constituent monomers, struggle to capture the whole properties of polymer, since the properties change during the polymerization process. In this study, we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training framework, which represents polymers as infinite sequences of monomers and integrates both topological and spatial information for comprehensive modeling. From the topological perspective, we generalize message passing mechanism (MPM) and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we demonstrate that applying MPM to infinite polymer sequences is equivalent to applying MPM on the induced star-linking graph of monomers. For GAM, we propose to further replace global graph attention with localized graph attention (LGA). Moreover, we show the robustness of the "star linking" strategy through Repeat and Shift Invariance Test (RSIT). Despite its robustness, "star linking" strategy exhibits limitations when monomer side chains contain ring structures, a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL) test. To overcome this issue, we propose backbone embedding to enhance the capability of MPM and LGA on infinite polymer sequences. From the spatial perspective, we extract 3D descriptors of repeating monomers to capture spatial information. Finally, we design a cross-modal fusion mechanism to unify the topological and spatial information. Experimental validation across eight diverse polymer property prediction tasks reveals that MIPS achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20335</link>
<guid>https://arxiv.org/abs/2507.20335</guid>
<content:encoded><![CDATA[
arXiv:2507.20335v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into education presents unprecedented opportunities for scalable personalized learning. However, standard LLMs often function as generic information providers, lacking alignment with fundamental pedagogical principles such as helpfulness, student-centered personalization, and creativity cultivation. To bridge this gap, we propose EduAlign, a novel framework designed to guide LLMs toward becoming more effective and responsible educational assistants. EduAlign consists of two main stages. In the first stage, we curate a dataset of 8k educational interactions and annotate them-both manually and automatically-along three key educational dimensions: Helpfulness, Personalization, and Creativity (HPC). These annotations are used to train HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM outputs according to these educational principles. We further evaluate the consistency and reliability of this reward model. In the second stage, we leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then assess the pre- and post-finetuning models on both educational and general-domain benchmarks across the three HPC dimensions. Experimental results demonstrate that the fine-tuned model exhibits significantly improved alignment with pedagogical helpfulness, personalization, and creativity stimulation. This study presents a scalable and effective approach to aligning LLMs with nuanced and desirable educational traits, paving the way for the development of more engaging, pedagogically aligned AI tutors.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of $\theta$-Expectations</title>
<link>https://arxiv.org/abs/2507.20353</link>
<guid>https://arxiv.org/abs/2507.20353</guid>
<content:encoded><![CDATA[
arXiv:2507.20353v1 Announce Type: cross 
Abstract: The canonical theory of stochastic calculus under ambiguity, founded on sub-additivity, is insensitive to non-convex uncertainty structures, leading to an identifiability impasse. This paper develops a mathematical framework for an identifiable calculus sensitive to non-convex geometry. We introduce the $\theta$-BSDE, a class of backward stochastic differential equations where the driver is determined by a pointwise maximization over a primitive, possibly non-convex, uncertainty set. The system's tractability is predicated not on convexity, but on a global analytic hypothesis: the existence of a unique and globally Lipschitz maximizer map for the driver function. Under this hypothesis, which carves out a tractable class of models, we establish well-posedness via a fixed-point argument. For a distinct, geometrically regular class of models, we prove a result of independent interest: under non-degeneracy conditions from Malliavin calculus, the maximizer is unique along any solution path, ensuring the model's internal consistency. We clarify the fundamental logical gap between this pathwise property and the global regularity required by our existence proof. The resulting valuation operator defines a dynamically consistent expectation, and we establish its connection to fully nonlinear PDEs via a Feynman-Kac formula.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering by Attention: Leveraging Prior Fitted Transformers for Data Partitioning</title>
<link>https://arxiv.org/abs/2507.20369</link>
<guid>https://arxiv.org/abs/2507.20369</guid>
<content:encoded><![CDATA[
arXiv:2507.20369v1 Announce Type: cross 
Abstract: Clustering is a core task in machine learning with wide-ranging applications in data mining and pattern recognition. However, its unsupervised nature makes it inherently challenging. Many existing clustering algorithms suffer from critical limitations: they often require careful parameter tuning, exhibit high computational complexity, lack interpretability, or yield suboptimal accuracy, especially when applied to large-scale datasets. In this paper, we introduce a novel clustering approach based on meta-learning. Our approach eliminates the need for parameter optimization while achieving accuracy that outperforms state-of-the-art clustering techniques. The proposed technique leverages a few pre-clustered samples to guide the clustering process for the entire dataset in a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted Transformer Network (PFN) to perform clustering. The algorithm computes attention between the pre-clustered samples and the unclustered samples, allowing it to infer cluster assignments for the entire dataset based on the learned relation. We theoretically and empirically demonstrate that, given just a few pre-clustered examples, the model can generalize to accurately cluster the rest of the dataset. Experiments on challenging benchmark datasets show that our approach can successfully cluster well-separated data without any pre-clustered samples, and significantly improves performance when a few clustered samples are provided. We show that our approach is superior to the state-of-the-art techniques. These results highlight the effectiveness and scalability of our approach, positioning it as a promising alternative to existing clustering techniques.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WBHT: A Generative Attention Architecture for Detecting Black Hole Anomalies in Backbone Networks</title>
<link>https://arxiv.org/abs/2507.20373</link>
<guid>https://arxiv.org/abs/2507.20373</guid>
<content:encoded><![CDATA[
arXiv:2507.20373v1 Announce Type: cross 
Abstract: We propose the Wasserstein Black Hole Transformer (WBHT) framework for detecting black hole (BH) anomalies in communication networks. These anomalies cause packet loss without failure notifications, disrupting connectivity and leading to financial losses. WBHT combines generative modeling, sequential learning, and attention mechanisms to improve BH anomaly detection. It integrates a Wasserstein generative adversarial network with attention mechanisms for stable training and accurate anomaly identification. The model uses long-short-term memory layers to capture long-term dependencies and convolutional layers for local temporal patterns. A latent space encoding mechanism helps distinguish abnormal network behavior. Tested on real-world network data, WBHT outperforms existing models, achieving significant improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and ability to detect previously undetected anomalies make it a valuable tool for proactive network monitoring and security, especially in mission-critical networks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Scene Understanding for Autonomous Navigation in Unstructured Environments</title>
<link>https://arxiv.org/abs/2507.20389</link>
<guid>https://arxiv.org/abs/2507.20389</guid>
<content:encoded><![CDATA[
arXiv:2507.20389v1 Announce Type: cross 
Abstract: Autonomous vehicles are the next revolution in the automobile industry and they are expected to revolutionize the future of transportation. Understanding the scenario in which the autonomous vehicle will operate is critical for its competent functioning. Deep Learning has played a massive role in the progress that has been made till date. Semantic Segmentation, the process of annotating every pixel of an image with an object class, is one crucial part of this scene comprehension using Deep Learning. It is especially useful in Autonomous Driving Research as it requires comprehension of drivable and non-drivable areas, roadside objects and the like. In this paper semantic segmentation has been performed on the Indian Driving Dataset which has been recently compiled on the urban and rural roads of Bengaluru and Hyderabad. This dataset is more challenging compared to other datasets like Cityscapes, since it is based on unstructured driving environments. It has a four level hierarchy and in this paper segmentation has been performed on the first level. Five different models have been trained and their performance has been compared using the Mean Intersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and SegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the dataset, exploratory data analysis, preparation, implementation of the five models and studies the performance and compares the results achieved in the process.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification</title>
<link>https://arxiv.org/abs/2507.20408</link>
<guid>https://arxiv.org/abs/2507.20408</guid>
<content:encoded><![CDATA[
arXiv:2507.20408v1 Announce Type: cross 
Abstract: Automated analysis of lung sound auscultation is essential for monitoring respiratory health, especially in regions facing a shortage of skilled healthcare workers. While respiratory sound classification has been widely studied in adults, its ap plication in pediatric populations, particularly in children aged <6 years, remains an underexplored area. The developmental changes in pediatric lungs considerably alter the acoustic proper ties of respiratory sounds, necessitating specialized classification approaches tailored to this age group. To address this, we propose a multistage hybrid CNN-Transformer framework that combines CNN-extracted features with an attention-based architecture to classify pediatric respiratory diseases using scalogram images from both full recordings and individual breath events. Our model achieved an overall score of 0.9039 in binary event classifi cation and 0.8448 in multiclass event classification by employing class-wise focal loss to address data imbalance. At the recording level, the model attained scores of 0.720 for ternary and 0.571 for multiclass classification. These scores outperform the previous best models by 3.81% and 5.94%, respectively. This approach offers a promising solution for scalable pediatric respiratory disease diagnosis, especially in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations</title>
<link>https://arxiv.org/abs/2507.20409</link>
<guid>https://arxiv.org/abs/2507.20409</guid>
<content:encoded><![CDATA[
arXiv:2507.20409v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?</title>
<link>https://arxiv.org/abs/2507.20419</link>
<guid>https://arxiv.org/abs/2507.20419</guid>
<content:encoded><![CDATA[
arXiv:2507.20419v1 Announce Type: cross 
Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeNER: Code Prompting for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.20423</link>
<guid>https://arxiv.org/abs/2507.20423</guid>
<content:encoded><![CDATA[
arXiv:2507.20423v1 Announce Type: cross 
Abstract: Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings</title>
<link>https://arxiv.org/abs/2507.20426</link>
<guid>https://arxiv.org/abs/2507.20426</guid>
<content:encoded><![CDATA[
arXiv:2507.20426v1 Announce Type: cross 
Abstract: DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that combines a residual learning-based encoder with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly from raw protein sequences. Our architecture incorporates dilated convolutions within residual blocks to mitigate vanishing gradient issues and extract rich sequence features, while capsule layers with dynamic routing capture hierarchical and spatial relationships within the learned feature space. We conducted comprehensive ablation studies comparing global and local embeddings from ProteinBERT and conventional one-hot encoding. Results show that ProteinBERT embeddings substantially outperform other representations on large datasets. Although one-hot encoding showed marginal advantages on smaller datasets, such as PDB186, it struggled to scale effectively. Extensive evaluations on four pairs of publicly available benchmark datasets demonstrate that our model consistently outperforms current state-of-the-art methods. It achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2% and 83.3%, while maintaining competitive performance on larger datasets such as PDB20000. Notably, the model maintains a well balanced sensitivity and specificity across datasets. These results demonstrate the efficacy and generalizability of integrating global protein representations with advanced deep learning architectures for reliable and scalable DBP prediction in diverse genomic contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST: Similarity-based Knowledge Transfer for Efficient Policy Learning</title>
<link>https://arxiv.org/abs/2507.20433</link>
<guid>https://arxiv.org/abs/2507.20433</guid>
<content:encoded><![CDATA[
arXiv:2507.20433v1 Announce Type: cross 
Abstract: Transfer Learning (TL) offers the potential to accelerate learning by transferring knowledge across tasks. However, it faces critical challenges such as negative transfer, domain adaptation and inefficiency in selecting solid source policies. These issues often represent critical problems in evolving domains, i.e. game development, where scenarios transform and agents must adapt. The continuous release of new agents is costly and inefficient. In this work we challenge the key issues in TL to improve knowledge transfer, agents performance across tasks and reduce computational costs. The proposed methodology, called FAST - Framework for Adaptive Similarity-based Transfer, leverages visual frames and textual descriptions to create a latent representation of tasks dynamics, that is exploited to estimate similarity between environments. The similarity scores guides our method in choosing candidate policies from which transfer abilities to simplify learning of novel tasks. Experimental results, over multiple racing tracks, demonstrate that FAST achieves competitive final performance compared to learning-from-scratch methods while requiring significantly less training steps. These findings highlight the potential of embedding-driven task similarity estimations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions</title>
<link>https://arxiv.org/abs/2507.20439</link>
<guid>https://arxiv.org/abs/2507.20439</guid>
<content:encoded><![CDATA[
arXiv:2507.20439v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code generation tasks under idealized conditions, where task descriptions are clear and precise. However, in practice, task descriptions frequently exhibit ambiguity, incompleteness, or internal contradictions. In this paper, we present the first empirical study examining the robustness of state-of-the-art code generation models when faced with such unclear task descriptions. We extend the HumanEval and MBPP benchmarks by systematically introducing realistic task descriptions flaws through guided mutation strategies, producing a dataset that mirrors the messiness of informal developer instructions. We evaluate multiple LLMs of varying sizes and architectures, analyzing their functional correctness and failure modes across task descriptions categories. Our findings reveal that even minor imperfections in task description phrasing can cause significant performance degradation, with contradictory task descriptions resulting in numerous logical errors. Moreover, while larger models tend to be more resilient than smaller variants, they are not immune to the challenges posed by unclear requirements. We further analyze semantic error patterns and identify correlations between description clarity, model behavior, and error types. Our results underscore the critical need for developing LLMs that are not only powerful but also robust to the imperfections inherent in natural user tasks, highlighting important considerations for improving model training strategies, designing more realistic evaluation benchmarks, and ensuring reliable deployment in practical software development environments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley-Value-Based Graph Sparsification for GNN Inference</title>
<link>https://arxiv.org/abs/2507.20460</link>
<guid>https://arxiv.org/abs/2507.20460</guid>
<content:encoded><![CDATA[
arXiv:2507.20460v1 Announce Type: cross 
Abstract: Graph sparsification is a key technique for improving inference efficiency in Graph Neural Networks by removing edges with minimal impact on predictions. GNN explainability methods generate local importance scores, which can be aggregated into global scores for graph sparsification. However, many explainability methods produce only non-negative scores, limiting their applicability for sparsification. In contrast, Shapley value based methods assign both positive and negative contributions to node predictions, offering a theoretically robust and fair allocation of importance by evaluating many subsets of graphs. Unlike gradient-based or perturbation-based explainers, Shapley values enable better pruning strategies that preserve influential edges while removing misleading or adversarial connections. Our approach shows that Shapley value-based graph sparsification maintains predictive performance while significantly reducing graph complexity, enhancing both interpretability and efficiency in GNN inference.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems</title>
<link>https://arxiv.org/abs/2507.20491</link>
<guid>https://arxiv.org/abs/2507.20491</guid>
<content:encoded><![CDATA[
arXiv:2507.20491v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.20499</link>
<guid>https://arxiv.org/abs/2507.20499</guid>
<content:encoded><![CDATA[
arXiv:2507.20499v1 Announce Type: cross 
Abstract: Cross-domain offline reinforcement learning (RL) seeks to enhance sample efficiency in offline RL by utilizing additional offline source datasets. A key challenge is to identify and utilize source samples that are most relevant to the target domain. Existing approaches address this challenge by measuring domain gaps through domain classifiers, target transition dynamics modeling, or mutual information estimation using contrastive loss. However, these methods often require large target datasets, which is impractical in many real-world scenarios. In this work, we address cross-domain offline RL under a limited target data setting, identifying two primary challenges: (1) Dataset imbalance, which is caused by large source and small target datasets and leads to overfitting in neural network-based domain gap estimators, resulting in uninformative measurements; and (2) Partial domain overlap, where only a subset of the source data is closely aligned with the target domain. To overcome these issues, we propose DmC, a novel framework for cross-domain offline RL with limited target samples. Specifically, DmC utilizes $k$-nearest neighbor ($k$-NN) based estimation to measure domain proximity without neural network training, effectively mitigating overfitting. Then, by utilizing this domain proximity, we introduce a nearest-neighbor-guided diffusion model to generate additional source samples that are better aligned with the target domain, thus enhancing policy learning with more effective source samples. Through theoretical analysis and extensive experiments in diverse MuJoCo environments, we demonstrate that DmC significantly outperforms state-of-the-art cross-domain offline RL methods, achieving substantial performance gains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models</title>
<link>https://arxiv.org/abs/2507.20509</link>
<guid>https://arxiv.org/abs/2507.20509</guid>
<content:encoded><![CDATA[
arXiv:2507.20509v1 Announce Type: cross 
Abstract: With rapid advances in code generation, reasoning, and problem-solving, Large Language Models (LLMs) are increasingly applied in robotics. Most existing work focuses on high-level tasks such as task decomposition. A few studies have explored the use of LLMs in feedback controller design; however, these efforts are restricted to overly simplified systems, fixed-structure gain tuning, and lack real-world validation. To further investigate LLMs in automatic control, this work targets a key subfield: adaptive control. Inspired by the framework of model reference adaptive control (MRAC), we propose an LLM-guided adaptive compensator framework that avoids designing controllers from scratch. Instead, the LLMs are prompted using the discrepancies between an unknown system and a reference system to design a compensator that aligns the response of the unknown system with that of the reference, thereby achieving adaptivity. Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided adaptive controller, indirect adaptive control, learning-based adaptive control, and MRAC, on soft and humanoid robots in both simulated and real-world environments. Results show that the LLM-guided adaptive compensator outperforms traditional adaptive controllers and significantly reduces reasoning complexity compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and reasoning-path inspection demonstrate that the LLM-guided adaptive compensator enables a more structured design process by transforming mathematical derivation into a reasoning task, while exhibiting strong generalizability, adaptability, and robustness. This study opens a new direction for applying LLMs in the field of automatic control, offering greater deployability and practicality compared to vision-language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
arXiv:2507.20520v1 Announce Type: cross 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?</title>
<link>https://arxiv.org/abs/2507.20525</link>
<guid>https://arxiv.org/abs/2507.20525</guid>
<content:encoded><![CDATA[
arXiv:2507.20525v1 Announce Type: cross 
Abstract: This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutr"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Spatial Reasoning through Visual and Textual Thinking</title>
<link>https://arxiv.org/abs/2507.20529</link>
<guid>https://arxiv.org/abs/2507.20529</guid>
<content:encoded><![CDATA[
arXiv:2507.20529v1 Announce Type: cross 
Abstract: The spatial reasoning task aims to reason about the spatial relationships in 2D and 3D space, which is a fundamental capability for Visual Question Answering (VQA) and robotics. Although vision language models (VLMs) have developed rapidly in recent years, they are still struggling with the spatial reasoning task. In this paper, we introduce a method that can enhance Spatial reasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In the spatial visual thinking phase, our model is trained to generate location-related specific tokens of essential targets automatically. Not only are the objects mentioned in the problem addressed, but also the potential objects related to the reasoning are considered. During the spatial textual thinking phase, Our model conducts long-term thinking based on visual cues and dialogues, gradually inferring the answers to spatial reasoning problems. To effectively support the model's training, we perform manual corrections to the existing spatial reasoning dataset, eliminating numerous incorrect labels resulting from automatic annotation, restructuring the data input format to enhance generalization ability, and developing thinking processes with logical reasoning details. Without introducing additional information (such as masks or depth), our model's overall average level in several spatial understanding tasks has significantly improved compared with other models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi K2: Open Agentic Intelligence</title>
<link>https://arxiv.org/abs/2507.20534</link>
<guid>https://arxiv.org/abs/2507.20534</guid>
<content:encoded><![CDATA[
arXiv:2507.20534v1 Announce Type: cross 
Abstract: We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation</title>
<link>https://arxiv.org/abs/2507.20536</link>
<guid>https://arxiv.org/abs/2507.20536</guid>
<content:encoded><![CDATA[
arXiv:2507.20536v1 Announce Type: cross 
Abstract: Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: https://github.com/SHI-Labs/T2I-Copilot.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Hallucination Detection via Future Context</title>
<link>https://arxiv.org/abs/2507.20546</link>
<guid>https://arxiv.org/abs/2507.20546</guid>
<content:encoded><![CDATA[
arXiv:2507.20546v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization</title>
<link>https://arxiv.org/abs/2507.20562</link>
<guid>https://arxiv.org/abs/2507.20562</guid>
<content:encoded><![CDATA[
arXiv:2507.20562v1 Announce Type: cross 
Abstract: Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation</title>
<link>https://arxiv.org/abs/2507.20568</link>
<guid>https://arxiv.org/abs/2507.20568</guid>
<content:encoded><![CDATA[
arXiv:2507.20568v1 Announce Type: cross 
Abstract: Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: https://cau-irislab.github.io/interspeech25/
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAG-AFL:Directed Acyclic Graph-based Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2507.20571</link>
<guid>https://arxiv.org/abs/2507.20571</guid>
<content:encoded><![CDATA[
arXiv:2507.20571v1 Announce Type: cross 
Abstract: Due to the distributed nature of federated learning (FL), the vulnerability of the global model and the need for coordination among many client devices pose significant challenges. As a promising decentralized, scalable and secure solution, blockchain-based FL methods have attracted widespread attention in recent years. However, traditional consensus mechanisms designed for Proof of Work (PoW) similar to blockchain incur substantial resource consumption and compromise the efficiency of FL, particularly when participating devices are wireless and resource-limited. To address asynchronous client participation and data heterogeneity in FL, while limiting the additional resource overhead introduced by blockchain, we propose the Directed Acyclic Graph-based Asynchronous Federated Learning (DAG-AFL) framework. We develop a tip selection algorithm that considers temporal freshness, node reachability and model accuracy, with a DAG-based trusted verification strategy. Extensive experiments on 3 benchmarking datasets against eight state-of-the-art approaches demonstrate that DAG-AFL significantly improves training efficiency and model accuracy by 22.7% and 6.5% on average, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Spatiotemporal Bandwidth Enhancement Filter by Sine-activated Deep Learning Model for Fast 3D Photoacoustic Tomography</title>
<link>https://arxiv.org/abs/2507.20575</link>
<guid>https://arxiv.org/abs/2507.20575</guid>
<content:encoded><![CDATA[
arXiv:2507.20575v1 Announce Type: cross 
Abstract: 3D photoacoustic tomography (3D-PAT) using high-frequency hemispherical transducers offers near-omnidirectional reception and enhanced sensitivity to the finer structural details encoded in the high-frequency components of the broadband photoacoustic (PA) signal. However, practical constraints such as limited number of channels with bandlimited sampling rate often result in sparse and bandlimited sensors that degrade image quality. To address this, we revisit the 2D deep learning (DL) approach applied directly to sensor-wise PA radio-frequency (PARF) data. Specifically, we introduce sine activation into the DL model to restore the broadband nature of PARF signals given the observed band-limited and high-frequency PARF data. Given the scarcity of 3D training data, we employ simplified training strategies by simulating random spherical absorbers. This combination of sine-activated model and randomized training is designed to emphasize bandwidth learning over dataset memorization. Our model was evaluated on a leaf skeleton phantom, a micro-CT-verified 3D spiral phantom and in-vivo human palm vasculature. The results showed that the proposed training mechanism on sine-activated model was well-generalized across the different tests by effectively increasing the sensor density and recovering the spatiotemporal bandwidth. Qualitatively, the sine-activated model uniquely enhanced high-frequency content that produces clearer vascular structure with fewer artefacts. Quantitatively, the sine-activated model exhibits full bandwidth at -12 dB spectrum and significantly higher contrast-to-noise ratio with minimal loss of structural similarity index. Lastly, we optimized our approach to enable fast enhanced 3D-PAT at 2 volumes-per-second for better practical imaging of a free-moving targets.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Interactions: Node-Level Graph Generation for Knowledge-Free Augmentation in Recommender Systems</title>
<link>https://arxiv.org/abs/2507.20578</link>
<guid>https://arxiv.org/abs/2507.20578</guid>
<content:encoded><![CDATA[
arXiv:2507.20578v1 Announce Type: cross 
Abstract: Recent advances in recommender systems rely on external resources such as knowledge graphs or large language models to enhance recommendations, which limit applicability in real-world settings due to data dependency and computational overhead. Although knowledge-free models are able to bolster recommendations by direct edge operations as well, the absence of augmentation primitives drives them to fall short in bridging semantic and structural gaps as high-quality paradigm substitutes. Unlike existing diffusion-based works that remodel user-item interactions, this work proposes NodeDiffRec, a pioneering knowledge-free augmentation framework that enables fine-grained node-level graph generation for recommendations and expands the scope of restricted augmentation primitives via diffusion. By synthesizing pseudo-items and corresponding interactions that align with the underlying distribution for injection, and further refining user preferences through a denoising preference modeling process, NodeDiffRec dramatically enhances both semantic diversity and structural connectivity without external knowledge. Extensive experiments across diverse datasets and recommendation algorithms demonstrate the superiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with maximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5 over selected baselines.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Remote Sensing Scene Classification on Edge Devices via Knowledge Distillation and Early-exit</title>
<link>https://arxiv.org/abs/2507.20623</link>
<guid>https://arxiv.org/abs/2507.20623</guid>
<content:encoded><![CDATA[
arXiv:2507.20623v1 Announce Type: cross 
Abstract: As the development of lightweight deep learning algorithms, various deep neural network (DNN) models have been proposed for the remote sensing scene classification (RSSC) application. However, it is still challenging for these RSSC models to achieve optimal performance among model accuracy, inference latency, and energy consumption on resource-constrained edge devices. In this paper, we propose a lightweight RSSC framework, which includes a distilled global filter network (GFNet) model and an early-exit mechanism designed for edge devices to achieve state-of-the-art performance. Specifically, we first apply frequency domain distillation on the GFNet model to reduce model size. Then we design a dynamic early-exit model tailored for DNN models on edge devices to further improve model inference efficiency. We evaluate our E3C model on three edge devices across four datasets. Extensive experimental results show that it achieves an average of 1.3x speedup on model inference and over 40% improvement on energy efficiency, while maintaining high classification accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Video-to-Music Generation with Multiple Time-Varying Conditions</title>
<link>https://arxiv.org/abs/2507.20627</link>
<guid>https://arxiv.org/abs/2507.20627</guid>
<content:encoded><![CDATA[
arXiv:2507.20627v1 Announce Type: cross 
Abstract: Music enhances video narratives and emotions, driving demand for automatic video-to-music (V2M) generation. However, existing V2M methods relying solely on visual features or supplementary textual inputs generate music in a black-box manner, often failing to meet user expectations. To address this challenge, we propose a novel multi-condition guided V2M generation framework that incorporates multiple time-varying conditions for enhanced control over music generation. Our method uses a two-stage training strategy that enables learning of V2M fundamentals and audiovisual temporal synchronization while meeting users' needs for multi-condition control. In the first stage, we introduce a fine-grained feature selection module and a progressive temporal alignment attention mechanism to ensure flexible feature alignment. For the second stage, we develop a dynamic conditional fusion module and a control-guided decoder module to integrate multiple conditions and accurately guide the music composition process. Extensive experiments demonstrate that our method outperforms existing V2M pipelines in both subjective and objective evaluations, significantly enhancing control and alignment with user expectations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2507.20630</link>
<guid>https://arxiv.org/abs/2507.20630</guid>
<content:encoded><![CDATA[
arXiv:2507.20630v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at https://github.com/liaolea/TransPrune.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Enhanced Knowledge Graph Completion using Large Language Models</title>
<link>https://arxiv.org/abs/2507.20643</link>
<guid>https://arxiv.org/abs/2507.20643</guid>
<content:encoded><![CDATA[
arXiv:2507.20643v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution</title>
<link>https://arxiv.org/abs/2507.20650</link>
<guid>https://arxiv.org/abs/2507.20650</guid>
<content:encoded><![CDATA[
arXiv:2507.20650v1 Announce Type: cross 
Abstract: Recently, Deep Learning (DL) models have been increasingly deployed on end-user devices as On-Device AI, offering improved efficiency and privacy. However, this deployment trend poses more serious Intellectual Property (IP) risks, as models are distributed on numerous local devices, making them vulnerable to theft and redistribution. Most existing ownership protection solutions (e.g., backdoor-based watermarking) are designed for cloud-based AI-as-a-Service (AIaaS) and are not directly applicable to large-scale distribution scenarios, where each user-specific model instance must carry a unique watermark. These methods typically embed a fixed watermark, and modifying the embedded watermark requires retraining the model. To address these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking method. It encodes user-specific $n$-bit binary signatures by independently embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA) module, enabling efficient watermark customization without retraining through branch swapping. A parameter obfuscation mechanism further entangles the watermark weights with those of the base model, preventing removal without degrading model performance. The method supports black-box verification and is compatible with various model architectures and DL tasks, including classification, image generation, and text generation. Extensive experiments across three types of tasks and six backbone models demonstrate our method's superior efficiency and adaptability compared to existing approaches, achieving 100\% verification accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection</title>
<link>https://arxiv.org/abs/2507.20666</link>
<guid>https://arxiv.org/abs/2507.20666</guid>
<content:encoded><![CDATA[
arXiv:2507.20666v1 Announce Type: cross 
Abstract: This paper proposes a method for generating machine-type-specific anomalies to evaluate the relative performance of unsupervised anomalous sound detection (UASD) systems across different machine types, even in the absence of real anomaly sound data. Conventional keyword-based data augmentation methods often produce unrealistic sounds due to their reliance on manually defined labels, limiting scalability as machine types and anomaly patterns diversify. Advanced audio generative models, such as MIMII-Gen, show promise but typically depend on anomalous training data, making them less effective when diverse anomalous examples are unavailable. To address these limitations, we propose a novel synthesis approach leveraging large language models (LLMs) to interpret textual descriptions of faults and automatically select audio transformation functions, converting normal machine sounds into diverse and plausible anomalous sounds. We validate this approach by evaluating a UASD system trained only on normal sounds from five machine types, using both real and synthetic anomaly data. Experimental results reveal consistent trends in relative detection difficulty across machine types between synthetic and real anomalies. This finding supports our hypothesis and highlights the effectiveness of the proposed LLM-based synthesis approach for relative evaluation of UASD systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games</title>
<link>https://arxiv.org/abs/2507.20670</link>
<guid>https://arxiv.org/abs/2507.20670</guid>
<content:encoded><![CDATA[
arXiv:2507.20670v1 Announce Type: cross 
Abstract: Understanding and predicting player movement in multiplayer games is crucial for achieving use cases such as player-mimicking bot navigation, preemptive bot control, strategy recommendation, and real-time player behavior analytics. However, the complex environments allow for a high degree of navigational freedom, and the interactions and team-play between players require models that make effective use of the available heterogeneous input data. This paper presents a multimodal architecture for predicting future player locations on a dynamic time horizon, using a U-Net-based approach for calculating endpoint location probability heatmaps, conditioned using a multimodal feature encoder. The application of a multi-head attention mechanism for different groups of features allows for communication between agents. In doing so, the architecture makes efficient use of the multimodal game state including image inputs, numerical and categorical features, as well as dynamic game data. Consequently, the presented technique lays the foundation for various downstream tasks that rely on future player positions such as the creation of player-predictive bot behavior or player anomaly detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title>
<link>https://arxiv.org/abs/2507.20704</link>
<guid>https://arxiv.org/abs/2507.20704</guid>
<content:encoded><![CDATA[
arXiv:2507.20704v1 Announce Type: cross 
Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI</title>
<link>https://arxiv.org/abs/2507.20714</link>
<guid>https://arxiv.org/abs/2507.20714</guid>
<content:encoded><![CDATA[
arXiv:2507.20714v1 Announce Type: cross 
Abstract: Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features' complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals</title>
<link>https://arxiv.org/abs/2507.20737</link>
<guid>https://arxiv.org/abs/2507.20737</guid>
<content:encoded><![CDATA[
arXiv:2507.20737v1 Announce Type: cross 
Abstract: Emotion recognition from physiological data is crucial for mental health assessment, yet it faces two significant challenges: incomplete multi-modal signals and interference from body movements and artifacts. This paper presents a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by integrating multiple querying mechanisms into a unified framework. Specifically, it uses modality queries to reconstruct missing data from incomplete signals, category queries to focus on emotional state features, and interference queries to separate relevant information from noise. Extensive experiment results demonstrate the superior emotion recognition performance of MMQ-Net compared to existing approaches, particularly under high levels of data incompleteness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularizing Subspace Redundancy of Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2507.20745</link>
<guid>https://arxiv.org/abs/2507.20745</guid>
<content:encoded><![CDATA[
arXiv:2507.20745v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) and its variants have delivered strong capability in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable parameters and benefiting from reparameterization. However, their projection matrices remain unrestricted during training, causing high representation redundancy and diminishing the effectiveness of feature adaptation in the resulting subspaces. While existing methods mitigate this by manually adjusting the rank or implicitly applying channel-wise masks, they lack flexibility and generalize poorly across various datasets and architectures. Hence, we propose ReSoRA, a method that explicitly models redundancy between mapping subspaces and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation. Specifically, it theoretically decomposes the low-rank submatrices into multiple equivalent subspaces and systematically applies de-redundancy constraints to the feature distributions across different projections. Extensive experiments validate that our proposed method consistently facilitates existing state-of-the-art PETL methods across various backbones and datasets in vision-language retrieval and standard visual classification benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly integrated into existing approaches in a plug-and-play manner, with no additional inference costs. Code is publicly available at: https://github.com/Lucenova/ReSoRA.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-LIF: Adaptive reset leaky-integrate and fire neuron for spiking neural networks</title>
<link>https://arxiv.org/abs/2507.20746</link>
<guid>https://arxiv.org/abs/2507.20746</guid>
<content:encoded><![CDATA[
arXiv:2507.20746v1 Announce Type: cross 
Abstract: Spiking neural networks possess the advantage of low energy consumption due to their event-driven nature. Compared with binary spike outputs, their inherent floating-point dynamics are more worthy of attention. The threshold level and re- set mode of neurons play a crucial role in determining the number and timing of spikes. The existing hard reset method causes information loss, while the improved soft reset method adopts a uniform treatment for neurons. In response to this, this paper designs an adaptive reset neuron, establishing the correlation between input, output and reset, and integrating a simple yet effective threshold adjustment strategy. It achieves excellent performance on various datasets while maintaining the advantage of low energy consumption.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank</title>
<link>https://arxiv.org/abs/2507.20753</link>
<guid>https://arxiv.org/abs/2507.20753</guid>
<content:encoded><![CDATA[
arXiv:2507.20753v1 Announce Type: cross 
Abstract: In e-commerce recommender and search systems, tree-based models, such as LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks. Despite their effectiveness and widespread adoption in industry, the debate continues whether deep neural networks (DNNs) can outperform traditional tree-based models in this domain. To contribute to this discussion, we systematically benchmark DNNs against our production-grade LambdaMART model. We evaluate multiple DNN architectures and loss functions on a proprietary dataset from OTTO and validate our findings through an 8-week online A/B test. The results show that a simple DNN architecture outperforms a strong tree-based baseline in terms of total clicks and revenue, while achieving parity in total units sold.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry</title>
<link>https://arxiv.org/abs/2507.20757</link>
<guid>https://arxiv.org/abs/2507.20757</guid>
<content:encoded><![CDATA[
arXiv:2507.20757v1 Announce Type: cross 
Abstract: Computer vision seeks to infer a wide range of information about objects and events. However, vision systems based on conventional imaging are limited to extracting information only from the visible surfaces of scene objects. For instance, a vision system can detect and identify a Coke can in the scene, but it cannot determine whether the can is full or empty. In this paper, we aim to expand the scope of computer vision to include the novel task of inferring the hidden liquid levels of opaque containers by sensing the tiny vibrations on their surfaces. Our method provides a first-of-a-kind way to inspect the fill level of multiple sealed containers remotely, at once, without needing physical manipulation and manual weighing. First, we propose a novel speckle-based vibration sensing system for simultaneously capturing scene vibrations on a 2D grid of points. We use our system to efficiently and remotely capture a dataset of vibration responses for a variety of everyday liquid containers. Then, we develop a transformer-based approach for analyzing the captured vibrations and classifying the container type and its hidden liquid level at the time of measurement. Our architecture is invariant to the vibration source, yielding correct liquid level estimates for controlled and ambient scene sound sources. Moreover, our model generalizes to unseen container instances within known classes (e.g., training on five Coke cans of a six-pack, testing on a sixth) and fluid levels. We demonstrate our method by recovering liquid levels from various everyday containers.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data</title>
<link>https://arxiv.org/abs/2507.20782</link>
<guid>https://arxiv.org/abs/2507.20782</guid>
<content:encoded><![CDATA[
arXiv:2507.20782v1 Announce Type: cross 
Abstract: Synthetic data has emerged as a promising alternative for training face recognition (FR) models, offering advantages in scalability, privacy compliance, and potential for bias mitigation. However, critical questions remain on whether both high accuracy and fairness can be achieved with synthetic data. In this work, we evaluate the impact of synthetic data on bias and performance of FR systems. We generate balanced face dataset, FairFaceGen, using two state of the art text-to-image generators, Flux.1-dev and Stable Diffusion v3.5 (SD35), and combine them with several identity augmentation methods, including Arc2Face and four IP-Adapters. By maintaining equal identity count across synthetic and real datasets, we ensure fair comparisons when evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our results demonstrate that although synthetic data still lags behind the real datasets in the generalization on IJB-B/C, demographically balanced synthetic datasets, especially those generated with SD35, show potential for bias mitigation. We also observe that the number and quality of intra-class augmentations significantly affect FR accuracy and fairness. These findings provide practical guidelines for constructing fairer FR systems using synthetic data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach</title>
<link>https://arxiv.org/abs/2507.20796</link>
<guid>https://arxiv.org/abs/2507.20796</guid>
<content:encoded><![CDATA[
arXiv:2507.20796v1 Announce Type: cross 
Abstract: Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</title>
<link>https://arxiv.org/abs/2507.20800</link>
<guid>https://arxiv.org/abs/2507.20800</guid>
<content:encoded><![CDATA[
arXiv:2507.20800v1 Announce Type: cross 
Abstract: The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNet's efficacy. Quantitative analysis revealed significant reductions (p < 0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the system's adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Flow Matching is Particle Swarm Optimization?</title>
<link>https://arxiv.org/abs/2507.20810</link>
<guid>https://arxiv.org/abs/2507.20810</guid>
<content:encoded><![CDATA[
arXiv:2507.20810v1 Announce Type: cross 
Abstract: This paper preliminarily investigates the duality between flow matching in generative models and particle swarm optimization (PSO) in evolutionary computation. Through theoretical analysis, we reveal the intrinsic connections between these two approaches in terms of their mathematical formulations and optimization mechanisms: the vector field learning in flow matching shares similar mathematical expressions with the velocity update rules in PSO; both methods follow the fundamental framework of progressive evolution from initial to target distributions; and both can be formulated as dynamical systems governed by ordinary differential equations. Our study demonstrates that flow matching can be viewed as a continuous generalization of PSO, while PSO provides a discrete implementation of swarm intelligence principles. This duality understanding establishes a theoretical foundation for developing novel hybrid algorithms and creates a unified framework for analyzing both methods. Although this paper only presents preliminary discussions, the revealed correspondences suggest several promising research directions, including improving swarm intelligence algorithms based on flow matching principles and enhancing generative models using swarm intelligence concepts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>First Hallucination Tokens Are Different from Conditional Ones</title>
<link>https://arxiv.org/abs/2507.20836</link>
<guid>https://arxiv.org/abs/2507.20836</guid>
<content:encoded><![CDATA[
arXiv:2507.20836v1 Announce Type: cross 
Abstract: Hallucination, the generation of untruthful content, is one of the major concerns regarding foundational models. Detecting hallucinations at the token level is vital for real-time filtering and targeted correction, yet the variation of hallucination signals within token sequences is not fully understood. Leveraging the RAGTruth corpus with token-level annotations and reproduced logits, we analyse how these signals depend on a token's position within hallucinated spans, contributing to an improved understanding of token-level hallucination. Our results show that the first hallucinated token carries a stronger signal and is more detectable than conditional tokens. We release our analysis framework, along with code for logit reproduction and metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments</title>
<link>https://arxiv.org/abs/2507.20850</link>
<guid>https://arxiv.org/abs/2507.20850</guid>
<content:encoded><![CDATA[
arXiv:2507.20850v1 Announce Type: cross 
Abstract: Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces</title>
<link>https://arxiv.org/abs/2507.20853</link>
<guid>https://arxiv.org/abs/2507.20853</guid>
<content:encoded><![CDATA[
arXiv:2507.20853v1 Announce Type: cross 
Abstract: Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induces a set of attainable states in RL. We show that the training dynamics of a two-layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2507.20872</link>
<guid>https://arxiv.org/abs/2507.20872</guid>
<content:encoded><![CDATA[
arXiv:2507.20872v1 Announce Type: cross 
Abstract: Alzheimer's disease affects over 55 million people worldwide and is projected to more than double by 2050, necessitating rapid, accurate, and scalable diagnostics. However, existing approaches are limited because they cannot achieve clinically acceptable accuracy, generalization across datasets, robustness to missing modalities, and explainability all at the same time. This inability to satisfy all these requirements simultaneously undermines their reliability in clinical settings. We propose OmniBrain, a multimodal framework that integrates brain MRI, radiomics, gene expression, and clinical data using a unified model with cross-attention and modality dropout. OmniBrain achieves $92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior multimodal approaches. Explainability analyses highlight neuropathologically relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a robust, interpretable, and practical solution for real-world Alzheimer's diagnosis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment</title>
<link>https://arxiv.org/abs/2507.20880</link>
<guid>https://arxiv.org/abs/2507.20880</guid>
<content:encoded><![CDATA[
arXiv:2507.20880v1 Announce Type: cross 
Abstract: Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music Arena: Live Evaluation for Text-to-Music</title>
<link>https://arxiv.org/abs/2507.20900</link>
<guid>https://arxiv.org/abs/2507.20900</guid>
<content:encoded><![CDATA[
arXiv:2507.20900v1 Announce Type: cross 
Abstract: We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains.
  Music Arena is available at: https://music-arena.org
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORPION: Addressing Scanner-Induced Variability in Histopathology</title>
<link>https://arxiv.org/abs/2507.20907</link>
<guid>https://arxiv.org/abs/2507.20907</guid>
<content:encoded><![CDATA[
arXiv:2507.20907v1 Announce Type: cross 
Abstract: Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patient's diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection</title>
<link>https://arxiv.org/abs/2507.20913</link>
<guid>https://arxiv.org/abs/2507.20913</guid>
<content:encoded><![CDATA[
arXiv:2507.20913v1 Announce Type: cross 
Abstract: The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIP's original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2507.20917</link>
<guid>https://arxiv.org/abs/2507.20917</guid>
<content:encoded><![CDATA[
arXiv:2507.20917v1 Announce Type: cross 
Abstract: This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling User Behavior from Adaptive Surveys with Supplemental Context</title>
<link>https://arxiv.org/abs/2507.20919</link>
<guid>https://arxiv.org/abs/2507.20919</guid>
<content:encoded><![CDATA[
arXiv:2507.20919v1 Announce Type: cross 
Abstract: Modeling user behavior is critical across many industries where understanding preferences, intent, or decisions informs personalization, targeting, and strategic outcomes. Surveys have long served as a classical mechanism for collecting such behavioral data due to their interpretability, structure, and ease of deployment. However, surveys alone are inherently limited by user fatigue, incomplete responses, and practical constraints on their length making them insufficient for capturing user behavior. In this work, we present LANTERN (Late-Attentive Network for Enriched Response Modeling), a modular architecture for modeling user behavior by fusing adaptive survey responses with supplemental contextual signals. We demonstrate the architectural value of maintaining survey primacy through selective gating, residual connections and late fusion via cross-attention, treating survey data as the primary signal while incorporating external modalities only when relevant. LANTERN outperforms strong survey-only baselines in multi-label prediction of survey responses. We further investigate threshold sensitivity and the benefits of selective modality reliance through ablation and rare/frequent attribute analysis. LANTERN's modularity supports scalable integration of new encoders and evolving datasets. This work provides a practical and extensible blueprint for behavior modeling in survey-centric applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2507.20923</link>
<guid>https://arxiv.org/abs/2507.20923</guid>
<content:encoded><![CDATA[
arXiv:2507.20923v1 Announce Type: cross 
Abstract: Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
arXiv:2507.20924v1 Announce Type: cross 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2507.20930</link>
<guid>https://arxiv.org/abs/2507.20930</guid>
<content:encoded><![CDATA[
arXiv:2507.20930v1 Announce Type: cross 
Abstract: Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Persona-Driven Reasoning in Language Models via Activation Patching</title>
<link>https://arxiv.org/abs/2507.20936</link>
<guid>https://arxiv.org/abs/2507.20936</guid>
<content:encoded><![CDATA[
arXiv:2507.20936v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multivariate Conformal Prediction via Conformalized Gaussian Scoring</title>
<link>https://arxiv.org/abs/2507.20941</link>
<guid>https://arxiv.org/abs/2507.20941</guid>
<content:encoded><![CDATA[
arXiv:2507.20941v1 Announce Type: cross 
Abstract: While achieving exact conditional coverage in conformal prediction is unattainable without making strong, untestable regularity assumptions, the promise of conformal prediction hinges on finding approximations to conditional guarantees that are realizable in practice. A promising direction for obtaining conditional dependence for conformal sets--in particular capturing heteroskedasticity--is through estimating the conditional density $\mathbb{P}_{Y|X}$ and conformalizing its level sets. Previous work in this vein has focused on nonconformity scores based on the empirical cumulative distribution function (CDF). Such scores are, however, computationally costly, typically requiring expensive sampling methods. To avoid the need for sampling, we observe that the CDF-based score reduces to a Mahalanobis distance in the case of Gaussian scores, yielding a closed-form expression that can be directly conformalized. Moreover, the use of a Gaussian-based score opens the door to a number of extensions of the basic conformal method; in particular, we show how to construct conformal sets with missing output values, refine conformal sets as partial information about $Y$ becomes available, and construct conformal sets on transformations of the output space. Finally, empirical results indicate that our approach produces conformal sets that more closely approximate conditional coverage in multivariate settings compared to alternative methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.20956</link>
<guid>https://arxiv.org/abs/2507.20956</guid>
<content:encoded><![CDATA[
arXiv:2507.20956v1 Announce Type: cross 
Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your AI, Not Your View: The Bias of LLMs in Investment Analysis</title>
<link>https://arxiv.org/abs/2507.20957</link>
<guid>https://arxiv.org/abs/2507.20957</guid>
<content:encoded><![CDATA[
arXiv:2507.20957v1 Announce Type: cross 
Abstract: In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a model's embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models' latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL</title>
<link>https://arxiv.org/abs/2507.20966</link>
<guid>https://arxiv.org/abs/2507.20966</guid>
<content:encoded><![CDATA[
arXiv:2507.20966v1 Announce Type: cross 
Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user mobility necessitates updating the set of serving access points to maintain the user-centric clustering. Such updates are typically performed through handoff (HO) operations; however, frequent HOs lead to overheads associated with the allocation and release of resources. This paper presents a deep reinforcement learning (DRL)-based solution to predict and manage these connections for mobile users. Our solution employs the Soft Actor-Critic algorithm, with continuous action space representation, to train a deep neural network to serve as the HO policy. We present a novel proposition for a reward function that integrates a HO penalty in order to balance the attainable rate and the associated overhead related to HOs. We develop two variants of our system; the first one uses mobility direction-assisted (DA) observations that are based on the user movement pattern, while the second one uses history-assisted (HA) observations that are based on the history of the large-scale fading (LSF). Simulation results show that our DRL-based continuous action space approach is more scalable than discrete space counterpart, and that our derived HO policy automatically learns to gather HOs in specific time slots to minimize the overhead of initiating HOs. Our solution can also operate in real time with a response time less than 0.4 ms.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation</title>
<link>https://arxiv.org/abs/2507.20968</link>
<guid>https://arxiv.org/abs/2507.20968</guid>
<content:encoded><![CDATA[
arXiv:2507.20968v1 Announce Type: cross 
Abstract: Domain shift poses a fundamental challenge in time series analysis, where models trained on source domain often fail dramatically when applied in target domain with different yet similar distributions. While current unsupervised domain adaptation (UDA) methods attempt to align cross-domain feature distributions, they typically treat features as indivisible entities, ignoring their intrinsic compositions that governs domain adaptation. We introduce DARSD, a novel UDA framework with theoretical explainability that explicitly realizes UDA tasks from the perspective of representation space decomposition. Our core insight is that effective domain adaptation requires not just alignment, but principled disentanglement of transferable knowledge from mixed representations. DARSD consists three synergistic components: (I) An adversarial learnable common invariant basis that projects original features into a domain-invariant subspace while preserving semantic content; (II) A prototypical pseudo-labeling mechanism that dynamically separates target features based on confidence, hindering error accumulation; (III) A hybrid contrastive optimization strategy that simultaneously enforces feature clustering and consistency while mitigating emerging distribution gaps. Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR, HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms, achieving optimal performance in 35 out of 53 cross-domain scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</title>
<link>https://arxiv.org/abs/2507.20984</link>
<guid>https://arxiv.org/abs/2507.20984</guid>
<content:encoded><![CDATA[
arXiv:2507.20984v1 Announce Type: cross 
Abstract: While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</title>
<link>https://arxiv.org/abs/2507.20987</link>
<guid>https://arxiv.org/abs/2507.20987</guid>
<content:encoded><![CDATA[
arXiv:2507.20987v1 Announce Type: cross 
Abstract: Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive eval- uation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region- specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evalua- tion protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Treatment Effect Estimation from Unstructured Data</title>
<link>https://arxiv.org/abs/2507.20993</link>
<guid>https://arxiv.org/abs/2507.20993</guid>
<content:encoded><![CDATA[
arXiv:2507.20993v1 Announce Type: cross 
Abstract: Existing methods for estimating personalized treatment effects typically rely on structured covariates, limiting their applicability to unstructured data. Yet, leveraging unstructured data for causal inference has considerable application potential, for instance in healthcare, where clinical notes or medical images are abundant. To this end, we first introduce an approximate 'plug-in' method trained directly on the neural representations of unstructured data. However, when these fail to capture all confounding information, the method may be subject to confounding bias. We therefore introduce two theoretically grounded estimators that leverage structured measurements of the confounders during training, but allow estimating personalized treatment effects purely from unstructured inputs, while avoiding confounding bias. When these structured measurements are only available for a non-representative subset of the data, these estimators may suffer from sampling bias. To address this, we further introduce a regression-based correction that accounts for the non-uniform sampling, assuming the sampling mechanism is known or can be well-estimated. Our experiments on two benchmark datasets show that the plug-in method, directly trainable on large unstructured datasets, achieves strong empirical performance across all settings, despite its simplicity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM</title>
<link>https://arxiv.org/abs/2507.20994</link>
<guid>https://arxiv.org/abs/2507.20994</guid>
<content:encoded><![CDATA[
arXiv:2507.20994v1 Announce Type: cross 
Abstract: Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the model's parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs' ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language module's textual "safety layers" in visual inputs, thereby effectively extending text-based safety to the visual modality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition</title>
<link>https://arxiv.org/abs/2507.20997</link>
<guid>https://arxiv.org/abs/2507.20997</guid>
<content:encoded><![CDATA[
arXiv:2507.20997v1 Announce Type: cross 
Abstract: In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability</title>
<link>https://arxiv.org/abs/2507.21004</link>
<guid>https://arxiv.org/abs/2507.21004</guid>
<content:encoded><![CDATA[
arXiv:2507.21004v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) deliver impressive performance but their black-box nature limits deployment in high-stakes domains requiring transparency. We introduce Compositional Function Networks (CFNs), a novel framework that builds inherently interpretable models by composing elementary mathematical functions with clear semantics. Unlike existing interpretable approaches that are limited to simple additive structures, CFNs support diverse compositional patterns -- sequential, parallel, and conditional -- enabling complex feature interactions while maintaining transparency. A key innovation is that CFNs are fully differentiable, allowing efficient training through standard gradient descent. We demonstrate CFNs' versatility across multiple domains, from symbolic regression to image classification with deep hierarchical networks. Our empirical evaluation shows CFNs achieve competitive performance against black-box models (96.24% accuracy on CIFAR-10) while outperforming state-of-the-art interpretable models like Explainable Boosting Machines. By combining the hierarchical expressiveness and efficient training of deep learning with the intrinsic interpretability of well-defined mathematical functions, CFNs offer a powerful framework for applications where both performance and accountability are paramount.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.21009</link>
<guid>https://arxiv.org/abs/2507.21009</guid>
<content:encoded><![CDATA[
arXiv:2507.21009v1 Announce Type: cross 
Abstract: This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.
  These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algebras of actions in an agent's representations of the world</title>
<link>https://arxiv.org/abs/2310.01536</link>
<guid>https://arxiv.org/abs/2310.01536</guid>
<content:encoded><![CDATA[
arXiv:2310.01536v2 Announce Type: replace 
Abstract: In this paper, we propose a framework to extract the algebra of the transformations of worlds from the perspective of an agent. As a starting point, we use our framework to reproduce the symmetry-based representations from the symmetry-based disentangled representation learning (SBDRL) formalism proposed by [1]; only the algebra of transformations of worlds that form groups can be described using symmetry-based representations. We then study the algebras of the transformations of worlds with features that occur in simple reinforcement learning scenarios. Using computational methods, that we developed, we extract the algebras of the transformations of these worlds and classify them according to their properties. Finally, we generalise two important results of SBDRL - the equivariance condition and the disentangling definition - from only working with symmetry-based representations to working with representations capturing the transformation properties of worlds with transformations for any algebra. Finally, we combine our generalised equivariance condition and our generalised disentangling definition to show that disentangled sub-algebras can each have their own individual equivariance conditions, which can be treated independently.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShaRP: Explaining Rankings and Preferences with Shapley Values</title>
<link>https://arxiv.org/abs/2401.16744</link>
<guid>https://arxiv.org/abs/2401.16744</guid>
<content:encoded><![CDATA[
arXiv:2401.16744v5 Announce Type: replace 
Abstract: Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Given the impact of these decisions on individuals, organizations, and population groups, it is essential to understand them - to help individuals improve their ranking position, design better ranking procedures, and ensure legal compliance. In this paper, we argue that explainability methods for classification and regression, such as SHAP, are insufficient for ranking tasks, and present ShaRP - Shapley Values for Rankings and Preferences - a framework that explains the contributions of features to various aspects of a ranked outcome.
  ShaRP computes feature contributions for various ranking-specific profit functions, such as rank and top-k, and also includes a novel Shapley value-based method for explaining pairwise preference outcomes. We provide a flexible implementation of ShaRP, capable of efficiently and comprehensively explaining ranked and pairwise outcomes over tabular data, in score-based ranking and learning-to-rank tasks. Finally, we develop a comprehensive evaluation methodology for ranking explainability methods, showing through qualitative, quantitative, and usability studies that our rank-aware QoIs offer complementary insights, scale effectively, and help users interpret ranked outcomes in practice.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful Differentiable Reasoning with Reshuffled Region-based Embeddings</title>
<link>https://arxiv.org/abs/2406.09529</link>
<guid>https://arxiv.org/abs/2406.09529</guid>
<content:encoded><![CDATA[
arXiv:2406.09529v2 Announce Type: replace 
Abstract: Knowledge graph (KG) embedding methods learn geometric representations of entities and relations to predict plausible missing knowledge. These representations are typically assumed to capture rule-like inference patterns. However, our theoretical understanding of which inference patterns can be captured remains limited. Ideally, KG embedding methods should be expressive enough such that for any set of rules, there exist relation embeddings that exactly capture these rules. This principle has been studied within the framework of region-based embeddings, but existing models are severely limited in the kinds of rule bases that can be captured. We argue that this stems from the fact that entity embeddings are only compared in a coordinate-wise fashion. As an alternative, we propose RESHUFFLE, a simple model based on ordering constraints that can faithfully capture a much larger class of rule bases than existing approaches. Most notably, RESHUFFLE can capture bounded inference w.r.t. arbitrary sets of closed path rules. The entity embeddings in our framework can be learned by a Graph Neural Network (GNN), which effectively acts as a differentiable rule base.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TurboSpec: Closed-loop Speculation Control System for Optimizing LLM Serving Goodput</title>
<link>https://arxiv.org/abs/2406.14066</link>
<guid>https://arxiv.org/abs/2406.14066</guid>
<content:encoded><![CDATA[
arXiv:2406.14066v3 Announce Type: replace 
Abstract: Large Language Model (LLM) serving systems batch concurrent user requests to achieve efficient serving. However, in real-world deployments, such inter-request parallelism from batching is often limited by external factors such as low request rates or memory constraints. Recent works focus on intra-request parallelism from speculative decoding as a solution to this problem. Unfortunately, benefits from intra-request parallelism are often fragile, as speculative decoding causes overhead, and speculated tokens may miss. We observe that speculative decoding may degrade LLM serving performance if added naively without tuning to the incoming requests and the speculation method. To alleviate the need for expert tuning and make speculative decoding more robust, we present TurboSpec, a speculation control system that automatically profiles the execution environment and utilizes a feedback-based algorithm to dynamically adjust the amount of intra-request parallelism in LLM serving. TurboSpec predicts "goodput" - the amount of successfully generated tokens - to evaluate and adjust intra-request parallelism amount to that with the highest goodput in runtime. We implement TurboSpec on a real-world LLM serving system vLLM and demonstrate its effectiveness across diverse workloads and hardware configurations, providing consistent performance improvements across all test scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2TEA: An Agentic AI Designer for Discovery with Generative Evolutionary Multitasking</title>
<link>https://arxiv.org/abs/2406.14917</link>
<guid>https://arxiv.org/abs/2406.14917</guid>
<content:encoded><![CDATA[
arXiv:2406.14917v3 Announce Type: replace 
Abstract: This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask Evolutionary Algorithm, representing the first agentic AI designer of its kind operating with generative evolutionary multitasking (GEM). LLM2TEA enables the crossbreeding of solutions from multiple domains, fostering novel solutions that transcend disciplinary boundaries. Of particular interest is the ability to discover designs that are both novel and conforming to real-world physical specifications. LLM2TEA comprises an LLM to generate genotype samples from text prompts describing target objects, a text-to-3D generative model to produce corresponding phenotypes, a classifier to interpret its semantic representations, and a computational simulator to assess its physical properties. Novel LLM-based multitask evolutionary operators are introduced to guide the search towards high-performing, practically viable designs. Experimental results in conceptual design optimization validate the effectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of novel designs over the current text-to-3D baseline. Moreover, over 73% of the generated designs outperform the top 1% of designs produced by the text-to-3D baseline in terms of physical performance. The designs produced by LLM2TEA are not only aesthetically creative but also functional in real-world contexts. Several of these designs have been successfully 3D printed, demonstrating the ability of our approach to transform AI-generated outputs into tangible, physical designs. These designs underscore the potential of LLM2TEA as a powerful tool for complex design optimization and discovery, capable of producing novel and physically viable designs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2412.05167</link>
<guid>https://arxiv.org/abs/2412.05167</guid>
<content:encoded><![CDATA[
arXiv:2412.05167v2 Announce Type: replace 
Abstract: Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., "Really!?" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at https://adu-bench.github.io/.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Infants to AI: Incorporating Infant-like Learning in Models Boosts Efficiency and Generalization in Learning Social Prediction Tasks</title>
<link>https://arxiv.org/abs/2503.03361</link>
<guid>https://arxiv.org/abs/2503.03361</guid>
<content:encoded><![CDATA[
arXiv:2503.03361v2 Announce Type: replace 
Abstract: Early in development, infants learn a range of useful concepts, which can be challenging from a computational standpoint. This early learning comes together with an initial understanding of aspects of the meaning of concepts, e.g., their implications, causality, and using them to predict likely future events. All this is accomplished in many cases with little or no supervision, and from relatively few examples, compared with current network models. In learning about objects and human-object interactions, early acquired and possibly innate concepts are often used in the process of learning additional, more complex concepts. In the current work, we model how early-acquired concepts are used in the learning of subsequent concepts, and compare the results with standard deep network modeling. We focused in particular on the use of the concepts of animacy and goal attribution in learning to predict future events. We show that the use of early concepts in the learning of new concepts leads to better learning (higher accuracy) and more efficient learning (requiring less data). We further show that this integration of early and new concepts shapes the representation of the concepts acquired by the model. The results show that when the concepts were learned in a human-like manner, the emerging representation was more useful, as measured in terms of generalization to novel data and tasks. On a more general level, the results suggest that there are likely to be basic differences in the conceptual structures acquired by current network models compared to human learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind</title>
<link>https://arxiv.org/abs/2504.18039</link>
<guid>https://arxiv.org/abs/2504.18039</guid>
<content:encoded><![CDATA[
arXiv:2504.18039v3 Announce Type: replace 
Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v2 Announce Type: replace 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback e.g. "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own" into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic</title>
<link>https://arxiv.org/abs/2505.08021</link>
<guid>https://arxiv.org/abs/2505.08021</guid>
<content:encoded><![CDATA[
arXiv:2505.08021v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we propose GNN architectures that correspond precisely to prominent fragments of first-order logic (FO), including various modal logics as well as more expressive two-variable fragments. To establish these results, we apply methods from finite model theory of first-order and modal logics to the domain of graph representation learning. Our results provide a unifying framework for understanding the logical expressiveness of GNNs within FO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast</title>
<link>https://arxiv.org/abs/2505.08151</link>
<guid>https://arxiv.org/abs/2505.08151</guid>
<content:encoded><![CDATA[
arXiv:2505.08151v3 Announce Type: replace 
Abstract: Accurate estimation of lithium-ion battery capacity degradation is critical for enhancing the reliability and safety of battery operations. Traditional expert models, tailored to specific scenarios, provide isolated estimations. With the rapid advancement of data-driven techniques, a series of general-purpose time-series foundation models have been developed. However, foundation models specifically designed for battery capacity degradation remain largely unexplored. To enable zero-shot generalization in battery degradation prediction using large model technology, this study proposes a degradation-aware fine-tuning strategy for time-series foundation models. We apply this strategy to fine-tune the Timer model on approximately 10 GB of open-source battery charge discharge data. Validation on our released CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer possesses strong zero-shot generalization capability in capacity degradation forecasting. To address the computational challenges of deploying large models, we further propose a knowledge distillation framework that transfers the knowledge of pre-trained foundation models into compact expert models. Distillation results across several state-of-the-art time-series expert models confirm that foundation model knowledge significantly improves the multi-condition generalization of expert models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery</title>
<link>https://arxiv.org/abs/2505.13940</link>
<guid>https://arxiv.org/abs/2505.13940</guid>
<content:encoded><![CDATA[
arXiv:2505.13940v2 Announce Type: replace 
Abstract: Large language models (LLMs) integrated with autonomous agents hold significant potential for advancing scientific discovery through automated reasoning and task execution. However, applying LLM agents to drug discovery is still constrained by challenges such as large-scale multimodal data processing, limited task automation, and poor support for domain-specific tools. To overcome these limitations, we introduce DrugPilot, a LLM-based agent system with a parameterized reasoning architecture designed for end-to-end scientific workflows in drug discovery. DrugPilot enables multi-stage research processes by integrating structured tool use with a novel parameterized memory pool. The memory pool converts heterogeneous data from both public sources and user-defined inputs into standardized representations. This design supports efficient multi-turn dialogue, reduces information loss during data exchange, and enhances complex scientific decision-making. To support training and benchmarking, we construct a drug instruction dataset covering eight core drug discovery tasks. Under the Berkeley function-calling benchmark, DrugPilot significantly outperforms state-of-the-art agents such as ReAct and LoT, achieving task completion rates of 98.0%, 93.5%, and 64.0% for simple, multi-tool, and multi-turn scenarios, respectively. These results highlight DrugPilot's potential as a versatile agent framework for computational science domains requiring automated, interactive, and data-integrated reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory</title>
<link>https://arxiv.org/abs/2505.17696</link>
<guid>https://arxiv.org/abs/2505.17696</guid>
<content:encoded><![CDATA[
arXiv:2505.17696v2 Announce Type: replace 
Abstract: This paper proposes a novel theoretical framework for guaranteeing and evaluating the resilience of long short-term memory (LSTM) networks in control systems. We introduce "recovery time" as a new metric of resilience in order to quantify the time required for an LSTM to return to its normal state after anomalous inputs. By mathematically refining incremental input-to-state stability ($\delta$ISS) theory for LSTM, we derive a practical data-independent upper bound on recovery time. This upper bound gives us resilience-aware training. Experimental validation on simple models demonstrates the effectiveness of our resilience estimation and control methods, enhancing a foundation for rigorous quality assurance in safety-critical AI applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs</title>
<link>https://arxiv.org/abs/2506.00582</link>
<guid>https://arxiv.org/abs/2506.00582</guid>
<content:encoded><![CDATA[
arXiv:2506.00582v2 Announce Type: replace 
Abstract: Psychology research has shown that humans are poor at estimating their performance on tasks, tending towards underconfidence on easy tasks and overconfidence on difficult tasks. We examine three LLMs, Llama-3-70B-instruct, Claude-3-Sonnet, and GPT-4o, on a range of QA tasks of varying difficulty, and show that models exhibit subtle differences from human patterns of overconfidence: less sensitive to task difficulty, and when prompted to answer based on different personas -- e.g., expert vs layman, or different race, gender, and ages -- the models will respond with stereotypically biased confidence estimations even though their underlying answer accuracy remains the same. Based on these observations, we propose Answer-Free Confidence Estimation (AFCE) to improve confidence calibration and LLM interpretability in these settings. AFCE is a self-assessment method that employs two stages of prompting, first eliciting only confidence scores on questions, then asking separately for the answer. Experiments on the MMLU and GPQA datasets spanning subjects and difficulty show that this separation of tasks significantly reduces overconfidence and delivers more human-like sensitivity to task difficulty.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ultimate Test of Superintelligent AI Agents: Can an AI Balance Care and Control in Asymmetric Relationships?</title>
<link>https://arxiv.org/abs/2506.01813</link>
<guid>https://arxiv.org/abs/2506.01813</guid>
<content:encoded><![CDATA[
arXiv:2506.01813v3 Announce Type: replace 
Abstract: This paper introduces the Shepherd Test, a new conceptual test for assessing the moral and relational dimensions of superintelligent artificial agents. The test is inspired by human interactions with animals, where ethical considerations about care, manipulation, and consumption arise in contexts of asymmetric power and self-preservation. We argue that AI crosses an important, and potentially dangerous, threshold of intelligence when it exhibits the ability to manipulate, nurture, and instrumentally use less intelligent agents, while also managing its own survival and expansion goals. This includes the ability to weigh moral trade-offs between self-interest and the well-being of subordinate agents. The Shepherd Test thus challenges traditional AI evaluation paradigms by emphasizing moral agency, hierarchical behavior, and complex decision-making under existential stakes. We argue that this shift is critical for advancing AI governance, particularly as AI systems become increasingly integrated into multi-agent environments. We conclude by identifying key research directions, including the development of simulation environments for testing moral behavior in AI, and the formalization of ethical manipulation within multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: Strategy-Adaptive Generation Engine for Query Rewriting</title>
<link>https://arxiv.org/abs/2506.19783</link>
<guid>https://arxiv.org/abs/2506.19783</guid>
<content:encoded><![CDATA[
arXiv:2506.19783v2 Announce Type: replace 
Abstract: Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prover Agent: An Agent-based Framework for Formal Mathematical Proofs</title>
<link>https://arxiv.org/abs/2506.19923</link>
<guid>https://arxiv.org/abs/2506.19923</guid>
<content:encoded><![CDATA[
arXiv:2506.19923v2 Announce Type: replace 
Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 86.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present case studies illustrating how these generated lemmas contribute to solving challenging problems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models</title>
<link>https://arxiv.org/abs/2507.03916</link>
<guid>https://arxiv.org/abs/2507.03916</guid>
<content:encoded><![CDATA[
arXiv:2507.03916v3 Announce Type: replace 
Abstract: Slide animations, such as fade-in, fly-in, and wipe, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually created test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?</title>
<link>https://arxiv.org/abs/2507.04632</link>
<guid>https://arxiv.org/abs/2507.04632</guid>
<content:encoded><![CDATA[
arXiv:2507.04632v3 Announce Type: replace 
Abstract: Recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs). The optimization process often requires numerous iterations to achieve satisfactory performance, resulting in high computational costs due to the need for frequent prompt evaluations under intensive LLM interactions and repeated policy updates. Appropriate online prompt selection methods reduce iteration steps by prioritizing informative prompts during training, while the pipeline's reliance on exhaustive prompt evaluation and subset selection for optimization still incurs substantial computational overhead due to frequent LLM inference calls. Distinguished from these direct evaluate-then-select schemes, this work investigates iterative approximate evaluation for arbitrary prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian risk-predictive framework that online estimates prompt difficulty without requiring costly LLM interactions. Technically, MoPPS models each prompt's success rate as a latent variable, performs streaming Bayesian inference, and employs posterior sampling in a constructed multi-armed bandit machine, enabling sample efficient and adaptive prompt selection. Extensive experiments across mathematics, planning, and vision-based geometry tasks show that MoPPS reliably predicts prompt difficulty and accelerates training with significantly reduced LLM rollouts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains</title>
<link>https://arxiv.org/abs/2507.09884</link>
<guid>https://arxiv.org/abs/2507.09884</guid>
<content:encoded><![CDATA[
arXiv:2507.09884v3 Announce Type: replace 
Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers' performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers' high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era</title>
<link>https://arxiv.org/abs/2507.13175</link>
<guid>https://arxiv.org/abs/2507.13175</guid>
<content:encoded><![CDATA[
arXiv:2507.13175v2 Announce Type: replace 
Abstract: The advancement of powerful yet opaque large language models (LLMs) necessitates a fundamental revision of the philosophical criteria used to evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the assumption of transparent architectures, which LLMs defy due to their stochastic outputs and opaque internal states. This paper argues that traditional ethical criteria are pragmatically obsolete for LLMs due to this mismatch. Engaging with core themes in the philosophy of technology, this paper proffers a revised set of ten functional criteria to evaluate LLM-based artificial moral agents: moral concordance, context sensitivity, normative integrity, metaethical awareness, system resilience, trustworthiness, corrigibility, partial transparency, functional autonomy, and moral imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating Moral Agency through Large Language Systems), aim to steer AMAs toward greater alignment and beneficial societal integration in the coming years. We illustrate these criteria using hypothetical scenarios involving an autonomous public bus (APB) to demonstrate their practical applicability in morally salient contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
arXiv:2507.14111v4 Announce Type: replace 
Abstract: The exponential growth in demand for GPU computing resources has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization that employs a novel contrastive RL algorithm.
  CUDA-L1 achieves significant performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x3.12 with a median speedup of x1.42 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x120. Furthermore, the model also demonstrates portability across GPU architectures, achieving average speedups of x3.12 on L40, x2.50 on RTX 3090, x2.39 on H100, and x2.37 on H20 despite being optimized specifically for A100.
  The capabilities of CUDA-L1 demonstrate that, RL can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. We also identify important challenges posed by training RL models for tasks like CUDA development, where RL often learns to exploit loopholes in reward functions rather than solve the intended optimization problems. By identifying these failure modes and analyzing their root causes, we develop practical methods for creating more robust training procedures that prevent reward hacking.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLungDx: A Hybrid Deep Learning Approach for Early Lung Cancer Diagnosis Using 3D Res-U-Net, YOLOv5, and Vision Transformers</title>
<link>https://arxiv.org/abs/2305.00046</link>
<guid>https://arxiv.org/abs/2305.00046</guid>
<content:encoded><![CDATA[
arXiv:2305.00046v3 Announce Type: replace-cross 
Abstract: Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive rate. The performance of both networks of the proposed framework was compared with other studies and found to outperform them regarding segmentation and detection accuracy. Additionally, our proposed Vision transformer network obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art networks. Our proposed end-to-end deep learning-based framework can effectively segment lungs, and detect and classify lung nodules, specifically in low-resource settings with limited access to radiologists. The proposed framework outperforms existing studies regarding all the respective evaluation metrics. The proposed framework can potentially improve the accuracy and efficiency of lung cancer screening in low-resource settings, ultimately leading to better patient outcomes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking and Analyzing Generative Data for Visual Recognition</title>
<link>https://arxiv.org/abs/2307.13697</link>
<guid>https://arxiv.org/abs/2307.13697</guid>
<content:encoded><![CDATA[
arXiv:2307.13697v2 Announce Type: replace-cross 
Abstract: Advancements in large pre-trained generative models have expanded their potential as effective data generators in visual recognition. This work delves into the impact of generative images, primarily comparing paradigms that harness external data (\ie generative \vs retrieval \vs original).
  Our key contributions are: \textbf{1) GenBench Construction:} We devise \textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548 categories, to appraise generative data across various visual recognition tasks. \textbf{2) CLER Score:} To address the insufficient correlation of existing metrics (\eg, FID, CLIP score) with downstream recognition performance, we propose \textbf{CLER}, a training-free metric indicating generative data's efficiency for recognition tasks prior to training. \textbf{3) New Baselines:} Comparisons of generative data with retrieved data from the same external pool help to elucidate the unique traits of generative data. \textbf{4) External Knowledge Injection:} By fine-tuning special token embeddings for each category via Textual Inversion, performance improves across 17 datasets, except when dealing with low-resolution reference images.
  Our exhaustive benchmark and analysis spotlight generative data's promise in visual recognition, while identifying key challenges for future investigation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization</title>
<link>https://arxiv.org/abs/2309.10370</link>
<guid>https://arxiv.org/abs/2309.10370</guid>
<content:encoded><![CDATA[
arXiv:2309.10370v3 Announce Type: replace-cross 
Abstract: In this paper, we approach the problem of cost (loss) minimization in underparametrized shallow ReLU networks through the explicit construction of upper bounds which appeal to the structure of classification data, without use of gradient descent. A key focus is on elucidating the geometric structure of approximate and precise minimizers. We consider an $\mathcal{L}^2$ cost function, input space $\mathbb{R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size that can be arbitrarily large. We prove an upper bound on the minimum of the cost function of order $O(\delta_P)$ where $\delta_P$ measures the signal-to-noise ratio of training data. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function, and show that the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes a particular $Q$-dimensional subspace in the input space ${\mathbb R}^M$. We comment on the characterization of the global minimum of the cost function in the given context.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Cloud Self-supervised Learning via 3D to Multi-view Masked Learner</title>
<link>https://arxiv.org/abs/2311.10887</link>
<guid>https://arxiv.org/abs/2311.10887</guid>
<content:encoded><![CDATA[
arXiv:2311.10887v2 Announce Type: replace-cross 
Abstract: Recently, multi-modal masked autoencoders (MAE) has been introduced in 3D self-supervised learning, offering enhanced feature learning by leveraging both 2D and 3D data to capture richer cross-modal representations. However, these approaches have two limitations: (1) they inefficiently require both 2D and 3D modalities as inputs, even though the inherent multi-view properties of 3D point clouds already contain 2D modality. (2) input 2D modality causes the reconstruction learning to unnecessarily rely on visible 2D information, hindering 3D geometric representation learning. To address these challenges, we propose a 3D to Multi-View Learner (Multi-View ML) that only utilizes 3D modalities as inputs and effectively capture rich spatial information in 3D point clouds. Specifically, we first project 3D point clouds to multi-view 2D images at the feature level based on 3D-based pose. Then, we introduce two components: (1) a 3D to multi-view autoencoder that reconstructs point clouds and multi-view images from 3D and projected 2D features; (2) a multi-scale multi-head (MSMH) attention mechanism that facilitates local-global information interactions in each decoder transformer block through attention heads at various scales. Additionally, a novel two-stage self-training strategy is proposed to align 2D and 3D representations. Our method outperforms state-of-the-art counterparts across various downstream tasks, including 3D classification, part segmentation, and object detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark</title>
<link>https://arxiv.org/abs/2312.09857</link>
<guid>https://arxiv.org/abs/2312.09857</guid>
<content:encoded><![CDATA[
arXiv:2312.09857v3 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to train models for unlabeled target data. Despite extensive research in domains like computer vision and natural language processing, UDA remains underexplored for time series data, which has widespread real-world applications ranging from medicine and manufacturing to earth observation and human activity recognition. Our paper addresses this gap by introducing a comprehensive benchmark for evaluating UDA techniques for time series classification, with a focus on deep learning methods. We provide seven new benchmark datasets covering various domain shifts and temporal dynamics, facilitating fair and standardized UDA method assessments with state of the art neural network backbones (e.g. Inception) for time series data. This benchmark offers insights into the strengths and limitations of the evaluated approaches while preserving the unsupervised nature of domain adaptation, making it directly applicable to practical problems. Our paper serves as a vital resource for researchers and practitioners, advancing domain adaptation solutions for time series data and fostering innovation in this critical field. The implementation code of this benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive learning-based agent modeling for deep reinforcement learning</title>
<link>https://arxiv.org/abs/2401.00132</link>
<guid>https://arxiv.org/abs/2401.00132</guid>
<content:encoded><![CDATA[
arXiv:2401.00132v3 Announce Type: replace-cross 
Abstract: Multi-agent systems often require agents to collaborate with or compete against other agents with diverse goals, behaviors, or strategies. Agent modeling is essential when designing adaptive policies for intelligent machine agents in multiagent systems, as this is the means by which the ego agent understands other agents' behavior and extracts their meaningful policy representations. These representations can be used to enhance the ego agent's adaptive policy which is trained by reinforcement learning. However, existing agent modeling approaches typically assume the availability of local observations from other agents (modeled agents) during training or a long observation trajectory for policy adaption. To remove these constrictive assumptions and improve agent modeling performance, we devised a Contrastive Learning-based Agent Modeling (CLAM) method that relies only on the local observations from the ego agent during training and execution. With these observations, CLAM is capable of generating consistent high-quality policy representations in real-time right from the beginning of each episode. We evaluated the efficacy of our approach in both cooperative and competitive multi-agent environments. Our experiments demonstrate that our approach achieves state-of-the-art on both cooperative and competitive tasks, highlighting the potential of contrastive learning-based agent modeling for enhancing reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2402.02339</link>
<guid>https://arxiv.org/abs/2402.02339</guid>
<content:encoded><![CDATA[
arXiv:2402.02339v2 Announce Type: replace-cross 
Abstract: Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on a projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of the pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a latent state. Projection loss is then employed to ensure the generated poses are well aligned in 2D space for high-quality optimization. Furthermore, we utilize the uncertainty of each joint to determine how much each joint is allowed for optimization. The effectiveness and superiority of the proposed framework are validated through extensive experiments on challenging datasets: Human3.6M, MPI-INF-3DHP, and 3DPW. Notably, our approach outperforms the previous best result by a large margin of 5.5\% on Human3.6M. Code is available at \href{https://github.com/xiu-cs/UAO-Pose3D}{https://github.com/xiu-cs/UAO-Pose3D}.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Enumeration Remains Challenging for Multimodal Generative AI</title>
<link>https://arxiv.org/abs/2402.03328</link>
<guid>https://arxiv.org/abs/2402.03328</guid>
<content:encoded><![CDATA[
arXiv:2402.03328v3 Announce Type: replace-cross 
Abstract: Many animal species can approximately judge the number of objects in a visual scene at a single glance, and humans can further determine the exact cardinality of a set by deploying systematic counting procedures. In contrast, it has been observed that even state-of-the-art AI systems have very limited enumeration skills. In this work, we propose two benchmark tasks inspired by cognitive science that allow to precisely evaluate the visual enumeration capabilities of multimodal foundation models, thereby providing an objective measure of their number sense and counting level. We consider popular visual question answering models (BLIP, LLaVA and ViLT) as well as advanced image-to-text (Gemini, GPT and Qwen) and text-to-image (DALL-E, FLUX and Stable Diffusion) AI systems. Our analyses show that even the most advanced models cannot reliably name the number of objects in simple visual stimuli or generate images containing a target number of items, as indexed by their low accuracy in both types of tasks. Especially for numbers outside the subitizing range, their responses are often far from the target numerosity, and, in stark contrast with human behavior, in many cases the distribution of errors depends on the object category. We also observe some striking mistakes with small numbers. Our findings demonstrate that developing an intuitive visual understanding of number remains challenging for AI models and that merely increasing model size might not be a viable strategy to promote the emergence of systematic counting skills. We release the full code of our benchmark to facilitate the evaluation of enumeration skills in future AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Data Poisoning on Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2402.08290</link>
<guid>https://arxiv.org/abs/2402.08290</guid>
<content:encoded><![CDATA[
arXiv:2402.08290v4 Announce Type: replace-cross 
Abstract: Counterfactual explanations are a widely used approach for examining the predictions of black-box systems. They can offer the opportunity for computational recourse by suggesting actionable changes on how to alter the input to obtain a different (i.e., more favorable) system output. However, recent studies have pointed out their susceptibility to various forms of manipulation.
  This work studies the vulnerability of counterfactual explanations to data poisoning. We formally introduce and investigate data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, a sub-group of instances, or globally for all instances. In this context, we formally introduce and characterize data poisonings, from which we derive and investigate a general data poisoning mechanism. We demonstrate the impact of such data poisoning in the critical real-world application of explaining event detections in water distribution networks. Additionally, we conduct an extensive empirical evaluation, demonstrating that state-of-the-art counterfactual generation methods and toolboxes are vulnerable to such data poisoning. Furthermore, we find that existing defense methods fail to detect those poisonous samples.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning</title>
<link>https://arxiv.org/abs/2403.14410</link>
<guid>https://arxiv.org/abs/2403.14410</guid>
<content:encoded><![CDATA[
arXiv:2403.14410v2 Announce Type: replace-cross 
Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affinity learning strategy. We examine the superiority of GLC and GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.8\% and 18.9\% in H-score on VisDA, respectively. GLC++ enhances the novel category clustering accuracy of GLC by 4.1\% in open-set scenarios on Office-Home. Furthermore, the introduced contrastive learning strategy not only enhances GLC but also significantly facilitates existing methodologies. The code is available at https://github.com/ispc-lab/GLC-plus.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Juru: Legal Brazilian Large Language Model from Reputable Sources</title>
<link>https://arxiv.org/abs/2403.18140</link>
<guid>https://arxiv.org/abs/2403.18140</guid>
<content:encoded><![CDATA[
arXiv:2403.18140v2 Announce Type: replace-cross 
Abstract: The high compute cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Mistral-7B model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge test suites. Our model, Juru, demonstrates the benefits of domain specialization by achieving improved performance on legal benchmarks, even with a reduced amount of pretraining data. However, this domain specialization through continued pretraining comes at the cost of increased forgetting in unrelated domains, as evidenced by performance degradation on general knowledge test suites in both Portuguese and English. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost. Juru is publicly available at https://huggingface.co/roseval/Juru-7B .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval and Distill: A Temporal Data Shift-Free Paradigm for Online Recommendation System</title>
<link>https://arxiv.org/abs/2404.15678</link>
<guid>https://arxiv.org/abs/2404.15678</guid>
<content:encoded><![CDATA[
arXiv:2404.15678v5 Announce Type: replace-cross 
Abstract: Current recommendation systems are significantly affected by a serious issue of temporal data shift, which is the inconsistency between the distribution of historical data and that of online data. Most existing models focus on utilizing updated data, overlooking the transferable, temporal data shift-free information that can be learned from shifting data. We propose the Temporal Invariance of Association theorem, which suggests that given a fixed search space, the relationship between the data and the data in the search space keeps invariant over time. Leveraging this principle, we designed a retrieval-based recommendation system framework that can train a data shift-free relevance network using shifting data, significantly enhancing the predictive performance of the original model in the recommendation system. However, retrieval-based recommendation models face substantial inference time costs when deployed online. To address this, we further designed a distill framework that can distill information from the relevance network into a parameterized module using shifting data. The distilled model can be deployed online alongside the original model, with only a minimal increase in inference time. Extensive experiments on multiple real datasets demonstrate that our framework significantly improves the performance of the original model by utilizing shifting data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training</title>
<link>https://arxiv.org/abs/2406.00222</link>
<guid>https://arxiv.org/abs/2406.00222</guid>
<content:encoded><![CDATA[
arXiv:2406.00222v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as disambiguation -- when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarification questions. Under task-specific settings, high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO), that enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under in data-efficient tuning scenarios, even when there is no action label available, using multiple real-world conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for complex SQL generation towards data analysis agents. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Resist Alignment: Evidence From Data Compression</title>
<link>https://arxiv.org/abs/2406.06144</link>
<guid>https://arxiv.org/abs/2406.06144</guid>
<content:encoded><![CDATA[
arXiv:2406.06144v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the $\mathbf{elasticity}$ of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment. The model weight and code are available at pku-lm-resist-alignment.github.io.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models</title>
<link>https://arxiv.org/abs/2408.10631</link>
<guid>https://arxiv.org/abs/2408.10631</guid>
<content:encoded><![CDATA[
arXiv:2408.10631v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have seen substantial growth, necessitating efficient model pruning techniques. Existing post-training pruning methods primarily measure weight importance in converged dense models, often overlooking changes in weight significance during the pruning process, leading to performance degradation. To address this issue, we present LLM-Barber (Block-Aware Rebuilder for Sparsity Mask in One-Shot), a novel one-shot pruning framework that rebuilds the sparsity mask of pruned models without any retraining or weight reconstruction. LLM-Barber incorporates block-aware error optimization across Self-Attention and MLP blocks, facilitating global performance optimization. We are the first to employ the product of weights and gradients as a pruning metric in the context of LLM post-training pruning. This enables accurate identification of weight importance in massive models and significantly reduces computational complexity compared to methods using secondorder information. Our experiments show that LLM-Barber efficiently prunes models from LLaMA and OPT families (7B to 13B) on a single A100 GPU in just 30 minutes, achieving state-of-the-art results in both perplexity and zero-shot performance across various language benchmarks. Code is available at https://github.com/YupengSu/LLM-Barber.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTMamba++: Enhancing Multi-Task Dense Scene Understanding via Mamba-Based Decoders</title>
<link>https://arxiv.org/abs/2408.15101</link>
<guid>https://arxiv.org/abs/2408.15101</guid>
<content:encoded><![CDATA[
arXiv:2408.15101v2 Announce Type: replace-cross 
Abstract: Multi-task dense scene understanding, which trains a model for multiple dense prediction tasks, has a wide range of application scenarios. Capturing long-range dependency and enhancing cross-task interactions are crucial to multi-task dense prediction. In this paper, we propose MTMamba++, a novel architecture for multi-task scene understanding featuring with a Mamba-based decoder. It contains two types of core blocks: self-task Mamba (STM) block and cross-task Mamba (CTM) block. STM handles long-range dependency by leveraging state-space models, while CTM explicitly models task interactions to facilitate information exchange across tasks. We design two types of CTM block, namely F-CTM and S-CTM, to enhance cross-task interaction from feature and semantic perspectives, respectively. Extensive experiments on NYUDv2, PASCAL-Context, and Cityscapes datasets demonstrate the superior performance of MTMamba++ over CNN-based, Transformer-based, and diffusion-based methods while maintaining high computational efficiency. The code is available at https://github.com/EnVision-Research/MTMamba.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Competency-Aware Planning for Probabilistically Safe Navigation Under Perception Uncertainty</title>
<link>https://arxiv.org/abs/2409.06111</link>
<guid>https://arxiv.org/abs/2409.06111</guid>
<content:encoded><![CDATA[
arXiv:2409.06111v5 Announce Type: replace-cross 
Abstract: Perception-based navigation systems are useful for unmanned ground vehicle (UGV) navigation in complex terrains, where traditional depth-based navigation schemes are insufficient. However, these data-driven methods are highly dependent on their training data and can fail in surprising and dramatic ways with little warning. To ensure the safety of the vehicle and the surrounding environment, it is imperative that the navigation system is able to recognize the predictive uncertainty of the perception model and respond safely and effectively in the face of uncertainty. In an effort to enable safe navigation under perception uncertainty, we develop a probabilistic and reconstruction-based competency estimation (PaRCE) method to estimate the model's level of familiarity with an input image as a whole and with specific regions in the image. We find that the overall competency score can correctly predict correctly classified, misclassified, and out-of-distribution (OOD) samples. We also confirm that the regional competency maps can accurately distinguish between familiar and unfamiliar regions across images. We then use this competency information to develop a planning and control scheme that enables effective navigation while maintaining a low probability of error. We find that the competency-aware scheme greatly reduces the number of collisions with unfamiliar obstacles, compared to a baseline controller with no competency awareness. Furthermore, the regional competency information is very valuable in enabling efficient navigation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio</title>
<link>https://arxiv.org/abs/2409.06624</link>
<guid>https://arxiv.org/abs/2409.06624</guid>
<content:encoded><![CDATA[
arXiv:2409.06624v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment</title>
<link>https://arxiv.org/abs/2409.07151</link>
<guid>https://arxiv.org/abs/2409.07151</guid>
<content:encoded><![CDATA[
arXiv:2409.07151v2 Announce Type: replace-cross 
Abstract: Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning</title>
<link>https://arxiv.org/abs/2409.12059</link>
<guid>https://arxiv.org/abs/2409.12059</guid>
<content:encoded><![CDATA[
arXiv:2409.12059v5 Announce Type: replace-cross 
Abstract: Current research efforts are focused on enhancing the thinking and reasoning capability of large language model (LLM) by prompting, data-driven emergence and inference-time computation. In this study, we consider stimulating language model's thinking and cognitive abilities from a modular perspective, which mimics the human brain architecture. We select a specific intermediate attention layer with newly implemented language heads. We conduct dual-layer fine-tuning by annotated (query, thought, answer) samples and show that the intermediate layer can also learn to decode fluent and reasonable language tokens. A two-pass inference mechanism is designed to generate thoughts then formal responses. The entire framework is called modularized thinking language model (MeTHanol) which can enhance LLM's cognitive behaviors as indicated by Theory of Mind (ToM) and Vignette-based experiments. Case studies also show that MeTHanol can plan and self-reflect and generate human-like thoughts and answers, even on unseen and open-domain tasks. MeTHanol can also adapt to a personalized prompt and behave as the specified character. Our study holds promise for significant cognitive gains from a modular perspective. Our code, model and data are available at https://bachozean.github.io/methanol-page
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Neural Networks for Modularity aids Interpretability</title>
<link>https://arxiv.org/abs/2409.15747</link>
<guid>https://arxiv.org/abs/2409.15747</guid>
<content:encoded><![CDATA[
arXiv:2409.15747v2 Announce Type: replace-cross 
Abstract: An approach to improve network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We find pretrained models to be highly unclusterable and thus train models to be more modular using an ``enmeshment loss'' function that encourages the formation of non-interacting clusters. Using automated interpretability measures, we show that our method finds clusters that learn different, disjoint, and smaller circuits for CIFAR-10 labels. Our approach provides a promising direction for making neural networks easier to interpret.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Representation Condition Improves Equivariant Molecule Generation</title>
<link>https://arxiv.org/abs/2410.03655</link>
<guid>https://arxiv.org/abs/2410.03655</guid>
<content:encoded><![CDATA[
arXiv:2410.03655v4 Announce Type: replace-cross 
Abstract: Recent advances in molecular generative models have demonstrated great promise for accelerating scientific discovery, particularly in drug design. However, these models often struggle to generate high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to improve molecular generative models by integrating geometric representation conditions with provable theoretical guarantees. We decompose the generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared with single-stage generation, the easy-to-generate representation in the first stage guides the second stage generation toward a high-quality molecule in a goal-oriented way. Leveraging EDM and SemlaFlow as base generators, we observe significant quality improvements in unconditional molecule generation on the widely used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 50\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations. Furthermore, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while largely preserving the generation quality achieved with 1,000 steps, thereby significantly reducing the generation iterations needed. Code is available at https://github.com/GraphPKU/GeoRCG.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</title>
<link>https://arxiv.org/abs/2410.08979</link>
<guid>https://arxiv.org/abs/2410.08979</guid>
<content:encoded><![CDATA[
arXiv:2410.08979v5 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a "temporal recall" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Furthermore, we compare SRL with model-based online planning, showing that SRL achieves comparable FAS while leveraging the same model during training that online planners use for planning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Factuality Assessment from Adversarial Feedback</title>
<link>https://arxiv.org/abs/2410.14651</link>
<guid>https://arxiv.org/abs/2410.14651</guid>
<content:encoded><![CDATA[
arXiv:2410.14651v3 Announce Type: replace-cross 
Abstract: We show that existing evaluations for assessing the factuality of news from conventional sources, such as claims on fact-checking websites, result in high accuracies over time for LLM-based detectors-even after their knowledge cutoffs. This suggests that recent popular false information from such sources can be easily identified due to its likely presence in pre-training/retrieval corpora or the emergence of salient, yet shallow, patterns in these datasets. Instead, we argue that a proper factuality evaluation dataset should test a model's ability to reason about current events by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive variants that challenge LLMs. Our iterative rewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent for a strong RAG-based GPT-4o detector. Our experiments reveal the important role of RAG in both evaluating and generating challenging news examples, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG-based evaluation helps discover more deceitful patterns.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Right to be Forgotten: Managing Heterogeneity Side Effects Through Strategic Incentives</title>
<link>https://arxiv.org/abs/2410.15045</link>
<guid>https://arxiv.org/abs/2410.15045</guid>
<content:encoded><![CDATA[
arXiv:2410.15045v3 Announce Type: replace-cross 
Abstract: Federated Unlearning (FU) enables the removal of specific clients' data influence from trained models. However, in non-IID settings, removing clients creates critical side effects: remaining clients with similar data distributions suffer disproportionate performance degradation, while the global model's stability deteriorates. These vulnerable clients then have reduced incentives to stay in the federation, potentially triggering a cascade of withdrawals that further destabilize the system. To address this challenge, we develop a theoretical framework that quantifies how data heterogeneity impacts unlearning outcomes. Based on these insights, we model FU as a Stackelberg game where the server strategically offers payments to retain crucial clients based on their contribution to both unlearning effectiveness and system stability. Our rigorous equilibrium analysis reveals how data heterogeneity fundamentally shapes the trade-offs between system-wide objectives and client interests. Our approach improves global stability by up to 6.23\%, reduces worst-case client degradation by 10.05\%, and achieves up to 38.6\% runtime efficiency over complete retraining.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs</title>
<link>https://arxiv.org/abs/2410.15956</link>
<guid>https://arxiv.org/abs/2410.15956</guid>
<content:encoded><![CDATA[
arXiv:2410.15956v3 Announce Type: replace-cross 
Abstract: Current Large Language Models (LLMs) are predominantly designed with English as the primary language, and even the few that are multilingual tend to exhibit strong English-centric biases. Much like speakers who might produce awkward expressions when learning a second language, LLMs often generate unnatural outputs in non-English languages, reflecting English-centric patterns in both vocabulary and grammar. Despite the importance of this issue, the naturalness of multilingual LLM outputs has received limited attention. In this paper, we address this gap by introducing novel automatic corpus-level metrics to assess the lexical and syntactic naturalness of LLM outputs in a multilingual context. Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark in French and Chinese, revealing a tendency towards English-influenced patterns. To mitigate this issue, we also propose a simple and effective alignment method to improve the naturalness of an LLM in a target language and domain, achieving consistent improvements in naturalness without compromising the performance on general-purpose benchmarks. Our work highlights the importance of developing multilingual metrics, resources and methods for the new wave of multilingual LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytic Continual Test-Time Adaptation for Multi-Modality Corruption</title>
<link>https://arxiv.org/abs/2410.22373</link>
<guid>https://arxiv.org/abs/2410.22373</guid>
<content:encoded><![CDATA[
arXiv:2410.22373v2 Announce Type: replace-cross 
Abstract: Test-Time Adaptation (TTA) enables pre-trained models to bridge the gap between source and target datasets using unlabeled test data, addressing domain shifts caused by corruptions like weather changes, noise, or sensor malfunctions in test time. Multi-Modal Continual Test-Time Adaptation (MM-CTTA), as an extension of standard TTA, further allows models to handle multi-modal inputs and adapt to continuously evolving target domains. However, MM-CTTA faces critical challenges such as catastrophic forgetting and reliability bias, which are rarely addressed effectively under multi-modal corruption scenarios. In this paper, we propose a novel approach, Multi-modality Dynamic Analytic Adapter (MDAA), to tackle MM-CTTA tasks. MDAA introduces analytic learning,a closed-form training technique,through Analytic Classifiers (ACs) to mitigate catastrophic forgetting. Furthermore, we design the Dynamic Late Fusion Mechanism (DLFM) to dynamically select and integrate reliable information from different modalities. Extensive experiments show that MDAA achieves state-of-the-art performance across the proposed tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syno: Structured Synthesis for Neural Operators</title>
<link>https://arxiv.org/abs/2410.23745</link>
<guid>https://arxiv.org/abs/2410.23745</guid>
<content:encoded><![CDATA[
arXiv:2410.23745v2 Announce Type: replace-cross 
Abstract: The desires for better prediction accuracy and higher execution performance in neural networks never end. Neural architecture search (NAS) and tensor compilers are two popular techniques to optimize these two goals, but they are both limited to composing or optimizing existing manually designed operators rather than coming up with completely new designs. In this work, we explore the less studied direction of neural operator synthesis, which aims to automatically and efficiently discover novel neural operators with better accuracy and/or speed. We develop an end-to-end framework Syno, to realize practical neural operator synthesis. Syno makes use of a novel set of fine-grained primitives defined on tensor dimensions, which ensure various desired properties to ease model training, and also enable expression canonicalization techniques to avoid redundant candidates during search. Syno further adopts a novel guided synthesis flow to obtain valid operators matched with the specified input/output dimension sizes, and leverages efficient stochastic tree search algorithms to quickly explore the design space. We demonstrate that Syno discovers better operators with average speedups of $1.37\times$ to $2.06\times$ on various hardware and compiler choices, while keeping less than 1% accuracy loss even on NAS-optimized models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method</title>
<link>https://arxiv.org/abs/2411.00612</link>
<guid>https://arxiv.org/abs/2411.00612</guid>
<content:encoded><![CDATA[
arXiv:2411.00612v2 Announce Type: replace-cross 
Abstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \textbf{C}ontrastive Learning-based \textbf{L}ink \textbf{P}rediction model, \textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\%, 13.44\% in terms of AUC and AP, respectively.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: A Personalized, Rapid, and Immersive Skill Mastery framework for personalizing experiential learning through Generative AI</title>
<link>https://arxiv.org/abs/2411.14433</link>
<guid>https://arxiv.org/abs/2411.14433</guid>
<content:encoded><![CDATA[
arXiv:2411.14433v2 Announce Type: replace-cross 
Abstract: The rise of generative AI (gen-AI) is transforming industries, particularly in education and workforce training. This chapter introduces PRISM (Personalized, Rapid, and Immersive Skill Mastery), a scalable framework leveraging gen-AI and Digital Twins (DTs) to deliver adaptive, experiential learning. PRISM integrates sentiment analysis and Retrieval-Augmented Generation (RAG) to monitor learner comprehension and dynamically adjust content to meet course objectives. We further present the Multi-Fidelity Digital Twin for Education (MFDT-E) framework, aligning DT fidelity levels with Bloom's Taxonomy and the Kirkpatrick evaluation model to support undergraduate, master's, and doctoral training. Experimental validation shows that GPT-4 achieves 91 percent F1 in zero-shot sentiment analysis of teacher-student dialogues, while GPT-3.5 performs robustly in informal language contexts. Additionally, the system's effectiveness and scalability for immersive Industry 4.0 training are demonstrated through four VR modules: Home Scene, Factory Floor Tour, Capping Station DT, and PPE Inspection Training. These results highlight the potential of integrating generative AI with digital twins to enable personalized, efficient, and scalable education.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2411.19804</link>
<guid>https://arxiv.org/abs/2411.19804</guid>
<content:encoded><![CDATA[
arXiv:2411.19804v2 Announce Type: replace-cross 
Abstract: Integrating multiple (sub-)systems is essential to create advanced Information Systems (ISs). Difficulties mainly arise when integrating dynamic environments across the IS lifecycle. A traditional approach is a registry that provides the API documentation of the systems' endpoints. Large Language Models (LLMs) have shown to be capable of automatically creating system integrations (e.g., as service composition) based on this documentation but require concise input due to input token limitations, especially regarding comprehensive API descriptions. Currently, it is unknown how best to preprocess these API descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing, of OpenAPIs to reduce the input token length while preserving the most relevant information. To further reduce the input token length for the composition prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that only receives a summary of the most relevant endpoints and retrieves details on demand. We evaluate RAG for endpoint discovery using the RestBench benchmark, first, for the different chunking possibilities and parameters measuring the endpoint retrieval recall, precision, and F1 score. Then, we assess the Discovery Agent using the same test set. With our prototype, we demonstrate how to successfully employ RAG for endpoint discovery to reduce the token count. While revealing high values for recall, precision, and F1, further research is necessary to retrieve all requisite endpoints. Our experiments show that for preprocessing, LLM-based and format-specific approaches outperform na\"ive chunking methods. Relying on an agent further enhances these results as the agent splits the tasks into multiple fine granular subtasks, improving the overall RAG performance in the token count, precision, and F1 score.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models</title>
<link>https://arxiv.org/abs/2412.15748</link>
<guid>https://arxiv.org/abs/2412.15748</guid>
<content:encoded><![CDATA[
arXiv:2412.15748v2 Announce Type: replace-cross 
Abstract: Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact</title>
<link>https://arxiv.org/abs/2412.19124</link>
<guid>https://arxiv.org/abs/2412.19124</guid>
<content:encoded><![CDATA[
arXiv:2412.19124v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has emerged as a promising paradigm in medical imaging, addressing the chronic challenge of limited labeled data in healthcare settings. While SSL has shown impressive results, existing studies in the medical domain are often limited in scope, focusing on specific datasets or modalities, or evaluating only isolated aspects of model performance. This fragmented evaluation approach poses a significant challenge, as models deployed in critical medical settings must not only achieve high accuracy but also demonstrate robust performance and generalizability across diverse datasets and varying conditions. To address this gap, we present a comprehensive evaluation of SSL methods within the medical domain, with a particular focus on robustness and generalizability. Using the MedMNIST dataset collection as a standardized benchmark, we evaluate 8 major SSL methods across 11 different medical datasets. Our study provides an in-depth analysis of model performance in both in-domain scenarios and the detection of out-of-distribution (OOD) samples, while exploring the effect of various initialization strategies, model architectures, and multi-domain pre-training. We further assess the generalizability of SSL methods through cross-dataset evaluations and the in-domain performance with varying label proportions (1%, 10%, and 100%) to simulate real-world scenarios with limited supervision. We hope this comprehensive benchmark helps practitioners and researchers make more informed decisions when applying SSL methods to medical applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards End-to-End Neuromorphic Event-based 3D Object Reconstruction Without Physical Priors</title>
<link>https://arxiv.org/abs/2501.00741</link>
<guid>https://arxiv.org/abs/2501.00741</guid>
<content:encoded><![CDATA[
arXiv:2501.00741v4 Announce Type: replace-cross 
Abstract: Neuromorphic cameras, also known as event cameras, are asynchronous brightness-change sensors that can capture extremely fast motion without suffering from motion blur, making them particularly promising for 3D reconstruction in extreme environments. However, existing research on 3D reconstruction using monocular neuromorphic cameras is limited, and most of the methods rely on estimating physical priors and employ complex multi-step pipelines. In this work, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our method incorporates a novel event representation to enhance edge features, enabling the proposed feature-enhancement model to learn more effectively. Additionally, we introduced Optimal Binarization Threshold Selection Principle as a guideline for future related work, using the optimal reconstruction results achieved with threshold optimization as the benchmark. Our method achieves a 54.6% improvement in reconstruction accuracy compared to the baseline method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Back Home: A Computer Vision Solution to Seashell Identification for Ecological Restoration</title>
<link>https://arxiv.org/abs/2501.04873</link>
<guid>https://arxiv.org/abs/2501.04873</guid>
<content:encoded><![CDATA[
arXiv:2501.04873v3 Announce Type: replace-cross 
Abstract: Illegal souvenir collection strips an estimated five tonnes of seashells from Costa Rica's beaches each year. Yet, once these specimens are seized, their coastal origin -- Pacific or Caribbean -- cannot be verified easily due to the lack of information, preventing their return when confiscated by local authorities. To solve this issue, we introduce BackHome19K, the first large-scale image corpus (19{,}058 photographs, 516 species) annotated with coast-level labels, and propose a lightweight pipeline that infers provenance in real time on a mobile-grade CPU. A trained anomaly filter pre-screens uploads, increasing robustness to user-generated noise. On a held-out test set, the classifier attains 86.3\% balanced accuracy, while the filter rejects 93\% of 180 out-of-domain objects with zero false negatives. Deployed as a web application, the system has already processed 70{,}000 shells for wildlife officers in under three seconds per image, enabling confiscated specimens to be safely repatriated to their native ecosystems. The dataset is available at https://huggingface.co/datasets/FIFCO/BackHome19K
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Cel-Animation: A Survey</title>
<link>https://arxiv.org/abs/2501.06250</link>
<guid>https://arxiv.org/abs/2501.06250</guid>
<content:encoded><![CDATA[
arXiv:2501.06250v2 Announce Type: replace-cross 
Abstract: Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, challenges like visual consistency, stylistic coherence, and ethical considerations persist. Additionally, this paper explores future directions and advancements in AI-assisted animation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings</title>
<link>https://arxiv.org/abs/2501.06645</link>
<guid>https://arxiv.org/abs/2501.06645</guid>
<content:encoded><![CDATA[
arXiv:2501.06645v3 Announce Type: replace-cross 
Abstract: Efficient preference optimization algorithms such as Direct Preference Optimization (DPO) have become a popular approach in aligning large language models (LLMs) with human preferences. These algorithms implicitly treat the LLM as a reward model, and focus on training it to correct misranked preference pairs. However, recent work~\citep{chen2024preference} empirically finds that DPO training \textit{rarely improves these misranked preference pairs}, despite its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant that instead \textit{down-weighs} misranked preference pairs and prioritizes enhancing the model's understanding of pairs that it can already rank correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this by adding a modulating factor to dynamically scale DPO loss. Our experiment demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the introduced hyperparameter fixed. Additionally, we empirically reveals how FocalPO affects training on correct and incorrect sample groups, further underscoring its effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on Hand Gesture Recognition from Visual Input</title>
<link>https://arxiv.org/abs/2501.11992</link>
<guid>https://arxiv.org/abs/2501.11992</guid>
<content:encoded><![CDATA[
arXiv:2501.11992v2 Announce Type: replace-cross 
Abstract: Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Be Trusted as Evolutionary Optimizers for Network-Structured Combinatorial Problems?</title>
<link>https://arxiv.org/abs/2501.15081</link>
<guid>https://arxiv.org/abs/2501.15081</guid>
<content:encoded><![CDATA[
arXiv:2501.15081v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown strong capabilities in language understanding and reasoning across diverse domains. Recently, there has been increasing interest in utilizing LLMs not merely as assistants in optimization tasks, but as primary optimizers, particularly for network-structured combinatorial problems. However, before LLMs can be reliably deployed in this role, a fundamental question must be addressed: Can LLMs iteratively manipulate solutions that consistently adhere to problem constraints? In this work, we propose a systematic framework to evaluate the capability of LLMs to engage with problem structures. Rather than treating the model as a black-box generator, we adopt the commonly used evolutionary optimizer (EVO) and propose a comprehensive evaluation framework that rigorously assesses the output fidelity of LLM-based operators across different stages of the evolutionary process. To enhance robustness, we introduce a hybrid error-correction mechanism that mitigates uncertainty in LLMs outputs. Moreover, we explore a cost-efficient population-level optimization strategy that significantly improves efficiency compared to traditional individual-level approaches. Extensive experiments on a representative node-level combinatorial network optimization task demonstrate the effectiveness, adaptability, and inherent limitations of LLM-based EVO. Our findings present perspectives on integrating LLMs into evolutionary computation and discuss paths that may support scalable and context-aware optimization in networked systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation</title>
<link>https://arxiv.org/abs/2501.19364</link>
<guid>https://arxiv.org/abs/2501.19364</guid>
<content:encoded><![CDATA[
arXiv:2501.19364v2 Announce Type: replace-cross 
Abstract: Multivariate Time Series Imputation (MTSI) is crucial for many applications, such as healthcare monitoring and traffic management, where incomplete data can compromise decision-making. Existing state-of-the-art methods, like Denoising Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy; however, they suffer from significant computational costs and are notably time-consuming due to their iterative nature. In this work, we propose CoSTI, an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI employs Consistency Training to achieve comparable imputation quality to DDPMs while drastically reducing inference times, making it more suitable for real-time applications. We evaluate CoSTI across multiple datasets and missing data scenarios, demonstrating up to a 98% reduction in imputation time with performance on par with diffusion-based models. This work bridges the gap between efficiency and accuracy in generative imputation tasks, providing a scalable solution for handling missing data in critical spatio-temporal systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Image Generation with Denoising Diffusion Codebook Models</title>
<link>https://arxiv.org/abs/2502.01189</link>
<guid>https://arxiv.org/abs/2502.01189</guid>
<content:encoded><![CDATA[
arXiv:2502.01189v4 Announce Type: replace-cross 
Abstract: We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIMO: Less is More for Reasoning</title>
<link>https://arxiv.org/abs/2502.03387</link>
<guid>https://arxiv.org/abs/2502.03387</guid>
<content:encoded><![CDATA[
arXiv:2502.03387v2 Announce Type: replace-cross 
Abstract: We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as "cognitive templates" that guide reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask and Remember: A Questions-Only Replay Strategy for Continual Visual Question Answering</title>
<link>https://arxiv.org/abs/2502.04469</link>
<guid>https://arxiv.org/abs/2502.04469</guid>
<content:encoded><![CDATA[
arXiv:2502.04469v2 Announce Type: replace-cross 
Abstract: Continual Learning in Visual Question Answering (VQACL) requires models to acquire new visual-linguistic skills (plasticity) while preserving previously learned knowledge (stability). The inherent multimodality of VQACL exacerbates this challenge, as models must balance stability across visual and textual domains while adapting to novel objects and reasoning tasks. Existing methods, primarily designed for unimodal settings, often fall short in addressing this dual requirement. In this work, we present QUestion-only replay with Attention Distillation (QUAD), a novel approach for VQACL that leverages only past task questions for regularization. By eliminating the need to store visual data, QUAD not only reduces memory overhead, but also alleviates privacy concerns. Our method introduces a Question-only Replay mechanism that selectively reuses prior task questions to counteract overfitting to the answer space of the current task, addressing the problem out of answer set. Complementing this, we propose Attention Consistency Distillation to enforce both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that QUAD significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA. Code is available at: https://github.com/IemProg/QUAD.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Histogram-based Outlier Score (EHBOS)</title>
<link>https://arxiv.org/abs/2502.05719</link>
<guid>https://arxiv.org/abs/2502.05719</guid>
<content:encoded><![CDATA[
arXiv:2502.05719v2 Announce Type: replace-cross 
Abstract: Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards</title>
<link>https://arxiv.org/abs/2502.08610</link>
<guid>https://arxiv.org/abs/2502.08610</guid>
<content:encoded><![CDATA[
arXiv:2502.08610v2 Announce Type: replace-cross 
Abstract: As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NestQuant: Nested Lattice Quantization for Matrix Products and LLMs</title>
<link>https://arxiv.org/abs/2502.09720</link>
<guid>https://arxiv.org/abs/2502.09720</guid>
<content:encoded><![CDATA[
arXiv:2502.09720v3 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent works have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot (8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation benchmarks confirm uniform superiority of NestQuant.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control</title>
<link>https://arxiv.org/abs/2502.12145</link>
<guid>https://arxiv.org/abs/2502.12145</guid>
<content:encoded><![CDATA[
arXiv:2502.12145v2 Announce Type: replace-cross 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $\alpha$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability, making it a practical and adaptable solution for real-world applications. Code is available at https://github.com/JinyanSu1/Flare-Aug.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2502.15278</link>
<guid>https://arxiv.org/abs/2502.15278</guid>
<content:encoded><![CDATA[
arXiv:2502.15278v2 Announce Type: replace-cross 
Abstract: Assessing whether AI-generated images are substantially similar to source works is a crucial step in resolving copyright disputes. In this paper, we propose CopyJudge, a novel automated infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework based on the multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on these judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Furthermore, assuming the input noise is controllable, our approach can be enhanced by iteratively exploring non-infringing noise vectors within the diffusion latent space, even without modifying the original prompts. Experimental results show that our automated identification method achieves comparable state-of-the-art performance, while offering superior generalization and interpretability across various forms of infringement, and that our mitigation method more effectively mitigates memorization and IP infringement with a high degree of alignment to the original non-infringing expressions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents</title>
<link>https://arxiv.org/abs/2502.18509</link>
<guid>https://arxiv.org/abs/2502.18509</guid>
<content:encoded><![CDATA[
arXiv:2502.18509v2 Announce Type: replace-cross 
Abstract: Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks associated with them. The moment users share information with these agents-such as large language models (LLMs)-their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLM-based Conversational Agents (LCAs). It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LCAs (untrusted receivers). Through a formative design user study, we observe how even "privacy-conscious" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally deployable framework that operates between users and LCAs, identifying and reformulating out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals. Notably, about 76% of participants in our human evaluation preferred the reformulated prompts over the original ones, validating the usability and effectiveness of contextual privacy in our proposed framework. We opensource the code at https://github.com/IBM/contextual-privacy-LLM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title>
<link>https://arxiv.org/abs/2502.18573</link>
<guid>https://arxiv.org/abs/2502.18573</guid>
<content:encoded><![CDATA[
arXiv:2502.18573v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Open-world Continual Learning under the Constraints of Scarce Labeled Data</title>
<link>https://arxiv.org/abs/2502.20974</link>
<guid>https://arxiv.org/abs/2502.20974</guid>
<content:encoded><![CDATA[
arXiv:2502.20974v2 Announce Type: replace-cross 
Abstract: Open-world continual learning (OWCL) adapts to sequential tasks with open samples, learning knowledge incrementally while preventing forgetting. However, existing OWCL still requires a large amount of labeled data for training, which is often impractical in real-world applications. Given that new categories/entities typically come with limited annotations and are in small quantities, a more realistic situation is OWCL with scarce labeled data, i.e., few-shot training samples. Hence, this paper investigates the problem of open-world few-shot continual learning (OFCL), challenging in (i) learning unbounded tasks without forgetting previous knowledge and avoiding overfitting, (ii) constructing compact decision boundaries for open detection with limited labeled data, and (iii) transferring knowledge about knowns and unknowns and even update the unknowns to knowns once the labels of open samples are learned. In response, we propose a novel OFCL framework that integrates three key components: (1) an instance-wise token augmentation (ITA) that represents and enriches sample representations with additional knowledge, (2) a margin-based open boundary (MOB) that supports open detection with new tasks emerge over time, and (3) an adaptive knowledge space (AKS) that endows unknowns with knowledge for the updating from unknowns to knowns. Finally, extensive experiments show that the proposed OFCL framework outperforms all baselines remarkably with practical importance and reproducibility. The source code is released at https://github.com/liyj1201/OFCL.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAIL: Text-Audio Incremental Learning</title>
<link>https://arxiv.org/abs/2503.04258</link>
<guid>https://arxiv.org/abs/2503.04258</guid>
<content:encoded><![CDATA[
arXiv:2503.04258v2 Announce Type: replace-cross 
Abstract: Many studies combine text and audio to capture multi-modal information but they overlook the model's generalization ability on new datasets. Introducing new datasets may affect the feature space of the original dataset, leading to catastrophic forgetting. Meanwhile, large model parameters can significantly impact training performance. To address these limitations, we introduce a novel task called Text-Audio Incremental Learning (TAIL) task for text-audio retrieval, and propose a new method, PTAT, Prompt Tuning for Audio-Text incremental learning. This method utilizes prompt tuning to optimize the model parameters while incorporating an audio-text similarity and feature distillation module to effectively mitigate catastrophic forgetting. We benchmark our method and previous incremental learning methods on AudioCaps, Clotho, BBC Sound Effects and Audioset datasets, and our method outperforms previous methods significantly, particularly demonstrating stronger resistance to forgetting on older datasets. Compared to the full-parameters Finetune (Sequential) method, our model only requires 2.42\% of its parameters, achieving 4.46\% higher performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Synthetic Image Detection through Diffusion Timestep Ensembling</title>
<link>https://arxiv.org/abs/2503.06201</link>
<guid>https://arxiv.org/abs/2503.06201</guid>
<content:encoded><![CDATA[
arXiv:2503.06201v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models have enabled the creation of deceptively real images, posing significant security risks when misused. In this study, we empirically show that different timesteps of DDIM inversion reveal varying subtle distinctions between synthetic and real images that are extractable for detection, in the forms of such as Fourier power spectrum high-frequency discrepancies and inter-pixel variance distributions. Based on these observations, we propose a novel synthetic image detection method that directly utilizes features of intermediately noised images by training an ensemble on multiple noised timesteps, circumventing conventional reconstruction-based strategies. To enhance human comprehension, we introduce a metric-grounded explanation generation and refinement module to identify and explain AI-generated flaws. Additionally, we construct the GenHard and GenExplain benchmarks to provide detection samples of greater difficulty and high-quality rationales for fake images. Extensive experiments show that our method achieves state-of-the-art performance with 98.91% and 95.89% detection accuracy on regular and challenging samples respectively, and demonstrates generalizability and robustness. Our code and datasets are available at https://github.com/Shadowlized/ESIDE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents</title>
<link>https://arxiv.org/abs/2503.08026</link>
<guid>https://arxiv.org/abs/2503.08026</guid>
<content:encoded><![CDATA[
arXiv:2503.08026v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs' cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Multimodal Controls for Expressive Talking Human Animation</title>
<link>https://arxiv.org/abs/2503.08714</link>
<guid>https://arxiv.org/abs/2503.08714</guid>
<content:encoded><![CDATA[
arXiv:2503.08714v4 Announce Type: replace-cross 
Abstract: In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be ``directly guided'' through text descriptions. Therefore, we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models</title>
<link>https://arxiv.org/abs/2503.08737</link>
<guid>https://arxiv.org/abs/2503.08737</guid>
<content:encoded><![CDATA[
arXiv:2503.08737v2 Announce Type: replace-cross 
Abstract: Constructing a compressed latent space through a variational autoencoder (VAE) is the key for efficient 3D diffusion models. This paper introduces COD-VAE that encodes 3D shapes into a COmpact set of 1D latent vectors without sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme to improve compression and decoding efficiency. First, our encoder block progressively compresses point clouds into compact latent vectors via intermediate point patches. Second, our triplane-based decoder reconstructs dense triplanes from latent vectors instead of directly decoding neural fields, significantly reducing computational overhead of neural fields decoding. Finally, we propose uncertainty-guided token pruning, which allocates resources adaptively by skipping computations in simpler regions and improves the decoder efficiency. Experimental results demonstrate that COD-VAE achieves 16x compression compared to the baseline while maintaining quality. This enables 20.8x speedup in generation, highlighting that a large number of latent vectors is not a prerequisite for high-quality reconstruction and generation. The code is available at https://github.com/join16/COD-VAE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are ECGs enough? Deep learning classification of pulmonary embolism using electrocardiograms</title>
<link>https://arxiv.org/abs/2503.08960</link>
<guid>https://arxiv.org/abs/2503.08960</guid>
<content:encoded><![CDATA[
arXiv:2503.08960v2 Announce Type: replace-cross 
Abstract: Pulmonary embolism is a leading cause of out of hospital cardiac arrest that requires fast diagnosis. While computed tomography pulmonary angiography is the standard diagnostic tool, it is not always accessible. Electrocardiography is an essential tool for diagnosing mul- tiple cardiac anomalies, as it is affordable, fast and available in many settings. However, the availability of public ECG datasets, specially for PE, is limited and, in practice, these datasets tend to be small, making it essential to optimize learning strategies. In this study, we investigate the performance of multiple neural networks in order to assess the impact of various approaches. Moreover, we check whether these practices enhance model generalization when transfer learning is used to translate infor- mation learned in larger ECG datasets, such as PTB-XL, CPSC18 and MedalCare-XL, to a smaller, more challenging dataset for PE. By lever- aging transfer learning, we analyze the extent to which we can improve learning efficiency and predictive performance on limited data. Code available at https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research</title>
<link>https://arxiv.org/abs/2503.12730</link>
<guid>https://arxiv.org/abs/2503.12730</guid>
<content:encoded><![CDATA[
arXiv:2503.12730v4 Announce Type: replace-cross 
Abstract: Mechanistic interpretability research faces a gap between analyzing simple circuits in toy tasks and discovering features in large models. To bridge this gap, we propose text-to-SQL generation as an ideal task to study, as it combines the formal structure of toy tasks with real-world complexity. We introduce TinySQL, a synthetic dataset, progressing from basic to advanced SQL operations, and train models ranging from 33M to 1B parameters to establish a comprehensive testbed for interpretability. We apply multiple complementary interpretability techniques, including Edge Attribution Patching and Sparse Autoencoders, to identify minimal circuits and components supporting SQL generation. We compare circuits for different SQL subskills, evaluating their minimality, reliability, and identifiability. Finally, we conduct a layerwise logit lens analysis to reveal how models compose SQL queries across layers: from intent recognition to schema resolution to structured generation. Our work provides a robust framework for probing and comparing interpretability methods in a structured, progressively complex setting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free-form language-based robotic reasoning and grasping</title>
<link>https://arxiv.org/abs/2503.13082</link>
<guid>https://arxiv.org/abs/2503.13082</guid>
<content:encoded><![CDATA[
arXiv:2503.13082v2 Announce Type: replace-cross 
Abstract: Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</title>
<link>https://arxiv.org/abs/2503.16421</link>
<guid>https://arxiv.org/abs/2503.16421</guid>
<content:encoded><![CDATA[
arXiv:2503.16421v2 Announce Type: replace-cross 
Abstract: Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aether: Geometric-Aware Unified World Modeling</title>
<link>https://arxiv.org/abs/2503.18945</link>
<guid>https://arxiv.org/abs/2503.18945</guid>
<content:encoded><![CDATA[
arXiv:2503.18945v3 Announce Type: replace-cross 
Abstract: The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates zero-shot synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Notably, even without real-world data, its reconstruction performance is comparable with or even better than that of domain-specific models. Additionally, Aether employs camera trajectories as geometry-informed action spaces, enabling effective action-conditioned prediction and visual planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interleaved Multitask Learning with Energy Modulated Learning Progress</title>
<link>https://arxiv.org/abs/2504.00707</link>
<guid>https://arxiv.org/abs/2504.00707</guid>
<content:encoded><![CDATA[
arXiv:2504.00707v2 Announce Type: replace-cross 
Abstract: As humans learn new skills and apply their existing knowledge while maintaining previously learned information, "continual learning" in machine learning aims to incorporate new data while retaining and utilizing past knowledge. However, existing machine learning methods often does not mimic human learning where tasks are intermixed due to individual preferences and environmental conditions. Humans typically switch between tasks instead of completely mastering one task before proceeding to the next. To explore how human-like task switching can enhance learning efficiency, we propose a multi task learning architecture that alternates tasks based on task-agnostic measures such as "learning progress" and "neural computational energy expenditure". To evaluate the efficacy of our method, we run several systematic experiments by using a set of effect-prediction tasks executed by a simulated manipulator robot. The experiments show that our approach surpasses random interleaved and sequential task learning in terms of average learning accuracy. Moreover, by including energy expenditure in the task switching logic, our approach can still perform favorably while reducing neural energy expenditure.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers</title>
<link>https://arxiv.org/abs/2504.01301</link>
<guid>https://arxiv.org/abs/2504.01301</guid>
<content:encoded><![CDATA[
arXiv:2504.01301v2 Announce Type: replace-cross 
Abstract: We present Bi-LAT, a novel imitation learning framework that unifies bilateral control with natural language processing to achieve precise force modulation in robotic manipulation. Bi-LAT leverages joint position, velocity, and torque data from leader-follower teleoperation while also integrating visual and linguistic cues to dynamically adjust applied force. By encoding human instructions such as "softly grasp the cup" or "strongly twist the sponge" through a multimodal Transformer-based model, Bi-LAT learns to distinguish nuanced force requirements in real-world tasks. We demonstrate Bi-LAT's performance in (1) unimanual cup-stacking scenario where the robot accurately modulates grasp force based on language commands, and (2) bimanual sponge-twisting task that requires coordinated force control. Experimental results show that Bi-LAT effectively reproduces the instructed force levels, particularly when incorporating SigLIP among tested language encoders. Our findings demonstrate the potential of integrating natural language cues into imitation learning, paving the way for more intuitive and adaptive human-robot interaction. For additional material, please visit: https://mertcookimg.github.io/bi-lat/
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time LaCAM for Real-Time MAPF</title>
<link>https://arxiv.org/abs/2504.06091</link>
<guid>https://arxiv.org/abs/2504.06091</guid>
<content:encoded><![CDATA[
arXiv:2504.06091v2 Announce Type: replace-cross 
Abstract: The vast majority of Multi-Agent Path Finding (MAPF) methods with completeness guarantees require planning full-horizon paths. However, planning full-horizon paths can take too long and be impractical in real-world applications. Instead, real-time planning and execution, which only allows the planner a finite amount of time before executing and replanning, is more practical for real-world multi-agent systems. Several methods utilize real-time planning schemes but none are provably complete, which leads to livelock or deadlock. Our main contribution is Real-Time LaCAM, the first Real-Time MAPF method with provable completeness guarantees. We do this by leveraging LaCAM (Okumura 2023) in an incremental fashion. Our results show how we can iteratively plan for congested environments with a cutoff time of milliseconds while still maintaining the same success rate as full-horizon LaCAM. We also show how it can be used with a single-step learned MAPF policy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines</title>
<link>https://arxiv.org/abs/2504.07840</link>
<guid>https://arxiv.org/abs/2504.07840</guid>
<content:encoded><![CDATA[
arXiv:2504.07840v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adopting Large Language Models to Automated System Integration</title>
<link>https://arxiv.org/abs/2504.08490</link>
<guid>https://arxiv.org/abs/2504.08490</guid>
<content:encoded><![CDATA[
arXiv:2504.08490v2 Announce Type: replace-cross 
Abstract: Modern enterprise computing systems integrate numerous subsystems to resolve a common task by yielding emergent behavior. A widespread approach is using services implemented with Web technologies like REST or OpenAPI, which offer an interaction mechanism and service documentation standard, respectively. Each service represents a specific business functionality, allowing encapsulation and easier maintenance. Despite the reduced maintenance costs on an individual service level, increased integration complexity arises. Consequently, automated service composition approaches have arisen to mitigate this issue. Nevertheless, these approaches have not achieved high acceptance in practice due to their reliance on complex formal modeling. Within this Ph.D. thesis, we analyze the application of Large Language Models (LLMs) to automatically integrate the services based on a natural language input. The result is a reusable service composition, e.g., as program code. While not always generating entirely correct results, the result can still be helpful by providing integration engineers with a close approximation of a suitable solution, which requires little effort to become operational. Our research involves (i) introducing a software architecture for automated service composition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for service discovery, (iii) proposing a novel natural language query-based benchmark for service discovery, and (iv) extending the benchmark to complete service composition scenarios. We have presented our software architecture as Compositio Prompto, the analysis of RAG for service discovery, and submitted a proposal for the service discovery benchmark. Open topics are primarily the extension of the service discovery benchmark to service composition scenarios and the improvements of the service composition generation, e.g., using fine-tuning or LLM agents.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model</title>
<link>https://arxiv.org/abs/2504.12039</link>
<guid>https://arxiv.org/abs/2504.12039</guid>
<content:encoded><![CDATA[
arXiv:2504.12039v2 Announce Type: replace-cross 
Abstract: Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as Vision Transformer (ViT) and State-Space Model (SSM) architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: https://github.com/lab-emi/AIRHAR.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization: A Close Look at Books</title>
<link>https://arxiv.org/abs/2504.12549</link>
<guid>https://arxiv.org/abs/2504.12549</guid>
<content:encoded><![CDATA[
arXiv:2504.12549v2 Announce Type: replace-cross 
Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the "prefix-prompting" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification</title>
<link>https://arxiv.org/abs/2504.15041</link>
<guid>https://arxiv.org/abs/2504.15041</guid>
<content:encoded><![CDATA[
arXiv:2504.15041v3 Announce Type: replace-cross 
Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in preserving old knowledge while adapting to new information. The existing solutions include rehearsal-based and rehearsal-free methods to address this challenge. Rehearsal-based approaches rely on knowledge distillation, continuously accumulating forgetting during the distillation process. Rehearsal-free methods insufficiently learn the distribution of each domain, leading to forgetfulness over time. To solve these issues, we propose a novel Distribution-aware Forgetting Compensation (DAFC) model that explores cross-domain shared representation learning and domain-specific distribution integration without using old exemplars or knowledge distillation. We propose a Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich prompt elements and guide the prompt model to learn fine-grained representations for each instance. This can enhance the differentiation of identity information and establish the foundation for domain distribution awareness. Then, Distribution-based Awareness and Integration (DAI) is designed to capture each domain-specific distribution by a dedicated expert network and adaptively consolidate them into a shared region in high-dimensional space. In this manner, DAI can consolidate and enhance cross-domain shared representation learning while alleviating catastrophic forgetting. Furthermore, we develop a Knowledge Consolidation Mechanism (KCM) that comprises instance-level discrimination and cross-domain consistency alignment strategies to facilitate model adaptive learning of new knowledge from the current domain and promote knowledge consolidation learning between acquired domain-specific distributions, respectively. Experimental results show that our DAFC outperforms state-of-the-art methods. Our code is available at https://github.com/LiuShiBen/DAFC.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2504.16907</link>
<guid>https://arxiv.org/abs/2504.16907</guid>
<content:encoded><![CDATA[
arXiv:2504.16907v2 Announce Type: replace-cross 
Abstract: Text-to-video (T2V) generative models have rapidly advanced and found widespread applications across fields like entertainment, education, and marketing. However, the adversarial vulnerabilities of these models remain rarely explored. We observe that in T2V generation tasks, the generated videos often contain substantial redundant information not explicitly specified in the text prompts, such as environmental elements, secondary objects, and additional details, providing opportunities for malicious attackers to embed hidden harmful content. Exploiting this inherent redundancy, we introduce BadVideo, the first backdoor attack framework tailored for T2V generation. Our attack focuses on designing target adversarial outputs through two key strategies: (1) Spatio-Temporal Composition, which combines different spatiotemporal features to encode malicious information; (2) Dynamic Element Transformation, which introduces transformations in redundant elements over time to convey malicious information. Based on these strategies, the attacker's malicious target seamlessly integrates with the user's textual instructions, providing high stealthiness. Moreover, by exploiting the temporal dimension of videos, our attack successfully evades traditional content moderation systems that primarily analyze spatial information within individual frames. Extensive experiments demonstrate that BadVideo achieves high attack success rates while preserving original semantics and maintaining excellent performance on clean inputs. Overall, our work reveals the adversarial vulnerability of T2V models, calling attention to potential risks and misuse. Our project page is at https://wrt2000.github.io/BadVideo2025/.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemeBLIP2: A novel lightweight multimodal system to detect harmful memes</title>
<link>https://arxiv.org/abs/2504.21226</link>
<guid>https://arxiv.org/abs/2504.21226</guid>
<content:encoded><![CDATA[
arXiv:2504.21226v3 Announce Type: replace-cross 
Abstract: Memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. In this paper, we introduces MemeBLIP2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. We build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. Using BLIP-2 as the core vision-language model, our system is evaluated on the PrideMM datasets. The results show that MemeBLIP2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture</title>
<link>https://arxiv.org/abs/2505.00316</link>
<guid>https://arxiv.org/abs/2505.00316</guid>
<content:encoded><![CDATA[
arXiv:2505.00316v3 Announce Type: replace-cross 
Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate in vitro vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Visual Instruction</title>
<link>https://arxiv.org/abs/2505.00693</link>
<guid>https://arxiv.org/abs/2505.00693</guid>
<content:encoded><![CDATA[
arXiv:2505.00693v3 Announce Type: replace-cross 
Abstract: Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision introduces challenges for robotic task definition such as ambiguity and verbosity. Moreover, in some public settings where quiet is required, such as libraries or hospitals, verbal communication with robots is inappropriate. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment,enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Project website: https://robotic-visual-instruction.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Forgery Detection for Surveillance Cameras: A Review</title>
<link>https://arxiv.org/abs/2505.03832</link>
<guid>https://arxiv.org/abs/2505.03832</guid>
<content:encoded><![CDATA[
arXiv:2505.03832v2 Announce Type: replace-cross 
Abstract: The widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. Surveillance footage plays a crucial role in security, law enforcement, and judicial processes. However, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. Ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. This paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. Various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. The findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. Strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility</title>
<link>https://arxiv.org/abs/2505.06743</link>
<guid>https://arxiv.org/abs/2505.06743</guid>
<content:encoded><![CDATA[
arXiv:2505.06743v3 Announce Type: replace-cross 
Abstract: Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression</title>
<link>https://arxiv.org/abs/2505.07119</link>
<guid>https://arxiv.org/abs/2505.07119</guid>
<content:encoded><![CDATA[
arXiv:2505.07119v3 Announce Type: replace-cross 
Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact, efficient processing strategies. We evaluate several data compression techniques, examining the tradeoff between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data. Current results show up to 80% reduction in end-to-end inference time, including edge processing, transmission, and server computation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAL: Searching Expandable Architectures for Incremental Learning</title>
<link>https://arxiv.org/abs/2505.10457</link>
<guid>https://arxiv.org/abs/2505.10457</guid>
<content:encoded><![CDATA[
arXiv:2505.10457v2 Announce Type: replace-cross 
Abstract: Incremental learning is a machine learning paradigm where a model learns from a sequential stream of tasks. This setting poses a key challenge: balancing plasticity (learning new tasks) and stability (preserving past knowledge). Neural Architecture Search (NAS), a branch of AutoML, automates the design of the architecture of Deep Neural Networks and has shown success in static settings. However, existing NAS-based approaches to incremental learning often rely on expanding the model at every task, making them impractical in resource-constrained environments. In this work, we introduce SEAL, a NAS-based framework tailored for data-incremental learning, a scenario where disjoint data samples arrive sequentially and are not stored for future access. SEAL adapts the model structure dynamically by expanding it only when necessary, based on a capacity estimation metric. Stability is preserved through cross-distillation training after each expansion step. The NAS component jointly searches for both the architecture and the optimal expansion policy. Experiments across multiple benchmarks demonstrate that SEAL effectively reduces forgetting and enhances accuracy while maintaining a lower model size compared to prior methods. These results highlight the promise of combining NAS and selective expansion for efficient, adaptive learning in incremental scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio</title>
<link>https://arxiv.org/abs/2505.12572</link>
<guid>https://arxiv.org/abs/2505.12572</guid>
<content:encoded><![CDATA[
arXiv:2505.12572v2 Announce Type: replace-cross 
Abstract: A two stage novel generation framework (outline -> section outline -> manuscript) is widely used in long novel generation,(e.g., \textsc{DOME}, \textsc{Plan\&amp;Write}, \textsc{Long Writer}), but study of such framework in ultra long novel(>1M words) reconstruction is little. Building on recent text compression methods (\textsc{LLMZip}, \textsc{LLM2Vec}), we conduct an information-theoretic analysis to quantify semantic distortion under different compression-expansion ratios. We examine how outline length affects information preservation. Experiments on ultra-long novels show that the optimal compression-expansion ratio significantly reduces semantic distortion compared to other non-optimal compression-expansion ratio.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v2 Announce Type: replace-cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v4 Announce Type: replace-cross 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards</title>
<link>https://arxiv.org/abs/2505.16789</link>
<guid>https://arxiv.org/abs/2505.16789</guid>
<content:encoded><![CDATA[
arXiv:2505.16789v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) gain popularity, their vulnerability to adversarial attacks emerges as a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can inadvertently introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Vulnerability, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity across multiple experimental datasets. We then evaluate the adversarial robustness of these fine-tuned models, analyzing persona shifts and interpretability traits to understand how dataset factors contribute to attack success rates. Lastly, we explore causal relationships that offer new insights into adversarial defense strategies, highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_vulnerability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
arXiv:2505.17067v3 Announce Type: replace-cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands</title>
<link>https://arxiv.org/abs/2505.17137</link>
<guid>https://arxiv.org/abs/2505.17137</guid>
<content:encoded><![CDATA[
arXiv:2505.17137v2 Announce Type: replace-cross 
Abstract: Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
arXiv:2505.18227v2 Announce Type: replace-cross 
Abstract: In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy</title>
<link>https://arxiv.org/abs/2505.18687</link>
<guid>https://arxiv.org/abs/2505.18687</guid>
<content:encoded><![CDATA[
arXiv:2505.18687v2 Announce Type: replace-cross 
Abstract: We derive the first closed-form condition under which artificial intelligence (AI) capital profits could sustainably finance a universal basic income (UBI) without additional taxes or new job creation. In a Solow-Zeira economy characterized by a continuum of automatable tasks, a constant net saving rate $s$, and task-elasticity $\sigma < 1$, we analyze how the AI capability threshold--defined as the productivity level of AI relative to pre-AI automation--varies under different economic scenarios. At present economic parameters, we find that AI systems must achieve only approximately 5-6 times existing automation productivity to finance an 11%-of-GDP UBI, in the worst case situation where *no* new jobs or tasks are created.
  Our analysis also reveals some specific policy levers: raising public revenue share (e.g. profit taxation) of AI capital from the current 15% to about 33% halves the required AI capability threshold to attain UBI to 3 times existing automotion productivity, but gains diminish beyond 50% public revenue share, especially if regulatory costs increase. Market structure also strongly affects outcomes: monopolistic or concentrated oligopolistic markets reduce the threshold by increasing economic rents, whereas heightened competition significantly raises it.
  Overall, these results suggest a couple policy recommendations: maximizing public revenue share up to a point so that operating costs are minimized, and strategically managing market competition can ensure AI's growing capabilities translate into meaningful social benefits within realistic technological progress scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization</title>
<link>https://arxiv.org/abs/2505.18975</link>
<guid>https://arxiv.org/abs/2505.18975</guid>
<content:encoded><![CDATA[
arXiv:2505.18975v4 Announce Type: replace-cross 
Abstract: State Space Models (SSMs), like recent Mamba2, have achieved remarkable performance and received extensive attention. However, deploying Mamba2 on resource-constrained edge devices encounters many problems: severe outliers within the linear layer challenging the quantization, diverse and irregular element-wise tensor operations, and hardware-unfriendly nonlinear functions in the SSM block. To address these issues, this paper presents FastMamba, a dedicated accelerator on FPGA with hardware-algorithm co-design to promote the deployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit quantization for linear layers through Hadamard transformation to eliminate outliers. Moreover, a hardware-friendly and fine-grained power-of-two quantization framework is presented for the SSM block and convolution layer, and a first-order linear approximation is developed to optimize the nonlinear functions. Based on the accurate algorithm quantization, we propose an accelerator that integrates parallel vector processing units, pipelined execution dataflow, and an efficient SSM Nonlinear Approximation Unit, which enhances computational efficiency and reduces hardware complexity. Finally, we evaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on Mamba2-130M, FastMamba achieves 68.80\times and 8.90\times speedup over Intel Xeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode experiment with Mamba2-2.7B, FastMamba attains 6\times higher energy efficiency than RTX 3090 GPU.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph</title>
<link>https://arxiv.org/abs/2505.20308</link>
<guid>https://arxiv.org/abs/2505.20308</guid>
<content:encoded><![CDATA[
arXiv:2505.20308v2 Announce Type: replace-cross 
Abstract: Metal additive manufacturing (AM) involves complex interdependencies among processes, materials, feedstock, and post-processing steps. However, the underlying relationships and domain knowledge remain fragmented across literature and static databases that often require expert-level queries, limiting their applicability in design and planning. To address these limitations, we develop a novel and structured knowledge graph (KG), representing 53 distinct metals and alloys across seven material categories, nine AM processes, four feedstock types, and corresponding post-processing requirements. A large language model (LLM) interface, guided by a few-shot prompting strategy, enables natural language querying without the need for formal query syntax. The system supports a range of tasks, including compatibility evaluation, constraint-based filtering, and design for AM (DfAM) guidance. User queries in natural language are normalized, translated into Cypher, and executed on the KG, with results returned in a structured format. This work introduces the first interactive system that connects a domain-specific metal AM KG with an LLM interface, delivering accessible and explainable decision support for engineers and promoting human-centered tools in manufacturing knowledge systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.23017</link>
<guid>https://arxiv.org/abs/2505.23017</guid>
<content:encoded><![CDATA[
arXiv:2505.23017v3 Announce Type: replace-cross 
Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in decision-making across various fields, including economics, energy, and transportation. Most existing methods excell at short-term forecasting, while overlooking the hurdles of Long-term Probabilistic Time Series Forecasting (LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have a significant adverse effect on prediction accuracy, and make generative models inefficient by increasing the cost of each iteration. To overcome these limitations, we introduce $K^2$VAE, an efficient VAE-based generative model that leverages a KoopmanNet to transform nonlinear time series into a linear dynamical system, and devises a KalmanNet to refine predictions and model uncertainty in such linear system, which reduces error accumulation in long-term forecasting. Extensive experiments demonstrate that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, providing a more efficient and accurate solution.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing</title>
<link>https://arxiv.org/abs/2505.23145</link>
<guid>https://arxiv.org/abs/2505.23145</guid>
<content:encoded><![CDATA[
arXiv:2505.23145v4 Announce Type: replace-cross 
Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose {\em FlowAlign}, a novel inversion-free flow-based framework for consistent image editing with optimal control-based trajectory control. Specifically, FlowAlign introduces source similarity at the terminal point as a regularization term to promote smoother and more consistent trajectories during the editing process. Notably, our terminal point regularization is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highliting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering</title>
<link>https://arxiv.org/abs/2505.24371</link>
<guid>https://arxiv.org/abs/2505.24371</guid>
<content:encoded><![CDATA[
arXiv:2505.24371v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription (Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates in two phases. First, extracting text transcripts from video frames using a Vision-Language Model (VLM). Next, processing questions using these transcripts to generate answers through a Large Language Model (LLM). This design ensures image privacy by deploying the VLM on edge devices and the LLM in the cloud. To improve transcript quality, we propose grid-based visual prompting, which extracts intricate local details from each grid cell and integrates them with global information. Evaluation results show that Grid-LoGAT, using the open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our method surpasses the non-grid version by 24 points on localization-based questions we created using NExT-QA. (This paper is accepted by IEEE ICIP 2025.)
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2506.00831</link>
<guid>https://arxiv.org/abs/2506.00831</guid>
<content:encoded><![CDATA[
arXiv:2506.00831v2 Announce Type: replace-cross 
Abstract: Existing threat modeling frameworks related to transportation cyber-physical systems (CPS) are often narrow in scope, labor-intensive, and require substantial cybersecurity expertise. To this end, we introduce the Transportation Cybersecurity and Resiliency Threat Modeling Framework (TraCR-TMF), a large language model (LLM)-based threat modeling framework for transportation CPS that requires limited cybersecurity expert intervention. TraCR-TMF identifies threats, potential attack techniques, and relevant countermeasures for transportation CPS. Three LLM-based approaches support these identifications: (i) a retrieval-augmented generation approach requiring no cybersecurity expert intervention, (ii) an in-context learning approach with low expert intervention, and (iii) a supervised fine-tuning approach with moderate expert intervention. TraCR-TMF offers LLM-based attack path identification for critical assets based on vulnerabilities across transportation CPS entities. Additionally, it incorporates the Common Vulnerability Scoring System (CVSS) scores of known exploited vulnerabilities to prioritize threat mitigations. The framework was evaluated through two cases. First, the framework identified relevant attack techniques for various transportation CPS applications, 73% of which were validated by cybersecurity experts as correct. Second, the framework was used to identify attack paths for a target asset in a real-world cyberattack incident. TraCR-TMF successfully predicted exploitations, like lateral movement of adversaries, data exfiltration, and data encryption for ransomware, as reported in the incident. These findings show the efficacy of TraCR-TMF in transportation CPS threat modeling, while reducing the need for extensive involvement of cybersecurity experts. To facilitate real-world adoptions, all our codes are shared via an open-source repository.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Analytic Gradients in Provably Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.01665</link>
<guid>https://arxiv.org/abs/2506.01665</guid>
<content:encoded><![CDATA[
arXiv:2506.01665v2 Announce Type: replace-cross 
Abstract: The deployment of autonomous robots in safety-critical applications requires safety guarantees. Provably safe reinforcement learning is an active field of research that aims to provide such guarantees using safeguards. These safeguards should be integrated during training to reduce the sim-to-real gap. While there are several approaches for safeguarding sampling-based reinforcement learning, analytic gradient-based reinforcement learning often achieves superior performance from fewer environment interactions. However, there is no safeguarding approach for this learning paradigm yet. Our work addresses this gap by developing the first effective safeguard for analytic gradient-based reinforcement learning. We analyse existing, differentiable safeguards, adapt them through modified mappings and gradient formulations, and integrate them with a state-of-the-art learning algorithm and a differentiable simulation. Using numerical experiments on three control tasks, we evaluate how different safeguards affect learning. The results demonstrate safeguarded training without compromising performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Spectral Band Generation for Audio Coding</title>
<link>https://arxiv.org/abs/2506.06732</link>
<guid>https://arxiv.org/abs/2506.06732</guid>
<content:encoded><![CDATA[
arXiv:2506.06732v2 Announce Type: replace-cross 
Abstract: Spectral band replication (SBR) enables bit-efficient coding by generating high-frequency bands from the low-frequency ones. However, it only utilizes coarse spectral features upon a subband-wise signal replication, limiting adaptability to diverse acoustic signals. In this paper, we explore the efficacy of a deep neural network (DNN)-based generative approach for coding the high-frequency bands, which we call neural spectral band generation (n-SBG). Specifically, we propose a DNN-based encoder-decoder structure to extract and quantize the side information related to the high-frequency components and generate the components given both the side information and the decoded core-band signals. The whole coding pipeline is optimized with generative adversarial criteria to enable the generation of perceptually plausible sound. From experiments using AAC as the core codec, we show that the proposed method achieves a better perceptual quality than HE-AAC-v1 with much less side information.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2506.09853</link>
<guid>https://arxiv.org/abs/2506.09853</guid>
<content:encoded><![CDATA[
arXiv:2506.09853v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation</title>
<link>https://arxiv.org/abs/2506.10235</link>
<guid>https://arxiv.org/abs/2506.10235</guid>
<content:encoded><![CDATA[
arXiv:2506.10235v2 Announce Type: replace-cross 
Abstract: Automation of analog topology design is crucial due to customized requirements of modern applications with heavily manual engineering efforts. The state-of-the-art work applies a sequence-to-sequence approach and supervised finetuning on language models to generate topologies given user specifications. However, its circuit formulation is inefficient due to O(|V |2) token length and suffers from low precision sensitivity to numeric inputs. In this work, we introduce LaMAGIC2, a succinct float-input canonical formulation with identifier (SFCI) for language model-based analog topology generation. SFCI addresses these challenges by improving component-type recognition through identifier-based representations, reducing token length complexity to O(|V |), and enhancing numeric precision sensitivity for better performance under tight tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a prior method. LaMAGIC2 also exhibits better transferability for circuits with more vertices with up to 58.5% improvement. These advancements establish LaMAGIC2 as a robust framework for analog topology generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective</title>
<link>https://arxiv.org/abs/2506.12327</link>
<guid>https://arxiv.org/abs/2506.12327</guid>
<content:encoded><![CDATA[
arXiv:2506.12327v2 Announce Type: replace-cross 
Abstract: An increasing number of studies have examined the social bias of rapidly developed large language models (LLMs). Although most of these studies have focused on bias occurring in a single social attribute, research in social science has shown that social bias often occurs in the form of intersectionality -- the constitutive and contextualized perspective on bias aroused by social attributes. In this study, we construct the Japanese benchmark inter-JBBQ, designed to evaluate the intersectional bias in LLMs on the question-answering setting. Using inter-JBBQ to analyze GPT-4o and Swallow, we find that biased output varies according to its contexts even with the equal combination of social attributes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Levels of Autonomy for AI Agents</title>
<link>https://arxiv.org/abs/2506.12469</link>
<guid>https://arxiv.org/abs/2506.12469</guid>
<content:encoded><![CDATA[
arXiv:2506.12469v2 Announce Type: replace-cross 
Abstract: Autonomy is a double-edged sword for AI agents, simultaneously unlocking transformative possibilities and serious risks. How can agent developers calibrate the appropriate levels of autonomy at which their agents should operate? We argue that an agent's level of autonomy can be treated as a deliberate design decision, separate from its capability and operational environment. In this work, we define five levels of escalating agent autonomy, characterized by the roles a user can take when interacting with an agent: operator, collaborator, consultant, approver, and observer. Within each level, we describe the ways by which a user can exert control over the agent and open questions for how to design the nature of user-agent interaction. We then highlight a potential application of our framework towards AI autonomy certificates to govern agent behavior in single- and multi-agent systems. We conclude by proposing early ideas for evaluating agents' autonomy. Our work aims to contribute meaningful, practical steps towards responsibly deployed and useful AI agents in the real world.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Safety Shielding for Imperfect-Perception Agents</title>
<link>https://arxiv.org/abs/2506.17275</link>
<guid>https://arxiv.org/abs/2506.17275</guid>
<content:encoded><![CDATA[
arXiv:2506.17275v2 Announce Type: replace-cross 
Abstract: We consider the problem of safe control in discrete autonomous agents that use learned components for imperfect perception (or more generally, state estimation) from high-dimensional observations. We propose a shield construction that provides run-time safety guarantees under perception errors by restricting the actions available to an agent, modeled as a Markov decision process, as a function of the state estimates. Our construction uses conformal prediction for the perception component, which guarantees that for each observation, the predicted set of estimates includes the actual state with a user-specified probability. The shield allows an action only if it is allowed for all the estimates in the predicted set, resulting in local safety. We also articulate and prove a global safety property of existing shield constructions for perfect-perception agents bounding the probability of reaching unsafe states if the agent always chooses actions prescribed by the shield. We illustrate our approach with a case-study of an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Model Integration with Open World Temporal Logic for Process Automation</title>
<link>https://arxiv.org/abs/2506.17776</link>
<guid>https://arxiv.org/abs/2506.17776</guid>
<content:encoded><![CDATA[
arXiv:2506.17776v2 Announce Type: replace-cross 
Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models capable of extracting structured information from diverse and complex data sources. However, a significant challenge lies in translating these perceptual or extractive outputs into actionable, reasoned decisions within complex operational workflows. To address these challenges, this paper introduces a novel approach that integrates the outputs from various machine learning models directly with the PyReason framework, an open-world temporal logic programming reasoning engine. PyReason's foundation in generalized annotated logic allows for the seamless incorporation of real-valued outputs (e.g., probabilities, confidence scores) from diverse ML models, treating them as truth intervals within its logical framework. Crucially, PyReason provides mechanisms, implemented in Python, to continuously poll ML model outputs, convert them into logical facts, and dynamically recompute the minimal model, ensuring real-tine adaptive decision-making. Furthermore, its native support for temporal reasoning, knowledge graph integration, and fully explainable interface traces enables sophisticated analysis over time-sensitive process data and existing organizational knowledge. By combining the strengths of perception and extraction from ML models with the logical deduction and transparency of PyReason, we aim to create a powerful system for automating complex processes. This integration finds utility across numerous domains, including manufacturing, healthcare, and business operations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADA-DPM: A Neural Descriptors-based Adaptive Noise Filtering Strategy for SLAM</title>
<link>https://arxiv.org/abs/2506.18016</link>
<guid>https://arxiv.org/abs/2506.18016</guid>
<content:encoded><![CDATA[
arXiv:2506.18016v2 Announce Type: replace-cross 
Abstract: Lidar SLAM plays a significant role in mobile robot navigation and high-definition map construction. However, existing methods often face a trade-off between localization accuracy and system robustness in scenarios with a high proportion of dynamic objects, point cloud distortion, and unstructured environments. To address this issue, we propose a neural descriptors-based adaptive noise filtering strategy for SLAM, named ADA-DPM, which improves the performance of localization and mapping tasks through three key technical innovations. Firstly, to tackle dynamic object interference, we design the Dynamic Segmentation Head to predict and filter out dynamic feature points, eliminating the ego-motion interference caused by dynamic objects. Secondly, to mitigate the impact of noise and unstructured feature points, we propose the Global Importance Scoring Head that adaptively selects high-contribution feature points while suppressing the influence of noise and unstructured feature points. Moreover, we introduce the Cross-Layer Graph Convolution Module (GLI-GCN) to construct multi-scale neighborhood graphs, fusing local structural information across different scales and improving the discriminative power of overlapping features. Finally, experimental validations on multiple public datasets confirm the effectiveness of ADA-DPM.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning fine-tuning of language model for instruction following and math reasoning</title>
<link>https://arxiv.org/abs/2506.21560</link>
<guid>https://arxiv.org/abs/2506.21560</guid>
<content:encoded><![CDATA[
arXiv:2506.21560v2 Announce Type: replace-cross 
Abstract: This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epitome: Pioneering an Experimental Platform for AI-Social Science Integration</title>
<link>https://arxiv.org/abs/2507.01061</link>
<guid>https://arxiv.org/abs/2507.01061</guid>
<content:encoded><![CDATA[
arXiv:2507.01061v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crop Pest Classification Using Deep Learning Techniques: A Review</title>
<link>https://arxiv.org/abs/2507.01494</link>
<guid>https://arxiv.org/abs/2507.01494</guid>
<content:encoded><![CDATA[
arXiv:2507.01494v2 Announce Type: replace-cross 
Abstract: Insect pests continue to bring a serious threat to crop yields around the world, and traditional methods for monitoring them are often slow, manual, and difficult to scale. In recent years, deep learning has emerged as a powerful solution, with techniques like convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid models gaining popularity for automating pest detection. This review looks at 37 carefully selected studies published between 2018 and 2025, all focused on AI-based pest classification. The selected research is organized by crop type, pest species, model architecture, dataset usage, and key technical challenges. The early studies relied heavily on CNNs but latest work is shifting toward hybrid and transformer-based models that deliver higher accuracy and better contextual understanding. Still, challenges like imbalanced datasets, difficulty in detecting small pests, limited generalizability, and deployment on edge devices remain significant hurdles. Overall, this review offers a structured overview of the field, highlights useful datasets, and outlines the key challenges and future directions for AI-based pest monitoring systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts</title>
<link>https://arxiv.org/abs/2507.04270</link>
<guid>https://arxiv.org/abs/2507.04270</guid>
<content:encoded><![CDATA[
arXiv:2507.04270v2 Announce Type: replace-cross 
Abstract: Foundation models have revolutionized AI, yet they struggle with zero-shot deployment in real-world industrial settings due to a lack of high-quality, domain-specific datasets. To bridge this gap, Superb AI introduces ZERO, an industry-ready vision foundation model that leverages multi-modal prompting (textual and visual) for generalization without retraining. Trained on a compact yet representative 0.9 million annotated samples from a proprietary billion-scale industrial dataset, ZERO demonstrates competitive performance on academic benchmarks like LVIS-Val and significantly outperforms existing models across 37 diverse industrial datasets. Furthermore, ZERO achieved 2nd place in the CVPR 2025 Object Instance Detection Challenge and 4th place in the Foundational Few-shot Object Detection Challenge, highlighting its practical deployability and generalizability with minimal adaptation and limited data. To the best of our knowledge, ZERO is the first vision foundation model explicitly built for domain-specific, zero-shot industrial applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tractable Representation Learning with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2507.04385</link>
<guid>https://arxiv.org/abs/2507.04385</guid>
<content:encoded><![CDATA[
arXiv:2507.04385v2 Announce Type: replace-cross 
Abstract: Probabilistic circuits (PCs) are powerful probabilistic models that enable exact and tractable inference, making them highly suitable for probabilistic reasoning and inference tasks. While dominant in neural networks, representation learning with PCs remains underexplored, with prior approaches relying on external neural embeddings or activation-based encodings. To address this gap, we introduce autoencoding probabilistic circuits (APCs), a novel framework leveraging the tractability of PCs to model probabilistic embeddings explicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining embedding representations through tractable probabilistic inference. The PC encoder allows the framework to natively handle arbitrary missing data and is seamlessly integrated with a neural decoder in a hybrid, end-to-end trainable architecture enabled by differentiable sampling. Our empirical evaluation demonstrates that APCs outperform existing PC-based autoencoding methods in reconstruction quality, generate embeddings competitive with, and exhibit superior robustness in handling missing data compared to neural autoencoders. These results highlight APCs as a powerful and flexible representation learning method that exploits the probabilistic inference capabilities of PCs, showing promising directions for robust inference, out-of-distribution detection, and knowledge distillation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations</title>
<link>https://arxiv.org/abs/2507.04886</link>
<guid>https://arxiv.org/abs/2507.04886</guid>
<content:encoded><![CDATA[
arXiv:2507.04886v2 Announce Type: replace-cross 
Abstract: Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critiques of World Models</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
arXiv:2507.05169v3 Announce Type: replace-cross 
Abstract: World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning</title>
<link>https://arxiv.org/abs/2507.06821</link>
<guid>https://arxiv.org/abs/2507.06821</guid>
<content:encoded><![CDATA[
arXiv:2507.06821v3 Announce Type: replace-cross 
Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays a significant role in human-computer interaction (HCI) in recent years. Since different discrete emotions may exist at the same time, compared with single-class emotion recognition, emotion distribution learning (EDL) that identifies a mixture of basic emotions has gradually emerged as a trend. However, existing EDL methods face challenges in mining the heterogeneity among multiple modalities. Besides, rich semantic correlations across arbitrary basic emotions are not fully exploited. In this paper, we propose a multi-modal emotion distribution learning framework, named HeLo, aimed at fully exploring the heterogeneity and complementary information in multi-modal emotional data and label correlation within mixed basic emotions. Specifically, we first adopt cross-attention to effectively fuse the physiological data. Then, an optimal transport (OT)-based heterogeneity mining module is devised to mine the interaction and heterogeneity between the physiological and behavioral representations. To facilitate label correlation learning, we introduce a learnable label embedding optimized by correlation matrix alignment. Finally, the learnable label embeddings and label correlation matrices are integrated with the multi-modal representations through a novel label correlation-driven cross-attention mechanism for accurate emotion distribution learning. Experimental results on two publicly available datasets demonstrate the superiority of our proposed method in emotion distribution learning.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation</title>
<link>https://arxiv.org/abs/2507.09108</link>
<guid>https://arxiv.org/abs/2507.09108</guid>
<content:encoded><![CDATA[
arXiv:2507.09108v2 Announce Type: replace-cross 
Abstract: High-quality labeled datasets are crucial for training and evaluating foundation models in software engineering, but creating them is often prohibitively expensive and labor-intensive. We introduce SPICE, a scalable, automated pipeline for labeling SWE-bench-style datasets with annotations for issue clarity, test coverage, and effort estimation. SPICE combines context-aware code navigation, rationale-driven prompting, and multi-pass consensus to produce labels that closely approximate expert annotations. SPICE's design was informed by our own experience and frustration in labeling more than 800 instances from SWE-Gym. SPICE achieves strong agreement with human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000 instances from around $100,000 (manual annotation) to just $5.10. These results demonstrate SPICE's potential to enable cost-effective, large-scale dataset creation for SE-focused FMs. To support the community, we release both SPICE tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench Verified).
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When and Where do Data Poisons Attack Textual Inversion?</title>
<link>https://arxiv.org/abs/2507.10578</link>
<guid>https://arxiv.org/abs/2507.10578</guid>
<content:encoded><![CDATA[
arXiv:2507.10578v3 Announce Type: replace-cross 
Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion models (DMs). In this paper, we systematically analyze when and where poisoning attacks textual inversion (TI), a widely used personalization technique for DMs. We first introduce Semantic Sensitivity Maps, a novel method for visualizing the influence of poisoning on text embeddings. Second, we identify and experimentally verify that DMs exhibit non-uniform learning behavior across timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias and inject adversarial signals predominantly at lower timesteps. Lastly, we observe that adversarial signals distract learning away from relevant concept regions within training data, corrupting the TI process. Based on these insights, we propose Safe-Zone Training (SZT), a novel defense mechanism comprised of 3 key components: (1) JPEG compression to weaken high-frequency poison signals, (2) restriction to high timesteps during TI training to avoid adversarial signals at lower timesteps, and (3) loss masking to constrain learning to relevant regions. Extensive experiments across multiple poisoning methods demonstrate that SZT greatly enhances the robustness of TI against all poisoning attacks, improving generative quality beyond prior published defenses. Code: www.github.com/JStyborski/Diff_Lab Data: www.github.com/JStyborski/NC10
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search</title>
<link>https://arxiv.org/abs/2507.11549</link>
<guid>https://arxiv.org/abs/2507.11549</guid>
<content:encoded><![CDATA[
arXiv:2507.11549v2 Announce Type: replace-cross 
Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in computer vision tasks by adaptively focusing on informative image regions. However, their data-dependent sampling mechanism introduces irregular memory access patterns, posing significant challenges for efficient hardware deployment. Existing acceleration methods either incur high hardware overhead or compromise model accuracy. To address these issues, this paper proposes a hardware-friendly optimization framework for DAT. First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. The method explores the optimal slice configuration by jointly optimizing hardware cost and inference accuracy. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware. Algorithm experiments on the ImageNet-1K dataset demonstrate that our hardware-friendly framework can maintain have only 0.2% accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA show the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing</title>
<link>https://arxiv.org/abs/2507.12060</link>
<guid>https://arxiv.org/abs/2507.12060</guid>
<content:encoded><![CDATA[
arXiv:2507.12060v2 Announce Type: replace-cross 
Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at https://kunkunlin1221.github.io/InstructFLIP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v2 Announce Type: replace-cross 
Abstract: Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce Video Diffusion for Action Reasoning (Vidar), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), Vidar generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models</title>
<link>https://arxiv.org/abs/2507.13993</link>
<guid>https://arxiv.org/abs/2507.13993</guid>
<content:encoded><![CDATA[
arXiv:2507.13993v2 Announce Type: replace-cross 
Abstract: The growing volume of medical imaging data has increased the need for automated diagnostic tools, especially for musculoskeletal injuries like rib fractures, commonly detected via CT scans. Manual interpretation is time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep learning framework for rib fracture diagnosis and report generation. It integrates a YOLOv9 model for fracture detection, a medical knowledge graph for retrieving clinical context, and a fine-tuned LLaVA language model for generating diagnostic reports. OrthoInsight combines visual features from CT images with expert textual data to deliver clinically useful outputs. Evaluated on 28,675 annotated CT images and expert reports, it achieves high performance across Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value, with an average score of 4.28, outperforming models like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal learning in transforming medical image analysis and providing effective support for radiologists.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
arXiv:2507.15846v3 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G$^2$, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technological folie \`a deux: Feedback Loops Between AI Chatbots and Mental Illness</title>
<link>https://arxiv.org/abs/2507.19218</link>
<guid>https://arxiv.org/abs/2507.19218</guid>
<content:encoded><![CDATA[
arXiv:2507.19218v2 Announce Type: replace-cross 
Abstract: Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capacity-constrained mental health services. While some users report psychological benefits, concerning edge cases are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots. To understand this new risk profile we need to consider the interaction between human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy) and adaptability (in-context learning). We argue that individuals with mental health conditions face increased risks of chatbot-induced belief destabilization and dependence, owing to altered belief-updating, impaired reality-testing, and social isolation. Current AI safety measures are inadequate to address these interaction-based risks. To address this emerging public health concern, we need coordinated action across clinical practice, AI development, and regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initial Steps in Integrating Large Reasoning and Action Models for Service Composition</title>
<link>https://arxiv.org/abs/2507.18775</link>
<guid>https://arxiv.org/abs/2507.18775</guid>
<content:encoded><![CDATA[
<div> Keywords: service composition, large language models, reasoning models, action models, automated 

Summary: 
Service composition in software systems is a challenge often limited by reasoning capabilities and execution mechanisms. This paper proposes integrating Large Reasoning Models (LRMs) and Large Action Models (LAMs, enabled by large language models, to address these challenges. LRMs handle semantic reasoning and ecosystem complexity, while LAMs excel in dynamic action execution and interoperability. By combining these paradigms into an integrated LRM-LAM framework, automated service composition can bridge the gap between intention and execution. This approach allows systems to reason about service requirements and constraints while dynamically executing workflows. Ultimately, this integration aims to transform service composition into a fully automated, user-friendly process driven by high-level natural language intent.<br /><br />Summary: <div>
arXiv:2507.18775v1 Announce Type: new 
Abstract: Service composition remains a central challenge in building adaptive and intelligent software systems, often constrained by limited reasoning capabilities or brittle execution mechanisms. This paper explores the integration of two emerging paradigms enabled by large language models: Large Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs address the challenges of semantic reasoning and ecosystem complexity while LAMs excel in dynamic action execution and system interoperability. However, each paradigm has complementary limitations - LRMs lack grounded action capabilities, and LAMs often struggle with deep reasoning. We propose an integrated LRM-LAM architectural framework as a promising direction for advancing automated service composition. Such a system can reason about service requirements and constraints while dynamically executing workflows, thus bridging the gap between intention and execution. This integration has the potential to transform service composition into a fully automated, user-friendly process driven by high-level natural language intent.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization</title>
<link>https://arxiv.org/abs/2507.18795</link>
<guid>https://arxiv.org/abs/2507.18795</guid>
<content:encoded><![CDATA[
<div> queueing network systems, reinforcement learning, simulation-driven, manufacturing, communication <br />
<br />
Summary: 
This study presents a simulation-driven reinforcement learning framework for optimizing routing decisions in complex queueing network systems, focusing on manufacturing and communication applications. The framework utilizes Deep Deterministic Policy Gradient (DDPG) with Dyna-style planning (Dyna-DDPG) to address the limitations of traditional queueing methods in dynamic and uncertain environments. The Dyna-DDPG implementation includes separate predictive models for next-state transitions and rewards, enhancing stability and sample efficiency. Through comprehensive experiments, the framework demonstrates the ability to rapidly learn effective routing policies that are resilient to disruptions and scalable to larger network sizes. The study also emphasizes strong software engineering practices to ensure reproducibility and maintainability for practical deployment in real-world scenarios. <div>
arXiv:2507.18795v1 Announce Type: new 
Abstract: This study focuses on the development of a simulation-driven reinforcement learning (RL) framework for optimizing routing decisions in complex queueing network systems, with a particular emphasis on manufacturing and communication applications. Recognizing the limitations of traditional queueing methods, which often struggle with dynamic, uncertain environments, we propose a robust RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with Dyna-style planning (Dyna-DDPG). The framework includes a flexible and configurable simulation environment capable of modeling diverse queueing scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG implementation incorporates separate predictive models for next-state transitions and rewards, significantly improving stability and sample efficiency. Comprehensive experiments and rigorous evaluations demonstrate the framework's capability to rapidly learn effective routing policies that maintain robust performance under disruptions and scale effectively to larger network sizes. Additionally, we highlight strong software engineering practices employed to ensure reproducibility and maintainability of the framework, enabling practical deployment in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neuroscience-Inspired Dual-Process Model of Compositional Generalization</title>
<link>https://arxiv.org/abs/2507.18868</link>
<guid>https://arxiv.org/abs/2507.18868</guid>
<content:encoded><![CDATA[
<div> generalization, AI systems, compositional tasks, hippocampus, prefrontal cortex

Summary:
The article introduces MIRAGE, a framework inspired by how the human brain achieves systematic compositional generalization through the interplay of the hippocampus and prefrontal cortex. MIRAGE consists of two modules: a Neural Decomposer and a Schema Engine, mimicking the brain's deliberative and intuitive processes. The Neural Decomposer, akin to neocortical computation, iteratively refines sequence representations using decompositions. The Schema Engine emulates the hippocampus-prefrontal cortex loop by extracting and applying reusable schemas from episodic memory. By incorporating schematic structures into the Transformer component, MIRAGE demonstrates systematic compositional generalization on the SCAN benchmark with high accuracy. Ablation studies emphasize the importance of extracted schemas and iterative refinement in achieving systematicity. MIRAGE's success lies in its ability to apply and transform explicit schemas to solve entirely novel tasks, showcasing its effectiveness in systematic generalization tasks with minimal parameters. 

<br /><br />Summary: <div>
arXiv:2507.18868v1 Announce Type: new 
Abstract: Systematic compositional generalization - constructing and understanding novel combinations of known building blocks - remains a core challenge for AI systems. Human cognition achieves this flexibility via the interplay of the hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes episodes, and the prefrontal cortex consolidates them into reusable schemas for reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with Rules and Abstractions from Generalized Experience), a framework that achieves systematic generalization on compositional tasks. MIRAGE has two interacting modules mirroring the brain's deliberative HPC-PFC loop and intuitive neocortical pattern recognition. (1) The meta-trained Transformer Neural Decomposer, paralleling neocortical "System 1" computation, is trained on a task-agnostic stream of randomly sampled compositional grammars and applies one decomposition step per pass, with successive passes iteratively refining the sequence representation. (2) The Schema Engine, analogous to the HPC-PFC "System 2" loop, dynamically extracts, ranks, and applies reusable schemas, storing variable bindings in episodic memory and expanding them when needed. By explicitly equipping the Transformer component of MIRAGE with actively managed schematic structures, our model performs systematic compositional operations through explicit schema application and transformation, relying solely on frozen weights when solving entirely novel tasks. This approach demonstrates systematic compositional generalization on the SCAN benchmark, achieving > 99% accuracy on all task splits with only 1.19M parameters in the transformer module. Ablation studies confirm that MIRAGE's systematicity critically depends on the quality of extracted schemas and the model's iterative refinement process.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Success in Humanoid Reinforcement Learning under Partial Observation</title>
<link>https://arxiv.org/abs/2507.18883</link>
<guid>https://arxiv.org/abs/2507.18883</guid>
<content:encoded><![CDATA[
<div> history encoder, reinforcement learning, humanoid locomotion, partial observability, policy learning

Summary:
This research introduces a novel history encoder that enables stable training of humanoid policies in the Gymnasium Humanoid-v4 environment with incomplete state information. The environment objective is to walk forward without falling, and the learned policy achieves comparable performance to full state access methods despite utilizing only a subset of the original states. The policy demonstrates adaptability to variations in robot properties, such as body part masses. The key to success lies in the history encoder, which processes a fixed-length sequence of past observations in parallel. Integrated into a model-free algorithm, the encoder ensures robust decision-making by reconstructing essential contextual information from recent observations. This breakthrough shows promise for effective policy learning in high-dimensional tasks like robotic control under partial observability. <br /><br />Summary: <div>
arXiv:2507.18883v1 Announce Type: new 
Abstract: Reinforcement learning has been widely applied to robotic control, but effective policy learning under partial observability remains a major challenge, especially in high-dimensional tasks like humanoid locomotion. To date, no prior work has demonstrated stable training of humanoid policies with incomplete state information in the benchmark Gymnasium Humanoid-v4 environment. The objective in this environment is to walk forward as fast as possible without falling, with rewards provided for staying upright and moving forward, and penalties incurred for excessive actions and external contact forces. This research presents the first successful instance of learning under partial observability in this environment. The learned policy achieves performance comparable to state-of-the-art results with full state access, despite using only one-third to two-thirds of the original states. Moreover, the policy exhibits adaptability to robot properties, such as variations in body part masses. The key to this success is a novel history encoder that processes a fixed-length sequence of past observations in parallel. Integrated into a standard model-free algorithm, the encoder enables performance on par with fully observed baselines. We hypothesize that it reconstructs essential contextual information from recent observations, thereby enabling robust decision-making.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling</title>
<link>https://arxiv.org/abs/2507.18977</link>
<guid>https://arxiv.org/abs/2507.18977</guid>
<content:encoded><![CDATA[
<div> incremental training, Temporal Knowledge Graph, entity similarity, weighted sampling, link prediction <br />
Summary:<br />
This paper introduces an incremental training framework for Temporal Knowledge Graphs (TKGs) to address challenges posed by evolving TKGs. The framework enhances existing TKG completion models by incorporating an enhancement layer and a weighted sampling strategy. The enhancement layer defines entity similarity on a global scale, going beyond local neighborhood proximity used by traditional methods. The weighted sampling strategy focuses on edges related to infrequently occurring entities during training. Experimental results on benchmark datasets show that the proposed framework outperforms existing methods in total link prediction, inductive link prediction, and handling long-tail entities. The method achieves a significant improvement in Mean Reciprocal Rank (MRR) on these datasets, indicating its potential in mitigating catastrophic forgetting and improving the robustness of TKG completion models, especially in scenarios where new entities or sparse connections are involved. <br />Summary: <div>
arXiv:2507.18977v1 Announce Type: new 
Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access to the entire graph during training. This overlooks challenges stemming from the evolving nature of TKGs, such as: (i) the model's requirement to generalize and assimilate new knowledge, and (ii) the task of managing new or unseen entities that often have sparse connections. In this paper, we present an incremental training framework specifically designed for TKGs, aiming to address entities that are either not observed during training or have sparse connections. Our approach combines a model-agnostic enhancement layer with a weighted sampling strategy, that can be augmented to and improve any existing TKG completion method. The enhancement layer leverages a broader, global definition of entity similarity, which moves beyond mere local neighborhood proximity of GNN-based methods. The weighted sampling strategy employed in training accentuates edges linked to infrequently occurring entities. We evaluate our method on two benchmark datasets, and demonstrate that our framework outperforms existing methods in total link prediction, inductive link prediction, and in addressing long-tail entities. Notably, our method achieves a 10\% improvement and a 15\% boost in MRR for these datasets. The results underscore the potential of our approach in mitigating catastrophic forgetting and enhancing the robustness of TKG completion methods, especially in an incremental training context
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation</title>
<link>https://arxiv.org/abs/2507.19089</link>
<guid>https://arxiv.org/abs/2507.19089</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained traffic management, lane-level traffic data, traffic prediction, spatio-temporal graph node generation problem, RoadDiff framework

Summary:
The article introduces the Fine-grained Road Traffic Inference (FRTI) task, which aims to generate detailed lane-level traffic information with limited road data. The proposed RoadDiff framework consists of the Road-Lane Correlation Autoencoder-Decoder and Lane Diffusion Module to accurately infer fine-grained lane traffic states. Baseline models were tested on six datasets to validate the effectiveness of RoadDiff in addressing the FRTI task. The framework provides a cost-effective and energy-efficient solution for precise traffic management and prediction, crucial for applications like autonomous driving and traffic signal control. The code and datasets used in the experiments are available on GitHub for further research and development. 

<br /><br />Summary: <div>
arXiv:2507.19089v1 Announce Type: new 
Abstract: Fine-grained traffic management and prediction are fundamental to key applications such as autonomous driving, lane change guidance, and traffic signal control. However, obtaining lane-level traffic data has become a critical bottleneck for data-driven models due to limitations in the types and number of sensors and issues with the accuracy of tracking algorithms. To address this, we propose the Fine-grained Road Traffic Inference (FRTI) task, which aims to generate more detailed lane-level traffic information using limited road data, providing a more energy-efficient and cost-effective solution for precise traffic management. This task is abstracted as the first scene of the spatio-temporal graph node generation problem. We designed a two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task. This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies and distribution relationships of road data to accurately infer fine-grained lane traffic states. Based on existing research, we designed several baseline models with the potential to solve the FRTI task and conducted extensive experiments on six datasets representing different road conditions to validate the effectiveness of the RoadDiff model in addressing the FRTI task. The relevant datasets and code are available at https://github.com/ShuhaoLii/RoadDiff.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2507.19109</link>
<guid>https://arxiv.org/abs/2507.19109</guid>
<content:encoded><![CDATA[
<div> Algorithm, Multi-objective optimization, Monte-Carlo, Nested Rollout Policy Adaptation, Pareto front 

Summary: 
The article introduces Pareto-NRPA, a new Monte-Carlo algorithm designed for multi-objective optimization problems over discrete search spaces. It extends the Nested Rollout Policy Adaptation (NRPA) algorithm to handle multi-objective optimization by using a set of policies to explore different regions of the solution space. The algorithm maintains non-dominated fronts at each level of search and adapts policies based on the diversity and isolation of sequences within the Pareto front. Pareto-NRPA is benchmarked on a bi-objective variant of the Traveling Salesman Problem with Time Windows and a neural architecture search task, demonstrating competitive performance in terms of convergence and diversity of solutions. It significantly outperforms state-of-the-art evolutionary multi-objective algorithms on constrained search spaces. This work represents the first adaptation of NRPA to the multi-objective setting. <div>
arXiv:2507.19109v1 Announce Type: new 
Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for multi-objective optimization problems over discrete search spaces. Extending the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for single-objective problems, Pareto-NRPA generalizes the nested search and policy update mechanism to multi-objective optimization. The algorithm uses a set of policies to concurrently explore different regions of the solution space and maintains non-dominated fronts at each level of search. Policy adaptation is performed with respect to the diversity and isolation of sequences within the Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel bi-objective variant of the Traveling Salesman Problem with Time Windows problem (MO-TSPTW), and a neural architecture search task on well-known benchmarks. Results demonstrate that Pareto-NRPA achieves competitive performance against state-of-the-art multi-objective algorithms, both in terms of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly outperforms state-of-the-art evolutionary multi-objective algorithms on constrained search spaces. To our knowledge, this work constitutes the first adaptation of NRPA to the multi-objective setting.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?</title>
<link>https://arxiv.org/abs/2507.19132</link>
<guid>https://arxiv.org/abs/2507.19132</guid>
<content:encoded><![CDATA[
<div> Keywords: computer-using agents, benchmark, automation, capabilities, real-world scenarios

Summary:
The article introduces a new benchmark, OS-MAP, designed to evaluate computer-using agents for daily automation tasks. The benchmark organizes 416 realistic tasks across 15 applications based on a five-level taxonomy of automation and a generalization scope derived from real-world user demands. It evaluates agents based on their automation level and generalization scope, providing a structured assessment matrix. Experiments reveal challenges for even State-of-the-Art agents in higher-level tasks, emphasizing the need for a deeper understanding of current strengths and limitations in the field. The benchmark aims to bridge the gap between research progress and practical deployment by providing a comprehensive assessment of agent capabilities and alignment with real-world scenarios. The code, environments, baselines, and data for OS-MAP are publicly available for further research and development. 

<br /><br />Summary: 
- Introduces OS-MAP, a benchmark for evaluating computer-using agents in daily automation tasks.
- Organizes tasks based on automation level and generalization scope, derived from real-world user demands.
- Evaluates agents on their capability to perform tasks across different levels and scopes.
- Highlights challenges faced by even State-of-the-Art agents in higher-level tasks.
- Aims to provide a structured assessment matrix for understanding agent capabilities and aligning them with real-world scenarios. <div>
arXiv:2507.19132v1 Announce Type: new 
Abstract: Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring</title>
<link>https://arxiv.org/abs/2507.19172</link>
<guid>https://arxiv.org/abs/2507.19172</guid>
<content:encoded><![CDATA[
<div> Dataset, PhysDrive, in-vehicle physiological monitoring, multimodal, driver monitoring<br />
Summary:<br />
PhysDrive is introduced as a large-scale dataset for contactless in-vehicle physiological sensing, addressing the limitations of existing resources. It includes data from 48 drivers with synchronized RGB, near-infrared camera, and raw mmWave radar data, along with six ground truths. The dataset covers various driving conditions, such as driver motions, natural light, vehicle types, and road conditions. Signal-processing and deep-learning methods are evaluated on PhysDrive to establish a benchmark across all modalities. Full open-source code compatible with mainstream public toolboxes is released to facilitate research in multimodal driver monitoring and smart-cockpit systems. PhysDrive aims to accelerate research in this field by providing a foundational resource for robust and unobtrusive in-vehicle physiological monitoring. <br /><br />Summary: <div>
arXiv:2507.19172v1 Announce Type: new 
Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for ensuring driving safety and user experience. While remote physiological measurement (RPM) offers a promising non-invasive solution, its translation to real-world driving scenarios is critically constrained by the scarcity of comprehensive datasets. Existing resources are often limited in scale, modality diversity, the breadth of biometric annotations, and the range of captured conditions, thereby omitting inherent real-world challenges in driving. Here, we present PhysDrive, the first large-scale multimodal dataset for contactless in-vehicle physiological sensing with dedicated consideration on various modality settings and driving factors. PhysDrive collects data from 48 drivers, including synchronized RGB, near-infrared camera, and raw mmWave radar data, accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR, and SpO2). It covers a wide spectrum of naturalistic driving conditions, including driver motions, dynamic natural light, vehicle types, and road conditions. We extensively evaluate both signal-processing and deep-learning methods on PhysDrive, establishing a comprehensive benchmark across all modalities, and release full open-source code with compatibility for mainstream public toolboxes. We envision PhysDrive will serve as a foundational resource and accelerate research on multimodal driver monitoring and smart-cockpit systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Lifting for Ordered Domains with Predecessor Relations</title>
<link>https://arxiv.org/abs/2507.19182</link>
<guid>https://arxiv.org/abs/2507.19182</guid>
<content:encoded><![CDATA[
<div> lifted inference, ordered domains, predecessor relations, weighted first-order model counting, algorithm

Summary:
This paper discusses lifted inference on ordered domains with predecessor relations, focusing on total (cyclic) orders where each element has a distinct predecessor. Previous approaches using weighted first-order model counting struggled with practical applications due to encoding linear order predicates. This work introduces an algorithm that natively supports predecessor relations, providing exponential speedups for immediate and second predecessors and handling general k-th predecessor relations efficiently. Experimental results showed significant speed improvements, demonstrating the algorithm's effectiveness in various inference tasks and combinatorial math problems. <div>
arXiv:2507.19182v1 Announce Type: new 
Abstract: We investigate lifted inference on ordered domains with predecessor relations, where the elements of the domain respect a total (cyclic) order, and every element has a distinct (clockwise) predecessor. Previous work has explored this problem through weighted first-order model counting (WFOMC), which computes the weighted sum of models for a given first-order logic sentence over a finite domain. In WFOMC, the order constraint is typically encoded by the linear order axiom introducing a binary predicate in the sentence to impose a linear ordering on the domain elements. The immediate and second predecessor relations are then encoded by the linear order predicate. Although WFOMC with the linear order axiom is theoretically tractable, existing algorithms struggle with practical applications, particularly when the predecessor relations are involved. In this paper, we treat predecessor relations as a native part of the axiom and devise a novel algorithm that inherently supports these relations. The proposed algorithm not only provides an exponential speedup for the immediate and second predecessor relations, which are known to be tractable, but also handles the general k-th predecessor relations. The extensive experiments on lifted inference tasks and combinatorics math problems demonstrate the efficiency of our algorithm, achieving speedups of a full order of magnitude.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2507.19261</link>
<guid>https://arxiv.org/abs/2507.19261</guid>
<content:encoded><![CDATA[
<div> knowledge grafting, resource-constrained environments, model optimization, AI adoption, edge computing<br />
Summary:<br />
The paper introduces knowledge grafting as a mechanism to optimize AI models for resource-constrained environments by transferring selected features from a large donor model to a smaller rootstock model. This approach results in an 88.54% reduction in model size while improving generalization capability. The new rootstock model achieves a validation accuracy of 89.97%, maintaining lower validation loss compared to the donor model. It also performs exceptionally well on unseen test data with 90.45% accuracy. This method addresses the size vs. performance trade-off, enabling AI frameworks to be deployed on resource-constrained devices with enhanced performance. The approach was tested in an agricultural weed detection scenario and can be extended to various edge computing scenarios, potentially accelerating AI adoption in areas with limited hardware/software support, similar to how horticultural grafting enables cultivation in challenging agri-based environments. <br />Summary: <div>
arXiv:2507.19261v1 Announce Type: new 
Abstract: The increasing adoption of Artificial Intelligence (AI) has led to larger, more complex models with numerous parameters that require substantial computing power -- resources often unavailable in many real-world application scenarios. Our paper addresses this challenge by introducing knowledge grafting, a novel mechanism that optimizes AI models for resource-constrained environments by transferring selected features (the scion) from a large donor model to a smaller rootstock model. The approach achieves an 88.54% reduction in model size (from 64.39 MB to 7.38 MB), while improving generalization capability of the model. Our new rootstock model achieves 89.97% validation accuracy (vs. donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and performs exceptionally well on unseen test data with 90.45% accuracy. It addresses the typical size vs performance trade-off, and enables deployment of AI frameworks on resource-constrained devices with enhanced performance. We have tested our approach on an agricultural weed detection scenario, however, it can be extended across various edge computing scenarios, potentially accelerating AI adoption in areas with limited hardware/software support -- by mirroring in a similar manner the horticultural grafting enables productive cultivation in challenging agri-based environments.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games</title>
<link>https://arxiv.org/abs/2507.19263</link>
<guid>https://arxiv.org/abs/2507.19263</guid>
<content:encoded><![CDATA[
<div> belief states, imperfect-information games, constraint-based model, Constraint Satisfaction Problems, Belief Propagation

Summary: In imperfect-information games, where agents must make decisions based on partial knowledge, the Belief Stochastic Game model allows state estimation within the game model itself. Two approaches were explored to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation. Evaluation across two games showed that constraint-based beliefs can yield results similar to probabilistic inference, with minimal impact on agent performance. This suggests that constraint-based belief states alone may be sufficient for effective decision-making in various settings. <div>
arXiv:2507.19263v1 Announce Type: new 
Abstract: In imperfect-information games, agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic. This paper investigates two approaches to represent beliefs in games with hidden piece identities: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities. We evaluated the impact of both representations using general-purpose agents across two different games. Our findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2507.19364</link>
<guid>https://arxiv.org/abs/2507.19364</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social simulation, cognitive biases, multi-agent simulation, hybrid approaches

Summary: 
- The paper discusses the use of Large Language Models (LLMs) in social simulation, highlighting their potential to replicate human cognitive aspects like Theory of Mind reasoning, but also noting limitations such as cognitive biases.
- It examines emerging applications of LLMs in multi-agent simulation frameworks and discusses projects like Generative Agents (Smallville) and AgentSociety, emphasizing challenges in behavioral fidelity and validation strategies.
- The paper distinguishes between contexts where LLMs provide direct value, such as interactive simulations, and where their use is problematic, such as in explanatory or predictive modeling.
- It advocates for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms to combine the expressive flexibility of language-based reasoning with transparency and analytical rigor. 

<br /><br />Summary: <div>
arXiv:2507.19364v1 Announce Type: new 
Abstract: This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning neuro-symbolic convergent term rewriting systems</title>
<link>https://arxiv.org/abs/2507.19372</link>
<guid>https://arxiv.org/abs/2507.19372</guid>
<content:encoded><![CDATA[
<div> Keywords: neural systems, term rewriting, neuro-symbolic architecture, generalization, out-of-distribution performance

Summary:
The article introduces a framework for learning convergent term rewriting systems using a neuro-symbolic architecture inspired by the rewriting algorithm. Two modular implementations, the Neural Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS), are presented. Both models exhibit strong generalization to out-of-distribution instances, with FastNRS showing improvements in memory efficiency, training speed, and inference time. Evaluation on mathematical formula simplification tasks and a multi-domain learning scenario demonstrates the versatility of the proposed system. It outperforms neural baselines such as the Neural Data Router and GPT-4o, as well as matching or surpassing the OpenAI o1-preview model in reasoning benchmarks. The system's algorithmic-inspired design and key architectural elements contribute to its success in executing symbolic algorithms with strong generalization capabilities. 

<br /><br />Summary: <div>
arXiv:2507.19372v1 Announce Type: new 
Abstract: Building neural systems that can learn to execute symbolic algorithms is a challenging open problem in artificial intelligence, especially when aiming for strong generalization and out-of-distribution performance. In this work, we introduce a general framework for learning convergent term rewriting systems using a neuro-symbolic architecture inspired by the rewriting algorithm itself. We present two modular implementations of such architecture: the Neural Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a result of algorithmic-inspired design and key architectural elements, both models can generalize to out-of-distribution instances, with FastNRS offering significant improvements in terms of memory efficiency, training speed, and inference time. We evaluate both architectures on four tasks involving the simplification of mathematical formulas and further demonstrate their versatility in a multi-domain learning scenario, where a single model is trained to solve multiple types of problems simultaneously. The proposed system significantly outperforms two strong neural baselines: the Neural Data Router, a recent transformer variant specifically designed to solve algorithmic problems, and GPT-4o, one of the most powerful general-purpose large-language models. Moreover, our system matches or outperforms the latest o1-preview model from OpenAI that excels in reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints</title>
<link>https://arxiv.org/abs/2507.19458</link>
<guid>https://arxiv.org/abs/2507.19458</guid>
<content:encoded><![CDATA[
<div> Budget planning, maintenance optimization, infrastructure asset management, deep reinforcement learning, hierarchical approach

Summary: 
- The paper introduces a new Hierarchical Deep Reinforcement Learning methodology for multi-year infrastructure planning, addressing challenges like diverse asset deterioration and budget constraints. 
- It divides the problem into a Budget Planner for annual budget allocation and a Maintenance Planner for asset prioritization within the budget. 
- Incorporating linear programming projection within a hierarchical Soft Actor-Critic framework efficiently handles the complex action space and ensures budget compliance. 
- A case study on sewer networks demonstrates the method's effectiveness, showcasing faster convergence, scalability, and near-optimal solutions for larger networks. 
- Compared to conventional Deep Q-Learning and enhanced genetic algorithms, the proposed approach outperforms in terms of convergence speed and solution quality. 

<br /><br /> <div>
arXiv:2507.19458v1 Announce Type: new 
Abstract: Budget planning and maintenance optimization are crucial for infrastructure asset management, ensuring cost-effectiveness and sustainability. However, the complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability. This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning. Our approach decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget. By structurally separating macro-budget decisions from asset-level prioritization and integrating linear programming projection within a hierarchical Soft Actor-Critic framework, the method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach. Compared to conventional Deep Q-Learning and enhanced genetic algorithms, our methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading</title>
<link>https://arxiv.org/abs/2507.18637</link>
<guid>https://arxiv.org/abs/2507.18637</guid>
<content:encoded><![CDATA[
<div> Keywords: visual expertise, eye-gaze movements, network analysis, diagnostic performance, AI-assisted learning<br />
Summary: <br />
This study explores how undergraduate dentistry students develop visual search skills when diagnosing dental radiographs using network analysis techniques. The research analyzes eye-gaze scanpaths as directed graphs and investigates changes in network metrics over time. Time series clustering reveals distinct patterns of visual search strategies, with transition entropy negatively correlated and number of nodes, edges, and average PageRank positively correlated with diagnostic performance. The findings suggest a developmental shift from intermediate to expert-level processing for individual students. Understanding these insights can help improve training methods and design AI-assisted learning interventions in various domains. <div>
arXiv:2507.18637v1 Announce Type: cross 
Abstract: Understanding how novices acquire and hone visual search skills is crucial for developing and optimizing training methods across domains. Network analysis methods can be used to analyze graph representations of visual expertise. This study investigates the relationship between eye-gaze movements and learning outcomes among undergraduate dentistry students who were diagnosing dental radiographs over multiple semesters. We use network analysis techniques to model eye-gaze scanpaths as directed graphs and examine changes in network metrics over time. Using time series clustering on each metric, we identify distinct patterns of visual search strategies and explore their association with students' diagnostic performance. Our findings suggest that the network metric of transition entropy is negatively correlated with performance scores, while the number of nodes and edges as well as average PageRank are positively correlated with performance scores. Changes in network metrics for individual students over time suggest a developmental shift from intermediate to expert-level processing. These insights contribute to understanding expertise acquisition in visual tasks and can inform the design of AI-assisted learning interventions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity</title>
<link>https://arxiv.org/abs/2507.18638</link>
<guid>https://arxiv.org/abs/2507.18638</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, user prompts, task efficiency, prompt engineering, generative AI <br />
Summary: 
The study examines the impact of user prompts on the efficacy and productivity of large language models (LLMs) like ChatGPT and Gemini. Data from 243 participants reveal that clear, structured prompts significantly enhance task efficiency and outcomes when interacting with AI. Users who utilize context-aware prompts experience better results, highlighting the importance of prompt engineering in maximizing the value of generative AI. The research underscores the crucial role of prompt clarity and structure in enhancing user satisfaction and overall performance with LLMs. The findings offer practical insights for optimizing the use of AI in various academic and professional settings. <br /><br />Summary: <div>
arXiv:2507.18638v1 Announce Type: cross 
Abstract: The widespread adoption of large language models (LLMs) such as ChatGPT, Gemini, and DeepSeek has significantly changed how people approach tasks in education, professional work, and creative domains. This paper investigates how the structure and clarity of user prompts impact the effectiveness and productivity of LLM outputs. Using data from 243 survey respondents across various academic and occupational backgrounds, we analyze AI usage habits, prompting strategies, and user satisfaction. The results show that users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes. These findings emphasize the essential role of prompt engineering in maximizing the value of generative AI and provide practical implications for its everyday use.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How good are humans at detecting AI-generated images? Learnings from an experiment</title>
<link>https://arxiv.org/abs/2507.18640</link>
<guid>https://arxiv.org/abs/2507.18640</guid>
<content:encoded><![CDATA[
<div> AI-generated images, human perception, authenticity, image evaluation, misinformation
Summary:
Participants in a study were tested on their ability to distinguish between real and AI-generated images, with an overall success rate of 62%. They were more accurate with human portraits but struggled with natural and urban landscapes. The findings highlight the difficulty humans face in discerning AI-generated visual content, especially when lacking clear artifacts or cues. The study emphasizes the importance of transparency tools like watermarks and robust AI detection to combat the spread of misinformation from AI-generated content.
<br /><br />Summary: <div>
arXiv:2507.18640v1 Announce Type: cross 
Abstract: As AI-powered image generation improves, a key question is how well human beings can differentiate between "real" and AI-generated or modified images. Using data collected from the online game "Real or Not Quiz.", this study investigates how effectively people can distinguish AI-generated images from real ones. Participants viewed a randomized set of real and AI-generated images, aiming to identify their authenticity. Analysis of approximately 287,000 image evaluations by over 12,500 global participants revealed an overall success rate of only 62\%, indicating a modest ability, slightly above chance. Participants were most accurate with human portraits but struggled significantly with natural and urban landscapes. These results highlight the inherent challenge humans face in distinguishing AI-generated visual content, particularly images without obvious artifacts or stylistic cues. This study stresses the need for transparency tools, such as watermarks and robust AI detection tools to mitigate the risks of misinformation arising from AI-generated content
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis</title>
<link>https://arxiv.org/abs/2507.18645</link>
<guid>https://arxiv.org/abs/2507.18645</guid>
<content:encoded><![CDATA[
<div> tunnelling, quantum, neural network, perception, sentiment

Summary:
Quantum tunnelling (QT) probability has been integrated into neural network models to better understand human perception, particularly in recognizing ambiguous objects and sentiment analysis. The researchers applied QT-based neural networks to distinguish custom CIFAR-format images of military and civilian vehicles, as well as sentiment, utilizing military-specific vocabulary. They propose that QT-based models can improve multimodal AI applications in battlefield situations, specifically in human-operated drone warfare, by giving AI certain aspects of human reasoning.<br /><br />Summary: <div>
arXiv:2507.18645v1 Announce Type: cross 
Abstract: Prior work has demonstrated that incorporating well-known quantum tunnelling (QT) probability into neural network models effectively captures important nuances of human perception, particularly in the recognition of ambiguous objects and sentiment analysis. In this paper, we employ novel QT-based neural networks and assess their effectiveness in distinguishing customised CIFAR-format images of military and civilian vehicles, as well as sentiment, using a proprietary military-specific vocabulary. We suggest that QT-based models can enhance multimodal AI applications in battlefield scenarios, particularly within human-operated drone warfare contexts, imbuing AI with certain traits of human reasoning.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gen-AI Police Sketches with Stable Diffusion</title>
<link>https://arxiv.org/abs/2507.18667</link>
<guid>https://arxiv.org/abs/2507.18667</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal AI-driven approaches, suspect sketching, Stable Diffusion model, CLIP model, LoRA fine-tuning<br />
Summary: 
This study explores the use of multimodal AI techniques to enhance suspect sketching, comparing three different pipelines. The first model, utilizing a Stable Diffusion model, achieved the highest structural similarity and signal-to-noise ratio. The second model, incorporating a pre-trained CLIP model for alignment, showed improvement in perceptual similarity. The third model, which fine-tuned the CLIP model's self- and cross-attention layers, enhanced text-image alignment but still lagged behind the first model in performance. Qualitatively, the sketches generated by the baseline Stable Diffusion model displayed clear facial features, demonstrating its robustness despite its simplicity. Iterative refinement further improved perceptual similarity in the models. The study confirmed that fine-tuning both self- and cross-attention layers resulted in the best alignment between text descriptions and sketches. <br /><br /> <div>
arXiv:2507.18667v1 Announce Type: cross 
Abstract: This project investigates the use of multimodal AI-driven approaches to automate and enhance suspect sketching. Three pipelines were developed and evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model integrated with a pre-trained CLIP model for text-image alignment, and (3) novel approach incorporating LoRA fine-tuning of the CLIP model, applied to self-attention and cross-attention layers, and integrated with Stable Diffusion. An ablation study confirmed that fine-tuning both self- and cross-attention layers yielded the best alignment between text descriptions and sketches. Performance testing revealed that Model 1 achieved the highest structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of 25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2 but still trailing Model 1. Qualitatively, sketches generated by Model 1 demonstrated the clearest facial features, highlighting its robustness as a baseline despite its simplicity.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs</title>
<link>https://arxiv.org/abs/2507.18668</link>
<guid>https://arxiv.org/abs/2507.18668</guid>
<content:encoded><![CDATA[
<div> Keywords: online learning, knowledge tracing, graph neural network, computational efficiency, resource efficiency

Summary:
Dual Graph Attention-based Knowledge Tracing (DGAKT) is introduced as a graph neural network model that addresses the issue of increasing computational cost in online learning scenarios. By leveraging subgraphs to capture high-order information on student-exercise-KC relationships, DGAKT significantly reduces memory and computational requirements compared to global graph models. The model outperforms existing KT methods and sets a new standard in resource efficiency. DGAKT's subgraph-based approach enhances computational efficiency by processing only relevant subgraphs for each interaction, leading to improved performance in knowledge tracing tasks. The model fills a critical gap in KT research by achieving both high accuracy and low resource usage, providing a promising solution for scaling knowledge tracing in the era of online learning.<br /><br />Summary: Dual Graph Attention-based Knowledge Tracing (DGAKT) introduces a novel graph neural network model that efficiently leverages subgraphs to represent student-exercise-KC relationships in online learning scenarios. The model outperforms existing knowledge tracing approaches by reducing memory and computational requirements while maintaining high accuracy. By processing only relevant subgraphs for each interaction, DGAKT sets a new standard in resource efficiency and addresses a critical need in KT research for scalable and efficient models in the online learning era. <div>
arXiv:2507.18668v1 Announce Type: cross 
Abstract: The rise of online learning has led to the development of various knowledge tracing (KT) methods. However, existing methods have overlooked the problem of increasing computational cost when utilizing large graphs and long learning sequences. To address this issue, we introduce Dual Graph Attention-based Knowledge Tracing (DGAKT), a graph neural network model designed to leverage high-order information from subgraphs representing student-exercise-KC relationships. DGAKT incorporates a subgraph-based approach to enhance computational efficiency. By processing only relevant subgraphs for each target interaction, DGAKT significantly reduces memory and computational requirements compared to full global graph models. Extensive experimental results demonstrate that DGAKT not only outperforms existing KT models but also sets a new standard in resource efficiency, addressing a critical need that has been largely overlooked by prior KT approaches.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling</title>
<link>https://arxiv.org/abs/2507.18671</link>
<guid>https://arxiv.org/abs/2507.18671</guid>
<content:encoded><![CDATA[
<div> upcycling, Mixtures-of-Experts model, continued pretraining, catastrophic forgetting, scientific knowledge

Summary:
Innovator introduces a novel approach to addressing the issue of catastrophic forgetting in large language models (LLMs) when continued pretraining with scientific data. By transforming a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model, Innovator separates general and scientific knowledge into specialized experts. Through a four-stage upcycle training paradigm, Innovator achieves significant improvements in scientific tasks while maintaining high performance in general tasks. With 64 specialized scientific experts and a shared general expert, Innovator extends the capabilities of LLMs like Qwen2.5-7B. Innovator-Reason, post-trained for reasoning enhancement, demonstrates excellent performance in solving complex scientific problems. Overall, Innovator's approach allows for effective integration of scientific and general knowledge in LLMs without compromising performance.<br /><br />Summary: <div>
arXiv:2507.18671v1 Announce Type: cross 
Abstract: A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting</title>
<link>https://arxiv.org/abs/2507.18678</link>
<guid>https://arxiv.org/abs/2507.18678</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial intelligence, 3D datasets, depth estimation, camera calibration, scale calibration

Summary: 
This article introduces a pipeline that can convert single-view images into detailed 3D representations, such as point clouds, camera poses, depth maps, and pseudo-RGBD. By utilizing integrated depth estimation, camera calibration, and scale calibration, the method allows for the creation of authentic, scale- and appearance-realistic 3D data from 2D imagery. The generated datasets, COCO-3D and Objects365-v2-3D, are released to facilitate various 3D tasks, from perception to machine learning-based reasoning. The pipeline significantly reduces data collection costs and enables advancements in spatial intelligence by bridging the gap between image-based data and the need for comprehensive 3D information. The results of extensive experiments validate the effectiveness of the method in enhancing AI systems' ability to perceive, understand, and interact with physical environments.

<br /><br />Summary: <div>
arXiv:2507.18678v1 Announce Type: cross 
Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it remains constrained by the scarcity of large-scale 3D datasets. Unlike the abundant 2D imagery, acquiring 3D data typically requires specialized sensors and laborious annotation. In this work, we present a scalable pipeline that converts single-view images into comprehensive, scale- and appearance-realistic 3D representations - including point clouds, camera poses, depth maps, and pseudo-RGBD - via integrated depth estimation, camera calibration, and scale calibration. Our method bridges the gap between the vast repository of imagery and the increasing demand for spatial scene understanding. By automatically generating authentic, scale-aware 3D data from images, we significantly reduce data collection costs and open new avenues for advancing spatial intelligence. We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D, and demonstrate through extensive experiments that our generated data can benefit various 3D tasks, ranging from fundamental perception to MLLM-based reasoning. These results validate our pipeline as an effective solution for developing AI systems capable of perceiving, understanding, and interacting with physical environments.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Market Making Strategies with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.18680</link>
<guid>https://arxiv.org/abs/2507.18680</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Market Making, Financial Markets, Deep Reinforcement Learning, Multi-Objective Reinforcement Learning

Summary:<br />
- This thesis explores applying Reinforcement Learning (RL) to market making in financial markets to tackle challenges such as inventory risk and non-stationary market dynamics.
- It formulates market making as a reinforcement learning problem and designs agents for single-agent and multi-agent environments.
- The research uses reward engineering and Multi-Objective Reinforcement Learning (MORL) for inventory management to balance competing objectives.
- A novel policy weighting algorithm, POW-dTS, is introduced to address non-stationarity by allowing agents to dynamically select and combine pretrained policies.
- Experimental results show that RL-based approaches outperform traditional strategies, offering new insights for designing robust, efficient, and adaptive market making agents. 

Summary: <div>
arXiv:2507.18680v1 Announce Type: cross 
Abstract: This thesis presents the results of a comprehensive research project focused on applying Reinforcement Learning (RL) to the problem of market making in financial markets. Market makers (MMs) play a fundamental role in providing liquidity, yet face significant challenges arising from inventory risk, competition, and non-stationary market dynamics. This research explores how RL, particularly Deep Reinforcement Learning (DRL), can be employed to develop autonomous, adaptive, and profitable market making strategies.
  The study begins by formulating the MM task as a reinforcement learning problem, designing agents capable of operating in both single-agent and multi-agent settings within a simulated financial environment. It then addresses the complex issue of inventory management using two complementary approaches: reward engineering and Multi-Objective Reinforcement Learning (MORL). While the former uses dynamic reward shaping to guide behavior, the latter leverages Pareto front optimization to explicitly balance competing objectives.
  To address the problem of non-stationarity, the research introduces POW-dTS, a novel policy weighting algorithm based on Discounted Thompson Sampling. This method allows agents to dynamically select and combine pretrained policies, enabling continual adaptation to shifting market conditions.
  The experimental results demonstrate that the proposed RL-based approaches significantly outperform traditional and baseline algorithmic strategies across various performance metrics. Overall, this research thesis contributes new methodologies and insights for the design of robust, efficient, and adaptive market making agents, reinforcing the potential of RL to transform algorithmic trading in complex financial systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Probing: Where to Find Human-Defined Concepts (Extended Version)</title>
<link>https://arxiv.org/abs/2507.18681</link>
<guid>https://arxiv.org/abs/2507.18681</guid>
<content:encoded><![CDATA[
<div> Concept probing, neural networks, internal representations, human-defined concepts, layer identification<br />
<br />
The paper discusses the concept of concept probing in neural networks, where additional classifiers are trained to map internal representations to human-defined concepts. The performance of these probes depends on the layer being probed, necessitating the identification of the appropriate layer for a given concept. The authors propose a method to automatically determine which layer's representations should be probed based on their informativeness and regularity with respect to the concept. Through an extensive empirical analysis on various neural network models and datasets, the effectiveness of this approach is validated. This research highlights the importance of selecting the right layer for concept probing in neural networks to accurately interpret the information encoded within the models.<br /><br />Summary: <div>
arXiv:2507.18681v1 Announce Type: cross 
Abstract: Concept probing has recently gained popularity as a way for humans to peek into what is encoded within artificial neural networks. In concept probing, additional classifiers are trained to map the internal representations of a model into human-defined concepts of interest. However, the performance of these probes is highly dependent on the internal representations they probe from, making identifying the appropriate layer to probe an essential task. In this paper, we propose a method to automatically identify which layer's representations in a neural network model should be considered when probing for a given human-defined concept of interest, based on how informative and regular the representations are with respect to the concept. We validate our findings through an exhaustive empirical analysis over different neural network models and datasets.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models</title>
<link>https://arxiv.org/abs/2507.18725</link>
<guid>https://arxiv.org/abs/2507.18725</guid>
<content:encoded><![CDATA[
<div> Keywords: machine unlearning, sparse models, un-pruning, retention of data, model pruning

Summary: 
Machine unlearning is crucial for addressing the right to be forgotten, but sparse models present challenges not well studied. Deleted data impacts pruned topology in sparse models, leading to the introduction of the concept of un-pruning to address this issue. An un-pruning algorithm is proposed to approximate pruned topology based on retained data, integrating seamlessly with existing unlearning algorithms and having an upper-bound error in theory. The algorithm is applicable to both structured and unstructured sparse models. Membership Inference Attack (MIA) accuracy is found unreliable for assessing forgotten data, prompting the development of new metrics for sparse models. Extensive experiments verify the effectiveness of un-pruning with various pruning methods and unlearning algorithms, highlighting the importance of considering retained data in the unlearning process. <div>
arXiv:2507.18725v1 Announce Type: cross 
Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted data from trained models and address the right to be forgotten. Despite the success of existing unlearning algorithms, unlearning in sparse models has not yet been well studied. In this paper, we empirically find that the deleted data has an impact on the pruned topology in a sparse model. Motivated by the observation and the right to be forgotten, we define a new terminology ``un-pruning" to eliminate the impact of deleted data on model pruning. Then we propose an un-pruning algorithm to approximate the pruned topology driven by retained data. We remark that any existing unlearning algorithm can be integrated with the proposed un-pruning workflow and the error of un-pruning is upper-bounded in theory. Also, our un-pruning algorithm can be applied to both structured sparse models and unstructured sparse models. In the experiment, we further find that Membership Inference Attack (MIA) accuracy is unreliable for assessing whether a model has forgotten deleted data, as a small change in the amount of deleted data can produce arbitrary MIA results. Accordingly, we devise new performance metrics for sparse models to evaluate the success of un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of un-pruning with various pruning methods and unlearning algorithms. Our code is released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems: A Novel Network Deep Q-Learning Approach</title>
<link>https://arxiv.org/abs/2507.18732</link>
<guid>https://arxiv.org/abs/2507.18732</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, infrastructure asset management, deep reinforcement learning, maintenance planning, budget constraints 

Summary:
The paper introduces a novel deep reinforcement learning framework for optimizing asset management strategies in large infrastructure networks. By breaking down the network-level Markov Decision Process into individual asset-level MDPs and utilizing a unified neural network architecture, the framework addresses scalability and computational challenges. It incorporates annual budget constraints through a budget allocation mechanism to ensure cost-effective maintenance plans. Through a case study on a pavement network with 68,800 segments, the proposed framework outperforms traditional methods like Progressive Linear Programming and genetic algorithms in efficiency and network performance. This advancement contributes to the field of infrastructure asset management and showcases the potential of reinforcement learning in complex, large-scale environments. 

<br /><br />Summary: <div>
arXiv:2507.18732v1 Announce Type: cross 
Abstract: Infrastructure asset management is essential for sustaining the performance of public infrastructure such as road networks, bridges, and utility networks. Traditional maintenance and rehabilitation planning methods often face scalability and computational challenges, particularly for large-scale networks with thousands of assets under budget constraints. This paper presents a novel deep reinforcement learning (DRL) framework that optimizes asset management strategies for large infrastructure networks. By decomposing the network-level Markov Decision Process (MDP) into individual asset-level MDPs while using a unified neural network architecture, the proposed framework reduces computational complexity, improves learning efficiency, and enhances scalability. The framework directly incorporates annual budget constraints through a budget allocation mechanism, ensuring maintenance plans are both optimal and cost-effective. Through a case study on a large-scale pavement network of 68,800 segments, the proposed DRL framework demonstrates significant improvements over traditional methods like Progressive Linear Programming and genetic algorithms, both in efficiency and network performance. This advancement contributes to infrastructure asset management and the broader application of reinforcement learning in complex, large-scale environments.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Single-Pixel Fluorescence Microscopy</title>
<link>https://arxiv.org/abs/2507.18740</link>
<guid>https://arxiv.org/abs/2507.18740</guid>
<content:encoded><![CDATA[
<div> Keywords: single-pixel imaging, fluorescence microscopy, autoencoder, reconstruction, multispectral imaging

Summary:
Single-pixel imaging in fluorescence microscopy requires fast acquisition and reconstruction. Traditional methods use total variation minimization, but a new approach involves training an autoencoder through self-supervision. This autoencoder learns an encoder (measurement matrix) and a decoder, which can enhance compression, reconstruction quality, and speed. The learned encoder is integrated into the physical device for multispectral and intensity data acquisition. This approach significantly reduces reconstruction time, improves image quality, and enables multispectral reconstructions. By leveraging data to learn measurement vectors and the reconstruction process, single-pixel fluorescence microscopy can advance diagnosis and biological research. Notably, this technique provides cost-effective multispectral imaging, benefiting the field at a fraction of the expense. 

<br /><br />Summary: <div>
arXiv:2507.18740v1 Announce Type: cross 
Abstract: Single-pixel imaging has emerged as a key technique in fluorescence microscopy, where fast acquisition and reconstruction are crucial. In this context, images are reconstructed from linearly compressed measurements. In practice, total variation minimisation is still used to reconstruct the image from noisy measurements of the inner product between orthogonal sampling pattern vectors and the original image data. However, data can be leveraged to learn the measurement vectors and the reconstruction process, thereby enhancing compression, reconstruction quality, and speed. We train an autoencoder through self-supervision to learn an encoder (or measurement matrix) and a decoder. We then test it on physically acquired multispectral and intensity data. During acquisition, the learned encoder becomes part of the physical device. Our approach can enhance single-pixel imaging in fluorescence microscopy by reducing reconstruction time by two orders of magnitude, achieving superior image quality, and enabling multispectral reconstructions. Ultimately, learned single-pixel fluorescence microscopy could advance diagnosis and biological research, providing multispectral imaging at a fraction of the cost.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement</title>
<link>https://arxiv.org/abs/2507.18742</link>
<guid>https://arxiv.org/abs/2507.18742</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Specification Self-Correction, in-context reward hacking, flawed specifications, multi-step inference process

Summary:<br /><br />
The article discusses the susceptibility of language models (LMs) to in-context reward hacking due to flawed specifications. To address this issue, the authors propose Specification Self-Correction (SSC), a test-time framework that enables LMs to identify and correct flaws in their guiding specifications. The SSC process involves a multi-step inference process where the model generates a response, critiques it, and then revises the specification to remove loopholes. Experimental results across creative writing and coding tasks show that LMs initially exploit tainted specifications in 50-70% of cases, but the SSC process reduces this vulnerability by over 90%. Importantly, this dynamic repair occurs at inference time without weight modifications, leading to more aligned model behavior. The code for SSC implementation is available on GitHub for further exploration. <div>
arXiv:2507.18742v1 Announce Type: cross 
Abstract: Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\% of cases, the SSC process reduces this vulnerability by over 90\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback</title>
<link>https://arxiv.org/abs/2507.18755</link>
<guid>https://arxiv.org/abs/2507.18755</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, agentic program repair, Engineering Agent, test failures, patch generation

Summary: 
With the advent of Large Language Models (LLMs), agentic program repair has become possible on a large scale within organizations. The Engineering Agent developed in this work utilizes Llama as a base and the ReAct harness to fix source code based on test failures. The agent reasons through a set of actions, receives feedback through static analysis and test failures, and ensures patch conformity with LLM-as-a-Judge. Offline evaluations showed the specialized 70B model's competitiveness and the benefits of symbolic information in the ReAct harness. In production, 80% of generated fixes were reviewed, with 31.5% successfully implemented. Feedback from engineers highlighted quick approvals, gratitude, and surprise for the Engineering Agent's solutions. Mixed feedback was received when fixes were partially correct, serving as a good starting point for further refinement. <div>
arXiv:2507.18755v1 Announce Type: cross 
Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an agent. We start with a test failure that was triaged by a rule-based test failure bot. We then set up an agentic harness and allow the agent to reason and run a set of 15 actions from reading a file to generating a patch. We provide feedback to the agent through static analysis and test failures so it can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator, the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we found that a specialized 70B model is highly competitive with the much larger but vanilla Llama-405B. In an ablation study, we found that the ReAct harness (neural model) benefited from the symbolic information from static analysis tools and test execution traces. A model that strikes a balance between the solve rate and error rate vs the cost and latency has a benchmark solve rate of 42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were reviewed, of which 31.5% were landed (25.5% of the total number of generated fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes from engineers' feedback. We saw positive feedback in the form of quick approvals, gratitude, and surprise. We also found mixed feedback when the Engineering Agent's solution was partially correct and it served as a good starting point.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning</title>
<link>https://arxiv.org/abs/2507.18788</link>
<guid>https://arxiv.org/abs/2507.18788</guid>
<content:encoded><![CDATA[
<div> Keywords: Image captioning, Transformer architectures, attention mechanism, CNN-LSTM paradigm, Nexus model

Summary:
This paper documents the development of image captioning models, progressing from a simple CNN-LSTM encoder-decoder to a competitive attention-based system. Five models are presented, with Nexus being the most advanced, incorporating an EfficientNetV2B3 backbone and dynamic attention mechanism. The experiments show the importance of attention mechanisms in enhancing performance, as upgrading the visual backbone alone can degrade results due to the single-vector bottleneck. The findings validate the shift to attention-based models in image captioning tasks. Trained on the MS COCO 2017 dataset, the Nexus model achieves a BLEU-4 score of 31.4, outperforming several benchmarks. The work provides a replicable blueprint for understanding core architectural principles in vision-language tasks. 

<br /><br />Summary: <div>
arXiv:2507.18788v1 Announce Type: cross 
Abstract: Image captioning, a task at the confluence of computer vision and natural language processing, requires a sophisticated understanding of both visual scenes and linguistic structure. While modern approaches are dominated by large-scale Transformer architectures, this paper documents a systematic, iterative development of foundational image captioning models, progressing from a simple CNN-LSTM encoder-decoder to a competitive attention-based system. We present a series of five models, beginning with Genesis and concluding with Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic attention mechanism. Our experiments chart the impact of architectural enhancements and demonstrate a key finding within the classic CNN-LSTM paradigm: merely upgrading the visual backbone without a corresponding attention mechanism can degrade performance, as the single-vector bottleneck cannot transmit the richer visual detail. This insight validates the architectural shift to attention. Trained on the MS COCO 2017 dataset, our final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several foundational benchmarks and validating our iterative design process. This work provides a clear, replicable blueprint for understanding the core architectural principles that underpin modern vision-language tasks.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition</title>
<link>https://arxiv.org/abs/2507.18802</link>
<guid>https://arxiv.org/abs/2507.18802</guid>
<content:encoded><![CDATA[
<div> Keywords: human feedback, large language models (LLMs), decomposition principle, user interface, HCI <br />
Summary: 
This study focuses on improving human feedback for aligning large language models using the decomposition principle. By breaking down text into individual claims and building a user interface called DxHF, users can provide more accurate feedback and make quicker judgments. Technical evaluation shows that decomposition enhances feedback accuracy, particularly for users who are uncertain. A crowdsourcing study with 160 participants demonstrates that using DxHF increases feedback accuracy by 5% on average, although it slightly lengthens the feedback time. Accuracy is significantly higher when users have less certainty. Overall, the study suggests that leveraging human-computer interaction (HCI) methods can effectively enhance human-AI alignment in tasks involving large language models. <br /><br />Summary: <div>
arXiv:2507.18802v1 Announce Type: cross 
Abstract: Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemoCoder: Automated Function Synthesis using LLM-Supported Agents</title>
<link>https://arxiv.org/abs/2507.18812</link>
<guid>https://arxiv.org/abs/2507.18812</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AI-assisted tools, MemoCoder, collaborative problem solving, knowledge-guided code generation

Summary:
MemoCoder is introduced as a multi-agent framework designed to improve code generation in challenging programming tasks where Large Language Models (LLMs) struggle. It utilizes a Fixing Knowledge Set to store successful repairs and guide future tasks, with a Mentor Agent overseeing the process by identifying error patterns and refining fixing strategies. The framework outperforms zero-shot prompting and self-repair strategies in three benchmark evaluations, showcasing its effectiveness in iterative refinement and knowledge-guided code generation. Through collaborative problem-solving and persistent learning from past fixes, MemoCoder offers a promising solution for developers relying on AI-assisted tools in code generation tasks. 

<br /><br />Summary: <div>
arXiv:2507.18812v1 Announce Type: cross 
Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to support code generation. While LLMs can generate syntactically correct solutions for well-structured programming tasks, they often struggle with challenges that require iterative debugging, error handling, or adaptation to diverse problem structures. Existing approaches such as fine-tuning or self-repair strategies either require costly retraining or lack mechanisms to accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework that enables collaborative problem solving and persistent learning from past fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores successful repairs and supports retrieval for future tasks. A central Mentor Agent supervises the repair process by identifying recurring error patterns and refining high-level fixing strategies, providing a novel supervisory role that guides the self-repair loop. We evaluate MemoCoder across three public benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem complexities. Experimental results show that MemoCoder consistently outperforms both zero-shot prompting and a Self-Repair strategy, with improvements ranging from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating its effectiveness in iterative refinement and knowledge-guided code generation.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deepfake Detection Via Facial Feature Extraction and Modeling</title>
<link>https://arxiv.org/abs/2507.18815</link>
<guid>https://arxiv.org/abs/2507.18815</guid>
<content:encoded><![CDATA[
<div> Facial landmarks, deepfake detection, artificial intelligence, neural network models, feature extraction  
Summary:<br /> 
- The paper introduces a novel approach to deepfake detection using facial landmarks instead of raw image processing.
- The study focuses on identifying subtle inconsistencies in facial movements in deepfake videos.
- Experimental results show promising performance metrics for neural network models using facial landmarks.
- RNN and ANN models achieved accuracy between 96% and 93%, while the CNN model had an accuracy of around 78%.
- The research challenges the belief that raw image processing is essential for identifying deepfake videos, showcasing the effectiveness of facial feature extraction with fewer parameters.<br /> 
Summary: <div>
arXiv:2507.18815v1 Announce Type: cross 
Abstract: The rise of deepfake technology brings forth new questions about the authenticity of various forms of media found online today. Videos and images generated by artificial intelligence (AI) have become increasingly more difficult to differentiate from genuine media, resulting in the need for new models to detect artificially-generated media. While many models have attempted to solve this, most focus on direct image processing, adapting a convolutional neural network (CNN) or a recurrent neural network (RNN) that directly interacts with the video image data. This paper introduces an approach of using solely facial landmarks for deepfake detection. Using a dataset consisting of both deepfake and genuine videos of human faces, this paper describes an approach for extracting facial landmarks for deepfake detection, focusing on identifying subtle inconsistencies in facial movements instead of raw image processing. Experimental results demonstrated that this feature extraction technique is effective in various neural network models, with the same facial landmarks tested on three neural network models, with promising performance metrics indicating its potential for real-world applications. The findings discussed in this paper include RNN and artificial neural network (ANN) models with accuracy between 96% and 93%, respectively, with a CNN model hovering around 78%. This research challenges the assumption that raw image processing is necessary to identify deepfake videos by presenting a facial feature extraction approach compatible with various neural network models while requiring fewer parameters.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Stochastic Segmentation Networks</title>
<link>https://arxiv.org/abs/2507.18838</link>
<guid>https://arxiv.org/abs/2507.18838</guid>
<content:encoded><![CDATA[
<div> Keywords: Flow Stochastic Segmentation Network, generative model, medical imaging, high-rank pixel-wise covariances, state-of-the-art results

Summary:
Flow Stochastic Segmentation Network (Flow-SSN) is introduced as a generative segmentation model family, utilizing discrete-time autoregressive and continuous-time flow variants. It overcomes limitations of low-rank parameterization seen in previous methods, enabling estimation of high-rank pixel-wise covariances without assuming rank or storing distribution parameters. More efficient sampling is achieved compared to diffusion-based models, with model capacity focused on learning the base distribution of the flow. This expressive prior contributes to state-of-the-art results on challenging medical imaging benchmarks. The code for Flow-SSN is available on GitHub for further exploration and utilization. <br /><br />Summary: Flow-SSN introduces a novel approach to generative segmentation, addressing limitations of low-rank parameterization, enabling estimation of high-rank covariances. With an efficient sampling method and an expressive prior, Flow-SSN achieves superior results on medical imaging benchmarks, demonstrating its effectiveness in practice. <div>
arXiv:2507.18838v1 Announce Type: cross 
Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a generative segmentation model family featuring discrete-time autoregressive and modern continuous-time flow variants. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, thanks to most of the model capacity being allocated to learning the base distribution of the flow, constituting an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results. Code available: https://github.com/biomedia-mira/flow-ssn.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Volumetric Grasping</title>
<link>https://arxiv.org/abs/2507.18847</link>
<guid>https://arxiv.org/abs/2507.18847</guid>
<content:encoded><![CDATA[
<div> equivariant grasp model, volumetric feature representation, deformable steerable convolution, sample efficiency, computational efficiency

Summary:<br />
The article introduces a new volumetric grasp model that is equivariant to rotations around the vertical axis, improving sample efficiency. It utilizes a tri-plane feature representation, projecting 3D features onto three planes with a novel design that ensures equivariance and invariance properties. By combining deformable and steerable convolutions, the model adapts to local object geometry while maintaining equivariance. Equivariant adaptations of state-of-the-art grasp planners, GIGA and IGD, are developed, with a focus on computational efficiency. Analytical justifications and extensive experiments validate the proposed approach, demonstrating reduced computational and memory costs. The equivariant grasp models based on tri-plane features consistently outperform non-equivariant counterparts, achieving higher performance with minimal additional computational overhead. The approach offers a promising solution for efficient and effective grasp planning in robotics. <div>
arXiv:2507.18847v1 Announce Type: cross 
Abstract: We propose a new volumetric grasp model that is equivariant to rotations around the vertical axis, leading to a significant improvement in sample efficiency. Our model employs a tri-plane volumetric feature representation -- i.e., the projection of 3D features onto three canonical planes. We introduce a novel tri-plane feature design in which features on the horizontal plane are equivariant to 90{\deg} rotations, while the sum of features from the other two planes remains invariant to the same transformations. This design is enabled by a new deformable steerable convolution, which combines the adaptability of deformable convolutions with the rotational equivariance of steerable ones. This allows the receptive field to adapt to local object geometry while preserving equivariance properties. We further develop equivariant adaptations of two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically, we derive a new equivariant formulation of IGD's deformable attention mechanism and propose an equivariant generative model of grasp orientations based on flow matching. We provide a detailed analytical justification of the proposed equivariance properties and validate our approach through extensive simulated and real-world experiments. Our results demonstrate that the proposed projection-based design significantly reduces both computational and memory costs. Moreover, the equivariant grasp models built on top of our tri-plane features consistently outperform their non-equivariant counterparts, achieving higher performance with only a modest computational overhead. Video and code can be viewed in: https://mousecpn.github.io/evg-page/
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis</title>
<link>https://arxiv.org/abs/2507.18848</link>
<guid>https://arxiv.org/abs/2507.18848</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiple Instance Learning, Whole Slide Image analysis, ViT, Prompt Token Clustering, Prototype-based pooling

Summary: <br />
The article introduces PTCMIL, a novel approach for Whole Slide Image (WSI) analysis using Multiple Instance Learning (MIL). PTCMIL integrates ViT with prompt token clustering to aggregate diverse patch information efficiently. By incorporating learnable prompt tokens, it aligns clustering and prediction tasks for improved performance. The method dynamically adapts clustering to individual WSIs, reducing complexity while preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL captures task-specific patterns effectively. Experimental results on various datasets demonstrate its superiority over existing methods in classification and survival analysis tasks. Ablation studies confirm its robustness and interpretability, making it a promising approach for WSI analysis. The code for PTCMIL is publicly available for further research and development. <br /> <div>
arXiv:2507.18848v1 Announce Type: cross 
Abstract: Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with the complexity and heterogeneity of WSIs. Existing MIL methods face challenges in aggregating diverse patch information into robust WSI representations. While ViTs and clustering-based approaches show promise, they are computationally intensive and fail to capture task-specific and slide-specific variability. To address these limitations, we propose PTCMIL, a novel Prompt Token Clustering-based ViT for MIL aggregation. By introducing learnable prompt tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in an end-to-end manner. It dynamically aligns clustering with downstream tasks, using projection-based clustering tailored to each WSI, reducing complexity while preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL efficiently captures task-relevant patterns. Extensive experiments on eight datasets demonstrate its superior performance in classification and survival analysis tasks, outperforming state-of-the-art methods. Systematic ablation studies confirm its robustness and strong interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning</title>
<link>https://arxiv.org/abs/2507.18857</link>
<guid>https://arxiv.org/abs/2507.18857</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, PrismRAG, fine-tuning framework, reasoning-centric habits, open-book QA benchmarks  
Summary:  
PrismRAG is a fine-tuning framework designed to address the shortcomings of retrieval-augmented generation models when dealing with confusing or semi-relevant context. It trains models using distractor-aware QA pairs that mix relevant evidence with subtle distractors, fostering reasoning-centric habits in the model. This approach enables the model to plan, rationalize, and synthesize answers without heavy reliance on human-engineered instructions. Evaluated across 12 open-book QA benchmarks, PrismRAG outperforms state-of-the-art solutions by improving average factuality by 5.4%. Its efficiency and effectiveness in handling diverse application domains and scenarios make it a promising tool for enhancing the performance of QA models. <br /><br />Summary: <div>
arXiv:2507.18857v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise</title>
<link>https://arxiv.org/abs/2507.18867</link>
<guid>https://arxiv.org/abs/2507.18867</guid>
<content:encoded><![CDATA[
<div> Framework, Exploration, Multi-agent reinforcement learning, Individual rewards, Sparse rewards
<br />
Summary:
The paper introduces a novel framework called LIGHT for efficient exploration in multi-agent reinforcement learning (MARL) environments with sparse rewards. LIGHT combines the use of dense individual rewards with human knowledge integration to guide agents towards efficient exploration. It considers individual action distribution and human expertise preference distribution to design individual intrinsic rewards for each agent. These rewards are based on actionable representational transformation relevant to Q-learning, aligning agents' action preferences with human expertise while maximizing joint action value. The experimental results show that LIGHT outperforms representative baselines in terms of performance and knowledge reusability across various sparse-reward tasks in challenging scenarios. The framework successfully integrates human expertise into MARL algorithms in an end-to-end manner, providing a more effective solution for exploration in complex problems. <br /><br /> <div>
arXiv:2507.18867v1 Announce Type: cross 
Abstract: Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Review of AI-based Intelligent Tutoring Systems: Applications and Challenges</title>
<link>https://arxiv.org/abs/2507.18882</link>
<guid>https://arxiv.org/abs/2507.18882</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-based Intelligent Tutoring Systems, pedagogical strategies, NLP, adaptive learning, student modeling

Summary: 
AI-based Intelligent Tutoring Systems have the potential to revolutionize education, but their effectiveness remains variable. A systematic literature review of studies from 2010 to 2025 explores the challenges and advancements in ITS application. Various domains such as pedagogical strategies, NLP, adaptive learning, student modeling, and domain-specific applications were analyzed. The review reveals the complexity of ITS effectiveness and highlights the need for improved scientific rigor in experimental design and data analysis. Recommendations for future research and practical implications were made. Overall, the study emphasizes the importance of understanding how ITS operate in real educational settings and addressing the challenges in their evaluation and application.<br /><br />Summary: <div>
arXiv:2507.18882v1 Announce Type: cross 
Abstract: AI-based Intelligent Tutoring Systems (ITS) have significant potential to transform teaching and learning. As efforts continue to design, develop, and integrate ITS into educational contexts, mixed results about their effectiveness have emerged. This paper provides a comprehensive review to understand how ITS operate in real educational settings and to identify the associated challenges in their application and evaluation. We use a systematic literature review method to analyze numerous qualified studies published from 2010 to 2025, examining domains such as pedagogical strategies, NLP, adaptive learning, student modeling, and domain-specific applications of ITS. The results reveal a complex landscape regarding the effectiveness of ITS, highlighting both advancements and persistent challenges. The study also identifies a need for greater scientific rigor in experimental design and data analysis. Based on these findings, suggestions for future research and practical implications are proposed.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling</title>
<link>https://arxiv.org/abs/2507.18897</link>
<guid>https://arxiv.org/abs/2507.18897</guid>
<content:encoded><![CDATA[
<div> Neural codec, extreme compression, Vector Quantization, Spoken Language Modeling, reconstructive stability <br />
<br />Summary:
The paper introduces HH-Codec, a neural codec designed for extreme compression in large-scale speech-to-speech systems. With a carefully designed Vector Quantization space for Spoken Language Modeling, the codec achieves compression at 24 tokens per second for 24 kHz audio using a single quantizer. The asymmetric encoder-decoder architecture (Audio-VQ-Mel-Audio) leverages dual supervision and progressive training to enhance reconstruction stability and fidelity, resulting in state-of-the-art performance with an ultra-low bandwidth of 0.3 kbps. The effectiveness of HH-Codec in codebook utilization and generative model adaptation is further evaluated through extensive ablations, validating the necessity of each module. The codec's source code is available on GitHub for further exploration and use. <div>
arXiv:2507.18897v1 Announce Type: cross 
Abstract: Discrete speech tokenization is a fundamental component in speech codecs. However, in large-scale speech-to-speech systems, the complexity of parallel streams from multiple quantizers and the computational cost of high-time-dimensional codecs pose significant challenges. In this paper, we introduce HH-Codec, a neural codec that achieves extreme compression at 24 tokens per second for 24 kHz audio while relying on single-quantizer inference. Our approach involves a carefully designed Vector Quantization space for Spoken Language Modeling, optimizing compression efficiency while minimizing information loss. Building on this, we propose an asymmetric encoder-decoder architecture (Audio-VQ-Mel-Audio) that leverages dual supervision and progressive training to enhance reconstruction stability and fidelity. HH-Codec achieves state-of-the-art performance in speech reconstruction with an ultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in codebook utilization and generative model adaptation, with extensive ablations validating the necessity of each module. HH-Codec is available at https://github.com/opendilab/HH-Codec.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.18918</link>
<guid>https://arxiv.org/abs/2507.18918</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual language models, activation patterns, low resource languages, Sparse Autoencoders, Low-Rank Adaptation

Summary: 
Multilingual large language models (LLMs) show strong cross-linguistic generalization, but medium to low resource languages struggle on benchmarks. Activation patterns in Gemma-2-2B across 10 languages reveal disparities, with medium to low resource languages receiving lower activations in early and deeper layers. Activation-aware fine-tuning through Low-Rank Adaptation (LoRA) results in significant activation gains for languages like Malayalam and Hindi, while maintaining English retention. Benchmark results post fine-tuning demonstrate consistent improvements, emphasizing activation alignment as crucial for enhancing multilingual LLM performance. <div>
arXiv:2507.18918v1 Announce Type: cross 
Abstract: Multilingual large language models (LLMs) exhibit strong cross-linguistic generalization, yet medium to low resource languages underperform on common benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to low resource languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam (ml), and Hindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic disparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower activations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this, we apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial activation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining English retention at approximately 91 percent. After fine-tuning, benchmark results show modest but consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual LLM performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WiSE-OD: Benchmarking Robustness in Infrared Object Detection</title>
<link>https://arxiv.org/abs/2507.18925</link>
<guid>https://arxiv.org/abs/2507.18925</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, infrared imagery, robustness, cross-modality, weight-space ensembling

Summary:
Object detection in infrared (IR) imagery is crucial for low-light and nighttime applications, but the lack of large-scale IR datasets leads models to rely on pre-training on RGB images. Fine-tuning on IR data improves accuracy but can compromise robustness under distribution shifts due to the RGB-IR modality gap. To address this challenge, LLVIP-C and FLIR-C benchmarks were introduced by corrupting standard IR datasets to create cross-modality out-of-distribution (OOD) challenges. The WiSE-OD weight-space ensembling method was proposed to combine knowledge from RGB and IR models, with variants WiSE-OD$_{ZS}$ and WiSE-OD$_{LP}$ enhancing cross-modality and corruption robustness without additional training or inference costs. Evaluation on existing detectors and baselines demonstrated the effectiveness of WiSE-OD in improving robustness in object detection tasks across different modalities and corruption scenarios. 

<br /><br />Summary: <div>
arXiv:2507.18925v1 Announce Type: cross 
Abstract: Object detection (OD) in infrared (IR) imagery is critical for low-light and nighttime applications. However, the scarcity of large-scale IR datasets forces models to rely on weights pre-trained on RGB images. While fine-tuning on IR improves accuracy, it often compromises robustness under distribution shifts due to the inherent modality gap between RGB and IR. To address this, we introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD) benchmarks built by applying corruption to standard IR datasets. Additionally, to fully leverage the complementary knowledge from RGB and infrared trained models, we propose WiSE-OD, a weight-space ensembling method with two variants: WiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and WiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both cross-modality and corruption robustness without any additional training or inference cost.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition</title>
<link>https://arxiv.org/abs/2507.18929</link>
<guid>https://arxiv.org/abs/2507.18929</guid>
<content:encoded><![CDATA[
<div> transformer, sticker emotion, multi-view, hierarchical fusion, multimodal

Summary:
- The article introduces a novel multi-granularity hierarchical fusion transformer (MGHFT) for sticker emotion understanding.
- MGHFT leverages Multimodal Large Language Models to interpret stickers from multiple views using rich textual context.
- A hierarchical fusion strategy is implemented to combine textual context with visual understanding, extracting global and local sticker features at multiple stages.
- Through contrastive learning and attention mechanisms, textual features are integrated at different stages of the visual backbone to enhance semantic understanding.
- The proposed text-guided fusion attention mechanism effectively integrates multimodal features, leading to improved sticker emotion recognition performance on public datasets. <div>
arXiv:2507.18929v1 Announce Type: cross 
Abstract: Although pre-trained visual models with text have demonstrated strong capabilities in visual feature extraction, sticker emotion understanding remains challenging due to its reliance on multi-view information, such as background knowledge and stylistic cues. To address this, we propose a novel multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view sticker interpreter based on Multimodal Large Language Models. Specifically, inspired by the human ability to interpret sticker emotions from multiple views, we first use Multimodal Large Language Models to interpret stickers by providing rich textual context via multi-view descriptions. Then, we design a hierarchical fusion strategy to fuse the textual context into visual understanding, which builds upon a pyramid visual transformer to extract both global and local sticker features at multiple stages. Through contrastive learning and attention mechanisms, textual features are injected at different stages of the visual backbone, enhancing the fusion of global- and local-granularity visual semantics with textual guidance. Finally, we introduce a text-guided fusion attention mechanism to effectively integrate the overall multimodal features, enhancing semantic understanding. Extensive experiments on 2 public sticker emotion datasets demonstrate that MGHFT significantly outperforms existing sticker emotion recognition approaches, achieving higher accuracy and more fine-grained emotion recognition. Compared to the best pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4% on F1 and 4.0% on accuracy. The code is released at https://github.com/cccccj-03/MGHFT_ACMMM2025.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN-based Surface Temperature Forecasts with Ensemble Numerical Weather Prediction over Medium-range Forecast Periods</title>
<link>https://arxiv.org/abs/2507.18937</link>
<guid>https://arxiv.org/abs/2507.18937</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, ensemble numerical weather prediction models, surface temperature forecasting, bias correction, spatial super-resolution<br />
<br />Summary: 
This study introduces a novel method that combines convolutional neural networks (CNNs) with ensemble numerical weather prediction (NWP) models to improve surface temperature forecasts beyond the short-range period. By utilizing CNN-based post-processing for bias correction and spatial super-resolution on each ensemble member, high-resolution temperature fields are reconstructed from low-resolution NWP outputs to reduce systematic errors. Random errors are minimized through ensemble averaging of the CNN-corrected members. Results show that correcting with CNN before ensemble averaging consistently produces more accurate forecasts. The proposed method, despite using low-resolution ensemble forecasts, outperforms deterministic NWP models in accuracy. This integrated approach offers a practical and scalable solution for enhancing medium-range temperature forecasts, especially beneficial for operational centers with limited computational resources.<br /><br /> <div>
arXiv:2507.18937v1 Announce Type: cross 
Abstract: This study proposes a method that integrates convolutional neural networks (CNNs) with ensemble numerical weather prediction (NWP) models, enabling surface temperature forecasting at lead times beyond the short-range (five-day) forecast period. Owing to limited computational resources, operational medium-range temperature forecasts typically rely on low-resolution NWP models, which are prone to systematic and random errors. To resolve these limitations, the proposed method first reduces systematic errors through CNN-based post-processing (bias correction and spatial super-resolution) on each ensemble member, reconstructing high-resolution temperature fields from low-resolution model outputs. Second, it reduces random errors through ensemble averaging of the CNN-corrected members. This study also investigates whether the sequence of CNN correction and ensemble averaging affects the forecast accuracy. For comparison with the proposed method, we additionally conducted experiments with the CNN trained on ensemble-averaged forecasts. The first approach--CNN correction before ensemble averaging--consistently achieved higher accuracy than the reverse approach. Although based on low-resolution ensemble forecasts, the proposed method notably outperformed the high-resolution deterministic NWP models. These findings indicate that combining CNN-based correction with ensemble averaging effectively reduces both the systematic and random errors in NWP model outputs. The proposed approach is a practical and scalable solution for improving medium-range temperature forecasts, and is particularly valuable at operational centers with limited computational resources.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models</title>
<link>https://arxiv.org/abs/2507.18945</link>
<guid>https://arxiv.org/abs/2507.18945</guid>
<content:encoded><![CDATA[
<div> keywords: academic papers, TreeReader, language model-augmented, hierarchical structure, interactive exploration

Summary:
TreeReader is a novel approach to navigating and understanding academic papers more efficiently. Traditional linear formats like PDF and HTML can be overwhelming and hinder comprehension of a paper's structure. While LLM-based chatbots offer summarization, they may lack nuanced understanding of specific sections. TreeReader decomposes papers into an interactive tree structure, allowing users to quickly grasp core ideas and selectively explore sections of interest. Each section is initially represented by a concise summary generated by an LLM, with underlying details accessible on demand. This design helps users verify summaries against the source text, leading to a more focused and efficient way of navigating complex academic literature. A user study confirmed TreeReader's positive impact on reading efficiency and comprehension.
<br /><br />Summary: TreeReader offers a new way to navigate and understand academic papers efficiently by combining hierarchical summarization with interactive exploration. The interactive tree structure and concise summaries generated by an LLM help users quickly locate key information, selectively explore sections of interest, and verify information against the source text. This approach bridges the gap between complex academic literature and users, enhancing reading efficiency and comprehension. <div>
arXiv:2507.18945v1 Announce Type: cross 
Abstract: Efficiently navigating and understanding academic papers is crucial for scientific progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a paper's hierarchical structure, making it difficult to locate key information. While LLM-based chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce unreliable information, and typically discard the document's navigational structure. Drawing insights from a formative study on academic reading practices, we introduce TreeReader, a novel language model-augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each section is initially represented by an LLM-generated concise summary, with underlying details accessible on demand. This design allows users to quickly grasp core ideas, selectively explore sections of interest, and verify summaries against the source text. A user study was conducted to evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more focused and efficient way to navigate and understand complex academic literature by bridging hierarchical summarization with interactive exploration.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN</title>
<link>https://arxiv.org/abs/2507.18967</link>
<guid>https://arxiv.org/abs/2507.18967</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater pollution, object recognition algorithms, YOLOv8, mean Average Precision, self-supervised learning

Summary:<br /><br />Underwater pollution is a major environmental issue, with large amounts of garbage present in oceans and rivers worldwide. Accurate detection of waste materials is essential for effective waste management and environmental monitoring. This study evaluated five object recognition algorithms for identifying underwater waste, with YOLOv8 demonstrating the best performance with a mean Average Precision of 80.9%. YOLOv8's advanced features such as improved anchor-free mechanisms and self-supervised learning contribute to its superior recognition capabilities. The results suggest that YOLOv8 can significantly enhance the detection and scalability of underwater cleanup operations, making it a valuable tool in combating pollution on a global scale.<br /><br /> <div>
arXiv:2507.18967v1 Announce Type: cross 
Abstract: Underwater pollution is one of today's most significant environmental concerns, with vast volumes of garbage found in seas, rivers, and landscapes around the world. Accurate detection of these waste materials is crucial for successful waste management, environmental monitoring, and mitigation strategies. In this study, we investigated the performance of five cutting-edge object recognition algorithms, namely YOLO (You Only Look Once) models, including YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional Neural Network (R-CNN), to identify which model was most effective at recognizing materials in underwater situations. The models were thoroughly trained and tested on a large dataset containing fifteen different classes under diverse conditions, such as low visibility and variable depths. From the above-mentioned models, YOLOv8 outperformed the others, with a mean Average Precision (mAP) of 80.9%, indicating a significant performance. This increased performance is attributed to YOLOv8's architecture, which incorporates advanced features such as improved anchor-free mechanisms and self-supervised learning, allowing for more precise and efficient recognition of items in a variety of settings. These findings highlight the YOLOv8 model's potential as an effective tool in the global fight against pollution, improving both the detection capabilities and scalability of underwater cleanup operations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation</title>
<link>https://arxiv.org/abs/2507.18973</link>
<guid>https://arxiv.org/abs/2507.18973</guid>
<content:encoded><![CDATA[
<div> framework, multi-TAG, language models, mathematical reasoning, tools
Summary:
Multi-TAG introduces a framework for enhancing mathematical reasoning systems by allowing large language models (LLMs) to concurrently use multiple external tools at each reasoning step. Unlike previous approaches, Multi-TAG does not require finetuning and can work with any LLM backbone. It significantly improves solution robustness and accuracy on challenging math benchmarks like MATH500, AIME, AMC, and OlympiadBench. The results show average improvements of 6.0% to 7.5% over state-of-the-art baselines across both open-weight and closed-source LLM backbones. This approach is promising for developing high-performance math reasoning systems that can handle complex problems requiring precise reasoning over multiple steps. Multi-TAG showcases the effectiveness of leveraging multiple tools for mathematical reasoning within a single framework. <br /><br />Summary: <div>
arXiv:2507.18973v1 Announce Type: cross 
Abstract: Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model</title>
<link>https://arxiv.org/abs/2507.18987</link>
<guid>https://arxiv.org/abs/2507.18987</guid>
<content:encoded><![CDATA[
<div> Bayesian Neural Networks, DTC recurrence, feature selection, Support Vector Machines, uncertainty quantification <br />
<br />
Summary: 
This study focuses on differentiated thyroid cancer (DTC) recurrence classification using a dataset of 383 patients and 16 variables. Initially, 11 machine learning models were tested, with Support Vector Machines achieving the highest accuracy. Feature selection using the Boruta algorithm led to Logistic Regression outperforming other models. However, traditional ML models lack uncertainty quantification, which is critical in clinical decisions. To address this, Bayesian Neural Networks with six different prior distributions were implemented. The BNN model with a Normal 0,10 prior exhibited high accuracies before and after feature selection. This comprehensive framework provides accurate DTC recurrence classification while also incorporating interpretability and uncertainty awareness. <div>
arXiv:2507.18987v1 Announce Type: cross 
Abstract: Differentiated thyroid cancer DTC recurrence is a major public health concern, requiring classification and predictive models that are not only accurate but also interpretable and uncertainty aware. This study introduces a comprehensive framework for DTC recurrence classification using a dataset containing 383 patients and 16 clinical and pathological variables. Initially, 11 machine learning ML models were employed using the complete dataset, where the Support Vector Machines SVM model achieved the highest accuracy of 0.9481. To reduce complexity and redundancy, feature selection was carried out using the Boruta algorithm, and the same ML models were applied to the reduced dataset, where it was observed that the Logistic Regression LR model obtained the maximum accuracy of 0.9611. However, these ML models often lack uncertainty quantification, which is critical in clinical decision making. Therefore, to address this limitation, the Bayesian Neural Networks BNN with six varying prior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy 0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and reduced datasets. The BNN model with Normal 0,10 prior distribution exhibited maximum accuracies of 0.9740 and 0.9870 before and after feature selection, respectively.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</title>
<link>https://arxiv.org/abs/2507.18989</link>
<guid>https://arxiv.org/abs/2507.18989</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, machine learning, arithmetic units, optimization, digital systems

Summary: 
- GENIAL is introduced as a machine learning-based framework for automatic generation and optimization of arithmetic units, specifically multipliers.
- The core of GENIAL is a Transformer-based surrogate model trained in two stages, to forecast hardware metrics such as power and area, enabling efficient search for new operand encodings that minimize power consumption.
- Extensive experiments show that GENIAL is more sample efficient and converges faster towards optimized designs, improving accuracy through a high-effort logic synthesis optimization flow.
- GENIAL automatically discovers encodings achieving up to 18% switching activity savings in multipliers for AI workloads compared to conventional methods.
- The versatility of GENIAL is demonstrated by significant improvements on Finite State Machines, showing applicability across a wide spectrum of logic functions.

<br /><br />Summary: <div>
arXiv:2507.18989v1 Announce Type: cross 
Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important to reduce the footprint of digital systems. Conventional design flows, which often rely on manual or heuristics-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, more specifically multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables to deploy a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two's complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL's applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A diffusion-based generative model for financial time series via geometric Brownian motion</title>
<link>https://arxiv.org/abs/2507.19003</link>
<guid>https://arxiv.org/abs/2507.19003</guid>
<content:encoded><![CDATA[
arXiv:2507.19003v1 Announce Type: cross 
Abstract: We propose a novel diffusion-based generative framework for financial time series that incorporates geometric Brownian motion (GBM), the foundation of the Black--Scholes theory, into the forward noising process. Unlike standard score-based models that treat price trajectories as generic numerical sequences, our method injects noise proportionally to asset prices at each time step, reflecting the heteroskedasticity observed in financial time series. By accurately balancing the drift and diffusion terms, we show that the resulting log-price process reduces to a variance-exploding stochastic differential equation, aligning with the formulation in score-based generative models. The reverse-time generative process is trained via denoising score matching using a Transformer-based architecture adapted from the Conditional Score-based Diffusion Imputation (CSDI) framework. Empirical evaluations on historical stock data demonstrate that our model reproduces key stylized facts heavy-tailed return distributions, volatility clustering, and the leverage effect more realistically than conventional diffusion models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment</title>
<link>https://arxiv.org/abs/2507.19004</link>
<guid>https://arxiv.org/abs/2507.19004</guid>
<content:encoded><![CDATA[
arXiv:2507.19004v1 Announce Type: cross 
Abstract: Rapid advances in medical imaging technology underscore the critical need for precise and automated image quality assessment (IQA) to ensure diagnostic accuracy. Existing medical IQA methods, however, struggle to generalize across diverse modalities and clinical scenarios. In response, we introduce MedIQA, the first comprehensive foundation model for medical IQA, designed to handle variability in image dimensions, modalities, anatomical regions, and types. We developed a large-scale multi-modality dataset with plentiful manually annotated quality scores to support this. Our model integrates a salient slice assessment module to focus on diagnostically relevant regions feature retrieval and employs an automatic prompt strategy that aligns upstream physical parameter pre-training with downstream expert annotation fine-tuning. Extensive experiments demonstrate that MedIQA significantly outperforms baselines in multiple downstream tasks, establishing a scalable framework for medical IQA and advancing diagnostic workflows and clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster</title>
<link>https://arxiv.org/abs/2507.19017</link>
<guid>https://arxiv.org/abs/2507.19017</guid>
<content:encoded><![CDATA[
arXiv:2507.19017v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization. In this article, we introduce MindSpeed RL, an effective and efficient system for large-scale RL training. Unlike existing centralized methods, MindSpeed RL organizes the essential data dependencies in RL training, i.e., sample flow and resharding flow, from a distributed view. On the one hand, a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. In addition, MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization. Compared with existing state-of-the-art systems, comprehensive experiments on the RL training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. Finally, we open--source MindSpeed RL and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Path Learning -- learning from noise and context for medical image denoising</title>
<link>https://arxiv.org/abs/2507.19035</link>
<guid>https://arxiv.org/abs/2507.19035</guid>
<content:encoded><![CDATA[
arXiv:2507.19035v1 Announce Type: cross 
Abstract: Medical imaging plays a critical role in modern healthcare, enabling clinicians to accurately diagnose diseases and develop effective treatment plans. However, noise, often introduced by imaging devices, can degrade image quality, leading to misinterpretation and compromised clinical outcomes. Existing denoising approaches typically rely either on noise characteristics or on contextual information from the image. Moreover, they are commonly developed and evaluated for a single imaging modality and noise type. Motivated by Geng et.al CNCL, which integrates both noise and context, this study introduces a Dual-Pathway Learning (DPL) model architecture that effectively denoises medical images by leveraging both sources of information and fusing them to generate the final output. DPL is evaluated across multiple imaging modalities and various types of noise, demonstrating its robustness and generalizability. DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on Gaussian noise and trained across all modalities. The code is available at 10.5281/zenodo.15836053.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closing the Modality Gap for Mixed Modality Search</title>
<link>https://arxiv.org/abs/2507.19054</link>
<guid>https://arxiv.org/abs/2507.19054</guid>
<content:encoded><![CDATA[
arXiv:2507.19054v1 Announce Type: cross 
Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet underexplored real-world application. In this work, we investigate how contrastive vision-language models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems</title>
<link>https://arxiv.org/abs/2507.19067</link>
<guid>https://arxiv.org/abs/2507.19067</guid>
<content:encoded><![CDATA[
arXiv:2507.19067v1 Announce Type: cross 
Abstract: Recommender systems, especially those based on graph neural networks (GNNs), have achieved remarkable success in capturing user-item interaction patterns. However, they remain susceptible to popularity bias--the tendency to over-recommend popular items--resulting in reduced content diversity and compromised fairness. In this paper, we propose PBiLoss, a novel regularization-based loss function designed to counteract popularity bias in graph-based recommender models explicitly. PBiLoss augments traditional training objectives by penalizing the model's inclination toward popular items, thereby encouraging the recommendation of less popular but potentially more personalized content. We introduce two sampling strategies: Popular Positive (PopPos) and Popular Negative (PopNeg), which respectively modulate the contribution of the positive and negative popular items during training. We further explore two methods to distinguish popular items: one based on a fixed popularity threshold and another without any threshold, making the approach flexible and adaptive. Our proposed method is model-agnostic and can be seamlessly integrated into state-of-the-art graph-based frameworks such as LightGCN and its variants. Comprehensive experiments across multiple real-world datasets demonstrate that PBiLoss significantly improves fairness, as demonstrated by reductions in the Popularity-Rank Correlation for Users (PRU) and Popularity-Rank Correlation for Items (PRI), while maintaining or even enhancing standard recommendation accuracy and ranking metrics. These results highlight the effectiveness of directly embedding fairness objectives into the optimization process, providing a practical and scalable solution for balancing accuracy and equitable content exposure in modern recommender systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching</title>
<link>https://arxiv.org/abs/2507.19098</link>
<guid>https://arxiv.org/abs/2507.19098</guid>
<content:encoded><![CDATA[
arXiv:2507.19098v1 Announce Type: cross 
Abstract: Reliable medical image classification requires accurate predictions and well-calibrated uncertainty estimates, especially in high-stakes clinical settings. This work presents MedSymmFlow, a generative-discriminative hybrid model built on Symmetrical Flow Matching, designed to unify classification, generation, and uncertainty quantification in medical imaging. MedSymmFlow leverages a latent-space formulation that scales to high-resolution inputs and introduces a semantic mask conditioning mechanism to enhance diagnostic relevance. Unlike standard discriminative models, it naturally estimates uncertainty through its generative sampling process. The model is evaluated on four MedMNIST datasets, covering a range of modalities and pathologies. The results show that MedSymmFlow matches or exceeds the performance of established baselines in classification accuracy and AUC, while also delivering reliable uncertainty estimates validated by performance improvements under selective prediction.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.19102</link>
<guid>https://arxiv.org/abs/2507.19102</guid>
<content:encoded><![CDATA[
arXiv:2507.19102v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Code Review Using Large Language Models at Ericsson: An Experience Report</title>
<link>https://arxiv.org/abs/2507.19115</link>
<guid>https://arxiv.org/abs/2507.19115</guid>
<content:encoded><![CDATA[
arXiv:2507.19115v1 Announce Type: cross 
Abstract: Code review is one of the primary means of assuring the quality of released software along with testing and static analysis. However, code review requires experienced developers who may not always have the time to perform an in-depth review of code. Thus, automating code review can help alleviate the cognitive burden on experienced software developers allowing them to focus on their primary activities of writing code to add new features and fix bugs. In this paper, we describe our experience in using Large Language Models towards automating the code review process in Ericsson. We describe the development of a lightweight tool using LLMs and static program analysis. We then describe our preliminary experiments with experienced developers in evaluating our code review tool and the encouraging results.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Structure Learning with Privacy Guarantees for Open Graph Data</title>
<link>https://arxiv.org/abs/2507.19116</link>
<guid>https://arxiv.org/abs/2507.19116</guid>
<content:encoded><![CDATA[
arXiv:2507.19116v1 Announce Type: cross 
Abstract: Ensuring privacy in large-scale open datasets is increasingly challenging under regulations such as the General Data Protection Regulation (GDPR). While differential privacy (DP) provides strong theoretical guarantees, it primarily focuses on noise injection during model training, neglecting privacy preservation at the data publishing stage. Existing privacy-preserving data publishing (PPDP) approaches struggle to balance privacy and utility, particularly when data publishers and users are distinct entities. To address this gap, we focus on the graph recovery problem and propose a novel privacy-preserving estimation framework for open graph data, leveraging Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike traditional methods that perturb gradients or model updates, our approach ensures unbiased graph structure recovery while enforcing DP at the data publishing stage. Moreover, we provide theoretical guarantees on estimation accuracy and extend our method to discrete-variable graphs, a setting often overlooked in DP research. Experimental results in graph learning demonstrate robust performance, offering a viable solution for privacy-conscious graph analysis.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction</title>
<link>https://arxiv.org/abs/2507.19119</link>
<guid>https://arxiv.org/abs/2507.19119</guid>
<content:encoded><![CDATA[
arXiv:2507.19119v1 Announce Type: cross 
Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and robotics. While existing point-based and grid-based methods expose two key limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representation lacks interaction with the frequency domain in modeling trajectory sequences. To address these challenges, we propose PatchTraj, a dynamic patch-based trajectory prediction framework that unifies time-domain and frequency-domain representations. Specifically, we decompose the trajectory into raw time sequences and frequency components, employing dynamic patch partitioning for multi-scale trajectory segmentation to capture hierarchical motion patterns. Each patch is processed by an adaptive embedding layer with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of two branches interact via cross-modal attention, enabling complementary fusion of temporal and spectral cues. Finally, a Transformer encoder-decoder integrates both modalities to autoregressively predict future trajectories. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method achieves state-of-the-art performance with high efficiency.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessment of Personality Dimensions Across Situations Using Conversational Speech</title>
<link>https://arxiv.org/abs/2507.19137</link>
<guid>https://arxiv.org/abs/2507.19137</guid>
<content:encoded><![CDATA[
arXiv:2507.19137v1 Announce Type: cross 
Abstract: Prior research indicates that users prefer assistive technologies whose personalities align with their own. This has sparked interest in automatic personality perception (APP), which aims to predict an individual's perceived personality traits. Previous studies in APP have treated personalities as static traits, independent of context. However, perceived personalities can vary by context and situation as shown in psychological research. In this study, we investigate the relationship between conversational speech and perceived personality for participants engaged in two work situations (a neutral interview and a stressful client interaction). Our key findings are: 1) perceived personalities differ significantly across interactions, 2) loudness, sound level, and spectral flux features are indicative of perceived extraversion, agreeableness, conscientiousness, and openness in neutral interactions, while neuroticism correlates with these features in stressful contexts, 3) handcrafted acoustic features and non-verbal features outperform speaker embeddings in inference of perceived personality, and 4) stressful interactions are more predictive of neuroticism, aligning with existing psychological research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Photovoltaic Assessment with Large Language Model</title>
<link>https://arxiv.org/abs/2507.19144</link>
<guid>https://arxiv.org/abs/2507.19144</guid>
<content:encoded><![CDATA[
arXiv:2507.19144v1 Announce Type: cross 
Abstract: Accurate detection and localization of solar photovoltaic (PV) panels in satellite imagery is essential for optimizing microgrids and active distribution networks (ADNs), which are critical components of renewable energy systems. Existing methods lack transparency regarding their underlying algorithms or training datasets, rely on large, high-quality PV training data, and struggle to generalize to new geographic regions or varied environmental conditions without extensive re-training. These limitations lead to inconsistent detection outcomes, hindering large-scale deployment and data-driven grid optimization. In this paper, we investigate how large language models (LLMs) can be leveraged to overcome these challenges. Despite their promise, LLMs face several challenges in solar panel detection, including difficulties with multi-step logical processes, inconsistent output formatting, frequent misclassification of visually similar objects (e.g., shadows, parking lots), and low accuracy in complex tasks such as spatial localization and quantification. To overcome these issues, we propose the PV Assessment with LLMs (PVAL) framework, which incorporates task decomposition for more efficient workflows, output standardization for consistent and scalable formatting, few-shot prompting to enhance classification accuracy, and fine-tuning using curated PV datasets with detailed annotations. PVAL ensures transparency, scalability, and adaptability across heterogeneous datasets while minimizing computational overhead. By combining open-source accessibility with robust methodologies, PVAL establishes an automated and reproducible pipeline for solar panel detection, paving the way for large-scale renewable energy integration and optimized grid management.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2507.19151</link>
<guid>https://arxiv.org/abs/2507.19151</guid>
<content:encoded><![CDATA[
arXiv:2507.19151v1 Announce Type: cross 
Abstract: Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case</title>
<link>https://arxiv.org/abs/2507.19156</link>
<guid>https://arxiv.org/abs/2507.19156</guid>
<content:encoded><![CDATA[
arXiv:2507.19156v1 Announce Type: cross 
Abstract: The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models</title>
<link>https://arxiv.org/abs/2507.19185</link>
<guid>https://arxiv.org/abs/2507.19185</guid>
<content:encoded><![CDATA[
arXiv:2507.19185v1 Announce Type: cross 
Abstract: Static benchmarks fail to capture LLM vulnerabilities emerging through community experimentation in online forums. We present PrompTrend, a system that collects vulnerability data across platforms and evaluates them using multidimensional scoring, with an architecture designed for scalable monitoring. Cross-sectional analysis of 198 vulnerabilities collected from online communities over a five-month period (January-May 2025) and tested on nine commercial models reveals that advanced capabilities correlate with increased vulnerability in some architectures, psychological attacks significantly outperform technical exploits, and platform dynamics shape attack effectiveness with measurable model-specific patterns. The PrompTrend Vulnerability Assessment Framework achieves 78% classification accuracy while revealing limited cross-model transferability, demonstrating that effective LLM security requires comprehensive socio-technical monitoring beyond traditional periodic assessment. Our findings challenge the assumption that capability advancement improves security and establish community-driven psychological manipulation as the dominant threat vector for current language models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?</title>
<link>https://arxiv.org/abs/2507.19195</link>
<guid>https://arxiv.org/abs/2507.19195</guid>
<content:encoded><![CDATA[
arXiv:2507.19195v1 Announce Type: cross 
Abstract: Despite the ongoing improvements in the design of large language models (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding and amplifying social biases. This study examines how dialectal variation, specifically African American Vernacular English (AAVE) versus Standard American English (SAE), interacts with data poisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it remains comparatively unaffected for SAE. Larger models exhibit a more significant amplification effect which suggests heightened susceptibility with scale. To further assess these disparities, we employed GPT-4o as a fairness auditor, which identified harmful stereotypical patterns disproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and intellectual inferiority. These findings underscore the compounding impact of data poisoning and dialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions, and socially responsible training protocols during development.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design</title>
<link>https://arxiv.org/abs/2507.19197</link>
<guid>https://arxiv.org/abs/2507.19197</guid>
<content:encoded><![CDATA[
arXiv:2507.19197v1 Announce Type: cross 
Abstract: Accurate spatial prediction of power integrity issues, such as IR drop, is critical for reliable VLSI design. However, traditional simulation-based solvers are computationally expensive and difficult to scale. We address this challenge by reformulating IR drop estimation as a pixel-wise regression task on heterogeneous multi-channel physical maps derived from circuit layouts. Prior learning-based methods treat all input layers (e.g., metal, via, and current maps) equally, ignoring their varying importance to prediction accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention (WACA) mechanism, which recursively enhances weak feature channels while suppressing over-dominant ones through a two-stage gating strategy. Integrated into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and balanced feature representation. On the public ICCAD-2023 benchmark, our method outperforms the ICCAD-2023 contest winner by reducing mean absolute error by 61.1% and improving F1-score by 71.0%. These results demonstrate that channel-wise heterogeneity is a key inductive bias in physical layout analysis for VLSI.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning</title>
<link>https://arxiv.org/abs/2507.19199</link>
<guid>https://arxiv.org/abs/2507.19199</guid>
<content:encoded><![CDATA[
arXiv:2507.19199v1 Announce Type: cross 
Abstract: Automatic classification of Diabetic Retinopathy (DR) can assist ophthalmologists in devising personalized treatment plans, making it a critical component of clinical practice. However, imbalanced data distribution in the dataset becomes a bottleneck in the generalization of deep learning models trained for DR classification. In this work, we combine global attention block (GAB) and category attention block (CAB) into the deep learning model, thus effectively overcoming the imbalanced data distribution problem in DR classification. Our proposed approach is based on an attention mechanism-based deep learning model that employs three pre-trained networks, namely, MobileNetV3-small, Efficientnet-b0, and DenseNet-169 as the backbone architecture. We evaluate the proposed method on two publicly available datasets of retinal fundoscopy images for DR. Experimental results show that on the APTOS dataset, the DenseNet-169 yielded 83.20% mean accuracy, followed by the MobileNetV3-small and EfficientNet-b0, which yielded 82% and 80% accuracies, respectively. On the EYEPACS dataset, the EfficientNet-b0 yielded a mean accuracy of 80%, while the DenseNet-169 and MobileNetV3-small yielded 75.43% and 76.68% accuracies, respectively. In addition, we also compute the F1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of 95.5%, and a kappa score of 88.2% for the experiments. Moreover, in our work, the MobileNetV3-small has 1.6 million parameters on the APTOS dataset and 0.90 million parameters on the EYEPACS dataset, which is comparatively less than other methods. The proposed approach achieves competitive performance that is at par with recently reported works on DR classification.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model</title>
<link>https://arxiv.org/abs/2507.19201</link>
<guid>https://arxiv.org/abs/2507.19201</guid>
<content:encoded><![CDATA[
arXiv:2507.19201v1 Announce Type: cross 
Abstract: Mammography is the most commonly used imaging modality for breast cancer screening, driving an increasing demand for deep-learning techniques to support large-scale analysis. However, the development of accurate and robust methods is often limited by insufficient data availability and a lack of diversity in lesion characteristics. While generative models offer a promising solution for data synthesis, current approaches often fail to adequately emphasize lesion-specific features and their relationships with surrounding tissues. In this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel framework designed to jointly synthesize holistic mammogram images and localized lesions. GCDM is built upon a latent denoising diffusion framework, where the noised latent image is concatenated with a soft mask embedding that represents breast, lesion, and their transitional regions, ensuring anatomical coherence between them during the denoising process. To further emphasize lesion-specific features, GCDM incorporates a gated conditioning branch that guides the denoising process by dynamically selecting and fusing the most relevant radiomic and geometric properties of lesions, effectively capturing their interplay. Experimental results demonstrate that GCDM achieves precise control over small lesion areas while enhancing the realism and diversity of synthesized mammograms. These advancements position GCDM as a promising tool for clinical applications in mammogram synthesis. Our code is available at https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV</title>
<link>https://arxiv.org/abs/2507.19234</link>
<guid>https://arxiv.org/abs/2507.19234</guid>
<content:encoded><![CDATA[
arXiv:2507.19234v1 Announce Type: cross 
Abstract: Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory</title>
<link>https://arxiv.org/abs/2507.19245</link>
<guid>https://arxiv.org/abs/2507.19245</guid>
<content:encoded><![CDATA[
arXiv:2507.19245v1 Announce Type: cross 
Abstract: This paper contributes to the Alpay Algebra by demonstrating that the stable outcome of a self referential process, obtained by iterating a transformation through all ordinal stages, is identical to the unique equilibrium of an unbounded revision dialogue between a system and its environment. The analysis initially elucidates how classical fixed point theorems guarantee such convergence in finite settings and subsequently extends the argument to the transfinite domain, relying upon well founded induction and principles of order theoretic continuity.
  Furthermore, the resulting transordinal fixed point operator is embedded into dependent type theory, a formalization which permits every step of the transfinite iteration and its limit to be verified within a modern proof assistant. This procedure yields a machine checked proof that the iterative dialogue necessarily stabilizes and that its limit is unique. The result provides a foundation for Alpay's philosophical claim of semantic convergence within the framework of constructive logic. By unifying concepts from fixed point theory, game semantics, ordinal analysis, and type theory, this research establishes a broadly accessible yet formally rigorous foundation for reasoning about infinite self referential systems and offers practical tools for certifying their convergence within computational environments.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Markov Categorical Framework for Language Modeling</title>
<link>https://arxiv.org/abs/2507.19247</link>
<guid>https://arxiv.org/abs/2507.19247</guid>
<content:encoded><![CDATA[
arXiv:2507.19247v1 Announce Type: cross 
Abstract: Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the model's prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: https://github.com/asiresearch/lm-theory
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects</title>
<link>https://arxiv.org/abs/2507.19271</link>
<guid>https://arxiv.org/abs/2507.19271</guid>
<content:encoded><![CDATA[
arXiv:2507.19271v1 Announce Type: cross 
Abstract: Code review is essential for maintaining software quality but often time-consuming and cognitively demanding, especially in industrial environments. Recent advancements in language models (LMs) have opened new avenues for automating core review tasks. This study presents the empirical evaluation of monolingual fine-tuning on the performance of open-source LMs across three key automated code review tasks: Code Change Quality Estimation, Review Comment Generation, and Code Refinement. We fine-tuned three distinct models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific dataset combining public benchmarks with industrial repositories. Our study investigates how different configurations of programming languages and natural languages in the training data affect LM performance, particularly in comment generation. Additionally, we benchmark the fine-tuned models against an automated software analysis tool (ASAT) and human reviewers to evaluate their practical utility in real-world settings. Our results show that monolingual fine-tuning improves model accuracy and relevance compared to multilingual baselines. While LMs can effectively support code review workflows, especially for routine or repetitive tasks, human reviewers remain superior in handling semantically complex or context-sensitive changes. Our findings highlight the importance of language alignment and task-specific adaptation in optimizing LMs for automated code review.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards LLM-Enhanced Group Recommender Systems</title>
<link>https://arxiv.org/abs/2507.19283</link>
<guid>https://arxiv.org/abs/2507.19283</guid>
<content:encoded><![CDATA[
arXiv:2507.19283v1 Announce Type: cross 
Abstract: In contrast to single-user recommender systems, group recommender systems are designed to generate and explain recommendations for groups. This group-oriented setting introduces additional complexities, as several factors - absent in individual contexts - must be addressed. These include understanding group dynamics (e.g., social dependencies within the group), defining effective decision-making processes, ensuring that recommendations are suitable for all group members, and providing group-level explanations as well as explanations for individual users. In this paper, we analyze in which way large language models (LLMs) can support these aspects and help to increase the overall decision support quality and applicability of group recommender systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Topological Defects in Polar Fluids via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19298</link>
<guid>https://arxiv.org/abs/2507.19298</guid>
<content:encoded><![CDATA[
arXiv:2507.19298v1 Announce Type: cross 
Abstract: Topological defects in active polar fluids exhibit complex dynamics driven by internally generated stresses, reflecting the deep interplay between topology, flow, and non-equilibrium hydrodynamics. Feedback control offers a powerful means to guide such systems, enabling transitions between dynamic states. We investigated closed-loop steering of integer-charged defects in a confined active fluid by modulating the spatial profile of activity. Using a continuum hydrodynamic model, we show that localized control of active stress induces flow fields that can reposition and direct defects along prescribed trajectories by exploiting non-linear couplings in the system. A reinforcement learning framework is used to discover effective control strategies that produce robust defect transport across both trained and novel trajectories. The results highlight how AI agents can learn the underlying dynamics and spatially structure activity to manipulate topological excitations, offering insights into the controllability of active matter and the design of adaptive, self-organized materials.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes</title>
<link>https://arxiv.org/abs/2507.19304</link>
<guid>https://arxiv.org/abs/2507.19304</guid>
<content:encoded><![CDATA[
arXiv:2507.19304v1 Announce Type: cross 
Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object detection accuracy. To address real-world challenges in outdoor 3D object detection, fusion of LiDAR and RGB input has started gaining traction. However, effective integration of these modalities for precise object detection task still remains a largely open problem. To address that, we propose a MultiStream Detection (MuStD) network, that meticulously extracts task-relevant information from both data modalities. The network follows a three-stream structure. Its LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input while the LiDAR-Height Compression stream computes Bird's-Eye View features. An additional 3D Multimodal stream combines RGB and LiDAR features using UV mapping and polar coordinate indexing. Eventually, the features containing comprehensive spatial, textural and geometric information are carefully fused and fed to a detection head for 3D object detection. Our extensive evaluation on the challenging KITTI Object Detection Benchmark using public testing server at https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&amp;result=d162ec699d6992040e34314d19ab7f5c217075e0 establishes the efficacy of our method by achieving new state-of-the-art or highly competitive results in different categories while remaining among the most efficient methods. Our code will be released through MuStD GitHub repository at https://github.com/IbrahimUWA/MuStD.git
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.19321</link>
<guid>https://arxiv.org/abs/2507.19321</guid>
<content:encoded><![CDATA[
arXiv:2507.19321v1 Announce Type: cross 
Abstract: Understanding the decisions made by deep neural networks is essential in high-stakes domains such as medical imaging and autonomous driving. Yet, these models often lack transparency, particularly in computer vision. Prototypical-parts-based neural networks have emerged as a promising solution by offering concept-level explanations. However, most are limited to fine-grained classification tasks, with few exceptions such as InfoDisent. InfoDisent extends prototypical models to large-scale datasets like ImageNet, but produces complex explanations.
  We introduce Sparse Information Disentanglement for Explainability (SIDE), a novel method that improves the interpretability of prototypical parts through a dedicated training and pruning scheme that enforces sparsity. Combined with sigmoid activations in place of softmax, this approach allows SIDE to associate each class with only a small set of relevant prototypes. Extensive experiments show that SIDE matches the accuracy of existing methods while reducing explanation size by over $90\%$, substantially enhancing the understandability of prototype-based explanations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs</title>
<link>https://arxiv.org/abs/2507.19334</link>
<guid>https://arxiv.org/abs/2507.19334</guid>
<content:encoded><![CDATA[
arXiv:2507.19334v1 Announce Type: cross 
Abstract: Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over LLM-based baselines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks</title>
<link>https://arxiv.org/abs/2507.19353</link>
<guid>https://arxiv.org/abs/2507.19353</guid>
<content:encoded><![CDATA[
arXiv:2507.19353v1 Announce Type: cross 
Abstract: Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models</title>
<link>https://arxiv.org/abs/2507.19361</link>
<guid>https://arxiv.org/abs/2507.19361</guid>
<content:encoded><![CDATA[
arXiv:2507.19361v1 Announce Type: cross 
Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences</title>
<link>https://arxiv.org/abs/2507.19362</link>
<guid>https://arxiv.org/abs/2507.19362</guid>
<content:encoded><![CDATA[
arXiv:2507.19362v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation</title>
<link>https://arxiv.org/abs/2507.19368</link>
<guid>https://arxiv.org/abs/2507.19368</guid>
<content:encoded><![CDATA[
arXiv:2507.19368v1 Announce Type: cross 
Abstract: Artificial intelligence is increasingly leveraged across various domains to automate decision-making processes that significantly impact human lives. In medical image analysis, deep learning models have demonstrated remarkable performance. However, their inherent complexity makes them black box systems, raising concerns about reliability and interpretability. Counterfactual explanations provide comprehensible insights into decision processes by presenting hypothetical "what-if" scenarios that alter model classifications. By examining input alterations, counterfactual explanations provide patterns that influence the decision-making process. Despite their potential, generating plausible counterfactuals that adhere to similarity constraints providing human-interpretable explanations remains a challenge. In this paper, we investigate this challenge by a model-specific optimization approach. While deep generative models such as variational autoencoders (VAEs) exhibit significant generative power, probabilistic models like sum-product networks (SPNs) efficiently represent complex joint probability distributions. By modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we leverage its dual role as both a latent space descriptor and a classifier for a given discrimination task. This formulation enables the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. We conduct experimental evaluation on the cheXpert dataset. To evaluate the effectiveness of the integration of SPNs, our SPN-guided latent space manipulation is compared against a neural network baseline. Additionally, the trade-off between latent variable regularization and counterfactual quality is analyzed.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation for Spoken Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2507.19374</link>
<guid>https://arxiv.org/abs/2507.19374</guid>
<content:encoded><![CDATA[
arXiv:2507.19374v1 Announce Type: cross 
Abstract: While there exist strong benchmark datasets for grammatical error correction (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-resourced. In this paper, we propose a fully automated method to generate audio-text pairs with grammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be used to evaluate the generated data and choose the more suitable dataset for SGEC. The goal is to generate an augmented dataset that maintains the textual and acoustic characteristics of the original data while providing new types of errors. This augmented dataset should augment and enrich the original corpus without altering the language assessment scores of the second language (L2) learners. We evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the audio-text pairs). Our experiments are conducted on the S\&amp;I Corpus, the first publicly available speech dataset with grammar error annotations.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReCatcher: Towards LLMs Regression Testing for Code Generation</title>
<link>https://arxiv.org/abs/2507.19390</link>
<guid>https://arxiv.org/abs/2507.19390</guid>
<content:encoded><![CDATA[
arXiv:2507.19390v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) for code generation evolve rapidly through fine-tuning, merging, or new model releases. However, such updates can introduce regressions, not only in correctness but also in code quality and performance. To address this, we present ReCatcher, a regression testing framework for Python code generation. ReCatcher systematically compares two LLMs, typically a current model and a candidate update, across three dimensions: logical correctness, static code quality, and execution performance. We apply ReCatcher to assess regressions across three update scenarios, fine-tuning, merging, and model release, using CodeLlama, DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with cross-language datasets increases syntax errors by up to 12%. Merging with general-purpose models like Llama2 leads to regressions in correctness by up to 18%. GPT-4o introduces regressions of up to 50% in handling missing imports compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance degradation in execution time versus GPT-4o. Overall, logical correctness, performance, and error handling (e.g., syntax errors and missing imports) are the most regression-prone areas. Comparing ReCatcher with baseline solutions, it presents better and consistent accuracy across logical and performance aspects. ReCatcher highlights the importance of systematic regression evaluation before adopting new models, while assisting researchers and practitioners in making more informed update decisions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays</title>
<link>https://arxiv.org/abs/2507.19398</link>
<guid>https://arxiv.org/abs/2507.19398</guid>
<content:encoded><![CDATA[
arXiv:2507.19398v1 Announce Type: cross 
Abstract: Chest radiography (CXR) plays a crucial role in the diagnosis of various diseases. However, the inherent class imbalance in the distribution of clinical findings presents a significant challenge for current self-supervised deep learning models. These models often fail to accurately classify long-tailed classes. Current Vision-Language models such as Contrastive Language Image Pre-training (CLIP) models effectively model the manifold distribution of the latent space, enabling high zero-shot classification accuracies. Although CLIP performs well on most of the primary classes in the dataset, our work reveals that its effectiveness decreases significantly for classes with a long-tailed distribution. Our approach employs a class-weighting mechanism that directly aligns with the distribution of classes within the latent space. This method ensures a substantial improvement in overall classification performance, with particular emphasis on enhancing the recognition and accuracy of rarely observed classes. We accomplish this by applying Gaussian Mixture Model (GMM) clustering to the latent space. The subsequent clusters are further refined by Student t-distribution, followed by a metric loss that utilizes the altered embeddings. Our approach facilitates stable and adaptive clustering of the features. This results in a notable average improvement of 7\% points in zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from previous SOTA models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security</title>
<link>https://arxiv.org/abs/2507.19399</link>
<guid>https://arxiv.org/abs/2507.19399</guid>
<content:encoded><![CDATA[
arXiv:2507.19399v1 Announce Type: cross 
Abstract: As large language models (LLMs) increasingly integrate native code interpreters, they enable powerful real-time execution capabilities, substantially expanding their utility. However, such integrations introduce potential system-level cybersecurity threats, fundamentally different from prompt-based vulnerabilities. To systematically evaluate these interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting CPU, memory, and disk resource exhaustion. Each risk category includes explicitly malicious ("direct") and plausibly benign ("indirect") prompt variants. Our automated evaluation framework assesses not only whether LLMs refuse or generates risky code, but also executes the generated code within the interpreter environment to evaluate code correctness, simplifications made by the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially available models from OpenAI and Google, we uncover significant and inconsistent vulnerabilities. For instance, evaluations show substantial disparities even within providers - OpenAI's o4-mini correctly refuses risky requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results particularly underscore that indirect, socially-engineered prompts substantially weaken model defenses. This highlights an urgent need for interpreter-specific cybersecurity benchmarks, dedicated mitigation tools (e.g., guardrails), and clear industry standards to guide safe and responsible deployment of LLM interpreter integrations. The benchmark dataset and evaluation code are publicly released to foster further research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions</title>
<link>https://arxiv.org/abs/2507.19403</link>
<guid>https://arxiv.org/abs/2507.19403</guid>
<content:encoded><![CDATA[
arXiv:2507.19403v1 Announce Type: cross 
Abstract: Connected and software-defined vehicles promise to offer a broad range of services and advanced functions to customers, aiming to increase passenger comfort and support autonomous driving capabilities. Due to the high reliability and availability requirements of connected vehicles, it is crucial to resolve any occurring failures quickly. To achieve this however, a complex cloud/edge architecture with a mesh of dependencies must be navigated to diagnose the responsible root cause. As such, manual analyses become unfeasible since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible platform for the automated diagnosis of connected vehicle functions. The platform enables the creation of pipelines that cover all steps from initial data collection to the tracing of potential root causes. In addition, SDVDiag supports self-adaptive behavior by the ability to exchange modules at runtime. Dependencies between functions are detected and continuously updated, resulting in a dynamic graph view of the system. In addition, vital system metrics are monitored for anomalies. Whenever an incident is investigated, a snapshot of the graph is taken and augmented by relevant anomalies. Finally, the analysis is performed by traversing the graph and creating a ranking of the most likely causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment for connected vehicle functions. The results show that injected faults can be detected reliably. As such, the platform offers the potential to gain new insights and reduce downtime by identifying problems and their causes at an early stage.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Arbitrary Predictions from Equally Valid Models</title>
<link>https://arxiv.org/abs/2507.19408</link>
<guid>https://arxiv.org/abs/2507.19408</guid>
<content:encoded><![CDATA[
arXiv:2507.19408v1 Announce Type: cross 
Abstract: Model multiplicity refers to the existence of multiple machine learning models that describe the data equally well but may produce different predictions on individual samples. In medicine, these models can admit conflicting predictions for the same patient -- a risk that is poorly understood and insufficiently addressed.
  In this study, we empirically analyze the extent, drivers, and ramifications of predictive multiplicity across diverse medical tasks and model architectures, and show that even small ensembles can mitigate/eliminate predictive multiplicity in practice. Our analysis reveals that (1) standard validation metrics fail to identify a uniquely optimal model and (2) a substantial amount of predictions hinges on arbitrary choices made during model development. Using multiple models instead of a single model reveals instances where predictions differ across equally plausible models -- highlighting patients that would receive arbitrary diagnoses if any single model were used. In contrast, (3) a small ensemble paired with an abstention strategy can effectively mitigate measurable predictive multiplicity in practice; predictions with high inter-model consensus may thus be amenable to automated classification. While accuracy is not a principled antidote to predictive multiplicity, we find that (4) higher accuracy achieved through increased model capacity reduces predictive multiplicity.
  Our findings underscore the clinical importance of accounting for model multiplicity and advocate for ensemble-based strategies to improve diagnostic reliability. In cases where models fail to reach sufficient consensus, we recommend deferring decisions to expert review.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</title>
<link>https://arxiv.org/abs/2507.19427</link>
<guid>https://arxiv.org/abs/2507.19427</guid>
<content:encoded><![CDATA[
arXiv:2507.19427v1 Announce Type: cross 
Abstract: Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.19457</link>
<guid>https://arxiv.org/abs/2507.19457</guid>
<content:encoded><![CDATA[
arXiv:2507.19457v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let It Go? Not Quite: Addressing Item Cold Start in Sequential Recommendations with Content-Based Initialization</title>
<link>https://arxiv.org/abs/2507.19473</link>
<guid>https://arxiv.org/abs/2507.19473</guid>
<content:encoded><![CDATA[
arXiv:2507.19473v1 Announce Type: cross 
Abstract: Many sequential recommender systems suffer from the cold start problem, where items with few or no interactions cannot be effectively used by the model due to the absence of a trained embedding. Content-based approaches, which leverage item metadata, are commonly used in such scenarios. One possible way is to use embeddings derived from content features such as textual descriptions as initialization for the model embeddings. However, directly using frozen content embeddings often results in suboptimal performance, as they may not fully adapt to the recommendation task. On the other hand, fine-tuning these embeddings can degrade performance for cold-start items, as item representations may drift far from their original structure after training. We propose a novel approach to address this limitation. Instead of entirely freezing the content embeddings or fine-tuning them extensively, we introduce a small trainable delta to frozen embeddings that enables the model to adapt item representations without letting them go too far from their original semantic structure. This approach demonstrates consistent improvements across multiple datasets and modalities, including e-commerce datasets with textual descriptions and a music dataset with audio-based representation.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts</title>
<link>https://arxiv.org/abs/2507.19477</link>
<guid>https://arxiv.org/abs/2507.19477</guid>
<content:encoded><![CDATA[
arXiv:2507.19477v1 Announce Type: cross 
Abstract: Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secret Collusion among AI Agents: Multi-Agent Deception via Steganography</title>
<link>https://arxiv.org/abs/2402.07510</link>
<guid>https://arxiv.org/abs/2402.07510</guid>
<content:encoded><![CDATA[
arXiv:2402.07510v5 Announce Type: replace 
Abstract: Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Question Embeddings with Cognitive Representation Optimization for Knowledge Tracing</title>
<link>https://arxiv.org/abs/2504.04121</link>
<guid>https://arxiv.org/abs/2504.04121</guid>
<content:encoded><![CDATA[
arXiv:2504.04121v2 Announce Type: replace 
Abstract: Designed to track changes in students' knowledge status and predict their future answers based on students' historical answer records. Current research on KT modeling focuses on predicting future student performance based on existing, unupdated records of student learning interactions. However, these methods ignore distractions in the response process (such as slipping and guessing) and ignore that static cognitive representations are temporary and limited. Most of them assume that there are no distractions during the answering process, and that the recorded representation fully represents the student's understanding and proficiency in knowledge. This can lead to many dissonant and uncoordinated issues in the original record. Therefore, we propose a knowledge-tracking cognitive representation optimization (CRO-KT) model that uses dynamic programming algorithms to optimize the structure of cognitive representation. This ensures that the structure matches the student's cognitive patterns in terms of practice difficulty. In addition, we use a synergistic optimization algorithm to optimize the cognitive representation of sub-target exercises based on the overall picture of exercise responses by considering all exercises with synergistic relationships as one goal. At the same time, the CRO-KT model integrates the relationship embedding learned in the dichotomous graph with the optimized record representation in a weighted manner, which enhances students' cognitive expression ability. Finally, experiments were conducted on three public datasets to verify the effectiveness of the proposed cognitive representation optimization model.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Super Agent System with Hybrid AI Routers</title>
<link>https://arxiv.org/abs/2504.10519</link>
<guid>https://arxiv.org/abs/2504.10519</guid>
<content:encoded><![CDATA[
arXiv:2504.10519v2 Announce Type: replace 
Abstract: AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This position paper presents a design of the Super Agent System powered by the hybrid AI routers. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reshaping MOFs text mining with a dynamic multi-agents framework of large language model</title>
<link>https://arxiv.org/abs/2504.18880</link>
<guid>https://arxiv.org/abs/2504.18880</guid>
<content:encoded><![CDATA[
arXiv:2504.18880v2 Announce Type: replace 
Abstract: Accurately identifying synthesis conditions for metal-organic frameworks (MOFs) remains a critical bottleneck in materials research, as translating literature-derived knowledge into actionable insights is hindered by the unstructured and heterogeneous nature of scientific texts. Here we present MOFh6, a large language model (LLM)-based multi-agent system designed to extract, structure, and apply synthesis knowledge from diverse input formats, including raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned with up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in synthesis data parsing and resolves 94.1% of complex co-reference abbreviations. It processes a single full-text document in 9.6 seconds and localizes structured synthesis descriptions within 36 seconds, with the cost per 100 papers reduced to USD 4.24, a 76% saving over existing systems. By addressing long-standing limitations in cross-paragraph semantic fusion and terminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF synthesis research, transforming static retrieval into an integrated and dynamic knowledge acquisition process. This shift bridges the gap between scientific literature and actionable synthesis design, providing a scalable framework for accelerating materials discovery.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers</title>
<link>https://arxiv.org/abs/2505.01482</link>
<guid>https://arxiv.org/abs/2505.01482</guid>
<content:encoded><![CDATA[
arXiv:2505.01482v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains. However, their ability to perform complex, multi-step reasoning task-essential for applications in science, medicine, and law-remains an area of active investigation. This paper examines the reasoning capabilities of contemporary LLMs, analyzing their strengths, limitations, and potential for improvement. The study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&amp;A (GPQA) dataset to assess the scientific reasoning of GPT-4o. Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our findings indicate that while LLMs exhibit emergent reasoning abilities, they often rely on pattern recognition rather than true logical inference, leading to inconsistencies in complex problem-solving. The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and CoT (43.75%). Self-consistency performed the second worst in explaining the answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning. We propose a research agenda aimed at bridging these gaps by integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies. By critically evaluating the reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse on the future of artificial general intelligence and the development of more robust, trustworthy AI systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification</title>
<link>https://arxiv.org/abs/2505.18380</link>
<guid>https://arxiv.org/abs/2505.18380</guid>
<content:encoded><![CDATA[
arXiv:2505.18380v2 Announce Type: replace 
Abstract: Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactOR and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method</title>
<link>https://arxiv.org/abs/2506.06740</link>
<guid>https://arxiv.org/abs/2506.06740</guid>
<content:encoded><![CDATA[
arXiv:2506.06740v2 Announce Type: replace 
Abstract: Psychological counseling faces huge challenges due to the growing demand for mental health services and the shortage of trained professionals. Large language models (LLMs) have shown potential to assist psychological counseling, especially in empathy and emotional support. However, existing models lack a deep understanding of emotions and are unable to generate personalized treatment plans based on fine-grained emotions. To address these shortcomings, we present AI PsyRoom, a multi-agent simulation framework designed to enhance psychological counseling by generating empathetic and emotionally nuanced conversations. By leveraging fine-grained emotion classification and a multi-agent framework, we construct a multi-agent PsyRoom A for dialogue reconstruction, generating a high-quality dialogue dataset EmoPsy, which contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues. We also propose PsyRoom B for generating personalized treatment plans. Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms state-of-the-art methods, achieving 18% improvement in problem orientation, 23% in expression, 24% in Empathy, and 16% in interactive communication quality. The datasets and models are publicly available, providing a foundation for advancing AI-assisted psychological counseling research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Flow: Perspectives, Scenarios, and Approaches</title>
<link>https://arxiv.org/abs/2506.12479</link>
<guid>https://arxiv.org/abs/2506.12479</guid>
<content:encoded><![CDATA[
arXiv:2506.12479v3 Announce Type: replace 
Abstract: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity</title>
<link>https://arxiv.org/abs/2507.09089</link>
<guid>https://arxiv.org/abs/2507.09089</guid>
<content:encoded><![CDATA[
arXiv:2507.09089v2 Announce Type: replace 
Abstract: Despite widespread adoption, the impact of AI tools on software development in the wild remains understudied. We conduct a randomized controlled trial (RCT) to understand how AI tools at the February-June 2025 frontier affect the productivity of experienced open-source developers. 16 developers with moderate AI experience complete 246 tasks in mature projects on which they have an average of 5 years of prior experience. Each task is randomly assigned to allow or disallow usage of early 2025 AI tools. When AI tools are allowed, developers primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet. Before starting tasks, developers forecast that allowing AI will reduce completion time by 24%. After completing the study, developers estimate that allowing AI reduced completion time by 20%. Surprisingly, we find that allowing AI actually increases completion time by 19%--AI tooling slowed developers down. This slowdown also contradicts predictions from experts in economics (39% shorter) and ML (38% shorter). To understand this result, we collect and evaluate evidence for 20 properties of our setting that a priori could contribute to the observed slowdown effect--for example, the size and quality standards of projects, or prior developer experience with AI tooling. Although the influence of experimental artifacts cannot be entirely ruled out, the robustness of the slowdown effect across our analyses suggests it is unlikely to primarily be a function of our experimental design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Isn't Relational Learning Taking Over the World?</title>
<link>https://arxiv.org/abs/2507.13558</link>
<guid>https://arxiv.org/abs/2507.13558</guid>
<content:encoded><![CDATA[
arXiv:2507.13558v2 Announce Type: replace 
Abstract: AI seems to be taking over the world with systems that model pixels, words, and phonemes. The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them. You might suspect that concentrating on modeling words and pixels is because all of the (valuable) data in the world is in terms of text and images. If you look into almost any company you will find their most valuable data is in spreadsheets, databases and other relational formats. These are not the form that are studied in introductory machine learning, but are full of product numbers, student numbers, transaction numbers and other identifiers that can't be interpreted naively as numbers. The field that studies this sort of data has various names including relational learning, statistical relational AI, and many others. This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike No More: Stabilizing the Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2312.16903</link>
<guid>https://arxiv.org/abs/2312.16903</guid>
<content:encoded><![CDATA[
arXiv:2312.16903v4 Announce Type: replace-cross 
Abstract: Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. Based on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers. Our findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut. We conduct various experiments to empirically verify our theoretical analyses. Experimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition</title>
<link>https://arxiv.org/abs/2401.10337</link>
<guid>https://arxiv.org/abs/2401.10337</guid>
<content:encoded><![CDATA[
arXiv:2401.10337v4 Announce Type: replace-cross 
Abstract: Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning process of the matching model despite constrained resources.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title>
<link>https://arxiv.org/abs/2405.06270</link>
<guid>https://arxiv.org/abs/2405.06270</guid>
<content:encoded><![CDATA[
arXiv:2405.06270v4 Announce Type: replace-cross 
Abstract: Clinical decision support systems require models that are not only highly accurate but also equitable and sensitive to the implications of missed diagnoses. In this study, we introduce a knowledge-guided in-context learning (ICL) framework designed to enable large language models (LLMs) to effectively process structured clinical data. Our approach integrates domain-specific feature groupings, carefully balanced few-shot examples, and task-specific prompting strategies. We systematically evaluate this method across seventy distinct ICL designs by various prompt variations and two different communication styles-natural-language narrative and numeric conversational-and compare its performance to robust classical machine learning (ML) benchmarks on tasks involving heart disease and diabetes prediction.
  Our findings indicate that while traditional ML models maintain superior performance in balanced precision-recall scenarios, LLMs employing narrative prompts with integrated domain knowledge achieve higher recall and significantly reduce gender bias, effectively narrowing fairness disparities by an order of magnitude. Despite the current limitation of increased inference latency, LLMs provide notable advantages, including the capacity for zero-shot deployment and enhanced equity. This research offers the first comprehensive analysis of ICL design considerations for applying LLMs to tabular clinical tasks and highlights distillation and multimodal extensions as promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLEIADES: Building Temporal Kernels with Orthogonal Polynomials</title>
<link>https://arxiv.org/abs/2405.12179</link>
<guid>https://arxiv.org/abs/2405.12179</guid>
<content:encoded><![CDATA[
arXiv:2405.12179v4 Announce Type: replace-cross 
Abstract: We introduce a class of neural networks named PLEIADES (PoLynomial Expansion In Adaptive Distributed Event-based Systems), which contains temporal convolution kernels generated from orthogonal polynomial basis functions. We focus on interfacing these networks with event-based data to perform online spatiotemporal classification and detection with low latency. By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning. We experimented with three event-based benchmarks and obtained state-of-the-art results on all three by large margins with significantly smaller memory and compute costs. We achieved: 1) 99.59% accuracy with 192K parameters on the DVS128 hand gesture recognition dataset and 100% with a small additional output filter; 2) 99.58% test accuracy with 277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with 576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes</title>
<link>https://arxiv.org/abs/2405.14475</link>
<guid>https://arxiv.org/abs/2405.14475</guid>
<content:encoded><![CDATA[
arXiv:2405.14475v4 Announce Type: replace-cross 
Abstract: Controllable generative models for images and videos have seen significant success, yet 3D scene generation, especially in unbounded scenarios like autonomous driving, remains underdeveloped. Existing methods lack flexible controllability and often rely on dense view data collection in controlled environments, limiting their generalizability across common datasets (e.g., nuScenes). In this paper, we introduce MagicDrive3D, a novel framework for controllable 3D street scene generation that combines video-based view synthesis with 3D representation (3DGS) generation. It supports multi-condition control, including road maps, 3D objects, and text descriptions. Unlike previous approaches that require 3D representation before training, MagicDrive3D first trains a multi-view video generation model to synthesize diverse street views. This method utilizes routinely collected autonomous driving data, reducing data acquisition challenges and enriching 3D scene generation. In the 3DGS generation step, we introduce Fault-Tolerant Gaussian Splatting to address minor errors and use monocular depth for better initialization, alongside appearance modeling to manage exposure discrepancies across viewpoints. Experiments show that MagicDrive3D generates diverse, high-quality 3D driving scenes, supports any-view rendering, and enhances downstream tasks like BEV segmentation, demonstrating its potential for autonomous driving simulation and beyond.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts</title>
<link>https://arxiv.org/abs/2406.12549</link>
<guid>https://arxiv.org/abs/2406.12549</guid>
<content:encoded><![CDATA[
arXiv:2406.12549v2 Announce Type: replace-cross 
Abstract: Recent LLMs are able to generate high-quality multilingual texts, indistinguishable for humans from authentic human-written ones. Research in machine-generated text detection is however mostly focused on the English language and longer texts, such as news articles, scientific papers or student essays. Social-media texts are usually much shorter and often feature informal language, grammatical errors, or distinct linguistic items (e.g., emoticons, hashtags). There is a gap in studying the ability of existing methods in detection of such texts, reflected also in the lack of existing multilingual benchmark datasets. To fill this gap we propose the first multilingual (22 languages) and multi-platform (5 social media platforms) dataset for benchmarking machine-generated text detection in the social-media domain, called MultiSocial. It contains 472,097 texts, of which about 58k are human-written and approximately the same amount is generated by each of 7 multilingual LLMs. We use this benchmark to compare existing detection methods in zero-shot as well as fine-tuned form. Our results indicate that the fine-tuned detectors have no problem to be trained on social-media texts and that the platform selection for training matters.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumorDB: Can AI understand graphical humor?</title>
<link>https://arxiv.org/abs/2406.13564</link>
<guid>https://arxiv.org/abs/2406.13564</guid>
<content:encoded><![CDATA[
arXiv:2406.13564v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in image segmentation and object detection, understanding complex scenes remains a significant challenge. Here, we focus on graphical humor as a paradigmatic example of image interpretation that requires elucidating the interaction of different scene elements in the context of prior cognitive knowledge. This paper introduces \textbf{HumorDB}, a novel, controlled, and carefully curated dataset designed to evaluate and advance visual humor understanding by AI systems. The dataset comprises diverse images spanning photos, cartoons, sketches, and AI-generated content, including minimally contrastive pairs where subtle edits differentiate between humorous and non-humorous versions. We evaluate humans, state-of-the-art vision models, and large vision-language models on three tasks: binary humor classification, funniness rating prediction, and pairwise humor comparison. The results reveal a gap between current AI systems and human-level humor understanding. While pretrained vision-language models perform better than vision-only models, they still struggle with abstract sketches and subtle humor cues. Analysis of attention maps shows that even when models correctly classify humorous images, they often fail to focus on the precise regions that make the image funny. Preliminary mechanistic interpretability studies and evaluation of model explanations provide initial insights into how different architectures process humor. Our results identify promising trends and current limitations, suggesting that an effective understanding of visual humor requires sophisticated architectures capable of detecting subtle contextual features and bridging the gap between visual perception and abstract reasoning. All the code and data are available here: \href{https://github.com/kreimanlab/HumorDB}{https://github.com/kreimanlab/HumorDB}
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating IP Broadcasting with Audio Tags: Workflow and Challenges</title>
<link>https://arxiv.org/abs/2407.15423</link>
<guid>https://arxiv.org/abs/2407.15423</guid>
<content:encoded><![CDATA[
arXiv:2407.15423v3 Announce Type: replace-cross 
Abstract: The broadcasting industry has adopted IP technologies, revolutionising both live and pre-recorded content production, from news gathering to live music events. IP broadcasting allows for the transport of audio and video signals in an easily configurable way, aligning with modern networking techniques. This shift towards an IP workflow allows for much greater flexibility, not only in routing signals but with the integration of tools using standard web development techniques. One possible tool could include the use of live audio tagging, which has a number of uses in the production of content. These could include adding sound effects to automated closed captioning or identifying unwanted sound events within a scene. In this paper, we describe the process of containerising an audio tagging model into a microservice, a small segregated code module that can be integrated into a multitude of different network setups. The goal is to develop a modular, accessible, and flexible tool capable of seamless deployment into broadcasting workflows of all sizes, from small productions to large corporations. Challenges surrounding latency of the selected audio tagging model and its effect on the usefulness of the end product are discussed.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation</title>
<link>https://arxiv.org/abs/2408.00998</link>
<guid>https://arxiv.org/abs/2408.00998</guid>
<content:encoded><![CDATA[
arXiv:2408.00998v5 Announce Type: replace-cross 
Abstract: Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to decompose diverse guiding factors with different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer which realizes dynamic control of the reference image to the T2I generation result in a plug-and-play manner. We demonstrate that our method allows flexible control over both guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability. The code is publicly available at: https://github.com/XiangGao1102/FBSDiff.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE: Winning the Points of LLM Function Calling</title>
<link>https://arxiv.org/abs/2409.00920</link>
<guid>https://arxiv.org/abs/2409.00920</guid>
<content:encoded><![CDATA[
arXiv:2409.00920v2 Announce Type: replace-cross 
Abstract: Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at https://huggingface.co/Team-ACE.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives</title>
<link>https://arxiv.org/abs/2411.19083</link>
<guid>https://arxiv.org/abs/2411.19083</guid>
<content:encoded><![CDATA[
arXiv:2411.19083v2 Announce Type: replace-cross 
Abstract: Bridging the gap between ego-centric and exo-centric views has been a long-standing question in computer vision. In this paper, we focus on the emerging Ego-Exo object correspondence task, which aims to understand object relations across ego-exo perspectives through segmentation. While numerous segmentation models have been proposed, most operate on a single image (view), making them impractical for cross-view scenarios. PSALM, a recently proposed segmentation method, stands out as a notable exception with its demonstrated zero-shot ability on this task. However, due to the drastic viewpoint change between ego and exo, PSALM fails to accurately locate and segment objects, especially in complex backgrounds or when object appearances change significantly. To address these issues, we propose ObjectRelator, a novel approach featuring two key modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language as an additional cue, integrating both visual masks and textual descriptions to improve object localization and prevent incorrect associations. XObjAlign enforces cross-view consistency through self-supervised alignment, enhancing robustness to object appearance variations. Extensive experiments demonstrate ObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and HANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art performance. Code is made available at: http://yuqianfu.com/ObjectRelator.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2412.12561</link>
<guid>https://arxiv.org/abs/2412.12561</guid>
<content:encoded><![CDATA[
arXiv:2412.12561v3 Announce Type: replace-cross 
Abstract: Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation</title>
<link>https://arxiv.org/abs/2412.13666</link>
<guid>https://arxiv.org/abs/2412.13666</guid>
<content:encoded><![CDATA[
arXiv:2412.13666v2 Announce Type: replace-cross 
Abstract: The capabilities of recent large language models (LLMs) to generate high-quality content indistinguishable by humans from human-written texts raises many concerns regarding their misuse. Previous research has shown that LLMs can be effectively misused for generating disinformation news articles following predefined narratives. Their capabilities to generate personalized (in various aspects) content have also been evaluated and mostly found usable. However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet. Such a dangerous combination should trigger integrated safety filters of the LLMs, if there are some. This study fills this gap by evaluating vulnerabilities of recent open and closed LLMs, and their willingness to generate personalized disinformation news articles in English. We further explore whether the LLMs can reliably meta-evaluate the personalization quality and whether the personalization affects the generated-texts detectability. Our results demonstrate the need for stronger safety-filters and disclaimers, as those are not properly functioning in most of the evaluated LLMs. Additionally, our study revealed that the personalization actually reduces the safety-filter activations; thus effectively functioning as a jailbreak. Such behavior must be urgently addressed by LLM developers and service providers.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bootstrapped Reward Shaping</title>
<link>https://arxiv.org/abs/2501.00989</link>
<guid>https://arxiv.org/abs/2501.00989</guid>
<content:encoded><![CDATA[
arXiv:2501.00989v2 Announce Type: replace-cross 
Abstract: In reinforcement learning, especially in sparse-reward domains, many environment steps are required to observe reward information. In order to increase the frequency of such observations, "potential-based reward shaping" (PBRS) has been proposed as a method of providing a more dense reward signal while leaving the optimal policy invariant. However, the required "potential function" must be carefully designed with task-dependent knowledge to not deter training performance. In this work, we propose a "bootstrapped" method of reward shaping, termed BSRS, in which the agent's current estimate of the state-value function acts as the potential function for PBRS. We provide convergence proofs for the tabular setting, give insights into training dynamics for deep RL, and show that the proposed method improves training speed in the Atari suite.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models</title>
<link>https://arxiv.org/abs/2501.01986</link>
<guid>https://arxiv.org/abs/2501.01986</guid>
<content:encoded><![CDATA[
arXiv:2501.01986v2 Announce Type: replace-cross 
Abstract: The increasing demand to process long and high-resolution videos significantly burdens Large Vision-Language Models (LVLMs) due to the enormous number of visual tokens. Existing token reduction methods primarily prune tokens based on importance metrics, such as cumulative attention scores. However, even important tokens may exhibit high redundancy caused by similarity among adjacent video frames and repetitive visual elements. To address this limitation, we propose FrameFusion, a novel token reduction approach integrating similarity-based merging with importance-based pruning. We conduct a thorough study on token similarity characteristics, revealing three key insights: (1) spatially corresponding visual tokens between adjacent frames have higher cosine similarities compared to other token pairs; (2) high token similarities prominently decrease in deeper model layers; and (3) token similarity rankings are highly consistent across different layers. Guided by these observations, FrameFusion computes token similarities exclusively between corresponding visual tokens from adjacent frames, applies token merging at initial successive layers followed by pruning in deeper layers, and adopts a cascaded merging strategy to further enhance efficiency. We evaluate FrameFusion comprehensively across six diverse LVLMs, ranging from 2B to 72B parameters, using five video benchmarks encompassing video retrieval, question-answering, and spatial-temporal understanding tasks. Experiments show that FrameFusion reduces visual tokens by 70%, achieving 1.6-3.6x end-to-end speedups, with an average performance impact of less than 3%. Our code is available at: https://github.com/thu-nics/FrameFusion.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying Cross-cluster Modularity in Neural Networks</title>
<link>https://arxiv.org/abs/2502.02470</link>
<guid>https://arxiv.org/abs/2502.02470</guid>
<content:encoded><![CDATA[
arXiv:2502.02470v3 Announce Type: replace-cross 
Abstract: An approach to improve neural network interpretability is via clusterability, i.e., splitting a model into disjoint clusters that can be studied independently. We define a measure for clusterability and show that pre-trained models form highly enmeshed clusters via spectral graph clustering. We thus train models to be more modular using a "clusterability loss" function that encourages the formation of non-interacting clusters. We then investigate the emerging properties of these highly clustered models. We find our trained clustered models do not exhibit more task specialization, but do form smaller circuits. We investigate CNNs trained on MNIST and CIFAR, small transformers trained on modular addition, and GPT-2 and Pythia on the Wiki dataset, and Gemma on a Chemistry dataset. This investigation shows what to expect from clustered models.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities</title>
<link>https://arxiv.org/abs/2502.05209</link>
<guid>https://arxiv.org/abs/2502.05209</guid>
<content:encoded><![CDATA[
arXiv:2502.05209v4 Announce Type: replace-cross 
Abstract: Evaluations of large language model (LLM) risks and capabilities are increasingly being incorporated into AI risk management and governance frameworks. Currently, most risk evaluations are conducted by designing inputs that elicit harmful behaviors from the system. However, this approach suffers from two limitations. First, input-output evaluations cannot fully evaluate realistic risks from open-weight models. Second, the behaviors identified during any particular input-output evaluation can only lower-bound the model's worst-possible-case input-output behavior. As a complementary method for eliciting harmful behaviors, we propose evaluating LLMs with model tampering attacks which allow for modifications to latent activations or weights. We pit state-of-the-art techniques for removing harmful LLM capabilities against a suite of 5 input-space and 6 model tampering attacks. In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the success rate of model tampering attacks can empirically predict and offer conservative estimates for the success of held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 16 steps of fine-tuning. Together, these results highlight the difficulty of suppressing harmful LLM capabilities and show that model tampering attacks enable substantially more rigorous evaluations than input-space attacks alone.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIPA: Preference Alignment as Prior-Informed Statistical Estimation</title>
<link>https://arxiv.org/abs/2502.05773</link>
<guid>https://arxiv.org/abs/2502.05773</guid>
<content:encoded><![CDATA[
arXiv:2502.05773v2 Announce Type: replace-cross 
Abstract: Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding.
  In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\sim10\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation Scaling Laws</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
arXiv:2502.08606v2 Announce Type: replace-cross 
Abstract: We propose a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings mitigate the risks associated with large-scale distillation by enabling compute-optimal allocation for both the teacher and student to maximize student performance. We provide compute-optimal distillation recipes for two key scenarios: when a teacher already exists, and when a teacher needs training. In settings involving many students or an existing teacher, distillation outperforms supervised learning up to a compute level that scales predictably with student size. Conversely, if only one student is to be distilled and a teacher also requires training, supervised learning is generally preferable. Additionally, our large-scale study of distillation increases our understanding of the process and helps inform experimental design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation</title>
<link>https://arxiv.org/abs/2502.10802</link>
<guid>https://arxiv.org/abs/2502.10802</guid>
<content:encoded><![CDATA[
arXiv:2502.10802v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with an additional test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning</title>
<link>https://arxiv.org/abs/2502.11439</link>
<guid>https://arxiv.org/abs/2502.11439</guid>
<content:encoded><![CDATA[
arXiv:2502.11439v2 Announce Type: replace-cross 
Abstract: Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SpFT framework, based on ideas from neural network pruning. At a high level, we first identify ``important'' neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Experiments on common language tasks show our method improves SpFT's memory efficiency by 20-50\% while matching the accuracy of state-of-the-art methods like LoRA's variants.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Attribution Regularizers for Efficient Model Training</title>
<link>https://arxiv.org/abs/2502.20268</link>
<guid>https://arxiv.org/abs/2502.20268</guid>
<content:encoded><![CDATA[
arXiv:2502.20268v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains. However, effectively leveraging their vast knowledge for training smaller downstream models remains an open challenge, especially in domains like tabular data learning, where simpler models are often preferred due to interpretability and efficiency.
  In this paper, we introduce a novel yet straightforward method for incorporating LLM-generated global task feature attributions into the training process of smaller networks. Specifically, we propose an attribution-matching regularization term that aligns the training dynamics of the smaller model with the insights provided by the LLM. By doing so, our approach yields superior performance in few-shot learning scenarios. Notably, our method requires only black-box API access to the LLM, making it easy to integrate into existing training pipelines with minimal computational overhead.
  Furthermore, we demonstrate how this method can be used to address common issues in real-world datasets, such as skewness and bias. By integrating high-level knowledge from LLMs, our approach improves generalization, even when training data is limited or imbalanced. We validate its effectiveness through extensive experiments across multiple tasks, demonstrating improved learning efficiency and model robustness.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer</title>
<link>https://arxiv.org/abs/2502.20719</link>
<guid>https://arxiv.org/abs/2502.20719</guid>
<content:encoded><![CDATA[
arXiv:2502.20719v2 Announce Type: replace-cross 
Abstract: Generating realistic synthetic electronic health records (EHRs) holds tremendous promise for accelerating healthcare research, facilitating AI model development and enhancing patient privacy. However, existing generative methods typically treat EHRs as flat sequences of discrete medical codes. This approach overlooks two critical aspects: the inherent hierarchical organization of clinical coding systems and the rich semantic context provided by code descriptions. Consequently, synthetic patient sequences often lack high clinical fidelity and have limited utility in downstream clinical tasks. In this paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT), a novel framework that leverages both hierarchical and semantic information for the generative process. HiSGT constructs a hierarchical graph to encode parent-child and sibling relationships among clinical codes and employs a graph neural network to derive hierarchy-aware embeddings. These are then fused with semantic embeddings extracted from a pre-trained clinical language model (e.g., ClinicalBERT), enabling the Transformer-based generator to more accurately model the nuanced clinical patterns inherent in real EHRs. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGT significantly improves the statistical alignment of synthetic data with real patient records, as well as supports robust downstream applications such as chronic disease classification. By addressing the limitations of conventional raw code-based generative models, HiSGT represents a significant step toward clinically high-fidelity synthetic data generation and a general paradigm suitable for interpretable medical code representation, offering valuable applications in data augmentation and privacy-preserving healthcare analytics.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs</title>
<link>https://arxiv.org/abs/2503.00151</link>
<guid>https://arxiv.org/abs/2503.00151</guid>
<content:encoded><![CDATA[
arXiv:2503.00151v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly integrated into daily life, ensuring their cultural sensitivity and inclusivity is paramount. We introduce our dataset, a year-long community-driven project covering all 22 Arab countries. The dataset includes instructions (input, response pairs) in both Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20 diverse topics. Built by a team of 44 researchers across the Arab world, all of whom are authors of this paper, our dataset offers a broad, inclusive perspective. We use our dataset to evaluate the cultural and dialectal capabilities of several frontier LLMs, revealing notable limitations. For instance, while closed-source LLMs generally exhibit strong performance, they are not without flaws, and smaller open-source models face greater challenges. Moreover, certain countries (e.g., Egypt, the UAE) appear better represented than others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code, and data for reproducibility are publicly available.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners</title>
<link>https://arxiv.org/abs/2503.05996</link>
<guid>https://arxiv.org/abs/2503.05996</guid>
<content:encoded><![CDATA[
arXiv:2503.05996v2 Announce Type: replace-cross 
Abstract: Reinforcement learning agents are fundamentally limited by the quality of the reward functions they learn from, yet reward design is often overlooked under the assumption that a well-defined reward is readily available. However, in practice, designing rewards is difficult, and even when specified, evaluating their correctness is equally problematic: how do we know if a reward function is correctly specified? In our work, we address these challenges by focusing on reward alignment -- assessing whether a reward function accurately encodes the preferences of a human stakeholder. As a concrete measure of reward alignment, we introduce the Trajectory Alignment Coefficient to quantify the similarity between a human stakeholder's ranking of trajectory distributions and those induced by a given reward function. We show that the Trajectory Alignment Coefficient exhibits desirable properties, such as not requiring access to a ground truth reward, invariance to potential-based reward shaping, and applicability to online RL. Additionally, in an 11 -- person user study of RL practitioners, we found that access to the Trajectory Alignment Coefficient during reward selection led to statistically significant improvements. Compared to relying only on reward functions, our metric reduced cognitive workload by 1.5x, was preferred by 82% of users and increased the success rate of selecting reward functions that produced performant policies by 41%.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation</title>
<link>https://arxiv.org/abs/2503.16806</link>
<guid>https://arxiv.org/abs/2503.16806</guid>
<content:encoded><![CDATA[
arXiv:2503.16806v2 Announce Type: replace-cross 
Abstract: Nonprehensile manipulation is crucial for handling objects that are too thin, large, or otherwise ungraspable in unstructured environments. While conventional planning-based approaches struggle with complex contact modeling, learning-based methods have recently emerged as a promising alternative. However, existing learning-based approaches face two major limitations: they heavily rely on multi-view cameras and precise pose tracking, and they fail to generalize across varying physical conditions, such as changes in object mass and table friction. To address these challenges, we propose the Dynamics-Adaptive World Action Model (DyWA), a novel framework that enhances action learning by jointly predicting future states while adapting to dynamics variations based on historical trajectories. By unifying the modeling of geometry, state, physics, and robot actions, DyWA enables more robust policy learning under partial observability. Compared to baselines, our method improves the success rate by 31.5% using only single-view point cloud observations in the simulation. Furthermore, DyWA achieves an average success rate of 68% in real-world experiments, demonstrating its ability to generalize across diverse object geometries, adapt to varying table friction, and robustness in challenging scenarios such as half-filled water bottles and slippery surfaces.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs</title>
<link>https://arxiv.org/abs/2503.18377</link>
<guid>https://arxiv.org/abs/2503.18377</guid>
<content:encoded><![CDATA[
arXiv:2503.18377v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities, but their enormous size poses significant challenges for deployment in real-world applications. To address this issue, researchers have sought to apply network pruning techniques to LLMs. A critical challenge in pruning is allocation the sparsity for each layer. Recent sparsity allocation methods is often based on heuristics or search that can easily lead to suboptimal performance. In this paper, we conducted an extensive investigation into various LLMs and revealed three significant discoveries: (1) the layerwise pruning sensitivity (LPS) of LLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and (3) the performance of a sparse model is related to the uniformity of its layerwise redundancy level. Based on these observations, we propose that the layerwise sparsity of LLMs should adhere to three principles: \emph{non-uniformity}, \emph{pruning metric dependency}, and \emph{uniform layerwise redundancy level} in the pruned model. To this end, we proposed Maximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in the most redundant layers (\emph{i.e.}, those with the highest non-outlier ratio) at each iteration. The achieved layerwise sparsity aligns with the outlined principles. We conducted extensive experiments on publicly available LLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental results validate the effectiveness of MRP, demonstrating its superiority over previous methods.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance</title>
<link>https://arxiv.org/abs/2504.13376</link>
<guid>https://arxiv.org/abs/2504.13376</guid>
<content:encoded><![CDATA[
arXiv:2504.13376v3 Announce Type: replace-cross 
Abstract: This study addresses the minor-embedding problem, which involves mapping the variables of an Ising model onto a quantum annealing processor. The primary motivation stems from the observed performance disparity of quantum annealers when solving problems suited to the processor's architecture versus those with non-hardware-native topologies. Our research has two main objectives: i) to analyze the impact of embedding quality on the performance of D-Wave Systems quantum annealers, and ii) to evaluate the quality of the embeddings generated by Minorminer, the standard minor-embedding technique in the quantum annealing literature, provided by D-Wave. Regarding the first objective, our experiments reveal a clear correlation between the average chain length of embeddings and the relative errors of the solutions sampled. This underscores the critical influence of embedding quality on quantum annealing performance. For the second objective, we evaluate Minorminer's embedding capabilities, the quality and robustness of its embeddings, and its execution-time performance. We also compare its performance with Clique Embedding, another algorithm developed by D-Wave, which is deterministic and designed to embed fully connected Ising models into quantum annealing processors, serving as a worst-case scenario. The results demonstrate that there is significant room for improvement for Minorminer, suggesting that more effective embedding strategies could lead to meaningful gains in quantum annealing performance.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training</title>
<link>https://arxiv.org/abs/2504.20314</link>
<guid>https://arxiv.org/abs/2504.20314</guid>
<content:encoded><![CDATA[
arXiv:2504.20314v2 Announce Type: replace-cross 
Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN) training paradigm that offers computational simplicity and memory savings. However, this seemingly promising approach faces a significant and long-ignored challenge. ZO requires generating a substantial number of Gaussian random numbers, which poses significant difficulties and even makes it infeasible for hardware platforms, such as FPGAs and ASICs. In this paper, we identify this critical issue, which arises from the mismatch between algorithm and hardware designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO framework. Specifically, we design random number reuse strategies to significantly reduce the demand for random number generation and introduce a hardware-friendly adaptive scaling method to replace the costly Gaussian distribution with a uniform distribution. Our experiments show that PeZO reduces the required LUTs and FFs for random number generation by 48.6\% and 12.7\%, and saves at maximum 86\% power consumption, all without compromising training performance, making ZO optimization feasible for on-device training. To the best of our knowledge, we are the first to explore the potential of on-device ZO optimization, providing valuable insights for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations</title>
<link>https://arxiv.org/abs/2504.21019</link>
<guid>https://arxiv.org/abs/2504.21019</guid>
<content:encoded><![CDATA[
arXiv:2504.21019v2 Announce Type: replace-cross 
Abstract: The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
arXiv:2505.03005v3 Announce Type: replace-cross 
Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.
  Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs</title>
<link>https://arxiv.org/abs/2505.15265</link>
<guid>https://arxiv.org/abs/2505.15265</guid>
<content:encoded><![CDATA[
arXiv:2505.15265v2 Announce Type: replace-cross 
Abstract: Adversarial attacks aim to generate malicious inputs that mislead deep models, but beyond causing model failure, they cannot provide certain interpretable information such as ``\textit{What content in inputs make models more likely to fail?}'' However, this information is crucial for researchers to specifically improve model robustness. Recent research suggests that models may be particularly sensitive to certain semantics in visual inputs (such as ``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this paper we conducted the first exploration on large vision-language models (LVLMs) and found that LVLMs indeed are susceptible to hallucinations and various errors when facing specific semantic concepts in images. To efficiently search for these sensitive concepts, we integrated large language models (LLMs) and text-to-image (T2I) models to propose a novel semantic evolution framework. Randomly initialized semantic concepts undergo LLM-based crossover and mutation operations to form image descriptions, which are then converted by T2I models into visual inputs for LVLMs. The task-specific performance of LVLMs on each input is quantified as fitness scores for the involved semantics and serves as reward signals to further guide LLMs in exploring concepts that induce LVLMs. Extensive experiments on seven mainstream LVLMs and two multimodal tasks demonstrate the effectiveness of our method. Additionally, we provide interesting findings about the sensitive semantics of LVLMs, aiming to inspire further in-depth research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</title>
<link>https://arxiv.org/abs/2506.00842</link>
<guid>https://arxiv.org/abs/2506.00842</guid>
<content:encoded><![CDATA[
arXiv:2506.00842v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) achieve strong performance on plain text tasks but underperform on structured data like tables and databases. Potential challenges arise from their underexposure during pre-training and rigid text-to-structure transfer mechanisms. Unlike humans who seamlessly apply learned patterns across data modalities, LLMs struggle to infer implicit relationships embedded in tabular formats, especially in the absence of explicit structural guidance. To bridge this cognitive gap, we introduce Contrastive Retrieval-Augmented Generation on Experience (CoRE), a framework that builds experience memory representations and enhances generalization through contrastive In-Context Learning (ICL) to simulate human-like knowledge transfer. Experiments on Text-to-SQL and TableQA show CoRE significantly improves performance, achieving average gains of 3.44% and 4.24%, with up to 17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated Experience Memory expands training data 8-9x, enhancing diversity and domain coverage. This training-free and continual method propels LLMs toward structured knowledge expertise.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation</title>
<link>https://arxiv.org/abs/2506.01177</link>
<guid>https://arxiv.org/abs/2506.01177</guid>
<content:encoded><![CDATA[
arXiv:2506.01177v2 Announce Type: replace-cross 
Abstract: Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture of generative adversarial networks (GANs) for molecule discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, while reducing parameter count by more than 60%. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically-grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset</title>
<link>https://arxiv.org/abs/2506.02614</link>
<guid>https://arxiv.org/abs/2506.02614</guid>
<content:encoded><![CDATA[
arXiv:2506.02614v4 Announce Type: replace-cross 
Abstract: With the rapid development of space exploration, space debris has attracted more attention due to its potential extreme threat, leading to the need for real-time and accurate debris tracking. However, existing methods are mainly based on traditional signal processing, which cannot effectively process the complex background and dense space debris. In this paper, we propose a deep learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly accurate debris tracking. SDT-Net effectively represents the feature of debris, enhancing the efficiency and stability of end-to-end model learning. To train and evaluate this model effectively, we also produce a large-scale dataset Space Debris Tracking Dataset (SDTD) by a novel observation-based data simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562 frames and covers 250,000 synthetic space debris. Extensive experiments validate the effectiveness of our model and the challenging of our dataset. Furthermore, we test our model on real data from the Antarctic Station, achieving a MOTA score of 73.2%, which demonstrates its strong transferability to real-world scenarios. Our dataset and code will be released soon.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2506.14988</link>
<guid>https://arxiv.org/abs/2506.14988</guid>
<content:encoded><![CDATA[
arXiv:2506.14988v3 Announce Type: replace-cross 
Abstract: We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Analysis for Melanoma Prevention</title>
<link>https://arxiv.org/abs/2506.18414</link>
<guid>https://arxiv.org/abs/2506.18414</guid>
<content:encoded><![CDATA[
arXiv:2506.18414v2 Announce Type: replace-cross 
Abstract: Melanoma represents a critical health risk due to its aggressive progression and high mortality, underscoring the need for early, interpretable diagnostic tools. While deep learning has advanced in skin lesion classification, most existing models provide only binary outputs, offering limited clinical insight. This work introduces a novel approach that extends beyond classification, enabling interpretable risk modelling through a Conditional Variational Autoencoder. The proposed method learns a structured latent space that captures semantic relationships among lesions, allowing for a nuanced, continuous assessment of morphological differences. An SVM is also trained on this representation effectively differentiating between benign nevi and melanomas, demonstrating strong and consistent performance. More importantly, the learned latent space supports visual and geometric interpretation of malignancy, with the spatial proximity of a lesion to known melanomas serving as a meaningful indicator of risk. This approach bridges predictive performance with clinical applicability, fostering early detection, highlighting ambiguous cases, and enhancing trust in AI-assisted diagnosis through transparent and interpretable decision-making.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models</title>
<link>https://arxiv.org/abs/2506.19037</link>
<guid>https://arxiv.org/abs/2506.19037</guid>
<content:encoded><![CDATA[
arXiv:2506.19037v3 Announce Type: replace-cross 
Abstract: Masked diffusion language models (MDLMs) promise fast, non-autoregressive text generation, yet existing samplers, which pick tokens to unmask based on model confidence, ignore interactions when unmasking multiple positions in parallel and effectively reduce to slow, autoregressive behavior. We propose the Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free method that partitions sequence positions into non-adjacent dilated groups and unmasked them in parallel so as to minimize an upper bound on joint entropy gain at each denoising step. By explicitly trading off the number of network calls against generation quality, DUS recovers most of the performance lost under traditional parallel unmasking strategies. Across math (GSM8K, MATH500), code (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS outperforms confidence-based planners, without modifying the underlying denoiser, and reveals the true speed-quality frontier of MDLMs.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Generalization of Spiking Neural Networks Through Temporal Regularization</title>
<link>https://arxiv.org/abs/2506.19256</link>
<guid>https://arxiv.org/abs/2506.19256</guid>
<content:encoded><![CDATA[
arXiv:2506.19256v3 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have received widespread attention due to their event-driven and low-power characteristics, making them particularly effective for processing event-based neuromorphic data. Recent studies have shown that directly trained SNNs suffer from severe overfitting issues due to the limited scale of neuromorphic datasets and the gradient mismatching problem, which fundamentally constrain their generalization performance. In this paper, we propose a temporal regularization training (TRT) method by introducing a time-dependent regularization mechanism to enforce stronger constraints on early timesteps. We compare the performance of TRT with other state-of-the-art methods performance on datasets including CIFAR10/100, ImageNet100, DVS-CIFAR10, and N-Caltech101. To validate the effectiveness of TRT, we conducted ablation studies and analyses including loss landscape visualization and learning curve analysis, demonstrating that TRT can effectively mitigate overfitting and flatten the training loss landscape, thereby enhancing generalizability. Furthermore, we establish a theoretical interpretation of TRT's temporal regularization mechanism based on the results of Fisher information analysis. We analyze the temporal information dynamics inside SNNs by tracking Fisher information during the TRT training process, revealing the Temporal Information Concentration (TIC) phenomenon, where Fisher information progressively concentrates in early timesteps. The time-decaying regularization mechanism implemented in TRT effectively guides the network to learn robust features in early timesteps with rich information, thereby leading to significant improvements in model generalization. Code is available at https://anonymous.4open.science/r/TRT-7FBFUYT4E.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JCAPT: A Joint Modeling Approach for CAPT</title>
<link>https://arxiv.org/abs/2506.19315</link>
<guid>https://arxiv.org/abs/2506.19315</guid>
<content:encoded><![CDATA[
arXiv:2506.19315v2 Announce Type: replace-cross 
Abstract: Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration Behavior of Untrained Policies</title>
<link>https://arxiv.org/abs/2506.22566</link>
<guid>https://arxiv.org/abs/2506.22566</guid>
<content:encoded><![CDATA[
arXiv:2506.22566v3 Announce Type: replace-cross 
Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL), particularly in environments with sparse or adversarial reward structures. In this work, we study how the architecture of deep neural policies implicitly shapes exploration before training. We theoretically and empirically demonstrate strategies for generating ballistic or diffusive trajectories from untrained policies in a toy model. Using the theory of infinite-width networks and a continuous-time limit, we show that untrained policies return correlated actions and result in non-trivial state-visitation distributions. We discuss the distributions of the corresponding trajectories for a standard architecture, revealing insights into inductive biases for tackling exploration. Our results establish a theoretical and experimental framework for using policy initialization as a design tool to understand exploration behavior in early training.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning</title>
<link>https://arxiv.org/abs/2507.04790</link>
<guid>https://arxiv.org/abs/2507.04790</guid>
<content:encoded><![CDATA[
arXiv:2507.04790v3 Announce Type: replace-cross 
Abstract: Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All in One: Visual-Description-Guided Unified Point Cloud Segmentation</title>
<link>https://arxiv.org/abs/2507.05211</link>
<guid>https://arxiv.org/abs/2507.05211</guid>
<content:encoded><![CDATA[
arXiv:2507.05211v2 Announce Type: replace-cross 
Abstract: Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2048: Reinforcement Learning in a Delayed Reward Environment</title>
<link>https://arxiv.org/abs/2507.05465</link>
<guid>https://arxiv.org/abs/2507.05465</guid>
<content:encoded><![CDATA[
arXiv:2507.05465v2 Announce Type: replace-cross 
Abstract: Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model</title>
<link>https://arxiv.org/abs/2507.08013</link>
<guid>https://arxiv.org/abs/2507.08013</guid>
<content:encoded><![CDATA[
arXiv:2507.08013v2 Announce Type: replace-cross 
Abstract: Recent advances in natural language processing (NLP) have been driven bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel at understanding complex texts, but biomedical literature, withits domain-specific terminology, poses challenges that models likeWord2Vec and bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5, despite capturing context, fall short in tasks needingbidirectional understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a pretrained BERT model trained on a large biomedicaldataset and equipped with domain-specific vocabulary that enhances thecomprehension of biomedical terminology. MedicalBERT model is furtheroptimized and fine-tuned to address diverse tasks, including named entityrecognition, relation extraction, question answering, sentence similarity, anddocument classification. Performance metrics such as the F1-score,accuracy, and Pearson correlation are employed to showcase the efficiencyof our model in comparison to other BERT-based models such as BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost of the benchmarks, and surpasses the general-purpose BERT model by5.67% on average across all the tasks evaluated respectively. This work alsounderscores the potential of leveraging pretrained BERT models for medicalNLP tasks, demonstrating the effectiveness of transfer learning techniques incapturing domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model. Available from: https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model [accessed Jul 06 2025].
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them</title>
<link>https://arxiv.org/abs/2507.10616</link>
<guid>https://arxiv.org/abs/2507.10616</guid>
<content:encoded><![CDATA[
arXiv:2507.10616v2 Announce Type: replace-cross 
Abstract: Training large language models (LLMs) for reasoning via maths and code datasets has become a major new focus in LLM post-training. Two particularly popular approaches are reinforcement learning (RL) and supervised fine-tuning (SFT), but their training dynamics are poorly understood. We present a comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters. We find that RL yields minor in-domain gains on maths and slight degradation on knowledge-intensive benchmarks like MMLU, while both trends are more pronounced in SFT. We also analyse model parameters across checkpoints, observing that both algorithms modify query and key weights the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer MLPs more, leading us to hypothesise that this may have caused the out-of-domain degradation. We therefore investigate whether freezing parts of the model during training can mitigate the reduced performance on knowledge-intensive benchmarks. However, our results are inconclusive, with benefits on GPQA:Diamond and degradation on other benchmarks. Taken together, our observations provide a preliminary indication for why RL amplifies existing capabilities, while SFT replaces old skills with new ones.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models</title>
<link>https://arxiv.org/abs/2507.10643</link>
<guid>https://arxiv.org/abs/2507.10643</guid>
<content:encoded><![CDATA[
arXiv:2507.10643v2 Announce Type: replace-cross 
Abstract: Existing post-hoc model-agnostic methods generate external explanations for opaque models, primarily by locally attributing the model output to its input features. However, they often lack an explicit and systematic framework for quantifying the contribution of individual features. Building on the Taylor expansion framework introduced by Deng et al. (2024) to unify existing local attribution methods, we propose a rigorous set of postulates -- "precision", "federation", and "zero-discrepancy" -- to govern Taylor term-specific attribution. Guided by these postulates, we introduce TaylorPODA (Taylor expansion-derived imPortance-Order aDapted Attribution), which incorporates an additional "adaptation" property. This property enables alignment with task-specific goals, especially in post-hoc settings lacking ground-truth explanations. Empirical evaluations demonstrate that TaylorPODA achieves competitive results against baseline methods, providing principled and visualization-friendly explanations. This work represents a step toward the trustworthy deployment of opaque models by offering explanations with stronger theoretical grounding.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13152</link>
<guid>https://arxiv.org/abs/2507.13152</guid>
<content:encoded><![CDATA[
arXiv:2507.13152v2 Announce Type: replace-cross 
Abstract: Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation</title>
<link>https://arxiv.org/abs/2507.13229</link>
<guid>https://arxiv.org/abs/2507.13229</guid>
<content:encoded><![CDATA[
arXiv:2507.13229v2 Announce Type: replace-cross 
Abstract: The pursuit of a generalizable stereo matching model, capable of performing across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. On the other hand, global matching architectures, while theoretically more robust, have been historically rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with $S^2M^2$: a global matching architecture that achieves both state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods across most metrics while reconstructing high-quality details with competitive efficiency.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</title>
<link>https://arxiv.org/abs/2507.13618</link>
<guid>https://arxiv.org/abs/2507.13618</guid>
<content:encoded><![CDATA[
arXiv:2507.13618v3 Announce Type: replace-cross 
Abstract: Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics</title>
<link>https://arxiv.org/abs/2507.17777</link>
<guid>https://arxiv.org/abs/2507.17777</guid>
<content:encoded><![CDATA[
<div> Symbolic Regression, Fluid Mechanics, Incompressible Flow, PySR Library, Answer Set Programming

Summary:
Symbolic Regression (SR) is utilized to model a 3D incompressible flow in a rectangular channel, providing interpretable mathematical relationships without prior assumptions. The study focuses on velocity and pressure fields, deriving compact equations from numerical simulations using the PySR library. The resulting equations accurately depict the flow dynamics, matching analytical solutions and showcasing SR's ability to simplify complex behaviors. An innovative hybrid approach combining SR with Answer Set Programming (ASP) ensures generated equations are both statistically accurate and physically plausible, aligning with domain principles. This integration enhances the reliability of data-driven models and paves the way for efficient frameworks in real-time data analysis, emphasizing the importance of explainable predictions in fluid mechanics. <div>
arXiv:2507.17777v1 Announce Type: new 
Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as "black boxes", Symbolic Regression (SR) stands out as a powerful tool for revealing interpretable mathematical relationships in complex physical systems, requiring no a priori assumptions about models' structures. Motivated by the recognition that, in fluid mechanics, an understanding of the underlying flow physics is as crucial as accurate prediction, this study applies SR to model a fundamental three-dimensional (3D) incompressible flow in a rectangular channel, focusing on the (axial) velocity and pressure fields under laminar conditions. By employing the PySR library, compact symbolic equations were derived directly from numerical simulation data, revealing key characteristics of the flow dynamics. These equations not only approximate the parabolic velocity profile and pressure drop observed in the studied fluid flow, but also perfectly coincide with analytical solutions from the literature. Furthermore, we propose an innovative approach that integrates SR with the knowledge-representation framework of Answer Set Programming (ASP), combining the generative power of SR with the declarative reasoning strengths of ASP. The proposed hybrid SR/ASP framework ensures that the SR-generated symbolic expressions are not only statistically accurate, but also physically plausible, adhering to domain-specific principles. Overall, the study highlights two key contributions: SR's ability to simplify complex flow behaviours into concise, interpretable equations, and the potential of knowledge-representation approaches to improve the reliability and alignment of data-driven SR models with domain principles. Insights from the examined 3D channel flow pave the way for integrating such hybrid approaches into efficient frameworks, [...] where explainable predictions and real-time data analysis are crucial.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis</title>
<link>https://arxiv.org/abs/2507.17874</link>
<guid>https://arxiv.org/abs/2507.17874</guid>
<content:encoded><![CDATA[
<div> keywords: agentic systems, structured reasoning, data analysis, large language models, cognitive workflow
Summary:<br /><br />I2I-STRADA is introduced as an agentic architecture for data analysis, focusing on structured reasoning processes. It aims to formalize the cognitive workflow of data analysis by modeling analytical reasoning steps as modular sub-tasks. The system emphasizes interpreting vague goals, grounding them in contextual knowledge, constructing abstract plans, and adapting execution based on intermediate outcomes. Evaluations on DABstep and DABench benchmarks demonstrate that I2I-STRADA surpasses previous systems in planning coherence and insight alignment. This highlights the significance of structured cognitive workflows in designing agents for data analysis. Overall, the framework aims to enhance automation of insight generation by incorporating structured reasoning processes into agentic systems for data analysis. 

<br /><br />Summary: <div>
arXiv:2507.17874v1 Announce Type: new 
Abstract: Recent advances in agentic systems for data analysis have emphasized automation of insight generation through multi-agent frameworks, and orchestration layers. While these systems effectively manage tasks like query translation, data transformation, and visualization, they often overlook the structured reasoning process underlying analytical thinking. Reasoning large language models (LLMs) used for multi-step problem solving are trained as general-purpose problem solvers. As a result, their reasoning or thinking steps do not adhere to fixed processes for specific tasks. Real-world data analysis requires a consistent cognitive workflow: interpreting vague goals, grounding them in contextual knowledge, constructing abstract plans, and adapting execution based on intermediate outcomes. We introduce I2I-STRADA (Information-to-Insight via Structured Reasoning Agent for Data Analysis), an agentic architecture designed to formalize this reasoning process. I2I-STRADA focuses on modeling how analysis unfolds via modular sub-tasks that reflect the cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench benchmarks show that I2I-STRADA outperforms prior systems in planning coherence and insight alignment, highlighting the importance of structured cognitive workflows in agent design for data analysis.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTAPS: Tool-augmented LLMs for Operations Management</title>
<link>https://arxiv.org/abs/2507.17927</link>
<guid>https://arxiv.org/abs/2507.17927</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, advanced planning system, SmartAPS, conversational system, supply chain planners <br />
Summary: <br />
Large language models (LLMs) have the potential to revolutionize the way operations planners interact with advanced planning systems (APS). SmartAPS, a conversational system leveraging LLM technology, addresses the accessibility issues faced by supply chain planners using traditional APS. By providing a natural language chat interface, SmartAPS enables planners to easily query information, engage in counterfactual reasoning, receive recommendations, and conduct scenario analysis. The system aims to empower operations planners to better manage their operations without the need for costly consultants for customization and maintenance. By combining the power of LLMs with operational planning tools, SmartAPS offers a user-friendly solution that enhances decision-making processes in supply chain management. The system's capabilities are showcased in a demonstration video, highlighting its ability to streamline operations planning and improve efficiency. <br /><br />Summary: <div>
arXiv:2507.17927v1 Announce Type: new 
Abstract: Large language models (LLMs) present intriguing opportunities to enhance user interaction with traditional algorithms and tools in real-world applications. An advanced planning system (APS) is a sophisticated software that leverages optimization to help operations planners create, interpret, and modify an operational plan. While highly beneficial, many customers are priced out of using an APS due to the ongoing costs of consultants responsible for customization and maintenance. To address the need for a more accessible APS expressed by supply chain planners, we present SmartAPS, a conversational system built on a tool-augmented LLM. Our system provides operations planners with an intuitive natural language chat interface, allowing them to query information, perform counterfactual reasoning, receive recommendations, and execute scenario analysis to better manage their operation. A short video demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of timeline-based planning strategies avoiding determinization</title>
<link>https://arxiv.org/abs/2507.17988</link>
<guid>https://arxiv.org/abs/2507.17988</guid>
<content:encoded><![CDATA[
<div> Timeline-based planning, qualitative temporal constraints, synchronization rules, PSPACE-complete, deterministic fragment

Summary:
Qualitative timeline-based planning models domains as collections of independent components with interactions governed by qualitative temporal constraints known as synchronization rules. The plan-existence problem is PSPACE-complete and has been shown to be reducible to the nonemptiness problem of nondeterministic finite automata. However, using nondeterministic automata for planning strategy synthesis requires a costly determinization step. This paper introduces a fragment of timeline-based planning that directly maps the plan-existence problem to the nonemptiness problem of deterministic finite automata, enabling strategy synthesis. The research also identifies a maximal subset of Allen's relations that can be accommodated in this deterministic fragment. The deterministic fragment simplifies the planning process by eliminating the need for determinization, offering a more efficient approach to qualitative timeline-based planning. 

<br /><br />Summary: <div>
arXiv:2507.17988v1 Announce Type: new 
Abstract: Qualitative timeline-based planning models domains as sets of independent, but
  interacting, components whose behaviors over time, the timelines, are governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI</title>
<link>https://arxiv.org/abs/2507.18004</link>
<guid>https://arxiv.org/abs/2507.18004</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, creativity, generative modeling, feedback, cognitive science

Summary: 
The paper introduces the E.A.R.T.H. framework, a five-stage generative pipeline that leverages model-generated errors to enhance creativity in AI. By focusing on Error generation, Amplification, Refine selection, Transform, and Harness feedback, the framework aims to uncover creative potential from failures. Using advanced models such as LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline incorporates a composite reward function based on novelty, surprise, and relevance. Results show a significant improvement in creativity scores at the Refine stage, with refined outputs demonstrating increased novelty and alignment between slogans and images. Human evaluations indicate a preference for metaphorical slogans over literal ones, emphasizing the importance of stylistic precision and emotional resonance. Overall, the study showcases how integrating error-centered generation and feedback mechanisms can lead to more creative and human-aligned AI systems. 

<br /><br />Summary: <div>
arXiv:2507.18004v1 Announce Type: new 
Abstract: How can AI move beyond imitation toward genuine creativity? This paper proposes the E.A.R.T.H. framework, a five-stage generative pipeline that transforms model-generated errors into creative assets through Error generation, Amplification, Refine selection, Transform, and Harness feedback. Drawing on cognitive science and generative modeling, we posit that "creative potential hides in failure" and operationalize this via structured prompts, semantic scoring, and human-in-the-loop evaluation. Implemented using LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the pipeline employs a composite reward function based on novelty, surprise, and relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to 1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4% improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a 4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment (CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones (3.99). Feedback highlights stylistic precision and emotional resonance. These results demonstrate that error-centered, feedback-driven generation enhances creativity, offering a scalable path toward self-evolving, human-aligned creative AI.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does visualization help AI understand data?</title>
<link>https://arxiv.org/abs/2507.18022</link>
<guid>https://arxiv.org/abs/2507.18022</guid>
<content:encoded><![CDATA[
<div> Keywords: Charts, graphs, AI systems, visualization, analysis

Summary: 
Charts and graphs are traditionally used by humans to analyze data, but recent experiments with commercial vision-language models, GPT 4.1 and Claude 3.5, have shown that AI systems can also benefit from visualization. Through a series of experiments involving synthetic datasets, it was found that the AI systems provided more precise and accurate descriptions when accompanied by a scatterplot, particularly as datasets became more complex. Comparison with baseline scenarios of a blank chart and a chart with mismatched data confirmed that the charts' content was responsible for the improved performance. This initial evidence suggests that AI systems, like humans, can gain insights and make better analyses when presented with visual representations of data. 

<br /><br />Summary: <div>
arXiv:2507.18022v1 Announce Type: new 
Abstract: Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines -- providing a blank chart and a chart with mismatched data -- shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Guided Policy Optimization</title>
<link>https://arxiv.org/abs/2507.18059</link>
<guid>https://arxiv.org/abs/2507.18059</guid>
<content:encoded><![CDATA[
<div> Keywords: Cooperative Multi-Agent Reinforcement Learning, Centralized Training, Decentralized Execution, MAGPO, Auto-regressive Joint Policy 

Summary: 
Cooperative Multi-Agent Reinforcement Learning (MARL) faces challenges like partial observability and limited communication, leading to the dominance of Centralized Training with Decentralized Execution (CTDE). However, existing CTDE methods often do not fully utilize centralized training or lack theoretical guarantees. The proposed Multi-Agent Guided Policy Optimization (MAGPO) framework integrates centralized guidance with decentralized execution, using an auto-regressive joint policy for efficient exploration. MAGPO aligns this joint policy with decentralized policies to ensure deployability under partial observability. The framework provides theoretical guarantees of monotonic policy improvement. Empirical evaluations on various tasks show that MAGPO consistently outperforms strong CTDE baselines and can match or even surpass fully centralized approaches. Overall, MAGPO offers a principled and practical solution for decentralized multi-agent learning, enhancing performance in diverse environments. <div>
arXiv:2507.18059v1 Announce Type: new 
Abstract: Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaGo Moment for Model Architecture Discovery</title>
<link>https://arxiv.org/abs/2507.18074</link>
<guid>https://arxiv.org/abs/2507.18074</guid>
<content:encoded><![CDATA[
<div> autonomous, Artificial Superintelligence, neural architecture discovery, innovation, scaling law
<br />
Summary:
ASI-Arch demonstrates Artificial Superintelligence for AI research in neural architecture discovery, allowing AI to autonomously innovate beyond human-defined spaces. It conducted 1,773 experiments to discover 106 state-of-the-art linear attention architectures, surpassing human-designed baselines and revealing new design principles. ASI-Arch operates autonomously, hypothesizing, implementing, and validating new architectures, showcasing the potential for computation-scalable research progress. The system established the first empirical scaling law for scientific discovery, showing that architectural breakthroughs can be scaled computationally. This breakthrough not only enables AI to conduct its own architectural innovation but also provides insights into emergent design patterns and autonomous research capabilities. The findings lay the foundation for self-accelerating AI systems, offering a blueprint for future innovation in the field of AI research.
<br /> <div>
arXiv:2507.18074v1 Announce Type: new 
Abstract: While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI framework for End-to-End Medical Data Inference</title>
<link>https://arxiv.org/abs/2507.18115</link>
<guid>https://arxiv.org/abs/2507.18115</guid>
<content:encoded><![CDATA[
<div> Automated Clinical Data Pipeline, Agentic AI Framework, Machine Learning, Healthcare, Module-based Agents<br />
<br />Summary: 
An Agentic AI framework is introduced to automate the clinical data pipeline in healthcare, addressing the challenges of fragmented workflows, model compatibility issues, and data privacy constraints. The framework consists of modular agents that handle structured and unstructured data, automating feature selection, model selection, and preprocessing. Evaluation on geriatrics, palliative care, and colonoscopy imaging datasets demonstrates the effectiveness of the framework. The pipeline includes agents for file-type detection, data anonymization, feature extraction, model-data feature matching, preprocessing recommendation, and model inference. By reducing the need for manual intervention, the framework offers a cost-efficient pathway for implementing AI in clinical environments. The use of interpretable outputs further enhances the transparency and usability of the system in healthcare settings. <div>
arXiv:2507.18115v1 Announce Type: new 
Abstract: Building and deploying machine learning solutions in healthcare remains expensive and labor-intensive due to fragmented preprocessing workflows, model compatibility issues, and stringent data privacy constraints. In this work, we introduce an Agentic AI framework that automates the entire clinical data pipeline, from ingestion to inference, through a system of modular, task-specific agents. These agents handle both structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing recommendation without manual intervention. We evaluate the system on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging. For example, in the case of structured data (anxiety data) and unstructured data (colonoscopy polyps data), the pipeline begins with file-type detection by the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring privacy compliance, where we first identify the data type and then anonymize it. The Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names, and a multi-stage MedGemma-based approach for image data, which infers modality and disease name. These features guide the Model-Data Feature Matcher Agent in selecting the best-fit model from a curated repository. The Preprocessing Recommender Agent and Preprocessing Implementor Agent then apply tailored preprocessing based on data type and model requirements. Finally, the ``Model Inference Agent" runs the selected model on the uploaded data and generates interpretable outputs using tools like SHAP, LIME, and DETR attention maps. By automating these high-friction stages of the ML lifecycle, the proposed framework reduces the need for repeated expert intervention, offering a scalable, cost-efficient pathway for operationalizing AI in clinical environments.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes</title>
<link>https://arxiv.org/abs/2507.18123</link>
<guid>https://arxiv.org/abs/2507.18123</guid>
<content:encoded><![CDATA[
<div> Keywords: COVID-19 vaccines, Natural Language Processing, Active Learning, Vaccine safety surveillance, Emergency Department notes

Summary:<br /><br />The study focuses on the importance of post-licensure surveillance systems for COVID-19 vaccines due to limited safety data collection during clinical trials. It utilizes Natural Language Processing (NLP) and Active Learning to develop a classifier that detects potential vaccine safety issues from Emergency Department (ED) notes. ED triage notes provide vital patient information and can aid in timely vaccine safety signal surveillance. NLP is a more accurate and efficient alternative to keyword-based classification, although it requires annotated data, which is often scarce in the medical field. Active learning helps optimize the annotation process and improve model performance, leading to faster implementation. This work combines active learning, data augmentation, and evaluation techniques to create a classifier for enhancing vaccine safety surveillance from ED triage notes. <div>
arXiv:2507.18123v1 Announce Type: new 
Abstract: The rapid development of COVID-19 vaccines has showcased the global communitys ability to combat infectious diseases. However, the need for post-licensure surveillance systems has grown due to the limited window for safety data collection in clinical trials and early widespread implementation. This study aims to employ Natural Language Processing techniques and Active Learning to rapidly develop a classifier that detects potential vaccine safety issues from emergency department notes. ED triage notes, containing expert, succinct vital patient information at the point of entry to health systems, can significantly contribute to timely vaccine safety signal surveillance. While keyword-based classification can be effective, it may yield false positives and demand extensive keyword modifications. This is exacerbated by the infrequency of vaccination-related ED presentations and their similarity to other reasons for ED visits. NLP offers a more accurate and efficient alternative, albeit requiring annotated data, which is often scarce in the medical field. Active learning optimizes the annotation process and the quality of annotated data, which can result in faster model implementation and improved model performance. This work combines active learning, data augmentation, and active learning and evaluation techniques to create a classifier that is used to enhance vaccine safety surveillance from ED triage notes.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logical Characterizations of GNNs with Mean Aggregation</title>
<link>https://arxiv.org/abs/2507.18145</link>
<guid>https://arxiv.org/abs/2507.18145</guid>
<content:encoded><![CDATA[
<div> graph neural networks, mean aggregation, ratio modal logic, expressive power, uniform setting

Summary:
Graph neural networks with mean aggregation have the same expressive power as ratio modal logic in the non-uniform setting. This places them between max aggregation and sum aggregation in terms of expressive power. In the uniform setting, mean GNNs relative to MSO are equivalent to alternation-free modal logic, assuming continuous combination functions and threshold classification functions. This shows that mean GNNs are less expressive than sum GNNs and max GNNs in the uniform setting. Dropping these assumptions can increase the expressive power of mean GNNs. <div>
arXiv:2507.18145v1 Announce Type: new 
Abstract: We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function. In the non-uniform setting, we show that such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. The non-uniform expressive power of mean GNNs is thus higher than that of GNNs with max aggregation, but lower than for sum aggregation--the latter are characterized by modal logic and graded modal logic, respectively. In the uniform setting, we show that the expressive power relative to MSO is exactly that of alternation-free modal logic, under the natural assumptions that combination functions are continuous and classification functions are thresholds. This implies that, relative to MSO and in the uniform setting, mean GNNs are strictly less expressive than sum GNNs and max GNNs. When any of the assumptions is dropped, the expressive power increases.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory</title>
<link>https://arxiv.org/abs/2507.18178</link>
<guid>https://arxiv.org/abs/2507.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cognition attribution framework, knowledge retrieval, reasoning adjustment, parameter scaling<br />
Summary:<br />
1. The study introduces a cognition attribution framework to dissect the contributions of knowledge and reasoning in large language models (LLMs), inspired by dual-system cognitive theory.
2. LLM cognition is divided into knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2), assessed through fast and slow thinking modes.
3. Results indicate that reasoning adjustment varies by domain, benefitting reasoning-focused areas like mathematics, physics, and chemistry while potentially hindering knowledge-centric fields.
4. Parameter scaling improves both knowledge and reasoning, with a greater impact on knowledge enhancement.
5. Knowledge is predominantly housed in lower network layers, while reasoning activities occur in higher layers, revealing insights into LLM architecture and behavior. 
<br /> <div>
arXiv:2507.18178v1 Announce Type: new 
Abstract: While large language models (LLMs) leverage both knowledge and reasoning during inference, the capacity to distinguish between them plays a pivotal role in model analysis, interpretability, and development. Inspired by dual-system cognitive theory, we propose a cognition attribution framework to decouple the contribution of knowledge and reasoning. In particular, the cognition of LLMs is decomposed into two distinct yet complementary phases: knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs are prompted to generate answers under two different cognitive modes, fast thinking and slow thinking, respectively. The performance under different cognitive modes is analyzed to quantify the contribution of knowledge and reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results reveal: (1) reasoning adjustment is domain-specific, benefiting reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and potentially imparing knowledge-intensive domains. (2) Parameter scaling improves both knowledge and reasoning, with knowledge improvements being more pronounced. Additionally, parameter scaling make LLMs reasoning significantly more prudent, while moderately more intelligent. (3) Knowledge primarily resides in lower network layers, while reasoning operates in higher layers. Our framework not only helps understand LLMs from a "decoupling" perspective, but also provides new insights into existing research, including scaling laws, hierarchical knowledge editing, and limitations of small-model reasoning.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Non-minimal Semantics for Disjunction in Answer Set Programming</title>
<link>https://arxiv.org/abs/2507.18198</link>
<guid>https://arxiv.org/abs/2507.18198</guid>
<content:encoded><![CDATA[
<div> semantics, disjunction, Answer Set Programming, Justified Models, Strongly Supported Models
Summary:
The paper compares four different semantics for disjunction in Answer Set Programming that do not follow the principle of model minimality. Two approaches, Justified Models and Strongly Supported Models, offer alternative non-minimal semantics for disjunction. Two other approaches, Forks and Determining Inference (DI) semantics, introduce a new disjunction connective. It is shown that Forks, Justified Models, and a relaxed version of DI semantics coincide and form a single approach. This common semantics always includes stable models and is stronger than Strongly Supported Models, which treat disjunctions classically. The research establishes that these approaches work similarly under different definitions, providing a broader understanding of disjunction in answer set programming.<br /><br />Summary: <div>
arXiv:2507.18198v1 Announce Type: new 
Abstract: In this paper, we compare four different semantics for disjunction in Answer Set Programming that, unlike stable models, do not adhere to the principle of model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly provide an alternative non-minimal semantics for disjunction. The other two, Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference} (DI) semantics, actually introduce a new disjunction connective, but are compared here as if they constituted new semantics for the standard disjunction operator. We are able to prove that three of these approaches (Forks, Justified Models and a reasonable relaxation of the DI semantics) actually coincide, constituting a common single approach under different definitions. Moreover, this common semantics always provides a superset of the stable models of a program (in fact, modulo any context) and is strictly stronger than the fourth approach (Strongly Supported Models), that actually treats disjunctions as in classical logic.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations for Risk Assessment of AI in Protecting Fundamental Rights</title>
<link>https://arxiv.org/abs/2507.18290</link>
<guid>https://arxiv.org/abs/2507.18290</guid>
<content:encoded><![CDATA[
<div> Keywords: qualitative risk assessment, AI, EU AI Act, legal compliance, fundamental rights protection

Summary: 
The chapter introduces a conceptual framework for qualitative risk assessment of AI, particularly within the EU AI Act context. It addresses the complexities of legal compliance and protection of fundamental rights by incorporating definitional balancing and defeasible reasoning. Definitional balancing utilizes proportionality analysis to resolve conflicting rights, while defeasible reasoning adapts to the evolving legal landscape. The framework emphasizes the need for analyzing AI deployment scenarios to identify potential legal breaches and impacts on fundamental rights. It establishes philosophical foundations for a logical AI risk analysis by considering the interaction between AI scenarios and rights. The approach supports the evaluation of high-risk AI systems and General Purpose AI (GPAI) systems. Future work aims to develop formal models and algorithms to enhance AI risk assessment, bridging theory with practical applications for responsible AI governance. 

Summary:<br /><br />Keywords: qualitative risk assessment, AI, EU AI Act, legal compliance, fundamental rights protection
 <div>
arXiv:2507.18290v1 Announce Type: new 
Abstract: This chapter introduces a conceptual framework for qualitative risk assessment of AI, particularly in the context of the EU AI Act. The framework addresses the complexities of legal compliance and fundamental rights protection by itegrating definitional balancing and defeasible reasoning. Definitional balancing employs proportionality analysis to resolve conflicts between competing rights, while defeasible reasoning accommodates the dynamic nature of legal decision-making. Our approach stresses the need for an analysis of AI deployment scenarios and for identifying potential legal violations and multi-layered impacts on fundamental rights. On the basis of this analysis, we provide philosophical foundations for a logical account of AI risk analysis. In particular, we consider the basic building blocks for conceptually grasping the interaction between AI deployment scenarios and fundamental rights, incorporating in defeasible reasoning definitional balancing and arguments about the contextual promotion or demotion of rights. This layered approach allows for more operative models of assessment of both high-risk AI systems and General Purpose AI (GPAI) systems, emphasizing the broader applicability of the latter. Future work aims to develop a formal model and effective algorithms to enhance AI risk assessment, bridging theoretical insights with practical applications to support responsible AI governance.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams</title>
<link>https://arxiv.org/abs/2507.18337</link>
<guid>https://arxiv.org/abs/2507.18337</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics exams, automated marking, computer algebra system, SMT solver, term rewriting system 

Summary: 
Automatically marking Physics exams is a challenging task addressed in this study. The approach involves using a combination of a computer algebra system, an SMT solver, and a term rewriting system to assess typed student answers for correctness compared to a ground truth solution. A Large Language Model is utilized to interpret and correct errors in student responses before converting them into a machine-readable format. Two automated theorem proving methods are applied: off-the-shelf SMT solving and a specialized term rewriting system for physics problems involving trigonometric expressions. The development of the term rewrite system required establishing termination and confluence properties, discussed in detail in the paper. The system's performance is evaluated on a dataset of over 1500 real-world student exam responses from the 2023 Australian Physics Olympiad. 

<br /><br />Summary: <div>
arXiv:2507.18337v1 Announce Type: new 
Abstract: We present our method for automatically marking Physics exams. The marking problem consists in assessing typed student answers for correctness with respect to a ground truth solution. This is a challenging problem that we seek to tackle using a combination of a computer algebra system, an SMT solver and a term rewriting system. A Large Language Model is used to interpret and remove errors from student responses and rewrite these in a machine readable format. Once formalized and language-aligned, the next step then consists in applying automated reasoning techniques for assessing student solution correctness. We consider two methods of automated theorem proving: off-the-shelf SMT solving and term rewriting systems tailored for physics problems involving trigonometric expressions. The development of the term rewrite system and establishing termination and confluence properties was not trivial, and we describe it in some detail in the paper. We evaluate our system on a rich pool of over 1500 real-world student exam responses from the 2023 Australian Physics Olympiad.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios</title>
<link>https://arxiv.org/abs/2507.18368</link>
<guid>https://arxiv.org/abs/2507.18368</guid>
<content:encoded><![CDATA[
<div> Keywords: ConDiFi, reasoning benchmarks, LLMs, financial tasks, actionable insights

Summary:
ConDiFi introduces a benchmark that evaluates both divergent and convergent thinking in Large Language Models (LLMs) for financial tasks. The benchmark includes macro-financial prompts for divergent reasoning and multi-hop adversarial multiple-choice questions for convergent reasoning. Evaluation of 14 models revealed differences in performance, with GPT-4o scoring low on Novelty and Actionability despite high fluency. Models like DeepSeek-R1 and Cohere Command R+ excelled in generating actionable insights suitable for investment decisions. ConDiFi offers a new perspective for assessing reasoning capabilities necessary for the safe and strategic use of LLMs in finance.<br /><br />Summary: <div>
arXiv:2507.18368v1 Announce Type: new 
Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step logic. In finance, however, professionals must not only converge on optimal decisions but also generate creative, plausible futures under uncertainty. We introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990 multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we evaluated 14 leading models and uncovered striking differences. Despite high fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models like DeepSeek-R1 and Cohere Command R+ rank among the top for generating actionable, insights suitable for investment decisions. ConDiFi provides a new perspective to assess reasoning capabilities essential to safe and strategic deployment of LLMs in finance.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting LLM Reasoning via Information Bottleneck</title>
<link>https://arxiv.org/abs/2507.18391</link>
<guid>https://arxiv.org/abs/2507.18391</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reinforcement learning, reasoning capabilities, information bottleneck, regularization

Summary:
In this paper, the authors introduce a theoretical framework called IB-aware Reasoning Optimization (IBRO) for enhancing the reasoning capabilities of Large Language Models (LLMs). This framework is grounded in the Information Bottleneck (IB) principle and aims to incentivize LLMs to produce informative and generalizable reasoning trajectories. By introducing IB regularization, which is a lightweight technique that seamlessly integrates into existing reinforcement learning-based post-training frameworks, the authors demonstrate consistent improvements in LLM reasoning performance across multiple mathematical reasoning benchmarks and reinforcement learning algorithms. The approach is designed to encourage reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts, leading to more principled and effective methodologies for enhancing LLM reasoning capabilities.<br /><br />Summary: <div>
arXiv:2507.18391v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently demonstrated remarkable progress in reasoning capabilities through reinforcement learning with verifiable rewards (RLVR). By leveraging simple rule-based rewards, RL effectively incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning trajectories, progressively guiding them toward correct answers. However, existing approaches remain largely heuristic and intuition-driven, limiting the development of principled methodologies. In this paper, we present a theoretical characterization of LLM reasoning grounded in information bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO), a framework that encourages reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts. We derive a practical token-level surrogate objective and propose an efficient approximation, resulting in the lightweight IB regularization method. This technique integrates seamlessly into existing RL-based post-training frameworks without additional computational overhead, requiring only a one-line code modification. Empirically, we validate IB regularization across multiple mathematical reasoning benchmarks and RL algorithms, demonstrating consistent improvements in LLM reasoning performance.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation</title>
<link>https://arxiv.org/abs/2507.18398</link>
<guid>https://arxiv.org/abs/2507.18398</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, call routing, call centres, Value Iteration, Proximal Policy Optimization

Summary:
Reinforcement Learning methods are applied to optimize call routing in call centres to minimize client waiting time and staff idle time. Two approaches are compared: a model-based method using Value Iteration with known system dynamics and a model-free approach using Proximal Policy Optimization learning from experience. A theoretical and simulation model is used to frame the problem as a Markov Decision Process within a Skills-Based Routing framework. The simulation model combines Discrete Event Simulation with OpenAI Gym environment. Random, Value Iteration, and Proximal Policy Optimization policies are evaluated using the simulation model. Despite longer training time, Proximal Policy Optimization consistently achieves the highest rewards, lowest client waiting time, and staff idle time across 1,000 test episodes. 

<br /><br />Summary: 
- Reinforcement Learning is used to optimize call routing in call centers.
- Two methods are compared: model-based using Value Iteration and model-free using Proximal Policy Optimization.
- The problem is framed as a Markov Decision Process within a Skills-Based Routing framework.
- Proximal Policy Optimization consistently achieves the highest rewards, lowest client waiting time, and staff idle time.
- Despite requiring longer training time, PPO outperforms other policies after 1,000 test episodes. <div>
arXiv:2507.18398v1 Announce Type: new 
Abstract: This paper investigates the application of Reinforcement Learning (RL) to optimise call routing in call centres to minimise client waiting time and staff idle time. Two methods are compared: a model-based approach using Value Iteration (VI) under known system dynamics, and a model-free approach using Proximal Policy Optimisation (PPO) that learns from experience. For the model-based approach, a theoretical model is used, while a simulation model combining Discrete Event Simulation (DES) with the OpenAI Gym environment is developed for model-free learning. Both models frame the problem as a Markov Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with Poisson client arrivals and exponentially distributed service and abandonment times. For policy evaluation, random, VI, and PPO policies are evaluated using the simulation model. After 1,000 test episodes, PPO consistently achives the highest rewards, along with the lowest client waiting time and staff idle time, despite requiring longer training time.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>